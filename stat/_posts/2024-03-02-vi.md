---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Advanced Variational Inference(VI), Variational Autoencoder(VAE)

- Paper : [Variational Inference : A Review for Statisticians](https://arxiv.org/abs/1601.00670)
- Paper : [Advances in Variational Inference](https://arxiv.org/pdf/1711.05597.pdf)
- Paper : [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)

## 1. Probabilistic ML Model
- $$x$$ : set of observed variables \
  $$y$$ : set of hidden / latent variables \
  $$\theta$$ : model parameters
- Discriminative probabilistic ML model
  - $$p(Y \mid X)$$ : 데이터 $$X$$가 주어졌을 때 결과 $$y$$ 예측하기 (Classification, Regression, ...)
- Generative probabilistic ML model

  - Bayes Theorem : $$p(Y \mid X)=\frac{p(X, Y)}{p(X)}=\frac{p(X \mid Y) p(Y)}{p(X)}=\frac{p(X \mid Y) p(Y)}{\int p(X \mid Y) p(Y) d Y}$$
  ![그림1](/assets/img/stat/vi/fig1.png)
- Discriminative model은 클래스(y) 사이의 차이를 의미하는 decision boundary를 학습하고, ($$p(Y\mid X)$$)
  Generative model은 분포 $$p(X), p(X,Y)$$를 학습하여 posterior $$p(Y\mid X)$$를 추정한다.
- $$Y$$의 차원이 높아질수록 분모에 있는 $$Y$$에 대한 적분이 어려워지기 때문에(intractable), 아래 두 가지 방법으로 $$p(Y\mid X)$$를 추정한다.
  - Variational Inference (optimization)
  - Markov chain Monte Carlo (sampling)
  - MCMC는 분포를 근사하기 위해 sampling으로 inference하는 것이고, VI는 분포를 근사하기 위해 optimization 문제로 바꾼 것이다.
  - 일반적으로 VI는 빠르고, MCMC는 정확하다.(상대적으로 그렇다는 것)

## 2. Variational Inference
- The model: $$p_\theta(x)$$ \
  The data: $$\stackrel{}{D}=\left\{x_1, \ldots, x_N\right\}$$ \
  Maximum likelihood fit: $$\theta \leftarrow \operatorname{argmax}_\theta \frac{1}{N} \sum_i \log p_\theta\left(x_i\right)$$ \
  - i.e. $$\theta \leftarrow \operatorname{argmax}_\theta \frac{1}{N} \sum_i \log \left(\int p_\theta\left(x_i \mid z\right) p(z) d z\right)$$
  - i.e. $$\theta \leftarrow \operatorname{argmax}_\theta \frac{1}{N} \sum_i E_{z \sim p\left(z \mid x_i\right)}\left[\log p_\theta\left(x_i, z\right)\right]$$
- log-likelihood $$\log p_\theta$$를 maximize : 
  $$\begin{aligned}
    \log p\left(x_i\right) & =\log \int p\left(x_i \mid z\right) p(z) d z \\
    & =\log \int p\left(x_i \mid z\right) p(z) \frac{q_i(z)}{q_i(z)} d z \\
    & =\log E_{z \sim q_i(z)}\left[\frac{p\left(x_i \mid z\right) p(z)}{q_i(z)}\right] \quad \left(\because E_{q(z)}[f(Z)]=\int f(z)q(z) dz\right) \\
    & \geq E_{z \sim q_i(z)}\left[\log \frac{p\left(x_i \mid z\right) p(z)}{q_i(z)}\right] \quad (\because \text{Jensen's Inequality} : \varphi(E[X]) \geq E[\varphi(X)] (\varphi \ \text{is concave fn})) \\
    & =E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right]-E_{z \sim q_i(z)}\left[\log q_i(z)\right] \\
    & =E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right]+H\left(q_i\right) \quad (\text{where} \ H \ \text{is Entropy}) \\
    & =E_{z \sim q_i(z)}\left[\log p_\theta \left(x_i, z\right)\right]+H\left(q_i\right) \\
    \\
    & = \mathcal{L}_{i}\left(p, q_{i}\right)
    \end{aligned}$$
- 위 식에서 $$E_{z \sim q_i(z)}\left[\log p\left(x_i, z\right)\right]$$은 $$q_i(z)$$가 $$p(x_i, z)$$의 density가 높은 곳에서 높은 density를 가질 때 커지고, $$H\left(q_i\right)$$는 $$q_i(z)$$가 고르게 퍼져있을 때 커진다.
- 위 전개식에서 부등식 앞뒤 식의 차이가 $$D_{\mathrm{KL}}\left(q_i\left(z_i\right) \| p\left(z \mid x_i\right)\right)$$가 되고, 그러므로 $$\mathcal{L}_{i}\left(p, q_{i}\right)$$를 maximize한다는 것은 $$D_{\mathrm{KL}}\left(q_i\left(z_i\right) \| p\left(z \mid x_i\right)\right)$$를 minimize한다는 것과 같다. (아래 전개식 참고)

  \\
  $$\begin{aligned}
    D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) & =E_{z \sim q_i(z)}\left[\log \frac{q_i(z)}{p\left(z \mid x_i\right)}\right]=E_{z \sim q_i(z)}\left[\log \frac{q_i(z) p\left(x_i\right)}{p\left(x_i, z\right)}\right] \\
    & =-E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right]+E_{z \sim q_i(z)}\left[\log q_i(z)\right]+E_{z \sim q_i(z)}\left[\log p\left(x_i\right)\right] \\
    \\
    & =-E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right]-\mathcal{H}\left(q_i\right)+\log p\left(x_i\right) \\
    & =-\mathcal{L}_i\left(p, q_i\right)+\log p\left(x_i\right)
    \end{aligned}$$
  $$\begin{aligned}
    &\log p\left(x_i\right)=D_{\mathrm{KL}}\left(q_i(z) \| p\left(z \mid x_i\right)\right)+\mathcal{L}_i\left(p, q_i\right)
    \end{aligned}$$
- $$\log p\left(x_i\right)$$가 고정되어 있다면, $$\mathcal{L}_i\left(p, q_i\right)$$를 maximize할 때 $$D_{\mathrm{KL}}\left(q_i(z) \| p\left(z \mid x_i\right)\right)$$가 minimize된다. \
  ($$\mathcal{L}_i\left(p, q_i\right)$$은 ELBO이고, $$D_{\mathrm{KL}}\left(q_i(z) \| p\left(z \mid x_i\right)\right)$$은 variational distribution)
- 이 때 학습되는 parameters는 아래와 같다.
  - $$z$$를 $$\hat x$$으로 mapping시키는 $$\theta$$ ($$\theta$$는 $$\hat x$$가 $$x$$와 비슷해지도록 학습)
  - $$z$$의 분포인 $$q_i(z)$$의 평균과 분산 (Gaussian을 가정)
  - 총 $$\mid \theta \mid +\left(\mid \mu_i\mid +\mid \sigma_i\mid \right) \times N$$개

## 2.1. Amortized Variational Inference
- $$\mid \theta \mid +\left(\mid \mu_i\mid +\mid \sigma_i\mid \right) \times N$$개의 parameters는 데이터 개수가 늘어날수록 커진다는 단점이 있다.
- Amortized Variational Inference는 $$q_i(z)$$가 아니라 $$q_\phi(z \mid x)=\mathcal{N}\left(\mu_\phi(x), \sigma_\phi(x)\right)$$가 $$p(x \mid z)$$와 비슷해지도록 학습한다.
  
  ![그림2](/assets/img/stat/vi/fig2.png)
- Basic Variational Inference는 sampled $$z$$로 $$\hat x$$를 만들어내는 것인데, Amortized Variational Inference는 $$x$$가 input으로 들어가면 latent vector $$z$$의 분포가 결정되고 그 분포에서 $$z$$를 sampling해서 $$\hat x$$를 만들어내기 때문에, autoencoder의 아이디어와 같다. 즉 VAE는 Amortized Variational Inference의 예시 중 하나이다.
- $$x$$가 input으로 들어가서 $$z$$의 분포가 결정되는 네트워크($$\phi$$)를 encoder, inference network가 되고, $$z$$로 $$\hat x$$을 만드는 네트워크($$\theta$$)를 decoder, generative network가 된다.
- $$p_\theta(x_i \mid z)$$를 Gaussian으로 가정한다는 것은, log를 씌웠을 때 exp 안에 있는 L2 term만 남기 때문에 $$\hat x$$과 $$x$$를 비교할 때 euclidean distance를 사용한다는 의미이다.
- 이외에도 많은 variants of VI가 있지만 Amortized VI만 소개하는 이유는 VAE와 관련이 있기 때문이다.

## 2.2. Mean-field Variational Inference
- Assumption : all latent variables are mutually independent (i.e. $$q(\mathbf{z})=\prod_{j=1}^m q_j\left(z_j\right) .$$)
- Mean-field 가정을 하면 true posterior의 variables가 highly dependent인 경우에 approximation의 정확도가 다소 떨어지는 단점이 있지만, fully factorized distribution으로 계산이 간단해진다.
- ELBO를 maximize하는 $$q(z_i)$$들을 각각 찾고 다 곱해서 $$q(z)=q(z_1) \times ... \times q(z_M)$$을 계산하기 때문이다
- Mean-filed 가정을 하면 아까 봤던 식이 아래와 같이 전개된다.
  $$\begin{aligned}
  \mathrm{ELBO} & =E_{q(z)}[\log p(x, z)-\log q(z)] \\
  & =E_{\prod_i q_i}\left[\log p(x, z)-\log \prod_i q_i\right] \ (\because \text{Mean-field assumption})\\
  & =\int \prod_i q_i\left\{\log p(x, z)-\log \prod_i q_i\right\} d z \ (\text{The definition of Expectation})\\
  \\
  & =\int \prod_i q_i\left\{\log p(x, z)-\sum_i \log q_i\right\} d z \ (\text{Property of logarithm}) \\
  & =\int q_j\left\{\int \log p(x, z) \prod_{i \neq j} q_i d z_i\right\} d z_j-\int \prod_i q_i \sum_i \log q_i d z \ (\text{j번째 적분만 바깥으로 뺀 것})\\
  & =\int q_j\left\{\int \log p(x, z) \prod_{i \neq j} q_i d z_i\right\} d z_j-\int \prod_i q_i \log q_1 d z+\ldots \int \prod_i q_i \log q_M d z \\
  & =\int q_j\left\{\int \log p(x, z) \prod_{i \neq j} q_i d z_i\right\} d z_j-\int \prod_i q_i \log q_j d z+\text{(Constant)} \\
  & =\int q_j\left\{\int \log p(x, z) \prod_{i \neq j} q_i d z_i\right\} d z_j-\int q_j \log q_j d z_j+\text{(Constant)} \ (q_j\text{와 무관한 항들은 constant}) \\
  & =\int q_j E_{i \neq j}[\log p(x, z)] d z_j-\int q_j \log q_j d z_j+\text{(Constant)} \\
  \\
  & =\int q_j \log \widetilde{p}\left(x, z_j\right) d z_j-\int q_j \log q_j d z_j+\text{(Constant)} \\
  & =\int q_j \log \frac{\widetilde{p}\left(x, z_j\right)}{q_j} d z_j+\text{(Constant)} \\
  & =-\mathrm{KL}\left[q_j \| \widetilde{p}\left(x, z_j\right)\right]+\text{(Constant)}
  \end{aligned}$$
- 위 결과로부터 $$q_j$$가 $$\tilde p(x,z_j)$$와 비슷해져야 한다는 것을 알 수 있다. 그러므로 $$q_j$$를 전개하고 normalization하면 아래와 같다. \
  $$\begin{aligned}
  q_j & =\widetilde{p}\left(x, z_j\right) \\
  \log q_j & =\log \widetilde{p}\left(x, z_j\right) \\
  \log q_j & \propto E_{i \neq j}[\log p(x, z)] \\
  q_j & \propto \exp \left(E_{i \neq j}[\log p(x, z)]\right) \\
  q_j & =\frac{\exp \left(E_{i \neq j}[\log p(x, z)]\right)}{\int \exp \left(E_{i \neq j}[\log p(x, z)]\right) d z_j}
  \end{aligned}$$
- optimal $$q^*_j$$를 알기 위해서 $$i\ne j$$에 대해 $$log\ p(x,z)$$의 expectation을 계산한다.

## 3. Variational Autoencoder
- VAE는 2개의 neural network로 구성된다. (2개의 NN)
  - 1) top-down generative model(=decoder) : mapping from the latent variable $$z$$ to the data $$x$$ 
  - 2) bottom-up inference model(=encoder) : approximates the posterior $$p(z \mid x)$$ (using amortized mean-field variational distribution)
  ![그림3](/assets/img/stat/vi/fig3.png)
- 위 그림에서 나와있듯이 encoder를 거치면 deterministic하게 latent variable이 output으로 나오는 것이 아니다. output은 mean vector와 std dev vector이고, 이렇게 결정된 gaussian 분포에서 sampling을 통해 latent variable을 만든다. (VAE가 generative model인 이유이다.)
- Reparameterization trick : $$N(mean, std)$$에서 sampling하지 않고 $$N(0,1)$$에서 생성한 뒤 std를 곱하고 mean을 더해줌으로써 미분이 가능하도록 (backpropagation이 가능하도록) 한다.
- latent variable이 decoder를 거치면 input과 유사한(ideally) 새로운 데이터가 생성된다.
- VAE의 loss는 다음과 같다.
  
  $$\arg \min _{\theta, \phi} \sum_i-\mathbb{E}_{q_\phi\left(z \mid x_i\right)}\left[\log \left(p\left(x_i \mid g_\theta(z)\right)\right)\right] \oplus K L\left(q_\phi\left(z \mid x_i\right) \| p(z)\right)$$
  - 학습 parameters는 encoder의 $$\phi$$와 decoder의 $$\theta$$이다.
  - Reconstruction Error $$-\mathbb{E}_{q_\phi\left(z \mid x_i\right)}\left[\log \left(p\left(x_i \mid g_\theta(z)\right)\right)\right]$$
    - $$x$$가 주어졌을 때 encoder $$q_\phi$$를 지나 $$z$$생성 : $$q_\phi(z \mid x)$$
    - 그 $$z$$가 decoder $$g_\theta$$를 지나 $$x$$ 생성 : $$g_\theta(z)$$
    - 이렇게 생성한 $$\hat x$$에 대한 $$x$$의 negative log-likelihood
  - Regularization Error $$K L\left(q_\phi\left(z \mid x_i\right) \| p(z)\right)$$
    - $$x$$가 주어졌을 때 encoder $$q_\phi$$를 지나 $$z$$생성 : $$q_\phi(z \mid x)$$
    - 그 $$z$$와 gaussian의 KL-divergence