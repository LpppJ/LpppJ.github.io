---
layout: post
related_posts:
  _
title: 
description: >
  [ICML 2025](https://openreview.net/pdf?id=bRa4JLPzii)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# CoMRes: Semi-Supervised Time Series Forecasting Utilizing Consensus Promotion of Multi-Resolution (ICLR 2025)

## Abstract

- 기존의 supervised learning approaches:
  - relyon cleaned and labeled data $$\to$$ unseen characteristics 못잡음
- 본 논문에서는 self-supervised approach 제안
  - Multi-view setting on augmented data 
  - without requiring explicit future values as labels
- **consensus promotion framework**을 통해
  - 여러 모델이 비슷하게 예측하도록 ! (agreement among multiple single-view models)
- 추가로 not only improves forecasting **accuracy** but also **mitigates** error accumulation inlong-horizon predictions
- impact of autoregressive and non-autoregressive decoding schemes on error propagation
  - $$\to$$ extending prediction horizons에 robust

## 1. Introduction

- Time series의 intricate temporal patterns 중에서 **multi-scale** dependencies !
  - Pathformer, Scaleformer, TimeMixer, Nhits 등 여러 시도 있었음
  - 하지만 different scales $$\to$$ increase in parameters $$\to$$ overfitting risk
  - 게다가 limited to labeled data
- 그렇다고 **data augmentation** 하면 underlying dynamics이 바뀌어 버려서 
  - slight shifts in trends or periodic patterns $$\to$$ prediction errors
  - 그러므로 overfitting risk 줄이고 generalization ability 늘리는 것 중요
    - (augmented data에 future values 붙이지 않은 채로 !)
- 그러므로 본 논문에서는 enhancing **consensus** among single-view models using **unseen** data
  - 기존의 co-training method와 비슷하게,
  - maximizing **agreement** among multi-view models on unseen data !
- 추가적으로, **error accumulation in long-horizon forecasting** 이슈 다룸
  - 기존의 former들은 non-autoregressive decoding scheme
  - how non-autoregressive models can be employed to minimize error accumulation ?

## 2. Related Work

- **Long-term Time Series Forecasting**
  - Transformer-based:
    - FEDformer(2022), Autoformer(2021): address the quadratic complexity
    - PatchTST(2023): patching to enhance local pattern recognition
    - iTransformer(2024), Crossformer(2023): attention으로 multivariate correlations
  - non-Transformer-based 역시 예측 성능 뛰어난ㅁ
    - tiDE(2023), MICN(2023), DLinear(2023), TimeMixer(2024)
- **Multi-scale Modeling for Time Series**
  - NHITS(2023): multi-rate data sampling and hierarchical interpolation
  - Pyraformer(2022), Scaleformer(2023): pyramidal attention mechanisms
  - TimeMixer(2024): decomposable mixing strategy
  - Pathformer(2024): features from both different **resolutions** and **temporal** distances
  - 대부분 ensemble of multiple predictions that rely on labeled data...
    - 본 논문에서는 semi-supervised learning으로
      - challenge of reconciling diverse representations 다룸
  - **Time Series Data Augmentation**
    - classification and anomaly detection에서는
      - small perturbations typically do not change the data label
    - 하지만 forecasting에서는 even small perturbations can lead to significant changes in the observations !
      - the improvement relies on the quality of the augmented data

## 3. Methodology

- $$L:\left(x_{t-L}, \ldots, x_t\right)$$보고 $$H \text { timestamps }\left(x_{t+1}, \ldots, x_{t+H}\right)$$예측
- Pathformer를 leverage unseen data하도록 확장 !

![그림1](/assets/img/timeseries/ComRes/fig1.png)

- Pathformer처럼
  - multi-scale patch division with various patch sizes
  - **intra-patch** attention within each divided patch and
    - **inter-patch** attention across different patches
    - 그 다음 MLP
- Pathformer와 다르게
  - **equal weighted** aggregator to combine these multi-scale characteristics
- 기존 supervised learning에서는 loss:
  - $$\mathcal{L}_s=\frac{1}{m+1}\left(\mid\widehat{x_{* t+1: t+H}}-x_{t+1: t+H}\mid^2+\sum_{i=1}^m \mid\widehat{x}_{i t+1: t+H}-x_{t+1: t+H}\mid^2\right)$$.

### 3.1. Consensus Promotion Learning

- Augmented data는 potentially noisy $$\to$$  future values 정하는 건 challenging
- 그래서 **consistency loss**: encourages mutual agreement amongthe models
  - $$\mathcal{L}_u=\frac{1}{m}\left(\sum_{i=1}^m\mid\widehat{x}_{i t+1: t+H}-\widehat{x_{* t+1}}{ }^2 t+H\mid^2\right)$$:
    - individual-view models to align their predictions with the comprehensive prediction
- 최종적인 unsupervised consistency loss는 $$\mathcal{L}=\mathcal{L}_s+w_u \mathcal{L}_u$$
  - $$w_u$$: represents the weight ot the unsupervised consistency loss
  - $$\mathcal L_s$$: minimizes the prediction error with respect to the ground truth
  - $$\mathcal L_u$$: aligns the individual model predictions

### 3.2. Unseen Data Generation

- Consensus promotion는 leveraging information from unseen data으로 model generalization을 향상시키는 것이 목표
- 그러면 Data augmentation은? explores new areas of the input space이 목표
  - **time warping, interpolation,and noise injection**으로 하겠다 !
  - Interpolation : estimating values between known data points to produce a smoother and more continuous time series.
  - Time Warping : DTW처럼 selects a random time range and then compresses(down-samples)
  - Noise Injection : selects a random time range and then compresses(down-samples)

### 3.3. Extending Prediction Horizons Beyond Training Ranges

- 현실적으로 autoregressive forecasting is essential !
- 하지만 Teacher forcing처럼 autoregressively training 없으면 error accumulation 발생

![그림1](/assets/img/timeseries/ComRes/fig2.png)

- consensus promotion through two approaches:

  - **Block-wise autoregressive**: extends the prediction horizon by utilizing the entire prediction range at once

    **Fine-grained autoregressive**: fine-grained autoregressive prediction involves a step-by-step process

## 4. Experiments

### 4.1. Time Series Forecasting

- **MRes (SL)**: MRes trained exclusively on labeled datasets using supervised learning (SL).
- **MRes w. augmentation**: MRes trained on pseudo-labeled datasets augmented with Time Warping.
- **MRes w. consensus**: MRes trained on labeled datasets with consensus promotion but withoutdata augmentation. Predictions on labeled data are learned not only from the ground truthbut also from the comprehensive prediction.

![그림1](/assets/img/timeseries/ComRes/table1.png)

- 그리고 일관되게 base-line model 보다 성능이 좋음 (아래 그림)

![그림1](/assets/img/timeseries/ComRes/fig3.png)

### 4.2. Error Accumulation in Auto-regressive Forecasting

![그림1](/assets/img/timeseries/ComRes/table2.png)

- 일단 table1보다 MSE가 커진건 inherent in autoregressive forecasting
- 첫째로 CoMRes가 MRes(SL)보다 일관되게 좋음
- 둘째로 block-wise autoregressive가 fine-grained autoregressive보다 좋음

![그림1](/assets/img/timeseries/ComRes/fig4.png)

- 미래 96시점 예측하게 학습하고 forecast longer horizons 해보았음
- 일단 CoMRes(초록, 빨강)가 MRes(오렌지)보다 stronger capacity to reduce error accumulation
- 그리고 미래 720시점 예측하게 학습한 파랑, 초록과 비교해보았음

### 4.3. Ablation Study

![그림1](/assets/img/timeseries/ComRes/table3.png)

- **Limited-Resource Scenarios** (table3)
  - reducing the amount of **labeled** training data 상태로 비교
  - train-test 사이에서 temporal variations이 다르면 성능 안좋아짐
  - ETTh2 and ETTm2는 labeled data 적어도 성능 좋음

![그림1](/assets/img/timeseries/ComRes/table4.png)

- **Combining Multiple Unseen Data Generation Techniques** (table4)
  - Combining all 3 augmentation techniques (Time Warp, Interpolation, and Noise Injection)가 항상 성능 개선으로 이어지는 건 아님
    - ETTh1 (높은 변동성): 성능 향상이 거의 X
      - data의 변동성이 높을 경우, 추가적인 데이터 변형이 예측 성능에 미치는 영향이 크지 않음
    - ETTm2 (명확한 주기성):  <720 장기 예측에서는 급격한 성능 저하 발생
      - 장기적인 추세 변화가 빠른 데이터에서는 augmentation이 부정적인 영향
    - Weather 데이터셋 (변수 간 강한 상관관계): 성능이 향상
      - 상관성이 높은 데이터에서는 다양한 증강 기법을 적용하는 것이 모델 학습에 도움

## 5. Conclusion

- Multi-view learning strategy for long-term time series forecasting
  - superior prediction accuracy and robustness
- Augmented unseen data to ensure that models
  - not only capturemulti-resolution patterns
  - but also provide consistent predictions
- (especially in autoregressive setups, where it mitigates the issue of error accumulation.)