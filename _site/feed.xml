<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-01T20:04:29+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">(GAFormer) Enhancing Timeseries Transformers Through Group-Aware Embeddings</title><link href="http://localhost:4000/timeseries/2024-03-01-GAFormer/" rel="alternate" type="text/html" title="(GAFormer) Enhancing Timeseries Transformers Through Group-Aware Embeddings" /><published>2024-03-01T00:00:00+09:00</published><updated>2024-03-01T20:04:25+09:00</updated><id>http://localhost:4000/timeseries/GAFormer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-01-GAFormer/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Multivariate TS의 복잡한 inter-channel relationship과 dynamic shifts로 인해 Robust and generalizable representation을 학습하기 어렵다.</li>
  <li>본 논문에서 제시하는 GAFormer는 set of group tokens를 학습하고 instance-specific group embedding layer를 만든다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Multivariate TS의 temporal dynamics(temporal structure) of each channel, 그리고 relationship across channels(channel-wise structure)는 TS의 representation을 만드는 요소</li>
  <li>TS는 <strong>no predetermined ordering</strong>, 그리고 <strong>instance-specific relationships across channels and time</strong>으로 인해 position embedding을 그대로 사용하기에 적절하지 않다.</li>
  <li>본 논문에서 제시하는 GAFormer는 channel structure와 temporal structure를 통합하여 token에 ‘group embedding’한다.</li>
</ul>

<h2 id="2-method">2. Method</h2>
<ul>
  <li>Instance-specific group embeddings : grouping across different tokens, either channel-wise (spatially) or time-wise (temporally)
    <h3 id="21-group-embeddings">2.1. Group Embeddings</h3>
  </li>
  <li>Sequence of tokens : \(X=\left[\mathbf{x}_1, \ldots, \mathbf{x}_N\right] \in \mathbb{R}^{N \times D}\)
    <ul>
      <li>\(N\) : the total # of tokens in a seq</li>
      <li>\(D\) : token dim</li>
    </ul>
  </li>
  <li>Linear weight matrix : \(W \in \mathbb{R}^{D \times K}\) project each token down to a space of \(K\) dim
    <ul>
      <li><code class="language-plaintext highlighter-rouge">operation</code> \(\operatorname{Encoder}(X) W \in \mathbb{R}^{N \times K}\)</li>
    </ul>
  </li>
  <li>그 다음 softmax : sparsify the coefficients that assign group tokens to input tokens (group awareness)
    <ul>
      <li><code class="language-plaintext highlighter-rouge">operation</code> \(\mathbb{S}(\operatorname{Encoder}(X) W)\)
        <ul>
          <li>where \(\mathbb{S}\) represents the softmax (along \((D)\) dim)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>각 tokens를 K차원으로 줄였기 때문에,  \(\mathbf{G} \in \mathbb{R}^{N \times D}\)을 곱해줄 수 있다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">operation</code> \(\operatorname{GE}(X)=\mathbb{S}(\operatorname{Encoder}(X) W) \cdot G\)</li>
    </ul>
  </li>
  <li>이제 \(X\)에 더해준다
    <ul>
      <li><code class="language-plaintext highlighter-rouge">operation</code> \(X \leftarrow X+\operatorname{GE}(X)\)</li>
    </ul>
  </li>
</ul>

<h3 id="22-gaformer-a-group-aware-spatiotemporal-transformer">2.2. GAFormer: A Group-Aware SpatioTemporal Transformer</h3>
<p>-</p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/forum?id=c56TWtYp0W)]]></summary></entry><entry><title type="html">iTransformer : Inverted Transformers are Effective for Time Series Forecasting</title><link href="http://localhost:4000/timeseries/2024-02-23-iTransformer/" rel="alternate" type="text/html" title="iTransformer : Inverted Transformers are Effective for Time Series Forecasting" /><published>2024-02-23T00:00:00+09:00</published><updated>2024-02-27T10:49:11+09:00</updated><id>http://localhost:4000/timeseries/iTransformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-23-iTransformer/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Transformer-based TS : larger lookback \(\to\) performance degradation &amp; computation explosion.</li>
  <li>iTransformer : attention과 feed-forward network(FFN)를 inverted dimension에서 적용
<img src="/assets/img/timeseries/iTransformer/fig2'.jpeg" alt="사진1" class="lead" width="00" height="100" /></li>
  <li>각 time point가 token이 되는 것이 아니라 각 series(variate)가 token이 된다.</li>
  <li>Inverted dimension만 다르고 이외의 Transformer의 components는 수정 없이 그대로 사용한다.</li>
</ul>

<h2 id="1-introduction">1. introduction</h2>
<ul>
  <li>Transformer가 다른 fields에서는 linear model보다 성능이 좋은데, multivariate ts forecasting에서는 not sutable하다. <br />
(특히 time points 사이에 semantic 관계보다 numerical 관계가 강한 경우에는 그냥 simple linear layer의 성능이 더 좋았다.)</li>
  <li>그 이유는 한 시점에 기록된 서로 다른 변수들의 값들이 하나의 token으로 기록되기 때문이다. 논문에서는 아래처럼 표현하고 있다. <br />
(= embed multiple variates of the same timestamp into indistinguishable channels…) <br />
(= the points of the same time step that represent different physical meanings recorded by inconsistent mesurements are embedded into one token …)</li>
  <li>이러한 single time step의 token 형태는 receptive field가 너무 좁아서 유용한 정보를 얻어내기가 어렵다.</li>
  <li>
    <p>또한 시계열은 데이터의 순서가 중요한데, transformer는 permutation-invariant attention mechanism이라서 temporal dimension을 잘 잡지 못한다.</p>
  </li>
  <li>그래서 iTransformer는 각각의 variate를 독립적으로 token으로 embedding해서 receptive field를 늘려준다.(Patching의 extreme case)</li>
  <li>그러면 token은 series의 global representation을 통합해서 variate-centric하다.</li>
  <li>FFN은 개별 변수에 대해 인코딩된 lookback series를 보고 예측할 수 있을 정도의 generalizable representation을 학습할 수 있다.</li>
  <li>본 논문의 contribution은 아래와 같다.
    <ul>
      <li>Transformer가 비효과적인 것이 아니라 아직 underexplored라서 잘못 사용되었으니 component를 개선한다.</li>
      <li>독립적인 개별 시계열을 token으로 간주하여 self-attention으로 multivariate correlations를 파악하고, FFN으로 forecasting을 위한 series-global representation을 학습한다.</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>
<ul>
  <li>TCN-based, RNN-base forecasters를 지나서 Transformer가 시퀀스 모델링과 확장 가능성으로 좋은 성능을 보여주며 많은 variant가 나왔다.</li>
  <li>Transformer의 variant는 component를 수정하는지, architecture를 수정하는지에 따라 4가지로 구분된다.
<img src="/assets/img/timeseries/iTransformer/fig3.jpeg" alt="사진2" />
    <ul>
      <li>1) Temporal dependency 모델링을 위한 attention module 수정 (Component adaptation)</li>
      <li>2) Linear model이 떠오르면서, transformer에서도 component나 architecture를 바꾸지 않고 Stationarization, Channel independence, Patching 등을 통해 효율적으로 성능 향상</li>
      <li>3) Transformer의 component와 architecture를 모두 수정하여 multivariate의 cross-time and cross-variate dependency를 파악</li>
      <li>4) iTransformer는 Transformer의 component는 그대로 가져오지만 inverted하게 가져온다. (architecture만 바뀜)</li>
    </ul>
  </li>
</ul>

<h2 id="3-itransformer">3. ITransformer</h2>
<ul>
  <li>현실 시나리오에서는 각각의 variate마다 발생 시점과 기록 시점의 delay 정도가 다를 수 있기 때문에 시점 \(t\)에서 모든 variates가 관측되지 않을 수도 있다. 뿐만 아니라 각각의 variate마다 통계적 분포 자체가 다를 수도 있다.</li>
</ul>

<h3 id="31-structure-overview">3.1. Structure Overview</h3>
<ul>
  <li><img src="/assets/img/timeseries/iTransformer/fig4.png" alt="사진3" /></li>
  <li>iTransformer는 Transformer의 encoder-only architecture (including the embedding, projection, and Transformer blocks)</li>
  <li><strong>Embedding the whole series as the token</strong> : 한 시점에서 많은 변수들을 하나의 token으로 간주하면 attention map을 학습하기 어렵다는 것은 patching으로 respective field를 늘리는 방식들이 좋은 성능을 낸다는 것으로부터 알 수 있다. 그러므로 each time series가 token이 되어 해당 변수의 properties를 다루고, self-attention으로 mutual interactions를, feed-forward networks로 series representations를 처리한다.</li>
  <li>iTransformer가 예측하는 future series \(\hat{\mathbf{Y}}_{:, n}\) based on \(\mathbf{X}_{:, n}\)의 formula :
<img src="/assets/img/timeseries/iTransformer/formula1.png" alt="사진4" />
    <ul>
      <li>\(\mathbf{H}=\left\{\mathbf{h}_1, \cdots, \mathbf{h}_N\right\} \in \mathbb{R}^{N \times D}\)은 \(D\)차원의 embedded tokens \(N\)개이고 \(h\)의 아래첨자는 layer index이다.</li>
      <li>Embedding \(\mathbb{R}^T \mapsto \mathbb{R}^D\)과 Projection \(\mathbb{R}^D \mapsto \mathbb{R}^S\)는 MLP가 한다.</li>
      <li>Inverted dimension으로 sequence의 순서가 FFN에 저장되므로 position embedding이 더이상 필요하지 않다.</li>
    </ul>
  </li>
  <li><strong>iTransformers</strong> : Attention에 multivariate correlation 외에는 requirements가 없어서 variates가 많아질 때 효율적이다. 또한 training과 inference에서 token의 개수가 다를 수 있어서 variates의 개수에 대해 유연한 모델이다.</li>
</ul>

<h3 id="32-inverted-transformer-components">3.2. Inverted Transformer Components</h3>
<ul>
  <li><strong>Layer normalization</strong> : Layer normalization은 훈련할 때 convergence speed and stability를 위한 것이다. 기존 Transformer에서는 한 시점에서 multivariate representation을 normalize했었는데, non-causal이나 앞서 언급한 delay를 고려하면 interaction noise의 원인이 될 수 있다. 그러므로 개별 variate를 normalize한다. 그러면 measurements(=variates, sensor, series)끼리의 불일치성도 해소된다.
\(\text { LayerNorm }(\mathbf{H})=\left\{\left.\frac{\mathbf{h}_n-\operatorname{Mean}\left(\mathbf{h}_n\right)}{\sqrt{\operatorname{Var}\left(\mathbf{h}_n\right)}} \right\rvert\, n=1, \cdots, N\right\}\)</li>
  <li><strong>Feed-Forward network</strong> : 각각의 variate token을 FFN에 태우면 universal approximation therem(Hornik, 1991)에 의해 시계열의 복잡한 representation을 추출할 수 있다. 이러한 inverted blocks를 쌓으면 observed를 encoding하고 future series를 decoding하는 과정을 <a href="/timeseries/2024-02-16-DLinear">MLP</a>처럼 할 수 있다. (MLP 방식은 시계열의 amplitude, periodicity, frequency spectrums까지 학습할 수 있고, time point self-attention보다도 좋은 성능을 낼 수 있다.)</li>
  <li><strong>Self-attention</strong> : Inverted dimenstion으로 self-attention을 계산하면 \(Q, K, V \in \mathbf R^{N \times d_k}\)를 linear projection으로 구한다. 사전에 feature-dimension으로 normalize를 해놓았으니 pre-Softmax score \(\mathbf{A}_{i, j}=\left(\mathbf{Q} \mathbf{K}^{\top} / \sqrt{d_k}\right)_{i, j} \propto \mathbf{q}_i^{\top} \mathbf{k}_j\)는 variate-wisecorrelation을 의미하고, whole score map \(\mathbf{A} \in \mathbb{R}^{N \times N}\)는 multivariate correlations btw paired variate tokens가 된다.</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<h3 id="41-forecasting-results">4.1. Forecasting Results</h3>
<ul>
  <li>7개의 데이터셋 사용(ECL, ETT, Exchange, Traffic, Weather, Solar-Energy, PEMS), 10개의 forecasting models과 비교
    <ul>
      <li>Transformer-based : Autoformer, FEDformer, Stationary, Crossformer, PatchTST</li>
      <li>Linear-based : DLinear, TiDE, RLinear</li>
      <li>TCN-based : SCINet, TimesNet
<img src="/assets/img/timeseries/iTransformer/table1.png" alt="사진5" /></li>
    </ul>
  </li>
  <li>SOTA였던 PatchTST는 변동이 심한 PEM 데이터를 처리하기 어렵고, 명시적으로 multivariate correlation을 파악하는 Crossformer보다 iTransformer의 성능이 뛰어나다.</li>
</ul>

<h3 id="42-itransformer-generality">4.2. ITransformer Generality</h3>
<ul>
  <li><strong>Performance promotion</strong>
<img src="/assets/img/timeseries/iTransformer/table2.png" alt="사진6" />
    <ul>
      <li>Transformer-based models에 inverted framework를 적용하여 성능을 비교한 결과 일관되게 성능이 향상되었다.</li>
    </ul>
  </li>
  <li><strong>Variate generalization</strong>
<img src="/assets/img/timeseries/iTransformer/fig5.png" alt="사진7" />
    <ul>
      <li>데이터의 20%만으로 training을 하더라도 100%로 training을 했을 때에 비해서 성능에 큰 차이가 없다는 것은 iTransformer가 효율적으로 훈련할 수 있는 모델임을 의미한다. 즉 unseen variates에 대해 generalization capability가 뛰어나다. 그 이유는 1) 각 variate를 token으로 embedding하니 variate의 개수에 제한이 없어지기 때문이고, 2) FFN이 각 token에 identically하게 적용되어 어떤 time series에서든 존재하는 본질적인 패턴을 학습할 수 있기 때문이다.</li>
    </ul>
  </li>
  <li><strong>Increasing lookback length</strong>
<img src="/assets/img/timeseries/iTransformer/fig6.png" alt="사진8" />
    <ul>
      <li>Transformer-based models는 look-back window가 길어져도 성능 향상으로 이어지지 않았지만, inverted framework는 look-back window가 길어지면 성능이 향상된다.</li>
    </ul>
  </li>
</ul>

<h3 id="43-model-analysis">4.3. Model Analysis</h3>
<ul>
  <li><strong>Ablation Study</strong>
<img src="/assets/img/timeseries/iTransformer/table3.png" alt="사진9" />
    <ul>
      <li>Time series의 multivariate correlation(Variate)와 series representation(Temporal)을 학습하기 위해 Attention을 FFN으로 바꿔도 보고 아예 안써보기도 하면서 iTransformer가 가장 좋은 성능을 낸다는 것을 확인하였다.</li>
    </ul>
  </li>
  <li><strong>Analysis of multivariate Representations and Correlations</strong>
<img src="/assets/img/timeseries/iTransformer/fig7.png" alt="사진10" />
    <ul>
      <li>The centered kernel alignment (CKA)는 높을수록 similar representation을 의미하는데, inverted frameworks가 유의미하게 높다.</li>
      <li>또한 얕은 layer의 attention map은 input series와 상관관계가 높고, 깊은 layer의 attention map은 future series와 상관관계가 높다는 점에서 interpretable attention이다.</li>
    </ul>
  </li>
  <li><strong>Efficient training strategy</strong>
<img src="/assets/img/timeseries/iTransformer/fig8.png" alt="사진11" />
    <ul>
      <li>각 배치에서 변수의 일부만 사용하여 훈련하더라도 MSE가 안정적이고, sample ratio가 낮아짐에 따라 memory가 적게 사용되므로 효율적인 모델이라 할 수 있다.</li>
    </ul>
  </li>
</ul>

<h2 id="5-conclusion-and-future-work">5. Conclusion and Future work</h2>
<ul>
  <li>iTransformer는 각 series를 variate token으로 보고 multivariate correlation을 파악하기 위해 attention과 FFN을 사용하였다.</li>
  <li>실험 결과를 통해 기존 Transformer-based time series forecasters보다 뛰어날 뿐만 아니라 interpretable한 성능을 보여주었다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/abs/2310.06625)]]></summary></entry><entry><title type="html">FITS: Modeling Time Series with 10k Parameters</title><link href="http://localhost:4000/timeseries/2024-02-22-FITS/" rel="alternate" type="text/html" title="FITS: Modeling Time Series with 10k Parameters" /><published>2024-02-22T00:00:00+09:00</published><updated>2024-02-23T02:34:43+09:00</updated><id>http://localhost:4000/timeseries/FITS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-22-FITS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>FITS(Frequency Interpolation Time Series)라는 lightweight yet powerful model 제안</li>
  <li>Raw time domain이 아니라 complex frequency domain에서 interpolation</li>
  <li>10k개의 parameters만 사용</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Time series domain에서는 시계열의 complexity와 dynamism 때문에 sparse and scattered하다.</li>
  <li>Frequency domain에서 time series를 representation하면 compact and efficient하다.</li>
  <li>Forecasting : simply extending the given look-back window with frequency interpolation</li>
  <li>Reconstruction : recover the original segment by interpolating the frequency representation of its downsampled counterpart</li>
  <li>FITS의 핵심은 <strong>complex-valued linear layer</strong> : amplitude scaling과 phase shift를 학습하여 complex domain에서 interpolation을 가능하게 한다.</li>
  <li>FITS의 <strong>low-pass filter</strong>를 통해 핵심적인 정보는 보존하면서 압축적인 정보를 representation한다.</li>
</ul>

<h2 id="2-related-work-and-motivation">2. Related Work and Motivation</h2>

<h3 id="21-frequency-aware-time-series-analysis-models">2.1. Frequency-Aware Time Series Analysis Models</h3>

<ul>
  <li>Frequency domain에서 시계열의 pattern을 잡으려는 시도 (FNet(2022), FEDFormer(2022), FiLM(2022), …)</li>
  <li>시계열의 periodicity(주기성)을 잡는 것도 중요 (DLinear(2023), TimesNet(2023))</li>
  <li>하지만 여전히 데이터 전처리를 포함하는 feature engineering에 의존하는 면이 있다.</li>
</ul>

<h3 id="22-divide-and-conquer-the-frequency-components">2.2. Divide and Conquer the Frequency Components</h3>

<ul>
  <li>Time series를 signal로 이해한다면 ?
    <ul>
      <li>시계열을 linear combination of sinusoidal components로 쪼갤 수 있다. (정보 손실 없이 !)</li>
      <li>그러면 sinusoidal waves에 phase bias만 추가해주면 예측값이 되니까 매우 간단해진다.</li>
    </ul>
  </li>
  <li>하지만 time domain에서 sinusoidal component를 예측하는 일은 어려우니 frequency domain에서 한다면 정보 손실도 없고 compact하다.</li>
</ul>

<h2 id="3-method">3. Method</h2>

<h3 id="31-preliminary--fft-and-complex-frequency-domain">3.1. Preliminary : FFT and Complex Frequency Domain</h3>

<ul>
  <li><strong>FFT(Fast Fourier Transform)</strong> : DFT(Discrete Fourier Transform)을 빠르게 계산, DFT는 discrete-time signals을 time domain에서 frequency domain으로 보내는 방법이다. (\(N\)개의 real numbers \(\to\) \(\frac{N}{2}+1\)개의 complex numbers)</li>
  <li><strong>Complex Frequency Domain</strong> : complex frequency는 signal의 representation인데, 각각의 frequency에 있는 complex number가 amplitude(magnitude or strength)와 phase(temporal shift or delay)를 파악한다.</li>
  <li>Complex number : \(X(f) = \mid X(f)\mid e^{j \theta (f)}\)
    <ul>
      <li>\(X(f)\) : frequency에서 component의 complex number</li>
      <li>\(\mid X(f)\mid\). : component의 amplitude</li>
      <li>\(e^{j \theta(f)}\) : component의 phase</li>
      <li>\(X(f)\)는 length가 amplitude이고 angle이 phase인 벡터로 visualize된다.</li>
      <li>다시 표현하면 \(X(f)=\mid X(f)\mid (\cos \theta(f)+j \sin \theta(f))\)</li>
      <li><img src="/assets/img/timeseries/FITS/fig1.png" alt="사진1" /></li>
    </ul>
  </li>
  <li><strong>Time Shift and Phase Shift</strong> : Time shift는 phase shift와 같다. 즉 unit complex exponential element를 곱하는 것과 같다. 예를 들어 만약 \(\tau\)만큼 time shift했다면(\(x(t-\tau)\)), Fourier transform은 \(X_\tau (f)=e^{-j 2 \pi f \tau } X(f)=\mid X(f)\mid e^{j(\theta(f)-2 \pi f \tau)}=[\cos (-2 \pi f \tau)+j \sin (-2 \pi f \tau)] X(f)\)이 된다. Amplitude는 변하지 않았고 phase만 \(\theta_\tau(f)=\theta(f)-2 \pi f \tau\)만큼 변했다. (time shift에 대해 linear)</li>
  <li>결론적으로, amplitude scaling과 phase shifting은 multiplication of complex numbers와 같다. (fig1)</li>
</ul>

<h3 id="32-fits-pipeline">3.2. FITS Pipeline</h3>

<ul>
  <li>Time series가 길수록 higher frequency resolution이 되기 때문에, 시계열의 frequency representation을 interpolate한다는 말은 = 시계열을 extend한다는 말과 같다. (시계열이 길어지면 더 작은 간격의 주파수로 변환될 수 있다는 걸 반대로 생각하면 된다.)</li>
  <li><img src="/assets/img/timeseries/FITS/fig2.png" alt="사진2" /></li>
  <li>FITS의 low-pass filter(LPF)는 말 그대로 낮은 주파수만 pass시키고 high-frequency components는 제거한다. 시계열의 essential한 정보, coarse한 정보는 남기고 fine한 정보는 제거하여 모델을 가볍게 만든다. (주기가 큰 파동만 남고 주기가 작아서 노이즈에 가까운 파동은 제거한다.)</li>
  <li><strong>Forecasting</strong> : 모델은 frequency interpolation을 통해 look-back window를 늘릴 수 있고(논문에서는 <code class="language-plaintext highlighter-rouge">extending</code>, <code class="language-plaintext highlighter-rouge">generate</code>, <code class="language-plaintext highlighter-rouge">reconstruct</code>로 표현하고 있다.) forecasting results가 된다.</li>
  <li><strong>Reconstruction</strong> : 우리는 원본 시계열을 downsampling했다가 reconstruction하게 되는데, downsampling된 segment를 원래 형태로 되돌릴 때 FITS가 사용된다. (frequency interpolation)</li>
</ul>

<h3 id="33-key-mechanisms-of-fits">3.3. Key Mechanisms of FITS</h3>

<ul>
  <li><strong>Complex Frequency Linear Interpolation</strong> : 모델의 output의 length를 조절해주기 위해서 interpolation rate를 일정하게 만들어준다. 원본 시계열의 절반 길이가 되는 frequency domain에서 interpolation을 하는데 output의 길이를 \(L_o\)로 맞춰주기 위해서는 \(\eta_{f r e q}=\frac{L_o / 2}{L_i / 2}=\frac{L_o}{L_i}=\eta\)가 되어야 한다.</li>
  <li><strong>Low Pass Filter(LPF)</strong> : 아래 그림처럼 frequency domain에서 high-frequency component에 해당하는 75%를 버리더라도 원래 시계열의 structure와 periodicity를 잘 보존한다. 왜냐하면 high-frequency component는 주기가 매우 짧은 성분들이라 노이즈에 가깝고 애초에 모델이 학습할 수 있는 영역 밖이라고 할 수 있기 때문이다.</li>
  <li><img src="/assets/img/timeseries/FITS/fig3.png" alt="사진3" /></li>
  <li><strong>Weight Sharing</strong> : 한 데이터셋 내에서 channels는 base frequency가 비슷하고 그러면 sharing weights로 효율적으로 multivariate task를 수행할 수 있다. 만약 채널마다 base frequency가 다르더라도 채널들을 클러스터링 해서 클러스터마다 개별적으로 학습하면 된다.</li>
</ul>

<h2 id="4-experiments-for-forecasting">4. Experiments for Forecasting</h2>

<h3 id="41-forecasting-as-frequency-interpolation">4.1. Forecasting as Frequency Interpolation</h3>

<ul>
  <li>일반적으로 look-back window의 길이 \(L\)이 forecasting horizon의 길이 \(H\)보다 기니까 단순하게 interpolation하기보다는 interpolation rate를 \(\eta_{\text {Fore }}=1+\frac{H}{L}\)로 한다.</li>
  <li><img src="/assets/img/timeseries/FITS/table12.png" alt="사진4" /></li>
</ul>

<h2 id="5-experiments-for-anomaly-detection">5. Experiments for Anomaly Detection</h2>

<ul>
  <li><img src="/assets/img/timeseries/FITS/table6.png" alt="사진5" /></li>
</ul>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Frequency interpolation으로 10k개의 parameters만으로 SOTA의 성능을 냈다.</li>
  <li>Future work : frequency domain에서의 large-scale complex-valued NN (ex. Complex-valued Transformer)</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://arxiv.org/abs/2307.03756)]]></summary></entry><entry><title type="html">\(2\). Geometric diatance (QGIS)</title><link href="http://localhost:4000/project/2024-02-21-prop2/" rel="alternate" type="text/html" title="\(2\). Geometric diatance (QGIS)" /><published>2024-02-21T00:00:00+09:00</published><updated>2024-02-22T19:04:56+09:00</updated><id>http://localhost:4000/project/prop2</id><content type="html" xml:base="http://localhost:4000/project/2024-02-21-prop2/"><![CDATA[<h2 id="1-shapefile-다운로드">1. Shapefile 다운로드</h2>
<ul>
  <li>지도에 두 주소 또는 좌표를 입력했을 때 출력되는 geometric distance를 구하려면 패키지가 아닌 지리 API를 활용해야 한다. 여기서는 <a href="https://qgis.org/en/site/forusers/download.html">QGIS</a>를 사용한다.</li>
  <li><code class="language-plaintext highlighter-rouge">서울시 도로망 layer</code>, <code class="language-plaintext highlighter-rouge">(지하철역, 버스정류장, 학교) layer</code>, 그리고 <code class="language-plaintext highlighter-rouge">아파트 layer</code>를 깔고 <strong>네트워크 분석</strong>을 진행할 것이다.</li>
  <li><code class="language-plaintext highlighter-rouge">서울시 도로망 layer</code>는 <a href="https://www.openstreetmap.org">OpenStreetMap</a>의 지도 정보를 추출해야 하는데, 3가지의 extractor가 있다.
    <ul>
      <li>1) <a href="https://download.geofabrik.de">Geofabrik</a>에서 sub region 지정
        <ul>
          <li>가장 하위 sub region이 South Korea이다. 즉 서울만 선택하는 것이 불가능하다.</li>
        </ul>
      </li>
      <li>2) <a href="https://extract.bbbike.org">BBBike</a>에서 region search (특정 도시 직접 search 가능)
        <ul>
          <li>특정 도시를 직접 search 가능하다. shp파일이 필요하므로 Shapefile을 선택하고 extract한다.</li>
        </ul>
      </li>
      <li>3) <a href="https://www.vworld.kr/v4po_main.do">국가공간정보포털(브이월드)</a>
        <ul>
          <li><code class="language-plaintext highlighter-rouge">공간정보 다운로드</code> \(\to\) <code class="language-plaintext highlighter-rouge">오픈마켓</code> \(\to\) <code class="language-plaintext highlighter-rouge">(도로명주소)도로구간</code>에서 지역별 도로 shp파일을 수집할 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>2) BBBike 또는 3) 국가공간정보포털을 선택하면 되는데, 2)를 선택했다.
    <ul>
      <li><img src="/assets/img/project/prop/prop2_1.png" alt="사진1" /></li>
      <li>왼쪽(국가공간정보포털)은 서울시에 포함되는 정확한 Node와 Link를 제공하지만, Node와 Link가 있어도 지나갈 수 없는 길 많다. 그래서 실제로 갈 수 있는 경로임에도 경로를 찾지 못하는 경우가 많다.</li>
      <li>오른쪽(BBBike)는 Node와 Link가 있어도 지나갈 수 없는 길이 훨씬 적지만, 실제와 다른 Link가 존재한다. (ex. 중앙에서 우측 하단으로 길게 그려진 직선)</li>
    </ul>
  </li>
</ul>

<h2 id="2-qgis-layer-추가">2. QGIS Layer 추가</h2>
<ul>
  <li>이제 QGIS를 실행한다. 기본 설정으로 <code class="language-plaintext highlighter-rouge">보기</code> \(\to\) <code class="language-plaintext highlighter-rouge">패널</code>에서 탐색기, 레이어, 공간 처리 툴박스를 선택한다.</li>
  <li><code class="language-plaintext highlighter-rouge">Z_KAIS_TL_SPRD_MANAGE_11000.shp</code>를 레이어로 추가한다. (<code class="language-plaintext highlighter-rouge">레이어</code> \(\to\) <code class="language-plaintext highlighter-rouge">레이어 추가</code> \(\to\) <code class="language-plaintext highlighter-rouge">벡터 레이어 추가</code> \(\to\) <code class="language-plaintext highlighter-rouge">*.shp 선택</code>하면 되는데 그냥 드래그해도 된다.)</li>
  <li>그리고 그 위에 미리 수집해놓은 지하철역 좌표를 레이어로 추가한다. 경도와 위도가 있는 csv파일이어야 한다. (<code class="language-plaintext highlighter-rouge">레이어</code> \(\to\) <code class="language-plaintext highlighter-rouge">레이어 추가</code> \(\to\) <code class="language-plaintext highlighter-rouge">구분자로 분리된 텍스트 레이어 추가</code> \(\to\) <code class="language-plaintext highlighter-rouge">지하철역.csv 파일 선택</code>)</li>
  <li><img src="/assets/img/project/prop/prop2_2.png" alt="사진2" /></li>
  <li>추가적으로 수집한 데이터(버스정류장, 학교 등)가 있다면 같은 방식으로 추가하면 되겠다.</li>
</ul>

<h2 id="3-최단경로-찾기">3. 최단경로 찾기</h2>
<ul>
  <li>우측 <code class="language-plaintext highlighter-rouge">공간 처리 툴박스</code> \(\to\) <code class="language-plaintext highlighter-rouge">네트워크 분석</code> \(\to\) <code class="language-plaintext highlighter-rouge">최단경로</code>를 선택하면 된다. <code class="language-plaintext highlighter-rouge">포인트에서 포인트</code>는 <code class="language-plaintext highlighter-rouge">ex.집에서 학교까지</code>이고, <code class="language-plaintext highlighter-rouge">포인트에서 레이어</code>는 <code class="language-plaintext highlighter-rouge">ex.집에서 모든 지하철역들까지</code>이다.</li>
  <li>네트워크 레이어, 시작점(출발지) 레이어, 그리고 종단 레이어를 지하철역으로 선택한다.</li>
  <li><img src="/assets/img/project/prop/prop2_3.png" alt="사진3" /></li>
  <li>연세대학교 정문(point)에서 모든 지하철역(layer)로 가는 최단거리를 표시했다. <code class="language-plaintext highlighter-rouge">레이어</code> \(\to\) <code class="language-plaintext highlighter-rouge">속성 테이블 열기</code>에서 cost가 낮은 순서대로 정렬하면 가까운 지하철역을 정렬할 수 있다.</li>
</ul>

<h2 id="4-최단경로-excel에-저장하기">4. 최단경로 excel에 저장하기</h2>
<ul>
  <li>좌측 하단 <code class="language-plaintext highlighter-rouge">레이어</code> 패널에서 <code class="language-plaintext highlighter-rouge">최단 경로</code>를 우클릭 후, export를 누르면 최단경로의 속성 테이블을 원하는 형태(ex. *.xlsx)로 추출할 수 있다.</li>
  <li><img src="/assets/img/project/prop/prop2_4.png" alt="사진4" /></li>
</ul>

<p>to be continued…</p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="project" /><summary type="html"><![CDATA[1. Shapefile 다운로드 지도에 두 주소 또는 좌표를 입력했을 때 출력되는 geometric distance를 구하려면 패키지가 아닌 지리 API를 활용해야 한다. 여기서는 QGIS를 사용한다. 서울시 도로망 layer, (지하철역, 버스정류장, 학교) layer, 그리고 아파트 layer를 깔고 네트워크 분석을 진행할 것이다. 서울시 도로망 layer는 OpenStreetMap의 지도 정보를 추출해야 하는데, 3가지의 extractor가 있다. 1) Geofabrik에서 sub region 지정 가장 하위 sub region이 South Korea이다. 즉 서울만 선택하는 것이 불가능하다. 2) BBBike에서 region search (특정 도시 직접 search 가능) 특정 도시를 직접 search 가능하다. shp파일이 필요하므로 Shapefile을 선택하고 extract한다. 3) 국가공간정보포털(브이월드) 공간정보 다운로드 \(\to\) 오픈마켓 \(\to\) (도로명주소)도로구간에서 지역별 도로 shp파일을 수집할 수 있다. 2) BBBike 또는 3) 국가공간정보포털을 선택하면 되는데, 2)를 선택했다. 왼쪽(국가공간정보포털)은 서울시에 포함되는 정확한 Node와 Link를 제공하지만, Node와 Link가 있어도 지나갈 수 없는 길 많다. 그래서 실제로 갈 수 있는 경로임에도 경로를 찾지 못하는 경우가 많다. 오른쪽(BBBike)는 Node와 Link가 있어도 지나갈 수 없는 길이 훨씬 적지만, 실제와 다른 Link가 존재한다. (ex. 중앙에서 우측 하단으로 길게 그려진 직선)]]></summary></entry><entry><title type="html">\(1\). 데이터 수집 및 전처리</title><link href="http://localhost:4000/project/2024-02-20-prop1/" rel="alternate" type="text/html" title="\(1\). 데이터 수집 및 전처리" /><published>2024-02-20T00:00:00+09:00</published><updated>2024-02-22T18:57:44+09:00</updated><id>http://localhost:4000/project/prop1</id><content type="html" xml:base="http://localhost:4000/project/2024-02-20-prop1/"><![CDATA[<h2 id="11-데이터-수집">1.1. 데이터 수집</h2>
<p><img src="/assets/img/project/prop/prop1_1.png" alt="사진1" /></p>
<ul>
  <li>아파트 매매 데이터는 <a href="https://rt.molit.go.kr/pt/xls/xls.do?mobileAt=">국토교통부 실거래가 공개시스템</a>에서 다운로드하였다.</li>
</ul>

<p><img src="/assets/img/project/prop/prop1_2.png" alt="사진2" /></p>
<ul>
  <li>아파트의 위치, 면적, 층, 매매가격 정도를 알 수가 있다. 부동산 분석을 위해서는 주변 정보도 수집할 필요가 있는데, 1.3.에서 포스팅한다.</li>
</ul>

<h2 id="12-좌표-변환--geocode">1.2. 좌표 변환 : Geocode</h2>
<ul>
  <li>Google Geocode를 활용하여 주소를 경도, 위도 좌표계로 변환하였다. 구글 spreadsheets에서 파일을 열고, 확장 프로그램에서 Geocode by Awesome Table을 다운받아 확장프로그램 \(\to\) Start Geocoding을 실행하였다.
<img src="/assets/img/project/prop/prop1_3.png" alt="사진3" /></li>
  <li>그러면 아래 사진처럼 Latitude와 Longitude 열이 생기면서 주소에 해당하는 좌표가 자동으로 입력된다. 다만 속도가 빠르지 않다는 점, Geocode premium을 사용하지 않으면 사용량에 제한이 있다.
<img src="/assets/img/project/prop/prop1_4.png" alt="사진4" /></li>
  <li>Geocode 중복 사용량을 없애기 위해서 같은 아파트의 여러 매매에 대해 매번 좌표를 계산하지 않도록, 아파트에 대해 unique하게 좌표를 계산하고 다시 merge하여 사용량을 1.2%로 줄였다.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">2013년 ~ 2023년 거래된 아파트의 수 : </span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">add20u</span><span class="p">))</span>
<span class="mi">2013</span><span class="n">년</span> <span class="o">~</span> <span class="mi">2023</span><span class="n">년</span> <span class="n">거래된</span> <span class="n">아파트의</span> <span class="n">수</span> <span class="p">:</span>  <span class="mi">9166</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">2013년 ~ 2023년 거래건수: </span><span class="sh">"</span><span class="p">,</span> <span class="nf">all_len</span><span class="p">(</span><span class="n">apt20</span><span class="p">))</span>
<span class="mi">2013</span><span class="n">년</span> <span class="o">~</span> <span class="mi">2023</span><span class="n">년</span> <span class="n">거래건수</span><span class="p">:</span>  <span class="mi">822029</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="13-주변-정보-수집">1.3. 주변 정보 수집</h2>
<ul>
  <li>부동산 분석을 위해서는 다양한 요인이 고려되어야겠지만, 그 중 하나가 주변 정보이다. 아파트의 주변에 지하철역이 있는지, 초/중/고등학교가 있는지 등등이다. 일단은 지하철역, 버스정류장, 학교의 위치를 수집하였다.</li>
  <li>수도권 지하철역 위치 : <a href="https://gaussian37.github.io/python-etc-수도권-지하철/">JINSOL KIM님 Blog</a></li>
  <li>서울시 버스정류장 위치 : <a href="https://topis.seoul.go.kr">서울시 교통정보 시스템</a></li>
  <li>서울시 초/중/고등학교 :  <a href="https://open.neis.go.kr/portal/mainPage.do">나이스 교육정보 개방 포털</a></li>
</ul>

<h2 id="14-euclidean-distance--haversine">1.4. Euclidean distance : haversine</h2>
<ul>
  <li>(* 1.4.는 trial and error 기록을 위한 포스팅일 뿐, 결과적으로 사용하지 않은 방법이니 넘어가도 된다.)</li>
  <li>이제 haversine 패키지를 사용해서 아파트별로 근처에 지하철역, 버스정류장, 그리고 학교가 몇 개인지 계산해주었다. harversine 사용법은 매우 쉽다. 예를 들어 서울역과 고속터미널역의 거리는 6.35km 정도 된다.
<img src="/assets/img/project/prop/prop1_5.png" alt="사진5" /></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">haversine</span> <span class="kn">import</span> <span class="n">haversine</span>
<span class="c1"># 서울역
</span><span class="n">seoul_station</span> <span class="o">=</span> <span class="p">(</span><span class="mf">37.5547278494914</span><span class="p">,</span> <span class="mf">126.969526329341</span><span class="p">)</span>
<span class="c1"># 고속터미널역
</span><span class="n">terminal_station</span> <span class="o">=</span> <span class="p">(</span><span class="mf">37.5049267445237</span><span class="p">,</span> <span class="mf">127.004949918697</span><span class="p">)</span>

<span class="nf">haversine</span><span class="p">(</span><span class="n">seoul_station</span><span class="p">,</span> <span class="n">terminal_station</span><span class="p">)</span>
<span class="c1"># 6.357909897291526
</span></code></pre></div></div>

<ul>
  <li>이제 아래와 같이 학교, 지하철역, 버스정류장의 좌표 데이터를 준비하고 harversine을 통해 아파트별로 가까운 지하철역이 몇 개인지를 센다.
<img src="/assets/img/project/prop/prop1_6.png" alt="사진6" /></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">NearStation</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">station</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="c1"># For all apts
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">(Station) (</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="s">)-th apt is completed !</span><span class="sh">"</span><span class="p">)</span>
        <span class="c1"># For all stations
</span>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">station</span><span class="p">)):</span>
            <span class="n">apt_lat</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">Latitude</span><span class="sh">"</span><span class="p">];</span> <span class="n">apt_long</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">Longitude</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">bus_lat</span> <span class="o">=</span> <span class="n">station</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="sh">"</span><span class="s">Latitude</span><span class="sh">"</span><span class="p">];</span> <span class="n">bus_long</span> <span class="o">=</span> <span class="n">station</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="sh">"</span><span class="s">Longitude</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="nf">haversine</span><span class="p">((</span><span class="n">apt_lat</span><span class="p">,</span> <span class="n">apt_long</span><span class="p">),</span> <span class="p">(</span><span class="n">bus_lat</span><span class="p">,</span> <span class="n">bus_long</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">dist</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
                <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">NearStation</span><span class="sh">"</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<ul>
  <li>하지만 우리가 실제로 아파트가 지하철역에서 얼마나 떨어져있는지를 생각할 때에는 <strong>euclidean distance</strong>가 아니라 실제로 지나가는 최단거리와 소요시간을 생각한다. 그러므로 최단거리에 해당하는 <strong>geometric distance</strong>을 사용할 필요가 있다. Geometric distance는 QGIS를 사용하므로 다음 게시글에서 다룬다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="project" /><summary type="html"><![CDATA[1.1. 데이터 수집 아파트 매매 데이터는 국토교통부 실거래가 공개시스템에서 다운로드하였다. 아파트의 위치, 면적, 층, 매매가격 정도를 알 수가 있다. 부동산 분석을 위해서는 주변 정보도 수집할 필요가 있는데, 1.3.에서 포스팅한다.]]></summary></entry><entry><title type="html">(PatchTST) A Time Series Is Worth 64 Words: Long-term Forecasting With Transformers</title><link href="http://localhost:4000/timeseries/2024-02-18-PatchTST/" rel="alternate" type="text/html" title="(PatchTST) A Time Series Is Worth 64 Words: Long-term Forecasting With Transformers" /><published>2024-02-18T00:00:00+09:00</published><updated>2024-02-22T18:57:44+09:00</updated><id>http://localhost:4000/timeseries/PatchTST</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-18-PatchTST/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>논문에서 요약을 잘 해놔서 굳이 번역하지 않고 그대로 가져왔다.</li>
  <li>2개의 Key components
    <ul>
      <li>Segmentation of time series into <strong>subseries-level patches</strong> which are served as input tokens to Transformer</li>
      <li><strong>Channel-independence</strong> where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series.</li>
    </ul>
  </li>
  <li>Patching design의 장점 3가지
    <ul>
      <li><strong>local semantic information</strong> is retained in the embedding</li>
      <li><strong>computation and memory usage</strong> of the attention maps are quadratically reduced</li>
      <li>the model can attend <strong>longer history</strong></li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Patching : 단일 time-step을 token으로 만들면 (<code class="language-plaintext highlighter-rouge">point-wise input token</code>) 시계열의 포괄적인 의미 정보를 파악할 수 없기 때문에, time-steps를 합쳐서 subseries-level patchs를 만들어 locality를 강화하고 포괄적인 의미 정보를 파악한다.</li>
  <li>Channel-independence : 각 token이 오직 하나의 채널(feature)의 정보만 담는 것이다. (반대로 <code class="language-plaintext highlighter-rouge">channel-mixing</code>은 token이 모든 채널(features)를 embedding space에 projection해서 정보를 섞는 방식이다.)</li>
  <li>PatchTST의 장점 3
    <ul>
      <li>Reduction on time and space complexity</li>
      <li>Longer look-back window</li>
      <li>Capability of representation learning</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related work</h2>
<ul>
  <li>Patching의 milestone은 ViT(2021)</li>
  <li>LogTrans(2019)
    <ul>
      <li>key, query는 point-wise 내적 안하지만 여전히 value는 single time step에 기반한다.</li>
    </ul>
  </li>
  <li>Autoformer(2021)
    <ul>
      <li>patch level connection을 얻기 위해 auto-correlation을 사용하지만 handcrafted design이라서 패치 내 의미 정보를 모두 파악하기 어렵다.</li>
    </ul>
  </li>
  <li>Triformer(2022)
    <ul>
      <li>patch attention을 제안하긴 하지만 patch를 input으로 사용하지 않는다는 점에서 의미 정보를 파악하기 어렵다.</li>
    </ul>
  </li>
  <li>Unlabelled data로 인해 self-supervised learning이 많이 떴는데, transformer를 통해 time series에 적용하기 위한 representation을 학습하는 시도는 아직 완전하지 않다.</li>
</ul>

<h2 id="3-proposed-method">3. Proposed Method</h2>

<h3 id="31-model-structure">3.1. Model Structure</h3>
<ul>
  <li>\((\boldsymbol{x_{1}}, ..., \boldsymbol x_L)\) 를 보고 \((\boldsymbol x_{L+1}, ..., \boldsymbol x_{L+T})\)를 예측하는 문제이고, PatchTST는 transformer의 encoder를 핵심으로 한다.
<img src="/assets/img/timeseries/PatchTST/fig1.jpeg" alt="사진1" /></li>
  <li><strong>Forward Process</strong> : 시계열에 있는 M개의 변수가 있고 길이가 L이라고 할 때, \(i\)번째 series는 \(\boldsymbol{x}_{1:L}^{(i)}=(x_{1}^{(i)}, ... , x_{L}^{(i)})\)이다.</li>
  <li>M개의 \(\boldsymbol{x}^{(i)} \in \mathbb R^{1 \times L}\) 가 각각 transformer backbone으로 들어가고 (channel-independence) 각각의 transformer는 \(\boldsymbol{\hat x}^{(i)} =(\hat x_{L+1}^{(i)}, ..., \hat x_{L+T}^{(i)})\in \mathbb R^{1 \times L}\)를 output으로 한다.</li>
  <li><strong>Patching</strong> : 아래 그림처럼 univariate time series \(\boldsymbol{x}^{(i)}\)를 \(\boldsymbol{x}_p^{(i)} \in \mathbb R^{P \times N}\)으로 patching한다.
<img src="/assets/img/timeseries/PatchTST/myfig1.jpeg" alt="사진2" /></li>
  <li>Input token의 개수가 \(L\)에서 \(N=\left\lfloor\frac{(L-P)}{S}\right\rfloor+2\)로 줄어들기 때문에, 사용할 수 있는 memory와 complexity가 확보되면서 더 긴 historical sequence를 볼 수 있어 성능이 향상된다.</li>
  <li><strong>Transformer Encoder</strong> :
    <ul>
      <li>1) Mapping to the transformer latent space : \(\boldsymbol{x}_d^{(i)}= \mathbf W_p\boldsymbol{x}_p^{(i)}+ \mathbf W_{pos}\)
        <ul>
          <li>where trainable linear projection \(\mathbf W_p \in \mathbb R^{D \times P}\), learnable addictive position encoding \(\mathbf W_{pos} \in \mathbb R^{D \times N}\))</li>
        </ul>
      </li>
      <li>
        <dl>
          <dt>2) Multi-head attention (with Batchnorm and Residual connection)</dt>
          <dd><code class="language-plaintext highlighter-rouge">Query</code> \(Q_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^Q\), <code class="language-plaintext highlighter-rouge">Key</code> \(K_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^K\) and <code class="language-plaintext highlighter-rouge">Value</code> \(V_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^V\)</dd>
        </dl>
        <ul>
          <li>where \(\mathbf{W}_h^Q, \mathbf{W}_h^K \in \mathbb{R}^{D \times d_k}\) and \(\mathbf W_h^V \in \mathbb R^{D \times D}\)</li>
        </ul>
      </li>
      <li>3) Getting attention : \(\mathbf O_h^{(i)} \in \mathbb R^{D \times N}\)
        <ul>
          <li>where \(\left(\mathbf{O}_h^{(i)}\right)^T=\operatorname{Attention}\left(Q_h^{(i)}, K_h^{(i)}, V_h^{(i)}\right)=\operatorname{Softmax}\left(\frac{Q_h^{(i)} K_h^{(i)^T}}{\sqrt{d_k}}\right) V_h^{(i)}\)</li>
        </ul>
      </li>
      <li>4) Flatten and Linear head : \(\boldsymbol{\hat x}^{(i)}=(\boldsymbol{\hat x}_{L+1}^{(i)}, ..., \boldsymbol{\hat x}_{L+T}^{(i)}) \in \mathbb R^{1 \times T}\)</li>
    </ul>
  </li>
  <li><strong>Loss function</strong> : MSE. \(\mathcal{L}=\mathbb{E}_{\boldsymbol{x}} \frac{1}{M} \sum_{i=1}^M\left\|\hat{\boldsymbol{x}}_{L+1: L+T}^{(i)}-\boldsymbol{x}_{L+1: L+T}^{(i)}\right\|_2^2\)</li>
  <li><strong>Instance Normalization</strong> : Pathcing 전에 각 univariate time series에 <code class="language-plaintext highlighter-rouge">mean=0</code>, <code class="language-plaintext highlighter-rouge">std=1</code>하고, output prediction 전에 다시 더해준다.</li>
</ul>

<h3 id="32-representation-learning">3.2. Representation Learning</h3>
<ul>
  <li>Self-supervised representation learning 방법 중에서 masked autoencoder를 사용했다. (input sequence의 일부를 0으로 masking하고 recover하도록 모델링)</li>
  <li>다만 이걸 그대로 Multivariate time series에 가져오면 두 가지 문제가 발생한다.
    <ul>
      <li>첫째, single time step에 masking하면 맞추기가 너무 쉽다. (interpolating 하면 끝) 그래서 다양한 크기의 group of time series를 랜덤하게 masking하는 기존의 방법을 사용했다.</li>
      <li>둘째, 각 time step을 D차원으로 representation하면 \(z_t \in \mathbb R^D\)가 되니, parameter matrix \(\mathbf W\)의 차원이 \((L\cdot D) \times (M\cdot T)\)이 되어 \(L, D, M, T\) 중 하나만 커지더라도 oversieze가 된다. 그래서 PatchTST에서는 \(D \times P\) size의 linear layer를 사용하였고, patch 단위로 masking을 했다.</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>사용한 데이터셋은 9개(ETTm1/2, ETTh1/2, ILI, Weather, Traffic, Exchange, Electrictiy)이고 비교한 모델은 6개(FEDformer, Autoformer, Informer, Pyraformer, LogTrans +LTSF-Linear)이다.
<img src="/assets/img/timeseries/AreTF/table12.jpeg" alt="사진3" />
<img src="/assets/img/timeseries/PatchTST/table34.jpeg" alt="사진4" /></li>
  <li>PatchTST/64는 input patches 64개, look-back window size L=512이다.
  PatchTST/42는 input patches 42개, look-back window size L=336이다.
  두 버전 모두 patch length P = 16, stride S = 8이다.
  Masking ratio = 40%이다.</li>
  <li>실험 결과 long-term forecasting에서 다른 transformer-based models 및 DLinear보다 성능이 뛰어났다.
<img src="/assets/img/timeseries/PatchTST/table56.jpeg" alt="사진5" /></li>
  <li>Transfer learning task에서도 다른 모델들보다 성능이 뛰어났다.
<img src="/assets/img/timeseries/PatchTST/table7.jpeg" alt="사진6" /></li>
  <li>Ablation study 결과 patching과 channel-independence 모두 성능에 중요한 역할을 하고 있음을 알 수 있다. 특히 patching의 motivation은 앞서 언급한 것처럼 직관적이다.
<img src="/assets/img/timeseries/PatchTST/fig2.jpeg" alt="사진7" /></li>
  <li>논문 <a href="/timeseries/2024-02-16-AreTF">Are Transformer Effective for Time Series Forecasting?</a>에서 transformer-based models의 경우 look-back windows size가 커져도 예측 성능이 좋아지지 않는다고 주장했고, 이는 temporal information을 잘 못잡아내는 것이 맞다. 하지만 PatchTST는 look-back windows가 길어질수록 성능이 좋아지므로 해당사항이 없다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>PatchTST의 key components 2가지는 : Patching과 Channel-independence이다.</li>
  <li>PatchTST는 longer look-back windows의 benefit을 가질 수 있으면서 local semantic information을 파악할 수 있다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://arxiv.org/abs/2211.14730)]]></summary></entry><entry><title type="html">(DLinear) Are Transformer Effective for Time Series Forecasting?</title><link href="http://localhost:4000/timeseries/2024-02-16-DLinear/" rel="alternate" type="text/html" title="(DLinear) Are Transformer Effective for Time Series Forecasting?" /><published>2024-02-16T00:00:00+09:00</published><updated>2024-03-01T17:26:18+09:00</updated><id>http://localhost:4000/timeseries/DLinear</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-16-DLinear/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Transformer-based solutions는 긴 시퀀스 속에 있는 semantic correlations를 잘 추출하기 때문에 long-term time series forecasting(LTSF)에 쓰인다.</li>
  <li>하지만 이러한 permutation-invariant self-attention의 특성상 temporal information loss가 불가피하다.</li>
  <li>그러므로 simple one-layer linear models (LTSF-Linear)를 제안한다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Transformer의 핵심인 multi-head self-attention은 <code class="language-plaintext highlighter-rouge">permutation-invariant</code> = <code class="language-plaintext highlighter-rouge">anti-order</code>한 semantic correlations를 찾아낸다.</li>
  <li>하지만 time series는 순서 자체가 굉장히 중요한데, NLP를 위한 transformer가 LTST에 효과적일까?</li>
  <li>결론적으로, 실험결과 : 복잡한 transformer 구조보다 단순한 LTSF-Linear의 성능이 더 뛰어났다.</li>
</ul>

<h2 id="2-prelininaries--tsf-problem-formulation">2. Prelininaries : TSF Problem Formulation</h2>
<ul>
  <li>본 논문에서 사용할 notation은 간단하다. \(\cal X = \{ X_1^t, ..., X_C^t\}_{t=1}^L\)을 보고 \(\cal \hat X = \{\hat X_1^t, ..., \hat X_C^t\}_{t=L+1}^{L+T}\)를 예측하는 것이다. (\(C\)는 변수의 개수, \(L\)은 look-back window의 size, \(T\)는 예측하고자 하는 time steps)</li>
</ul>

<h2 id="3-transformer-based-ltsf-solutions">3. Transformer-based LTSF solutions</h2>
<ul>
  <li>Vanilla Transformer는 quadratic time/memory complexity, error accumulation by the autoregressive decoder로 인한 한계가 있고, 이걸 해결하기 위한 많은 transformer-based models가 있는데, 요약하면 아래 그림과 같다.
<img src="/assets/img/timeseries/AreTF/fig1.jpeg" alt="사진1" /></li>
  <li>Autoformer, LogTrans, Pyraformer, FEDformer, Informer 등 transformer를 time series에 활용하기 위한 다양한 시도들이 있었는데, 파이프라인 단계별로 간단하게만 보겠다.
    <ul>
      <li>(a) Preprocessing : normalization with zero-mean, seasonal-trend decomposition, …</li>
      <li>(b) Embedding : fixed positional encoding, channel projection embedding, learnable temporal embedding, …</li>
      <li>(c) Encoder : Logsparse mask(LogTrans), pyramidal attention(Pyraformer), ProbSparse(Informer), Frequency enhanced block(FEDformer), series-wise auto-correlation(Autoformer), …</li>
      <li>(d) Decoder : IMS(Iterative Multi-step forecasting, 단일 시점 예측 반복하여 여러 시점 예측) 대신 DMS(Direct Multi-step forecasting, 각 미래 시점 예측 위해 별도 모델) 사용</li>
    </ul>
  </li>
  <li>하지만 앞서 언급한 것처럼 permutation-invariant한 semantic correlations은 temporal relation과 다르다.</li>
</ul>

<h2 id="4-an-embarrassingly-simple-baseline">4. An Embarrassingly Simple Baseline</h2>
<ul>
  <li>기존의 transformer-based LTSF solutions는 non-transformer에 비해 성능이 좋다는 실험 결과를 내놓았지만, 그건 IMS를 사용한 non-transformer와 달리 DMS를 사용했기 때문이다.</li>
  <li>이걸 뒷받침하는 근거가 본 논문에서 제시하는 DMS 모델인 LTSF-Linear인데, \(\hat X_i = WX_i\) where \(W \in \mathbb R^{T\times L}\) 구조이다.</li>
  <li>LTSF-Linear는 DLinear, NLinear 두 종류가 있다.
    <ul>
      <li>DLinear : trend component와 seasonal component로 decompose하고 각각을 one-layer에 넣어 다시 sum한다.</li>
      <li>NLinear : Input의 각 값에서 마지막 값을 빼고 linear layer를 통과한 뒤 다시 더해주는 simple normalization
<img src="/assets/img/timeseries/AreTF/fig2.jpeg" alt="사진2" /></li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>
<ul>
  <li>사용한 데이터셋은 9개(ETTm1/2, ETTh1/2, ILI, Weather, Traffic, Exchange, Electrictiy)이고 비교한 모델은 5개(FEDformer, Autoformer, Informer, Pyraformer, LogTrans)이다. 아래는 quantitative and qualitative results이다.
<img src="/assets/img/timeseries/AreTF/table12.jpeg" alt="사진3" />
<img src="/assets/img/timeseries/AreTF/fig3.jpeg" alt="사진4" /></li>
  <li>Q) Can existing LTSF-Transformers extract temporal relations well from longer input sequences?
    <ul>
      <li><img src="/assets/img/timeseries/AreTF/fig4.jpeg" alt="사진5" /></li>
      <li>Look-back window size가 커짐에 따라, transformer-based models의 성능은 크게 좋아지지 않지만, LTSF-Linear의 성능은 유의미하게 좋아진다.</li>
    </ul>
  </li>
  <li>Q) Are the self-attention scheme effective for LTSF?
    <ul>
      <li><img src="/assets/img/timeseries/AreTF/table4.jpeg" alt="사진6" /></li>
      <li>Informer에서 one linear layer까지 구조를 점점 단순하게 할수록 성능이 좋아졌다. 즉 복잡한 모듈이 불필요하다.</li>
    </ul>
  </li>
  <li>Q) Can existing LTSF-Transformers preserve temporal order well?
    <ul>
      <li><img src="/assets/img/timeseries/AreTF/table5.jpeg" alt="사진7" /></li>
      <li>데이터를 섞거나 반 나눠서 순서를 바꿨을 때, 다른 transformer-based models은 성능이 크게 떨어지지 않았지만, LTSF-Linear의 경우 유의미하게 떨어졌다.</li>
      <li>그러므로 LTSF-Linear가 다른 transformer-based models보다 temporal order를 잘 보존한다.</li>
    </ul>
  </li>
  <li>Q) Is training data size a limiting factor for existing LTSF- Transformers?
    <ul>
      <li><img src="/assets/img/timeseries/AreTF/table7.jpeg" alt="사진8" /></li>
      <li>Full dataset(ori.)로 학습했을 때보다 1년치 데이터(short)로 학습했을 때 성능이 더 좋았다.</li>
      <li>단순하게 긴 training data가 필요한게 아니라, whole-year data에 더 명확한 temporal features가 있다는 걸 알 수 있다.</li>
    </ul>
  </li>
</ul>

<h2 id="6-conclusion-and-future-work">6. Conclusion and Future Work</h2>
<ul>
  <li>본 논문의 contribution은 linear model을 제안한 데에 있는 것이 아니라, 비교 실험을 통해 왜 LTSF-transformers가 효율적이지 못한지에 대한 질문을 던지는 데에 있다.</li>
  <li>본 논문의 비교 실험 결과를 통해 transformer의 구조에 대해 더 잘 이해하고 효율적으로 사용하기 위한 방법을 고민해볼 수 있다.</li>
  <li>본 논문에서 제시하는 LTSF-Linear는 change points로 인한 temporal dynamics를 포착하기 어려운 등 한계점이 분명하기 때문에, 앞으로의 연구를 위해 단순하면서 경쟁력 있는 기준선 정도로 생각하면 되겠다.</li>
</ul>

<h2 id="추가">추가</h2>
<ul>
  <li>본 논문이 발표되기 전에 transformer-based time series models가 많이 나왔다. Informer, Autoformer, Pyraformer, Fedformer 등이다. 본 논문은 이러한 solutions을 반박하면서 conv를 제안하였다. 이제 본 논문을 반박하면서 다시 transformer를 제시하는 PatchTST를 읽어보러 가자.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2022](https://arxiv.org/abs/2205.13504)]]></summary></entry><entry><title type="html">깃블로그 만들기 6 : 흔히 발생할 수 있는 에러</title><link href="http://localhost:4000/gitblog/2024-02-15-gitblog6/" rel="alternate" type="text/html" title="깃블로그 만들기 6 : 흔히 발생할 수 있는 에러" /><published>2024-02-15T00:00:00+09:00</published><updated>2024-02-17T00:26:39+09:00</updated><id>http://localhost:4000/gitblog/gitblog6</id><content type="html" xml:base="http://localhost:4000/gitblog/2024-02-15-gitblog6/"><![CDATA[<ul>
  <li>이번에는 Hydejack 테마를 사용하면서 내가 자주 했던 실수들을 살펴보겠다. 처음에는 에러의 이유를 몰라서 gitblog repository를 몇 번이나 삭제하고 다시 만들면서 시간을 많이 썼는데, 앞으로는 그러지 않기 위해 남긴다.</li>
</ul>

<h2 id="1-git-commit-push할-때-_configyml의-theme-수정">1. git commit, push할 때 _config.yml의 theme 수정</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">깃 블로그 만들기 2 : 테마 적용하기</code> 10. 에서 git에 commit, push하기 전에 <code class="language-plaintext highlighter-rouge">_config.yml</code>파일에 수정을 해야 한다고 말했다. 무슨 말이냐면 내가 로컬에서 깃블로그를 수정하고 <code class="language-plaintext highlighter-rouge">bundle exec jekyll serve</code>로 서버에서 확인을 한 뒤 이제 push를 통해 반영하기 위해서는 터미널에 아래처럼 입력해야 한다는 것이다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sed</span> <span class="o">-</span><span class="n">i</span> <span class="sh">''</span> <span class="sh">'</span><span class="s">s/theme: jekyll-theme-hydejack/# theme: jekyll-theme-hydejack/</span><span class="sh">'</span> <span class="n">_config</span><span class="p">.</span><span class="n">yml</span>
<span class="n">sed</span> <span class="o">-</span><span class="n">i</span> <span class="sh">''</span> <span class="sh">'</span><span class="s">s/# remote_theme: hydecorp\/hydejack@v9/remote_theme: hydecorp\/hydejack@v9/</span><span class="sh">'</span> <span class="n">_config</span><span class="p">.</span><span class="n">yml</span>

<span class="n">ga</span> <span class="p">.</span>
<span class="n">git</span> <span class="n">commit</span> <span class="o">-</span><span class="n">m</span> <span class="err">“</span><span class="n">update</span><span class="err">”</span>
<span class="n">gp</span> <span class="o">--</span><span class="nb">set</span><span class="o">-</span><span class="n">upstream</span> <span class="n">origin</span> <span class="n">main</span>
<span class="n">gp</span>

<span class="n">sed</span> <span class="o">-</span><span class="n">i</span> <span class="sh">''</span> <span class="sh">'</span><span class="s">s/^# theme: jekyll-theme-hydejack/theme: jekyll-theme-hydejack/</span><span class="sh">'</span> <span class="n">_config</span><span class="p">.</span><span class="n">yml</span>
<span class="n">sed</span> <span class="o">-</span><span class="n">i</span> <span class="sh">''</span> <span class="sh">'</span><span class="s">s/^remote_theme: hydecorp\/hydejack@v9/# remote_theme: hydecorp\/hydejack@v9/</span><span class="sh">'</span> <span class="n">_config</span><span class="p">.</span><span class="n">yml</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ga .</code>는 <code class="language-plaintext highlighter-rouge">git add .</code>, <code class="language-plaintext highlighter-rouge">gp</code>는 <code class="language-plaintext highlighter-rouge">git push</code>를 의미한다.</li>
  <li>위 2줄은 <code class="language-plaintext highlighter-rouge">_config.yml</code>파일에서 <code class="language-plaintext highlighter-rouge">theme: jekyll-theme-hydejack</code> (이하 <code class="language-plaintext highlighter-rouge">theme</code>) 부분을 주석 처리하고 <code class="language-plaintext highlighter-rouge">remote_theme: hydecorp/hydejack@v9</code> (이하 <code class="language-plaintext highlighter-rouge">remote_theme</code>)부분의 주석 처리를 해제하는 명령어이다.</li>
  <li>
    <p>중앙 4줄은 변경 사항들을 git에 push하는 명령어이고, 아래 2줄은 원래대로 <code class="language-plaintext highlighter-rouge">_config.yml</code>파일에서 <code class="language-plaintext highlighter-rouge">theme</code> 부분의 주석을 해제하고 <code class="language-plaintext highlighter-rouge">remote_theme</code> 부분을 주석 처리하는 명령어이다.</p>
  </li>
  <li>그러면 <code class="language-plaintext highlighter-rouge">theme</code> 부분이 주석 처리되고, <code class="language-plaintext highlighter-rouge">remote_theme</code> 부분이 주석 해제된 채로 (즉 git push하는 상태) 터미널에 <code class="language-plaintext highlighter-rouge">bundle exec jekyll serve</code>를 입력하면 어떻게 될까? 아래처럼 에러가 발생하여 서버에 페이지를 띄울 수가 없다.</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog6/gitblog6_1.png" alt="사진1" /></p>

<ul>
  <li>그러면 반대로 <code class="language-plaintext highlighter-rouge">theme</code> 부분이 주석 해제되고, <code class="language-plaintext highlighter-rouge">remote_theme</code> 부분이 주석 처리된 채로 (즉 로컬 서버에 띄울 수 있는 상태)에서 git push를 하면 어떻게 될까? 아래처럼 github에 에러가 발생한다.</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog6/gitblog6_2.png" alt="사진2" /></p>

<ul>
  <li>어떤 에러인지 들어가보면 아래 사진처럼 <code class="language-plaintext highlighter-rouge">Build with Jekyll</code>에 에러가 발생했고, 자세히 보면 테마를 찾을 수 없다고 한다.</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog6/gitblog6_3.png" alt="사진3" />
<img src="/assets/img/gitblog/gitblog6/gitblog6_4.png" alt="사진4" /></p>

<ul>
  <li>그러므로 이러한 에러가 발생한다면 <code class="language-plaintext highlighter-rouge">_config.yml</code> 파일을 확인해보자.</li>
</ul>

<h2 id="2-게시글-파일-이름-에러">2. 게시글 파일 이름 에러</h2>

<p><img src="/assets/img/gitblog/gitblog3/gitblog3_3.png" alt="사진5" /></p>

<ul>
  <li>나는 원래 위 사진처럼 게시글 파일 이름을 <code class="language-plaintext highlighter-rouge">2024-02-12-깃블로그 만들기 4 : 새로운 게시글 작성하기.md </code>처럼 작성일자와 제목으로 관리해왔다. 이렇게 블로그에 표시될 게시글 제목과 게시글 파일 이름을 동일하게 하면 내가 로컬에서 관리하기 쉽기 때문이다. 하지만 역시 git push 했을 때 페이지가 정상적으로 뜨지 않는 에러가 발생했다. 통상적으로 파일 이름에 한글과 띄워쓰기를 쓰지 않는다는 점에서 혹시나 하는 마음에 아래 사진처럼 게시글 파일 이름을 모두 변경하였더니, 정상적으로 페이지가 떴다.</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog6/gitblog6_5.png" alt="사진6" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="gitblog" /><summary type="html"><![CDATA[이번에는 Hydejack 테마를 사용하면서 내가 자주 했던 실수들을 살펴보겠다. 처음에는 에러의 이유를 몰라서 gitblog repository를 몇 번이나 삭제하고 다시 만들면서 시간을 많이 썼는데, 앞으로는 그러지 않기 위해 남긴다.]]></summary></entry><entry><title type="html">(ModernTCN) A Modern Pure Convolution Structure for General Time Series Analysis</title><link href="http://localhost:4000/timeseries/2024-02-14-ModernTCN/" rel="alternate" type="text/html" title="(ModernTCN) A Modern Pure Convolution Structure for General Time Series Analysis" /><published>2024-02-14T00:00:00+09:00</published><updated>2024-02-23T02:34:43+09:00</updated><id>http://localhost:4000/timeseries/ModernTCN</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-14-ModernTCN/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>최근 Transformer-based 모델과 MLP-based 모델이 time series에서 우위에 있지만, 본 논문에서는 convolution을 time series에서 사용하는 모델 ModernTCN을 제안한다.</li>
  <li>Time series의 5개 mainstream task (long-term and short-term forecasting, imputation, classification and anomaly detection)에서 SOTA</li>
  <li>Convolution의 sharing params로 효율적이면서도 넓은 receptive fields를 가진다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Transformer-based 모델과 MLP-based 모델이 우위에 있는 이유는 global한 efective receptive fields (ERFs)가 cross-time dependency를 파악하기 때문이다.</li>
  <li>하지만 지금까지 convolution in time series 연구들은 그저 모델의 구조를 복잡하게 해왔고, 이와 다르게 본 논문에서는 convolution 자체를 업데이트해서 ERF를 키운다.</li>
  <li>왜냐하면 CV에서는 이미 Transformer을 보고 convolution을 optimizing하려고 시도하고 있기 때문이다.</li>
  <li>시계열에서 convolution을 쓴다는 것은 cross-time and cross-variable dependency를 포착하겠다는 의미이다.</li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>
<ul>
  <li>MICN(2023), SCINet(2023) 등 최근까지도 convolution을 time series에 활용하려는 시도가 많았지만 long-term dependency가 중요한 time series에서 limited ERFs는 transformer를 이길 수가 없었다.</li>
  <li>CNN(2017) 이후 ViTs(2020)이 등장했고, ViTs를 따라잡기 위해 다양한 modern convolution이 등장하는데, 예를 들면 conv block을 transformer와 비슷하게 하거나 (ConvNeXt, 2022), kernel size를 51 \(\times\) 51로 늘려버리기도 했다. (SLaK, 2022)</li>
  <li>본 논문에서는 conv를 time series에 쓰기 위해 1D conv를 수정한 ModernTCN을 제안한다.</li>
</ul>

<h2 id="3-moderntcn">3. ModernTCN</h2>

<h3 id="31-modernize-the-1d-convolution-block">3.1. Modernize the 1D Convolution block</h3>
<p><img src="/assets/img/timeseries/modernTCN/fig2.jpeg" alt="사진1" /></p>

<ul>
  <li>1D conv를 Figure-2:(b)처럼 DWConv(depth-wise)와 ConvFFN(feed-forward NN)으로 re-design하였다.
    <ul>
      <li>DWconv는 transformer의 self-attention와 같은 역할 : learning the temporal information among tokens on a <strong>per-feature</strong> basis</li>
      <li>ConvFFN은 transformer의 FFN과 같은 역할 : learn the new feature representation of each token <strong>independently</strong></li>
    </ul>
  </li>
  <li>위 디자인은 temporal and feature information을 분리한다. 이것이 jointly mix하던 tranditional conv와의 차이점이다.</li>
  <li>하지만 multivariate time series에서는 cross-variable information도 중요하니 추가적인 수정이 필요하긴 하다.</li>
</ul>

<h3 id="32-time-series-related-modifications">3.2. Time series related Modifications</h3>
<ul>
  <li>CV에서는 RGB \(\to\) D-dim embedding 하지만, 그대로 특정 t시점에서 M개 변수 \(\to\) D-dim embedding하면 안된다.
    <ul>
      <li>RGB차이보다 t시점에서 M개 변수 사이의 차이가 더 크고, cross-variable dependency를 반영 못하기 때문</li>
    </ul>
  </li>
  <li>그래서 아래와 같은 방식으로 patchify embedding을 거친다.
    <ul>
      <li>1) \(X_{in} \in \mathbb R^{M\times L}\)을 \(X_{in} \in \mathbb R^{M\times 1\times L}\)로 unsqueeze</li>
      <li>2) \(X_{in}\) 뒤에 \(P-S\)만큼 패딩 (\(P\)는 patch size, \(S\)는 stride)</li>
      <li>3) 1D conv를 통과, 각 patch는 D차원으로 embedding</li>
      <li>그림으로 표현하면 아래와 같다.
<img src="/assets/img/timeseries/modernTCN/myfig1.jpeg" alt="사진2" /></li>
      <li>예시로 이해해보자. patch size가 10이고 stride가 2이므로 총 50개의 patch를 보게 되므로 N=50이 된다. 
<img src="/assets/img/timeseries/modernTCN/myfig2.png" alt="사진3" /></li>
    </ul>
  </li>
  <li>DWConv는 feature와 variable 모두에 대해 independent하게, 그리고 kernel을 크게 해서 ERFs를 넓게 가져가 temporal information을 포착하도록 했다.</li>
  <li>ConvFFN은 information across feature and variable을 섞는 역할을 해야 하는데, 연산의 효율을 위해 jointly하게 학습하기보다는 두 개의 ConvFFN으로 decople했다.
    <ul>
      <li>ConvFFN 1 : learning the new feature representations per variable</li>
      <li>ConvFFN 2 : learning the cross-variable dependency per feature</li>
    </ul>
  </li>
</ul>

<h3 id="33-overall-structure">3.3 Overall Structure</h3>
<ul>
  <li>\(\mathbf{Z}=\operatorname{Backbone}(\mathbf X_{emb})\), Backbone(\(\cdot\))은 ModernTCN을 쌓아서 만든 구조이다.</li>
  <li>즉 다음과 같이 표현할 수 있다.</li>
  <li>\(\mathbf{Z}_{i+1}=\operatorname{Block}\left(\mathbf{Z}_i\right)+\mathbf{Z}_i\) , 즉 \(\mathbf{Z}_i= \begin{cases}\mathbf{X}_{e m b} &amp; , i=1 \\ \operatorname{Block}\left(\mathbf{Z}_{i-1}\right)+\mathbf{Z}_{i-1} &amp; , i&gt;1\end{cases}\), 이 때 Blcok(\(\cdot\))은 ModernTCN block이다.</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<p><img src="/assets/img/timeseries/modernTCN/fig3.jpeg" alt="사진4" /></p>
<ul>
  <li>Time series의 5가지 mainstream analysis task에서 performance, efficiency 측면에서 SOTA를 달성했다.</li>
</ul>

<h2 id="5-model-analysis">5. Model Analysis</h2>
<ul>
  <li>Performance, efficiency 측면에서 ModernTCN은 SOTA를 달성했다.</li>
  <li>MLP-based time series model TimesNet(2023)도 ModernTCN만큼 성능이 좋은데, 그 이유가 두 모델 모두 CV 분야에서 convolution을 사용하는 아이디어에서 영감을 얻었기 때문이다.</li>
  <li>다만, TimesNet은 conv를 사용하기 위해 1D time series를 2D 공간으로 보낸거고, ModernTCN은 conv 자체를 1D time series에 사용할 수 있도록 modernize했기 때문에 training speed가 빠르다.</li>
</ul>

<h2 id="6-conclusion-and-future-work">6. Conclusion and Future Work</h2>
<ul>
  <li>본 논문의 contribution은 단순히 conv-based model로 transformer-based 모델보다 좋은 성능을 냈다 정도가 아니라, 다시 한 번 time series에 다양한 conv-based models가 등장할 수 있음을 의미한다는 것이다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[ICLR 2024]]></summary></entry><entry><title type="html">(FTS-Diffusion) Generative Learning for Financial Time Series with Irregular and Scale-invariant Patterns</title><link href="http://localhost:4000/timeseries/2024-02-13-FTS-Diffusion/" rel="alternate" type="text/html" title="(FTS-Diffusion) Generative Learning for Financial Time Series with Irregular and Scale-invariant Patterns" /><published>2024-02-13T00:00:00+09:00</published><updated>2024-02-15T16:47:11+09:00</updated><id>http://localhost:4000/timeseries/FTS-Diffusion</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-13-FTS-Diffusion/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Financial deep learning 모델을 훈련시키기 위한 데이터가 부족한데, 그렇다고 synthetic data를 만들어내려 하니 irregular and scale-invariant patterns 때문에 어려움이 있음</li>
  <li>패턴이 irregular하다는 말은 패턴이 발생하는 간격이 일정하지 않아서 예측하기 어렵다는 것</li>
  <li>패턴이 scale-invariant하다는 말은 scale을 변화시켜도 형태가 유지된다는 말인데, 특정 패턴이 다양한 너비(폭)나 높이(진폭)로 나타날 수 있다는 말이다. 또한 프랙탈 구조처럼 축소해도 비슷한 패턴이 보이게 된다.</li>
  <li>본 논문에서는 irregular and scale-invariant patterns을 학습하고 생성하는 모델 FTS-Diffusion을 제시한다.</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/fig1.jpeg" alt="사진1" />
<img src="/assets/img/timeseries/fts-diff/fig2.jpeg" alt="사진2" /></p>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>FTS-Diffusion은 3가지의 modules로 구성된다.</li>
  <li>Pattern recognition - Pattern generation - Pattern evolution</li>
  <li>패턴을 인식하고, 패턴을 생성한 뒤, 패턴을 이어붙여서 하나의 time series를 만든다는 것이다. 기존 time series generation 모델들이 어려워하던 irregular and scale-invariant 패턴을 모델링할 수 있다.</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>
<ul>
  <li>본 논문에서 제시하는 모델은 time series를 생성하는 모델이다. 생성 모델은 크게 VAE 계열, GAN 계열, Diffusion 계열이 있다.</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/gm.jpeg" alt="사진3" /></p>

<ul>
  <li>일반적으로 좋은 생성 모델을 3가지 특성으로 정의하는데, 세 가지 모두 뛰어난 모델은 없고 상대적인 장단점이 존재한다.</li>
  <li>Diffusion이 속도는 상대적으로 느리지만 높은 퀄리티와 다양성 측면에서 뛰어나 많은 주목 받았고, 본 논문에서도 diffusion을 사용한다.</li>
</ul>

<h2 id="3-problem-statement">3. Problem Statement</h2>
<ul>
  <li>시계열 \(X=\{x_1,...,x_M\}\)은 \(M\)개의 segments로 이루어지고 \(x_m=\{x_{m,1},...,x_{m,t_m}\}\) 각 segment의 길이는 \(t_m\)</li>
  <li>conditional distribution \(f(\cdot\mid p,\alpha,\beta)\)에서 샘플링을 하는 것이고, \(p\)는 패턴, \(\alpha\)는 duration, \(\beta\)는 magnitude이다.</li>
  <li>tuple \((p,\alpha,\beta)\)는 하나의 state이고, 패턴끼리의 dynamic across를 모델링하기 위해 Markov chain을 사용한다. 즉 transition probability \(Q(p_j,\alpha_j,\beta_j \mid p_i,\alpha_i,\beta_i)\)를 통해 time series를 생성한다.</li>
  <li>이제 FTS-Diffusion의 각 모듈을 다시 살펴보면 아래와 같다.
    <ul>
      <li>Pattern recognition : 패턴 \(p\)를 인식하고 반복되는 패턴의 구조 \(\cal P\) 학습</li>
      <li>Pattern Generation : conditional distribution \(f(\cdot\mid p,\alpha,\beta), \forall p \in \cal P\) 학습</li>
      <li>Pattern Evolution : pattern transition probability \(Q(p_j,\alpha_j,\beta_j \mid p_i,\alpha_i,\beta_i)\) 학습</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/fig3.jpeg" alt="사진4" /></p>

<h2 id="4-framework">4. Framework</h2>

<h3 id="1-pattern-recognition">(1) Pattern recognition</h3>
<ul>
  <li>전체 time series를 여러 개의 segments로 나누고 비슷한 segments끼리 묶어 K개의 clusters를 만드는 알고리즘 (Scale-Invariant Subsequence Clustering, SISC)</li>
  <li>SISC는 각 segment의 length는, 가장 가까운 centriod와의 거리가 최소가 되는 segment length로 결정한다.</li>
  <li>이 때 거리 metric은 일반적으로 사용하는 euclidean이 아니라 length나 magnitude에 구애받지 않는 dynamic time wraping (DTW)를 사용하였다.</li>
  <li>centroid initialization은 처음 1개만 랜덤하게 고른 뒤 먼 segment일수록 다음 centroid가 될 확률이 높도록 하였다. (k-Center-Greedy와 비슷)</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/fig4.jpeg" alt="사진5" /></p>

<h3 id="2-pattern-generation">(2) Pattern generation</h3>
<ul>
  <li>패턴에 gaussian noise를 씌우고 denoising gradient를 학습하는 DDPM의 방식을 사용하여 패턴을 생성하였다.</li>
  <li>Diffusion으로 생성된 패턴을 (scaling) autoencoder에 통과시켜 원하는 length로 transform한다.</li>
  <li>Objective를 아래 식으로 사용하여 diffusion 모델과 autoencoder를 같이 학습시킨다.
\(\mathcal{L}(\theta)=\mathbb{E}_{\boldsymbol{x}_m}\left[\left\|\boldsymbol{x}_m-\hat{\boldsymbol{x}}_m\right\|_2^2\right]+\mathbb{E}_{\boldsymbol{x}_m^0, i, \epsilon}\left[\left\|\epsilon^i-\epsilon_\theta\left(\boldsymbol{x}_m^{i+1}, i, \boldsymbol{p}\right)\right\|_2^2\right]\)</li>
</ul>

<h3 id="3-pattern-generation">(3) Pattern generation</h3>
<ul>
  <li>Pattern evolution network \(\phi\)는 현재 state가 주어졌을 때 다음 state에 올 패턴들의 확률을 학습한다.
\((\hat p_{m+1}, \hat \alpha_{m+1}, \hat \beta_{m+1}) = \phi(p_m, \alpha_m, \beta_m)\)</li>
  <li>Pattern evolution objective는 아래와 같다.
\(\mathcal{L}(\phi)=\mathbb{E}_{\boldsymbol{x}_m}\left[\ell_{C E}\left(p_{m+1}, \hat{p}_{m+1}\right)+\left\|\alpha_{m+1}-\hat{\alpha}_{m+1}\right\|_2^2+\left\|\beta_{m+1}-\hat{\beta}_{m+1}\right\|_2^2\right]\)</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<ul>
  <li>S&amp;P500, GOOG, ZC=F(옥수수 선물) 데이터를 활용하였고, 자산 가격은 non-stationary random walk를 따른다고 알려져있으므로, 통계적 특성을 가지는 수익률(return)을 사용하였다.</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/table1.jpeg" alt="사진6" /></p>

<ul>
  <li>위 결과는 실제 return의 분포와 synthesized 분포의 적합도를 테스트하는 KS test와 AD test 결과이다.</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/fig6.jpeg" alt="사진7" /></p>

<ul>
  <li>Mixed data(생성된 synthesized data + 실제 observed data)로 training을 하고 real data로 test를 했을 때에도(TMTR, TATR), mixed data의 비율이 달라졌을 때에도 예측 성능이 일정하다는 것으로부터 FTS-Diffusion으로 생성한 synthesized data가 observed data와 유사하다고 볼 수 있다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>Pattern recognition : SISC designed to identify patterns</li>
  <li>Pattern generation : diffusion-based network to synthesize the segments of patterns</li>
  <li>Pattern evolution : assemble generated segments with proper temporal evoution</li>
</ul>

<h2 id="implementation">Implementation</h2>
<ul>
  <li>under review</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[ICLR 2024]]></summary></entry></feed>