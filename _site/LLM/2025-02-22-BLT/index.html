<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v9.1.6 <https://hydejack.com/>
-->







<head>
  






  
    
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Byte Latent Transformer: Patches Scale Better Than Tokens (Arxiv 2024) | LpppJ</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Byte Latent Transformer: Patches Scale Better Than Tokens (Arxiv 2024)" />
<meta name="author" content="GW Jeong" />
<meta property="og:locale" content="en" />
<meta name="description" content="Arxiv 2024" />
<meta property="og:description" content="Arxiv 2024" />
<link rel="canonical" href="http://localhost:4000/llm/2025-02-22-BLT/" />
<meta property="og:url" content="http://localhost:4000/llm/2025-02-22-BLT/" />
<meta property="og:site_name" content="LpppJ" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-22T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Byte Latent Transformer: Patches Scale Better Than Tokens (Arxiv 2024)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"GW Jeong"},"dateModified":"2025-02-23T20:28:58+09:00","datePublished":"2025-02-22T00:00:00+09:00","description":"Arxiv 2024","headline":"Byte Latent Transformer: Patches Scale Better Than Tokens (Arxiv 2024)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/llm/2025-02-22-BLT/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/me/logo.jpg"},"name":"GW Jeong"},"url":"http://localhost:4000/llm/2025-02-22-BLT/"}</script>
<!-- End Jekyll SEO tag -->


  

  



  <meta name="theme-color" content="rgb(230, 217, 195)">


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="LpppJ">
<meta name="apple-mobile-web-app-status-bar-style" content="default">

<meta name="application-name" content="LpppJ">

<meta name="generator" content="Hydejack v9.1.6" />


<link rel="alternate" href="http://localhost:4000/llm/2025-02-22-BLT/" hreflang="en">

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="LpppJ" />


<link rel="shortcut icon"    href="/assets/icons/favicon.ico">
<link rel="apple-touch-icon" href="/assets/icons/icon-192x192.png">

<link rel="manifest" href="/assets/site.webmanifest">

<link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com">



<link rel="preload" href="/assets/img/swipe.svg" as="image" id="_hrefSwipeSVG">






<script>!function(r,c){"use strict";function a(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}r.loadJS=function(e,t){var n=c.createElement("script"),e=(n.src=e,t&&a(n,"load",t,{once:!0}),c.scripts[0]);return e.parentNode.insertBefore(n,e),n},r._loaded=!1,r.loadJSDeferred=function(e,t){var n=c.createElement("script");function o(){r._loaded=!0,t&&a(n,"load",t,{once:!0});var e=c.scripts[0];e.parentNode.insertBefore(n,e)}return n.src=e,r._loaded?o():a(r,"load",o,{once:!0}),n},r.setRel=r.setRelStylesheet=function(e){a(c.getElementById(e),"load",function(){this.rel="stylesheet"},{once:!0})}}(window,document);
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
!function(w) {
  w._baseURL = '/';
  w._publicPath = '/assets/js/';
  w._noPushState = false;
  w._noDrawer = false;
  w._noNavbar = false;
  w._noToc = false;
  w._noSearch = false;
  w._search = {
    DATA_URL: '/assets/sitedata.json?no-cache',
    STORAGE_KEY: 'mini-search/',
    INDEX_KEY: 'index--2025-02-23T20:29:01+09:00',
  };
  w._clapButton = true;
}(window);</script>


<script async src="/assets/bower_components/MathJax/es5/tex-mml-chtml.js" id="_MathJax"></script>


<!--[if gt IE 8]><!---->

  




<link rel="stylesheet" href="/assets/css/hydejack-9.1.6.css" id="_stylePreload">
<link rel="stylesheet" href="/assets/icomoon/style.css" id="_iconsPreload">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap" id="_fontsPreload">



  <style id="_pageStyle">

html{--accent-color: rgb(94, 97, 94);--accent-color-faded: rgba(94, 97, 94, 0.5);--accent-color-highlight: rgba(94, 97, 94, 0.1);--accent-color-darkened: #4b4e4b;--theme-color: rgb(230, 217, 195)}
</style>


<!--<![endif]-->




</head>

<body class="no-break-layout">
  


<hy-push-state
  id="_pushState"
  replace-selector="#_main"
  link-selector="a[href]:not([href^='/assets/']):not(.external):not(.no-push-state)"
  script-selector="script"
  duration="500"
  hashchange
>
  
  
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <div class="nav-btn-bar">
      <a id="_menu" class="nav-btn no-hover" href="#_drawer--opened">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <div class="nav-span"></div>
    </div>
  </div>
</div>
<hr class="sr-only" hidden />

  <main
  id="_main"
  class="content layout-post"
  role="main"
>
  <nav id="breadcrumbs" class="screen-only"><ul>
  
  
    <li><a href="/">home</a></li>
    
      <li>
        
          <span>/</span>
          
          
          <a href="/llm/">llm</a>
        
      </li>
    
      <li>
        
          <span>/</span>
          <span>2025-02-22-BLT</span>
        
      </li>
    
  
</ul></nav>
  










<article id="post-llm-BLT" class="page post mb6" role="article">
  <header>
    <h1 class="post-title flip-project-title">
      
        Byte Latent Transformer: Patches Scale Better Than Tokens (Arxiv 2024)
      
    </h1>

    <div class="post-date">
      
      <span class="ellipsis mr1">
        <time datetime="2025-02-22T00:00:00+09:00">22 Feb 2025</time> in <span>Llm</span> 
      </span>
      
    </div>

    
    

    



  
    <p class="note-sm" >
      <a href="https://arxiv.org/pdf/2412.09871">Arxiv 2024</a>

    </p>
  


  </header>

  
    <h2 id="abstract">Abstract</h2>

<ul>
  <li>Byte Latent Transformer (BLT)
    <ul>
      <li><strong>new byte-level LLM architecture</strong></li>
      <li>tokenizer-free</li>
    </ul>
  </li>
  <li>BLT encodes bytes into <strong>dynamically sized patches</strong> (primary units of computation)</li>
  <li>Patches are segmented based on the <strong>entropy of the next byte</strong>
    <ul>
      <li>allocating more compute and model capacity where increased data complexity demands it</li>
    </ul>
  </li>
  <li>Feasibility of scaling models trained on raw bytes without a fixed vocabulary
    <ul>
      <li>By dynamically selecting long patches when data is predictable</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>기존 LLM : end-to-end train… but <strong>except for tokenization !</strong>
    <ul>
      <li>groups bytes into a <strong>static</strong> set of tokens (heuristic)</li>
      <li>\(\to\) bias how a string is compressed</li>
      <li>\(\to\) shortcomings such as
        <ul>
          <li>domain/modality sensitivity</li>
          <li>sensitivity to input noise</li>
          <li>a lack of orthographic knowledge</li>
          <li>and multilingual inequity</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>LLMs에서 long sequence 다루려면 Tokenization은 필수
    <ul>
      <li>training on bytes : costly at scale</li>
      <li>그래서 self-attention or attention-free architectures
        <ul>
          <li>But small models일때나 말이 됨</li>
          <li>왜냐면 Transformer로 효율적으로 하려고 해봐야 어차피  Scale 관점에서 <strong>large FFN on every byte</strong>가 문제</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>본 논문에서 dynamic, learnable method for <strong>grouping bytes into patches</strong> 제안
    <ul>
      <li>mixes byte and patch information</li>
      <li>tokenizaiton과 다르게 no fixed vocabulary for patches</li>
      <li>Arbitrary groups of bytes are mapped to latent patch representations !</li>
    </ul>
  </li>
  <li>기존 Tokenization-based LLMs allocate the <strong>same</strong> amount of compute to <strong>every</strong> token.
    <ul>
      <li>\(\to\) trades efficiency for performance 불가피</li>
      <li>왜냐면 예측의 complexity랑 token이랑 not always correlated</li>
    </ul>
  </li>
  <li>그러니 compute를 필요한 곳에 할당하자
    <ul>
      <li>ending of most words 예측은 쉬워서 large transformer 필요 X (low-entropy)</li>
    </ul>
  </li>
  <li>이 생각을 반영한 BLT:
    <ul>
      <li>3 transformer blocks: <strong>two small byte-level local</strong> models and <strong>a large global latent transformer</strong></li>
      <li>how to dynamically allocate compute = how to group bytes into patches</li>
      <li>BLT segments data based on the <strong>entropy of the next-byte prediction</strong></li>
    </ul>
  </li>
</ul>

<h2 id="2-patching-from-individual-bytes-to-groups-of-bytes">2. Patching: From Individual Bytes to Groups of Bytes</h2>

<p><img src="/assets/img/LLM/BLT/fig3.png" alt="그림1" /></p>

<ul>
  <li>Patching : segmenting the sequence of byte \(\boldsymbol{x}=\left\{x_i, \mid i=1, \ldots n\right\}\) into \(p=\left\{p_j \mid j=1, \ldots, m\right\}, m&lt;n\)
    <ul>
      <li>by mapping each \(x_i\) to the set \(\{0,1\}\) where 1 indicates the start of a new patch</li>
      <li>the computational cost = the number of main Transformer execution
        <ul>
          <li>BLT에서는 = the number of patches !</li>
          <li>따라서 average size of a patch, or simply patch size 매우 중요함</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>그러므로 여기서는 patching functions을 다룸
    <ul>
      <li>patching with a fixed number of bytes per patch</li>
      <li>whitespace patching</li>
      <li>dynamically patching with entropies from a small byte LM</li>
    </ul>
  </li>
</ul>

<h3 id="21-strided-patching-every-k-bytes">2.1. Strided Patching Every K Bytes</h3>

<ul>
  <li>일반적으로는 bytes into patches of fixed size \(k\) as done in MegaByte
    <ul>
      <li>changing the average patch size 쉽고 control the FLOP cost 쉬움</li>
      <li>But ! compute is not dynamically allocated
        <ul>
          <li>공백(whitespace) 예측하려고 Transformer 쓴다거나</li>
          <li>byte가 많은 math에 compute를 안쓴다거나…</li>
        </ul>
      </li>
      <li>그러다보니 inconsistent and non-contextual patching of similar byte sequences (같은 걸 다르게 split)</li>
    </ul>
  </li>
</ul>

<h3 id="22-space-patching">2.2. Space Patching</h3>

<ul>
  <li>creates new patches after any space-like bytes (= 공백 이후에 patch 시작)</li>
  <li>natural boundaries for linguistic units in many languages (=언어의 관점에서 자연스러운 접근)</li>
  <li>words are patched in the same way across sequences
    <ul>
      <li>FLOPs are allocated for hard predictions which often follow spaces (공백 바로 뒤)</li>
      <li>ex. “Who composed the Magic Flute?”에 답하기 위해서 바로 맞추라고 하면 어렵지만 M을 주면 Mozart 맞추기 쉬워짐 !</li>
    </ul>
  </li>
  <li>But ! cannot gracefully handle all languages and domains
    <ul>
      <li>무엇보다도 cannot vary thepatch size</li>
    </ul>
  </li>
</ul>

<h3 id="23-entropy-patching-using-next-byte-entropies-from-a-small-byte-lm">2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM</h3>

<ul>
  <li>2.2 같은 rule-based heuristic such as whitespace 말고 data-driven approach 원함
    <ul>
      <li>identify high uncertainty next-byte predictions, 즉 entropy patching</li>
    </ul>
  </li>
  <li>먼저 a small byte-level auto-regressive language model 하나 학습시키고
    <ul>
      <li>그 다음 compute next byte entropies under the LM distribution \(p_e\)  over the byte vocabulary \(\mathcal{V} :\)</li>
      <li>\(H\left(x_i\right)=\sum_{v \in \mathcal{V}} p_e\left(x_i=v \mid x_{&lt;i}\right) \log p_e\left(x_i=v \mid \boldsymbol{x}_{&lt;i}\right)\).</li>
      <li>Entropy가 주어졌을 때 patch boundaries는 어떻게 찾냐
        <ul>
          <li>첫째, entropy가 그냥 threshold보다 높은 지점</li>
          <li>둘째, 갑자기 entropy가 커진 지점</li>
          <li>i.e., \(\begin{aligned}\text{Global Constraint} \quad H\left(x_t\right)&gt;\theta_g$ \\
\text{Approx. Monotonic Constraint} \quad H\left(x_t\right)-H\left(x_{t-1}\right)&gt;\theta_r\end{aligned}\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="24-the-byte-pair-encoding-bpe-tokenizer-and-incremental-patching">2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching</h3>

<ul>
  <li>대부분의 LLMs (Llama 3 포함) use a subword tokenizer like BPE</li>
  <li>본 논문에서는
    <ul>
      <li>“<strong>tokens</strong>”: byte-groups drawn from a finite vocabulary determined prior totraining</li>
      <li>“<strong>patches</strong>”: dynamically grouped sequences without a fixed vocabulary</li>
      <li>둘의 차이는 with tokens, the model has no direct access to theunderlying byte features</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/LLM/BLT/fig4.png" alt="그림1" /></p>

<ul>
  <li>BLT는 the trade off between the vocabulary <strong>size</strong> and <strong>compute</strong>를 re-define함
    <ul>
      <li>원래는 vocabulary size 커짐 = larger tokens = larger final projection layer</li>
      <li>Llama 3도 Llama 2에 비해 average token size를 3.7에서 4.4 bytes로 늘리면서 embedding table size는 4배 됨</li>
    </ul>
  </li>
  <li>BLT는 결국 byte sequence 내에서 지금이 patch boundary인가 아닌지 판단하면 되고
    <ul>
      <li>즉 = Latent Transformer로 more compute 할지말지 판단</li>
      <li>이건 아직 생성 안된 rest of sequence랑 독립</li>
      <li>즉 incremental patching \(f_p\left(x_{&lt;i}\right)=f_p(\boldsymbol{x})_{&lt;i}\)이 아님</li>
      <li>즉 같은 prefix라도 다르게 tokenize 가능</li>
    </ul>
  </li>
</ul>

<h2 id="3-blt-architecture">3. BLT Architecture</h2>

<p><img src="/assets/img/LLM/BLT/fig2.png" alt="그림1" /></p>

<ul>
  <li>BLT의 구성은:
    <ul>
      <li><strong>A large</strong> global autoregressive language model
        <ul>
          <li>that operates on patch representations,</li>
        </ul>
      </li>
      <li><strong>Two</strong> smaller local models
        <ul>
          <li>encode sequences of bytes into patches</li>
          <li>decode patch representations back into bytes</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="31-latent-global-transformer-model">3.1 Latent Global Transformer Model</h3>

<ul>
  <li>The Latent Global Transformer is an autoregressive transformer model \(\mathcal{G}\) with \(l_{\mathcal{G}}\) layers
    <ul>
      <li>sequence of latent input patch representations \(p_j\) \(\to\) sequence of output patch representations \(o_j\)로 변환</li>
    </ul>
  </li>
  <li>block-causal attention mask 사용
    <ul>
      <li>현재 문서 내 현재 배치까지 범위에서만 attention</li>
      <li>pre-training, inference에서 FLOP 대부분이라 언제 실행할지가 complexity 결정</li>
    </ul>
  </li>
</ul>

<h3 id="32-local-encoder">3.2. Local Encoder</h3>

<p><img src="/assets/img/LLM/BLT/fig5.png" alt="그림1" /></p>

<ul>
  <li>The Local Encoder Model, denoted by \(\mathcal{E}\)
    <ul>
      <li>lightweight transformer-based model with layers \(l_{\mathcal E} &lt;&lt; l_{\mathcal G}\)</li>
      <li>input bytes \(b_i\) \(\to\) patch representation \(p_j\)</li>
      <li>Transformer랑 다른 건 cross-attention layer after each transformer layer (pool byte representations into patch representations 하는 부분) - 3.2.2에서 이해됨</li>
      <li>input sequence of bytes, \(b_i\), are embedded using a \(\mathbb{R}^{256 \times h_{\mathcal E}}\) matrix, denoted as \(x_i\)</li>
      <li>hash-embeddings는 optional</li>
    </ul>
  </li>
  <li>즉 alternating transformer and cross-attention layers가 input bytes \(b_i\)의 representation을 \(\to\) patch representation \(p_j\)</li>
  <li>local block causal attention mask; 사용해서 each byte attends to a fixed window of \(\mathcal w_{\mathcal{E}}\)
    <ul>
      <li>다른 patch 볼 수 있지만 현재 문서 내에서만 !</li>
    </ul>
  </li>
</ul>

<h3 id="321-encoder-hash-n-gram-embeddings">3.2.1. Encoder Hash n-gram Embeddings</h3>

<ul>
  <li>결국 to incorporate information about the preceding bytes가 중요</li>
  <li>byte \(b_i\)를 as individual로, 그리고 as part of a byte n-gram으로 둘다 봄
    <ul>
      <li>Byte-gram: \(g_{i, n}=\left\{b_{i-n+1}, \ldots, b_i\right\}\) (with a fixed size for each size \(n \in\{3,4,5,6,7,8\}\))</li>
      <li>그 다음 individual embedding에 다 더함. i.e.,
        <ul>
          <li>\(e_i  =x_i+\sum_{n=3, \ldots, 8} E_n^{\text {hash }}\left(\operatorname{Hash}\left(g_{i, n}\right)\right)\).</li>
          <li>where \(\operatorname{Hash}\left(g_{i, n}\right)=\operatorname{RollPolyHash}\left(g_{i, n}\right) \% \mid E_n^{\text {hash }}\mid\)</li>
        </ul>
      </li>
      <li>그 다음에 normalizing !</li>
    </ul>
  </li>
</ul>

<h3 id="322-encoder-multi-headed-cross-attention">3.2.2 Encoder Multi-Headed Cross-Attention</h3>

<ul>
  <li>이제 진짜 bytes-sequence를 patch-sequence로 바꿔보자</li>
  <li>Perceiver의 입력 cross-attention과 비슷하지만 patch 크기가 동적 !</li>
  <li>Patch represenation은 관련된 byte만으로 학습됨
    <ul>
      <li>initialization은 byte representation에 pooling</li>
      <li>Cross-Attention을 통해 Patch represenation이 풍부한 맥락을 반영</li>
      <li>\(\begin{aligned} P_{0, j} &amp; =\mathcal{E}_C\left(f_{\text {bytes }}\left(\left(p_j\right)\right), f \text { is a pooling function }\right. \\ P_l &amp; =P_{l-1}+W_o\left(\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\right) \\ \text { where } Q_j &amp; =W_q\left(P_{l-1, j}\right), K_i=W_k\left(h_{l-1, i}\right), V_i=W_v\left(h_{l-1, i}\right) \\ h_l &amp; =\text { Encoder-Transformer-Layer }\left(h_{l-1}\right)\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h3 id="33-local-decoder">3.3. Local Decoder</h3>

<ul>
  <li>Local encoder와 비슷하게 lightweight transformer-based model with \(l_{\mathcal D} &lt;&lt; l_{\mathcal G}\)</li>
  <li>patch representation \(o_j\) \(\to\) raw bytes \(y_i\)</li>
  <li>이번에는 Cross-Attention으로 byte representation으로 변환
    <ul>
      <li>\(\begin{aligned}
&amp; D_0=h_{l_{\varepsilon}} \\
&amp; B_l=D_{l-1}+W_o\left(\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\right) \text {, } \\
&amp; \text { where } Q_i=W_q\left(d_{l-1, i}\right), K_i=W_k\left(\mathcal{D}_C\left(o_j\right)\right), V_i=W_v\left(\mathcal{D}_C\left(o_j\right)\right) \\
&amp; D_l=\text { Decoder-Transformer-layer }{ }_l\left(B_l\right)
\end{aligned}\).</li>
      <li>Encoder는 byte가 K/V, patch가 Q / Decoder는 patch가 K/V, byte가 Q</li>
    </ul>
  </li>
</ul>

<h2 id="4-experimental-setup">4. Experimental Setup</h2>

<h3 id="41-pre-training-datasets">4.1. Pre-training Datasets</h3>

<ul>
  <li>LLaMA 2 (Touvron et al., 2023)으로 BLT model scaling law experiments</li>
  <li>BLT-1Tdㅡ로 Llama3와 비교</li>
</ul>

<h3 id="42-entropy-model">4.2. Entropy Model</h3>

<ul>
  <li>BLT의 Entropy-Based Patching을 위한 language model
    <ul>
      <li>Byte-level auto-regressive transformer 사용</li>
      <li>trained on the same training distribution as the full BLT model</li>
    </ul>
  </li>
</ul>

<h3 id="43-entropy-threshold-and-equalizing-context-length">4.3. Entropy Threshold and Equalizing Context Length</h3>

<ul>
  <li>공정한 비교를 위해 patch size가 동적으로 변하더라도 same average context length i.e.,
    <ul>
      <li>the number of bytes ineach <strong>batch</strong> remains constant <strong>in expectation</strong></li>
      <li>모든 배치가 동일한 패치 개수를 가지도록 하여 large patch size로 인한 memory spikes 방지</li>
    </ul>
  </li>
</ul>

<h3 id="44-entropy-model-context">4.4. Entropy Model Context</h3>

<p><img src="/assets/img/LLM/BLT/fig9.png" alt="그림1" /></p>

<ul>
  <li>entropy patching yields progressively <strong>larger patches</strong> in structured content like multiple choice tasks</li>
  <li>반복되다보니 entropy가 낮아지는 것 (lower entropy on the repeated content)</li>
  <li>따라서 <em>new lines</em>에서 entropy context 리셋하고 앞서 설명한 approximate monontonicity constraint 적용</li>
</ul>

<h3 id="45-flops-estimation">4.5. FLOPs Estimation</h3>

<ul>
  <li>주로 FLOPs가 발생하는 곳 3:
    <ul>
      <li>FFN, Self-attention, Output projection !</li>
      <li>\(\begin{aligned} \mathrm{FL}_{\mathrm{BLT}} &amp; =\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{G}}, l_{\mathcal{G}}, m=n_{c t x} / n_p, V=0\right) / n_p \\ &amp; +\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{E}}, l_{\mathcal{E}}, m=w_{\mathcal{E}}, V=0\right) \\ &amp; +\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{D}}, l_{\mathcal{D}}, m=w_{\mathcal{D}}, V=256\right) \\ &amp; +\operatorname{Cross} \operatorname{Attn} . \mathrm{FL}\left(h_{\mathcal{E}}, l_{\mathcal{E}}, m=n_p, r=n_p / k\right) \times k / n_p \\ &amp; + \text { Cross Attn. FL }\left(h_{\mathcal{D}}, l_{\mathcal{D}}, m=k, r=k / n_p\right)\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h3 id="46-bits-per-byte-estimation">4.6. Bits-Per-Byte Estimation</h3>

<ul>
  <li>Perplexity per <strong>token</strong>는 비교가 어렵기 때문에, <strong>Byte</strong>-Level에서 평가하는 Bits-Per-Byte(<strong>BPB</strong>) 사용
    <ul>
      <li>\(\operatorname{BPB}(x)=\frac{\mathcal{L}_{C E}(\boldsymbol{x})}{\ln (2) \cdot n_{\text {bytes }}}\).</li>
    </ul>
  </li>
</ul>

<h3 id="47-transformer-architecture-hyperparameters">4.7. Transformer Architecture Hyperparameters</h3>

<ul>
  <li>Llama 3와 동일한 hyperparameters setting
    <ul>
      <li>Activation: SwiGLU</li>
      <li>Positional embedding: RoPE</li>
      <li>Normalization: RMSNorm</li>
      <li>Attention optimization: Flash Attention</li>
    </ul>
  </li>
</ul>

<h3 id="48-blt-specific-hyperparameters">4.8. BLT-Specific Hyperparameters</h3>

<ul>
  <li>N-gram Hash Embeddings : 3~8</li>
</ul>

<p>pass</p>

<h2 id="5-scaling-trends">5. Scaling Trends</h2>

<h3 id="51-parameter-matched-compute-optimal-scaling-trends">5.1. Parameter Matched Compute Optimal Scaling Trends</h3>

<ul>
  <li>compute-optimal scaling of BLT
    <ul>
      <li>compared in terms of training <strong>FLOPs</strong> and language modeling <strong>performance</strong></li>
    </ul>
  </li>
  <li>Larger patch sizes in BLT (6 or 8 bytes) <strong>reduce inference FLOPs by up to 50%</strong> while maintaining comparable performance to BPE models.</li>
</ul>

<h3 id="52-beyond-compute-optimal-task-evaluations">5.2. Beyond Compute Optimal Task Evaluations</h3>

<p><img src="/assets/img/LLM/BLT/table1.png" alt="그림1" /></p>

<ul>
  <li>Evaluations include :<strong>common sense reasoning, world knowledge, and code generation</strong> tasks</li>
  <li>Llama 3 보다 뛰어난 이유는
    <ul>
      <li>Better use of compute through dynamic patching !</li>
      <li>Direct modeling of byte-level information rather than tokens</li>
      <li>Larger patch size로 inference FLOPs reduction !</li>
    </ul>
  </li>
</ul>

<h3 id="53-patches-scale-better-than-tokens">5.3. Patches Scale Better Than Tokens</h3>

<p><img src="/assets/img/LLM/BLT/table2.png" alt="그림1" /></p>

<ul>
  <li>모델 크기와 패치 크기를 동시에 증가시키면서도 동일한 학습 및 추론 FLOP 예산을 유지</li>
</ul>

<h2 id="6-byte-modeling-improves-robustness">6. Byte Modeling Improves Robustness</h2>

<h3 id="61-character-level-tasks">6.1. Character-Level Tasks</h3>

<p><img src="/assets/img/LLM/BLT/fig7.png" alt="그림1" /></p>

<ul>
  <li>Byte-level models are inherently more robust to <strong>input noise</strong> and better at handling <strong>character-level variations.</strong></li>
</ul>

<p><img src="/assets/img/LLM/BLT/table3.png" alt="그림1" /></p>

<h3 id="62-training-blt-from-llama-3">6.2. Training BLT from Llama 3</h3>

<ul>
  <li>Pre-traind tokenizer-based model을 leverage해서 더 빠르게 수렴할 수 있는가</li>
  <li>BLT from Llama 3.1이 from Llama 3보다 좋음 !</li>
</ul>

<p><img src="/assets/img/LLM/BLT/table5.png" alt="그림1" /></p>

<h2 id="7-ablations-and-discussion">7. Ablations and Discussion</h2>

<p><img src="/assets/img/LLM/BLT/fig8.png" alt="그림1" /></p>

<p><img src="/assets/img/LLM/BLT/table6.png" alt="그림1" /></p>

<p><img src="/assets/img/LLM/BLT/table78.png" alt="그림1" /></p>

<p><img src="/assets/img/LLM/BLT/table9.png" alt="그림1" /></p>

<h2 id="8-related-work">8. Related Work</h2>

<p>Pass</p>

<h2 id="9-limitations-and-futurework">9. Limitations and FutureWork</h2>

<ul>
  <li>Scaling laws were calculated for BPE-level transformers and may lead to suboptimal (data, parameter sizes) ratios in the case of BLT</li>
  <li>실험들이 &lt;1B parameters에서 진행. &gt;8B parameters에서는 확장 가능성만 제시했을 뿐</li>
  <li>Entropy model을 별도로 훈련해서 patching하지만 이것까지도 end-to-end 하고자 함</li>
</ul>

<h2 id="10-conclusion">10. Conclusion</h2>

<ul>
  <li>Fixed-vocabulary tokenization에 의존하지 않는 Byte Latent Transformer (BLT)
    <ul>
      <li>Byte를 learnable, dynamic하게 patching</li>
      <li>소폭의 성능 손실을 감수하는 대신, 추론 시 FLOP 연산량을 최대 50%까지 줄일 수 있음</li>
    </ul>
  </li>
  <li>기존 LLM은 fixed context에서 model size만 증가
    <ul>
      <li>BLT는 동일한 inference FLOP 내에서 model size와 patch size를 동시에 확장할 수 있음</li>
    </ul>
  </li>
</ul>

  
</article>



  <hr class="dingbat related mb6" />






  
     


  <aside class="about related mt4 mb4" role="complementary">
    
    

<div class="author mt4">
  

  
    


<img
  
    src="https://via.placeholder.com/128x128"
    srcset="/assets/img/me/me.JPG 1x,/assets/img/me/me.JPG 2x"
    
  
  alt="GW Jeong"
  class="avatar"
  
  width="120"
  height="120"
  loading="lazy"
/>

  

  
  
  <h2  class="page-title hr-bottom">
    About
  </h2>

  <p>Bachelor’s degree in Applied Statistics. Yonsei Univ. (2018~2024) <br />
DataScienceLab 8th 학회장 (2022~2023) <br />
Master’s degree in Statitstics. Yonsei Univ. (2024~)</p>


  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>

  </aside>


  

  
  

  
    


  

  
  

  
    

  


  
<footer class="content" role="contentinfo">
  <hr/>
  
    <p><small class="copyright">© Geonwoo Jeong.
</small></p>
  
  
    <p><small>Powered by <a class="external" href="https://hydejack.com/">Hydejack</a> v<span id="_version">9.1.6</span></small></p>
  <hr class="sr-only"/>
</footer>


</main>

  <hy-drawer
  id="_drawer"
  class=""
  side="left"
  threshold="10"
  noscroll
  
>
  <header id="_sidebar" class="sidebar" role="banner">
    




<div class="sidebar-bg sidebar-overlay" style="background-color:rgb(230, 217, 195);background-image:url(/assets/img/me/sidebar.jpg)"></div>

    <div class="sidebar-sticky">
  <div class="sidebar-about">
    
      <a class="no-hover" href="/" tabindex="-1">
        <img src="/assets/img/me/logo.jpg" class="avatar" alt="LpppJ" width="120" height="120" loading="lazy" />
      </a>
    
    <a class="sidebar-title" href="/"><h2 class="h1">LpppJ</h2></a>
    
    
      <p class="">
        DataScience and AI

      </p>
    
  </div>

  <nav class="sidebar-nav heading" role="navigation">
    <span class="sr-only">Navigation:</span>
<ul>
  
    
      
      <li>
        <a
          id="_drawer--opened"
          href="/timeseries/"
          class="sidebar-nav-item "
          
        >
          TimeSeries
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/mamba/"
          class="sidebar-nav-item "
          
        >
          Mamba
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/LLM/"
          class="sidebar-nav-item "
          
        >
          LLM
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/pytorch/"
          class="sidebar-nav-item "
          
        >
          Pytorch
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/stat/"
          class="sidebar-nav-item "
          
        >
          Statistics
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/project/"
          class="sidebar-nav-item "
          
        >
          Project
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/presentation/"
          class="sidebar-nav-item "
          
        >
          Presentation
        </a>
      </li>
    
  
</ul>

  </nav>

  
  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>
  </header>
</hy-drawer>
<hr class="sr-only" hidden />

</hy-push-state>


  <!--[if gt IE 10]><!---->
  <script nomodule>!function(){var t,n=document.createElement("script");!("noModule"in n)&&"onbeforeload"in n&&(t=!1,document.addEventListener("beforeload",function(e){if(e.target===n)t=!0;else if(!e.target.hasAttribute("nomodule")||!t)return;e.preventDefault()},!0),n.type="module",n.src=".",document.head.appendChild(n),n.remove())}();
</script>
  <script src="/assets/js/hydejack-9.1.6.js" type="module"></script>
  <script src="/assets/js/LEGACY-hydejack-9.1.6.js" nomodule defer></script>
  

  

<!--<![endif]-->
  <!-- <script>
  document.querySelector('hy-push-state').setAttribute('prefetch', '');

  document.querySelectorAll('.sidebar a[href^="/"]').forEach(function (el) { 
    el.addEventListener('click', function (e) {
      if (el.pathname === window.location.pathname) {
        e.preventDefault();
        e.stopPropagation();
        document.querySelector('hy-drawer').close();
      }
    });
  });
</script> -->

<!--
Code for integrating CloudFlare's email protection with Hydejack's single page app loading.
-->
<script>
  document.getElementById('_pushState').addEventListener('hy-push-state-after', function (e) {
    function e(e){
      (console.error?console.error:console.log).call(console,e)
    }

    function t(e){
      return l.innerHTML='<a href="'+e.replace(/"/g,"&quot;")+'"></a>',l.childNodes[0].getAttribute("href")
    }

    function r(e,t){
      var r=e.substr(t,2);return parseInt(r,16)
    }

    function n(e,n){
      for(var o="",c=r(e,n),a=n+2;a<e.length;a+=2){
        var l=r(e,a)^c;
        o+=String.fromCharCode(l)
      }
      return t(o)
    }

    var o="/cdn-cgi/l/email-protection#",
        c=".__cf_email__",
        a="data-cfemail",
        l=document.createElement("div");

    !function(){
      for(var t=document.getElementsByTagName("a"),r=0;r<t.length;r++)
        try{
          var c=t[r],a=c.href.indexOf(o);
          a>-1&&(c.href="mailto:"+n(c.href,a+o.length))
        }catch(t){
          e(t)
        }
    }(),
    function(){
      for(var t=document.querySelectorAll(c),r=0;r<t.length;r++)
        try{
          var o=t[r],l=n(o.getAttribute(a),0),i=document.createTextNode(l);
          o.parentNode.replaceChild(i,o)
        }catch(t){
          e(t)
        }
    }()
  });
</script>





<div hidden>
  
  <h2 class="sr-only">Templates (for web app):</h2>

  <template id="_animation-template">
  <div class="animation-main fixed-top">
    <nav id="breadcrumbs" class="screen-only"><ul>
  
  
</ul></nav>
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

  <template id="_loading-template">
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

  <template id="_error-template">
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

  <template id="_permalink-template">
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="content-hash"></span>
  </a>
</template>

</div>


</body>
</html>
