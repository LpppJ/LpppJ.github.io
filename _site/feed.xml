<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-11-25T17:10:16+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-11-25-MoU/" rel="alternate" type="text/html" title="Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)" /><published>2024-11-25T00:00:00+09:00</published><updated>2024-11-25T17:09:56+09:00</updated><id>http://localhost:4000/timeseries/MoU</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-11-25-MoU/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer는 높은 계산 비용(quadratic computational cost)이 문제,</li>
  <li>Mamba는 longterm forecasting에서 성능이 effective하지 않음 (potential information loss 때문)</li>
  <li>본 논문에서 제시하는 <strong>Mixture of Universals (MoU)</strong>의 components:
    <ul>
      <li><strong>Mixture of Feature Extractors (MoF)</strong>: adaptive patch representations
        <ul>
          <li>(for <strong>short</strong>-term dependency)</li>
        </ul>
      </li>
      <li><strong>Mixture of Architectures (MoA)</strong>:  Mamba, FeedForward, Convolution, and Self-Attention 연결한 것
        <ul>
          <li>(for  <strong>long</strong>-term dependency )</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>PatchTST</strong>:  patch embedding에서 uniform linear transformations 사용 \(\to\) varying semantic contexts 손실
    <ul>
      <li>Vision에서 Dynamic Convolution, Conditional Convolution (for informative representations)등장했지만</li>
      <li>Time series에서는 poor performances</li>
    </ul>
  </li>
  <li><strong>Transformer</strong>: 장기 의존성은 잘 처리하지만 계산 비용이 큼</li>
  <li><strong>Mamba</strong>: 효율적이지만 정보 손실로 인해 장기 예측에서 성능이 떨어질 수 있음</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig2.png" alt="그림1" /></p>

<ul>
  <li><strong>MoF(Mixture of Feature Extractors)</strong>:
    <ul>
      <li>multiple sub-extractors로 구성되어 있고</li>
      <li>sparse activation을 사용해서 input patch에 적합한 sub-extractor만 활성화</li>
      <li>learning of diverse contexts and <strong>minimal parameter increase</strong> !</li>
    </ul>
  </li>
  <li><strong>MoA(Mixture of Architectures)</strong> Mamba에서 시작해 국소적인 관점에서 전역적인 Self-Attention 계층으로 확장하며 장기 의존성을 효율적으로 캡처하는 계층적 구조.
    <ul>
      <li>hierarchical structure를 가진 encoder</li>
      <li><strong>Mamba layer</strong> that selects and learns key dependencies using a Selective State-Space Model (SSM).</li>
      <li><strong>FeedForward transition layer</strong> and a <strong>Convolution-layer</strong> that broadens the receptive field to capture longer dependencies.</li>
      <li><strong>Self-Attention layer</strong> integrates information globally to fully capture long-term dependencies</li>
    </ul>
  </li>
</ul>

<h2 id="2-approach">2. Approach</h2>

<h3 id="21-problem-setting-and-model-structure">2.1 Problem Setting and Model Structure</h3>

<ul>
  <li>\(\mathbf{X}_{\text {input }}=\left[\mathbf{X}^1, \mathbf{X}^2, \ldots, \mathbf{X}^M\right] \in \mathbb{R}^{M \times L}\) where \(\mathbf{X}^i=\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_L\right] \in \mathbb{R}^L\)</li>
  <li>\(\hat{\mathbf{X}}_{\text {output }}=\left[\hat{\mathbf{X}}^1, \hat{\mathbf{X}}^2, \ldots, \hat{\mathbf{X}}^M\right] \in \mathbb{R}^{M \times T}\).</li>
  <li>
    <p>Goal : learning a function \(\mathcal{F}: \mathbf{X} \rightarrow \hat{\mathbf{X}}\)</p>
  </li>
  <li>Overall process는 아래와 같음.
    <ul>
      <li>처음에는 raw time series \(\mathbf{X} \in \mathbb{R}^L\)에서 시작 (variate independence setting이라서 channel =1)</li>
      <li>\(N\)개의 patch tokens를 만듬 :
        <ul>
          <li>\(\mathbf{X}_p=\operatorname{Patch}(\mathbf{X}) \in \mathbb{R}^{N \times P}\) with fixed size \(P\), stride \(S\)</li>
        </ul>
      </li>
      <li><strong>MoF</strong> module에 넣어서 adaptive representations를 얻음 :
        <ul>
          <li>\(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\).</li>
          <li>MoF는 parameters를 조절하면서 계산 비용 최적화 (<strong>2.2</strong>에서 자세히 설명)</li>
        </ul>
      </li>
      <li>이제 <strong>MoA</strong>에 넣어서 long-term dependencies (among tokens) 잡음
        <ul>
          <li>\(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoA}\left(\mathbf{X}_{\mathrm{rep}}\right)\in \mathbb{R}^{N \times D}\).</li>
          <li>MoA는 long-term encoder based on the Mixture of Architectures (<strong>2.3</strong>에서 자세히 설명)</li>
        </ul>
      </li>
      <li>마지막으로 linear projector에 넣어서 final prediction 얻음
        <ul>
          <li>\(\hat{\mathbf{X}}=\mathbf{P}\left(\operatorname{Flatten}\left(\mathbf{X}_{\mathrm{rep}^{\prime}}\right)\right)\in \mathbb{R}^{M \times T}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-mixture-of-feature-extractors">2.2 Mixture of Feature Extractors</h3>

<ul>
  <li>MoF의 목적은 <strong>patch의 representative embedding</strong>을 만드는 것</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig3.png" alt="그림1" /></p>

<ul>
  <li>Sub-extractors \(\left\{F_1, F_2, \ldots, F_c\right\}\)가 있고 각각은 independent linear mapping</li>
  <li>MoF를 통과하면 \(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\)를 얻음
    <ul>
      <li>where \(R_i(\cdot)\)는  input-relevant router (sub-extractor를 sparse하게 활성화)
        <ul>
          <li>즉 \(R\left(\mathbf{X}_p\right)_i=\operatorname{Softmax}\left(\operatorname{Top}_{\mathrm{k}}\left(H\left(\mathbf{X}_p\right)_i, k\right)\right)\)
            <ul>
              <li>\(\operatorname{Softmax}(\cdot)\)는  \(\operatorname{Topk}(\cdot, k)\)에 의해 선택된 상위 \(k\)개의 점수를 정규화</li>
              <li>\(H\left(X_p\right)=\left[H\left(\mathbf{X}_p\right)_1, H\left(\mathbf{X}_p\right)_2, \ldots H\left(\mathbf{X}_p\right)_c\right]\) 는 sub-extractors의 score vector</li>
              <li>여기서 \(H\left(\mathbf{X}_p\right)_i=\left(\mathbf{X}_p \cdot W_g\right)_i+\text { SN } \cdot \text { Softplus }\left(\left(\mathbf{X}_p \cdot W_{\text {noise }}\right)_i\right)\)
                <ul>
                  <li>where \(W_g\)는 linear layer이고 두번째 항은 load balancin을 위한 tunable noise를 넣는 것</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이렇게 하면 patch token를 \(c\)개의 서로 다른 patterns들의 조합으로 분할할 수 있음
    <ul>
      <li>each pattern은 최적의 sub-extractors에 의해 뽑힌 것이니</li>
      <li>most representative embedding (for patches with divergent context)라고 할 수 있음</li>
    </ul>
  </li>
</ul>

<h3 id="23-mixture-of-architectures">2.3. Mixture of Architectures</h3>

<ul>
  <li>MoA의 목적은 <strong>comprehensive long-term dependencies</strong>를 모델링하는 것</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig4.png" alt="그림1" /></p>

<ul>
  <li><strong>Mamba, FeedForward, Convolution</strong>, and <strong>Self-Attention layer</strong>로 구성되어
    <ul>
      <li>각각이 다른 관점에서 long-term dependencie를 학습</li>
      <li><strong>gradually expanding perspective</strong>를 통해 effectiveness and efficiency 둘 다 챙김</li>
    </ul>
  </li>
  <li><strong>Mamba-layer in Time Series</strong> : <strong>relevant data를 선택하고 time-variant dependencies를 학습하는 곳</strong>
    <ul>
      <li>input은 MoF의 output \(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\)인데 \(x\)로 denote</li>
      <li>\(\begin{gathered}
\boldsymbol{x}^{\prime}=\sigma(\text { Conv1D }(\text { Linear }(\boldsymbol{x}))) \\
\boldsymbol{z}=\sigma(\text { Linear }(\boldsymbol{x}))
\end{gathered}\), where \(\sigma\)는 \(SiLU\) activation</li>
      <li>다음으로 \(\begin{gathered}
\boldsymbol{y}^{\prime}=\operatorname{Linear}\left(\operatorname{SelectiveSSM}\left(\boldsymbol{x}^{\prime}\right) \otimes \boldsymbol{z}\right) \\
\boldsymbol{y}=\operatorname{LayerNorm}\left(\boldsymbol{y}^{\prime}+\boldsymbol{x}\right)
\end{gathered}\), where \(\begin{gathered}
\text { SelectiveSSM }\left(\boldsymbol{x}_t^{\prime}\right)=\boldsymbol{y}_t \\
\boldsymbol{y}_t=C \boldsymbol{h}_t, \quad \boldsymbol{h}_t=\bar{A} \boldsymbol{h}_{t-1}+\bar{B} \boldsymbol{x}_t^{\prime}
\end{gathered}\)
        <ul>
          <li>\(h_t\)는 latent state, \(y_t\)는 output representation</li>
          <li>The discrete matrices는 \(\begin{gathered}
B_t=S_B\left(\boldsymbol{x}_t^{\prime}\right), \quad C_t=S_C\left(\boldsymbol{x}_t^{\prime}\right) \\
\Delta_t=\operatorname{softplus}\left(S_{\Delta}\left(\boldsymbol{x}_t^{\prime}\right)\right)
\end{gathered}\)
            <ul>
              <li>\(S\)들은 linear layers이고, \(\begin{gathered}
f_A\left(\Delta_t, A\right)=\exp \left(\Delta_t A\right) \\
f_B\left(\Delta_t, A, B_t\right)=\left(\Delta_t A\right)^{-1}\left(\exp \left(\Delta_t A\right)-I\right) \cdot \Delta B_t \\
\bar{A}_t=f_A\left(\Delta_t, A\right), \quad \bar{B}_t=f_B\left(\Delta_t, A, B_t\right)
\end{gathered}\)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>좀 복잡한데 위에 있는 fig4 보는 것이 낫겠다.</li>
    </ul>
  </li>
  <li><strong>FeedForward-layer</strong> : <strong>non-linearity를 강화하는 곳</strong>
    <ul>
      <li>\(\boldsymbol{x}_{\mathrm{ffn}}=\text { FeedForward }\left(\boldsymbol{y}_t ; w_1, \sigma, w_2\right)\), where
        <ul>
          <li>\(w_1\) and \(w_2\) are parameters, \(\sigma\) is activation function</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Convolution-layer</strong> : <strong>MoA의 receptive field를 넓히는 곳</strong>
    <ul>
      <li>partial long-term dependencies를 담고 있는 tokens끼리의 정보 교환을 촉진</li>
      <li>\(\boldsymbol{x}_{\mathrm{conv}}= \operatorname{Conv}\left(\boldsymbol{x}_{\mathrm{ffn}} ; \mathbf{k}, s, p, c_{\mathrm{out}}\right)\),
        <ul>
          <li>where \(\mathbf{k}\) is the kernel size, \(s\) is the stride, \(p\) is the padding,</li>
          <li>and \(c_{\text {out }}\) is the number of output channels</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Self-Attention-layer</strong> : <strong>global perspective에서 포괄적인 long-term dependencies를 파악하는 곳</strong>
    <ul>
      <li>: \(\begin{aligned} &amp; x_{\mathrm{att}}=\operatorname{FeedForward}(\operatorname{Attention}(Q, K, V)) \\ &amp; \operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V \\ &amp; Q=x_{\text {conv }} W_Q, K=x_{\text {conv }} W_K, V=x_{\text {conv }} W_V\end{aligned}\)</li>
    </ul>
  </li>
  <li><strong>Partial-to-global Design for Time Series</strong>
    <ul>
      <li><strong>gradually expanding perspective</strong>라는 말의 의미는
        <ul>
          <li>Mamba layer : Selective SSM을 사용하여 시간적으로 변화하는 의존성을 처리</li>
          <li>FeedForward layer : 이러한 부분 의존성을 더 복잡한 표현으로 전환</li>
          <li>Convolution layer : 수용 영역을 확장하여 보다 넓은 시간적 관계를 학습</li>
          <li>Self-Attention layer : 로컬화된 정보를 통합하여 장기 의존성에 대한 포괄적인 이해</li>
        </ul>
      </li>
      <li>를 거치면서 선택적으로 일부 의존성에 초점을 맞춘 후, 이를 점차 확장하여 전체적(global) 관점으로 발전시킨다는 뜻</li>
    </ul>
  </li>
</ul>

<h3 id="24-computational-complexity-and-model-parameter">2.4. Computational Complexity and Model Parameter</h3>

<ul>
  <li>Tokens \(T\)개가 주어졌을 때 top-\(k\) experts를 선택한다고 하면
    <ul>
      <li>\(C_{\mathrm{MOU}}=\underbrace{k T \times d^2}_{\text {MoF }}+\underbrace{T \times d^2}_{\text {Mamba }}+\underbrace{T \times d^2}_{\text {FFN }}+\underbrace{k T d^2}_{\text {Conv }}+\underbrace{T^2 \times d+T \times d^2}_{\text {Transformer }}\),
        <ul>
          <li>where \(\mathbf{k}\) is the kernel size in the convolutional layer,</li>
          <li>\(d\) is the dimension of the vector representations</li>
        </ul>
      </li>
      <li>Transformer 블록을 제외하면 선형적인 복잡도</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig1.png" alt="그림1" /></p>

<h2 id="3-experiments">3. Experiments</h2>

<h3 id="31-dataset">3.1. Dataset</h3>

<ul>
  <li>7 commonly used datasets</li>
  <li>Pass</li>
</ul>

<h3 id="32-baselines-and-setup">3.2. Baselines and Setup</h3>

<ul>
  <li>Mamba-based Models (S-Mamba)</li>
  <li>Linear-based Models (D-Linear)</li>
  <li>Convolution-based Models (ModernTCN)</li>
  <li>Transformer-based Models (PatchTST)</li>
</ul>

<h3 id="33-main-results">3.3. Main Results</h3>

<p><img src="/assets/img/timeseries/MoU/table1.png" alt="그림1" /></p>

<h3 id="34-ablation-study">3.4. Ablation Study</h3>

<p><img src="/assets/img/timeseries/MoU/table2.png" alt="그림1" /></p>

<ul>
  <li>3개의 adaptive feature extractors를 비교했을 때 MoF(in MoU)가 가장 좋았으며
    <ul>
      <li>Dyconv가 parameters 수를 크게 증가시키기 때문에 time series patch와 같은 작은 데이터셋에는 적합하지 않음</li>
      <li>SE-M의  calibration strategy는 representation을 normalized gating vector에 곱하는 방식이라서 다양한 컨텍스트 정보를 처리하는 데에는 한계</li>
    </ul>
  </li>
  <li>특히 MoF가 uniform transformation method (Linear)보다 좋다는 점이 주목할만함</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/table3.png" alt="그림1" /></p>

<ul>
  <li>
    <p>AA, MM, MFA, AAA, MMA, AMM, MAM, AMA, AFM, AFCM</p>

    <ul>
      <li>여기서 A, M, F, C는 각각 Self-Attention, Mamba, FeedForward, Convolution</li>
      <li>글자의 순서는 레이어의 배치 순서</li>
    </ul>
  </li>
  <li>
    <p>M-A 순서(MAM, AMA, MMA) &gt; M-A 순서를 가지지 않은 모델(AMM)</p>

    <ul>
      <li>
        <p>Mamba를 Self-Attention 이전에 배치하는 것이 장기 의존성을 캡처하는 데 더 효과적</p>
      </li>
      <li>
        <p>A-M 순서보다 M-A 순서가 장기 의존성 학습에서 더 중요한 역할</p>
      </li>
    </ul>
  </li>
  <li>
    <p>F-C 순서 &gt; F</p>

    <ul>
      <li>Convolution 레이어가 Mamba 레이어의 수용 영역을 확장하여</li>
      <li>Mamba 레이어의 부분적 관점과 Self-Attention 레이어의 글로벌 관점을 연결하는 중간 관점을 제공한다고 해석됨</li>
    </ul>
  </li>
</ul>

<h3 id="35-model-analysis">3.5. Model Analysis</h3>

<ul>
  <li>Does MoF actually learn contexts within patches?</li>
  <li>What is learned by the layers of MoA?</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig56.png" alt="그림1" /></p>

<h2 id="5-conclusion">5 Conclusion</h2>

<ul>
  <li>Mixture of Universals (MoU)
    <ul>
      <li>Mixture of Feature Extractors (MoF)
        <ul>
          <li>an adaptive method specifically designed to enhance time series patch representations for capturing short-term dependencies</li>
        </ul>
      </li>
      <li>Mixture of Architectures (MoA)
        <ul>
          <li>hierarchically integrates multiple architectures in a structured sequence to model long-term dependencies from a hybrid perspective</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2408.15997)]]></summary></entry><entry><title type="html">Sequential Order-Robust Mamba for Time Series Forecasting (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-11-14-SOR-Mamba/" rel="alternate" type="text/html" title="Sequential Order-Robust Mamba for Time Series Forecasting (Arxiv 2024)" /><published>2024-11-14T00:00:00+09:00</published><updated>2024-11-14T00:47:46+09:00</updated><id>http://localhost:4000/timeseries/SOR-Mamba</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-11-14-SOR-Mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Mamba : near-linear complexity in processing sequential data
    <ul>
      <li><strong>하지만 일반적으로 Time series에서 변수의 순서는 존재하지 않기 때문에</strong></li>
      <li><strong>Mamba에서 channel dependencies (CD)를 잡다보면 sequential order bias가 발생</strong></li>
    </ul>
  </li>
  <li>그러므로 본 논문에서는 <strong>SOR-Mamba</strong>를 제안
    <ul>
      <li><strong>Regularization strategy</strong>
        <ul>
          <li>to minimize the discrepancy btw two embedding vectors (reversed channel orders)</li>
          <li>\(\to\)  robustness to channel order</li>
        </ul>
      </li>
      <li><strong>Eliminates the 1D-convolution</strong>
        <ul>
          <li>(originally designed to capture local information in sequential data)</li>
        </ul>
      </li>
      <li><strong>Channel correlation modeling (CCM)</strong>
        <ul>
          <li>pretraining task aimed at preserving <strong>correlations between channels</strong>
            <ul>
              <li>from the data space to the latent space</li>
              <li>in order to enhance the ability to capture CD</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer
    <ul>
      <li>ability to capture long-term dependencies but, quadratic computational complexity</li>
      <li>reduce the complexity 하려다보니 performance degradations</li>
    </ul>
  </li>
  <li><strong>SSM</strong>
    <ul>
      <li>employing <strong>convolutional</strong> operations to process sequences with linear complexity</li>
    </ul>
  </li>
  <li><strong>Mamba</strong>
    <ul>
      <li>incorporating a <strong>selective</strong> mechanism to prioritize important information</li>
      <li>balance btw performance and computational efficiency</li>
    </ul>
  </li>
  <li>Temporal dependencies (TD), channel dependencies (CD) 둘 다 잡아야 하는데,
    <ul>
      <li>iTransformer에서는 CD는 complex attention mechanisms으로,</li>
      <li>TD는 simple multi-layer perceptrons (MLPs)으로 했었음</li>
    </ul>
  </li>
  <li>Mamba는 <strong>sequential order bias</strong>가 있다보니 <strong>Bidirectional Mamba로는 충분하지 않음</strong> (table1)</li>
  <li>그렇다고 MambaTS처럼 channel을 섞자니 추가적인 작업이 소요됨</li>
</ul>

<p>table1</p>

<ul>
  <li>그래서 <strong>Sequential Order-Robust Mamba for TS forecasting (SOR-Mamba)</strong>를 제안
    <ul>
      <li>(간단한 설명은 abstract에 잘 정리되어 있으므로 pass)</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li><strong>CD-Mamba Block</strong>
    <ul>
      <li>1D Conv 제거</li>
      <li>Time series의 channels는 애초에 inherent sequential order가 존재하지 않기 때문</li>
    </ul>
  </li>
  <li><strong>Regulararization with CD-Mamba Block</strong>
    <ul>
      <li>Reversed channel order로 발생하는 두 embedding vectors의 차이를 줄이도록 학습</li>
      <li>\(\to\) Enhancing robustness to channel order !</li>
    </ul>
  </li>
  <li><strong>Channel correlation modeling</strong>
    <ul>
      <li>Data space에서의 correlation (btw channels)과</li>
      <li>Embedding space에서의 correlation (btw channels)의 차이를 줄이도록 학습</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2410.23356)]]></summary></entry><entry><title type="html">Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-11-12-BiMamba+/" rel="alternate" type="text/html" title="Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)" /><published>2024-11-12T00:00:00+09:00</published><updated>2024-11-14T00:13:05+09:00</updated><id>http://localhost:4000/timeseries/BiMamba+</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-11-12-BiMamba+/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>LTSF :  long-term dependencies capturing and <strong>sparse semantic characteristics</strong></li>
  <li>Mamba
    <ul>
      <li>the selective capability on input data</li>
      <li>the hardware-aware parallel computing algorithm</li>
    </ul>
  </li>
  <li><strong>Mamba+ block</strong>
    <ul>
      <li>by adding a forget gate inside Mamba,</li>
      <li>to selectively combine the new features with the historical features in a complementary manner</li>
    </ul>
  </li>
  <li><strong>Bi-Mamba+</strong>
    <ul>
      <li>apply Mamba+ both forward and backward</li>
    </ul>
  </li>
  <li>MTS는 시나리오마다 dependency가 다름
    <ul>
      <li>(varying emphasis on intra- or inter-series dependencies)</li>
      <li>\(\to\) series-relation-aware decider
        <ul>
          <li>: controls the utilization of channel-independent or channel-mixing tokenization strategy</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer-based models
    <ul>
      <li>implicitly models the <strong>inter-series dependencies</strong> through channel-mixing embeddings</li>
      <li>However the <strong>quadratic complexity</strong> of the self-attention mechanism</li>
      <li>Informer, Autoformer : sparse attention
        <ul>
          <li>But balancing computational efficiency and predicting performance는 본질적 해결 X</li>
          <li>게다가 not explicitly capture the inter-series dependencies</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>state-space models (SSM) : design of selective scanning</li>
  <li><strong>Long-term time series modeling</strong> : patching manner (patch-wise tokens)으로 하겠다</li>
  <li><strong>Emphasis on intra- or inter-series dependencies</strong> : 데이터셋마다 intra- or inter-sequence dependencies 둘 중 뭐가 중요한지가 다름</li>
  <li>그래서 <strong>Mamba+</strong>를 디자인함
    <ul>
      <li>adding a forget gate in Mamba,</li>
      <li>to selectively combine the new features with the historical features in a complementary manner</li>
      <li>therefore preserving historical information in a longer range</li>
    </ul>
  </li>
  <li><strong>Mamba+</strong>에 기반한 <strong>Bidirectional Mamba+ (BiMamba+)</strong>를 제안
    <ul>
      <li>to model the MTS data from both forward and backward,
        <ul>
          <li>enhancing the model’s robustness and ability to capture interactions between time series elements</li>
        </ul>
      </li>
      <li>Series-Relation-Aware (SRA) decider
        <ul>
          <li>measures the proportion of highly correlated series pairs in the MTS data</li>
          <li>to automatically choose channel-independent or channelmixing tokenization strategies</li>
        </ul>
      </li>
      <li>patch-wise tokens
        <ul>
          <li>contain richer semantic information</li>
          <li>and encourage the model to learn the long-term dependencies</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="time-series-forecasting">Time Series Forecasting</h3>

<ul>
  <li>Transformer-based models : quadratic complexity to the length of the sequence
    <ul>
      <li>Informer(Zhou et al. 2021) : ProbSparse mechanism</li>
      <li>Autoformer(Wu et al. 2021) : time series decomposition</li>
      <li>Pyraformer(Liu et al. 2021) : pyramidal attention module</li>
      <li>FEDformer(Zhou et al. 2022) : frequency enhanced Transformer through frequency domain mapping</li>
      <li>PatchTST(Nie et al. 2023) : divides each univariate sequence into patches
        <ul>
          <li>and uses patch-wise self-attention to model temporal dependencies</li>
        </ul>
      </li>
      <li>Crossformer(Zhang and Yan 2023) : Cross-Dimension attention</li>
      <li>iTransformer(Liu et al. 2023) : inverts the attention layers
        <ul>
          <li>to straightly model inter-series dependencies</li>
          <li>But, the tokenization approach is simply passing the whole sequence through a Multilayer Perceptron (MLP),
            <ul>
              <li>which overlooks the complex evolutionary patterns inside the time series</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="ssm-based-models">SSM-based models</h3>

<ul>
  <li>RNN-based models :
    <ul>
      <li>maintain a hidden state which is updated with each input element</li>
      <li>limits the training speed and leads to forgetting long-term information</li>
    </ul>
  </li>
  <li>CNN-based models :
    <ul>
      <li>parallel computing and have faster training speed</li>
      <li>limits the inference speed and overlook the long-term global information</li>
    </ul>
  </li>
  <li>State Space Models (SSM) :
    <ul>
      <li>trained in parallel like CNN and inferences fastly like RNN</li>
    </ul>
  </li>
  <li>Mamba
    <ul>
      <li>parameterized matrices and a hardware-aware parallel computing algorithm to SSM</li>
      <li><strong>S-Mamba</strong>(Wang et al. 2024)</li>
      <li>embeds each univariate time series like iTransformer</li>
      <li>and feeds the embeddings into Mamba blocks
        <ul>
          <li>to model the relationships of different time series</li>
        </ul>
      </li>
      <li>However, the tokenization approach may overlook the complex evolutionary patterns</li>
      <li><strong>MambaMixer</strong>(Behrouz et al. 2024)
        <ul>
          <li>adjusts the Mamba block to bidirectional</li>
          <li>and uses two improved blocks to capture inter/intra-series dependencies simultaneously</li>
          <li>However, the gating branch is used to filter new features
            <ul>
              <li>(of both forward and backward directions)</li>
              <li>which may cause challenges for extracting new features</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>TimeMachine</strong>(Ahamed and Cheng 2024)
        <ul>
          <li>a multi-scale quadruple-Mamba architecture
            <ul>
              <li>to unify the handling of channel-mixing and channelindependence situations</li>
            </ul>
          </li>
          <li>However, simply based on the length of historical observations and variable number of different datasets
            <ul>
              <li>the characteristics of the MTS data are not fully considered.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<h3 id="31-preliminaries">3.1. Preliminaries</h3>

<ul>
  <li>Long-term multivariate time series forecasting
    <ul>
      <li>\(\mathbf{X}_{i n}=\left[x_1, x_2, \ldots, x_L\right] \in \mathbb{R}^{L \times M}\), we predict the future values \(\mathbf{X}_{\text {out }}=\left[x_{L+1}, x_{L+2}, \ldots, x_{L+H}\right] \in \mathbb{R}^{H \times M}\)</li>
    </ul>
  </li>
  <li>State Space Models
    <ul>
      <li>using first-order differential equations, \(h^{\prime}(t)=\mathbf{A} h(t)+\mathbf{B} x(t), \quad y(t)=\mathbf{C} h(t)\)
        <ul>
          <li>where \(\mathbf{A} \in \mathbb{R}^{N \times N}, \mathbf{B} \in \mathbb{R}^{D \times N} \text { and } \mathbf{C} \in \mathbb{R}^{N \times D}\)</li>
        </ul>
      </li>
      <li>can be discretized :
        <ul>
          <li>\(\begin{aligned}
&amp; \overline{\mathbf{A}}=\exp (\Delta \mathbf{A}), \\
&amp; \overline{\mathbf{B}}=(\Delta \mathbf{A})^{-1}(\exp (\Delta \mathbf{A})-\mathbf{I}) \cdot \Delta \mathbf{B}
\end{aligned}\),</li>
          <li>\(h_k=\overline{\mathbf{A}} h_{k-1}+\overline{\mathbf{B}} x_k, \quad y_k=\mathbf{C} h_k\),</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>S4</strong>(Gu et al. 2021b) :
    <ul>
      <li>HIPPO Matrix(Gu et al. 2020) to the initialization of matrix A</li>
    </ul>
  </li>
  <li><strong>Mamba</strong>(Gu and Dao 2023) :
    <ul>
      <li>parameterizes the matrices \(\mathbf{B}, \mathbf{C}\) and \(\Delta\) in a data-driven manner,</li>
      <li>introducing a selection mechanism into S4 model</li>
    </ul>
  </li>
</ul>

<h3 id="32-overview">3.2. Overview</h3>

<p><img src="/assets/img/timeseries/BiMamba+/fig1.png" alt="그림1" /></p>

<ul>
  <li>step 1) calculate the tokenization strategy indicator through the SRA decider</li>
  <li>step 2) divide the input series into patches and generate patch-wise tokens
    <ul>
      <li>based on the tokenization strategy indicator</li>
    </ul>
  </li>
  <li>step 3) obtained tokens are fed into multiple Bi-Mamba+ encoders</li>
  <li>step 4) a flatten head and a linear projector to get the final output</li>
</ul>

<h3 id="33-instance-normalization">3.3. Instance Normalization</h3>

<ul>
  <li>the input sequence의 non-stationary statistics를 제거하기 위해 RevIN (Kim et al. 2022) 사용</li>
</ul>

<h3 id="34-token-generalization">3.4. Token Generalization</h3>

<ul>
  <li>SRA Decider
    <ul>
      <li>Channel-independence / dependence는 데이터셋마다 다름</li>
      <li>\(\to\) automatic tokenization process
        <ul>
          <li>데이터셋마다 \(T=\left\{t^1, t^2, \ldots, t^M\right\}\)에 대해
            <ul>
              <li>Spearman correlation coefficients of different series \(t^i \text { and } t^j\) 계산 \(\rho_{i, j}\)</li>
              <li>\(\rho_{i, j}=1-\frac{6 \sum_{k=0}^n\left(\operatorname{Rank}\left(t_k^i\right)-\operatorname{Rank}\left(t_k^j\right)\right)^2}{n\left(n^2-1\right)}\),
                <ul>
                  <li>where \(n\) : the number of observations</li>
                  <li>\(\operatorname{Rank}\left(t_k^i\right)\) : the rank level of the \(k\)-th element in the specific time series \(t^i\)</li>
                </ul>
              </li>
              <li>threshold \(\lambda\)를 정하고 relevant series \(\rho_{\max }^\lambda\) and \(\rho_{\max }^0\)를 센 다음
                <ul>
                  <li>relation ratio \(r=\rho_{\max }^\lambda / \rho_{\max }^0 \geq 1-\lambda\)이면 channel-mixing</li>
                  <li>그렇지 않으면 channelindependent strategy</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/alg1.png" alt="그림1" /></p>

<h3 id="35-tokenization-process">3.5. Tokenization Process</h3>

<ul>
  <li>PatchTST처럼 \(x_{1: L}^i\)를 \(J = \left\lceil\frac{L-P}{S}+1\right\rceil\)개의 patch로 나누고
    <ul>
      <li>(S는 stride, P는 patch에 들어가는 시점의 개수)</li>
      <li>channel-independent strategy에서는 각 patch를 D차원으로 embedding
        <ul>
          <li>\(\to\) \(\mathbb{E}_{\text {ind }} \in \mathbb{R}^{M \times J \times D}\)</li>
        </ul>
      </li>
      <li>channel-mixing strategy에서는 같은 시점의 다른 변수들도 group으로 만들고 각 group을 tokenization
        <ul>
          <li>\(\to \mathbb{E}_{\operatorname{mix}} \in\mathbb{R}^{J \times M \times D}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="36-mamba-block">3.6. Mamba+ Block</h3>

<p><img src="/assets/img/timeseries/BiMamba+/mamba.png" alt="그림1" /></p>

<ul>
  <li><strong>기존 Mamba</strong>는 2개의 branches를 사용
    <ul>
      <li>\(b_1\)에서는 1d-conv와 SSM을 통과, 다른 하나 \(b_2\)에서는 그냥 SiLU activation 통과</li>
      <li>\(b_1\)의 SSM 안에 HIPPO가 있긴 해도 \(b_2\) 때문에 최근 정보가 더 우선시되는 문제</li>
    </ul>
  </li>
  <li>그래서 <strong>Mamba+ block</strong>에서는
    <ul>
      <li>forget gate \(\text{gate}_f=1-\text{gate}_{b_2}\)를 추가</li>
      <li>\(\text{gate}_f\)와 \(\text{gate}_{b_2}\)는 new features와 forgotten historical features를 선택적 결합
        <ul>
          <li>\(\to\) preserving historical information !</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/fig2.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/BiMamba+/alg2.png" alt="그림1" /></p>

<h3 id="37-bidirectional-mamba-encoder">3.7. Bidirectional Mamba+ Encoder</h3>

<ul>
  <li>Channel-mixing이면 \(\mathbb{E}_x^{(l)} \in \mathbb{R}^{J \times M \times D}\) and \(\mathbb{E}_x^{(0)}=\mathbb{E}_{m i x}\)
    <ul>
      <li>otherwise \(\mathbb{E}_x^{(l)} \in \mathbb{R}^{M \times J \times D}\) and \(\mathbb{E}_x^{(0)}=\mathbb{E}_{\text {ind }}\)</li>
    </ul>
  </li>
  <li>2개의 Bi-Mamba+ block, 각각 forward and backward
    <ul>
      <li>각각의 input을 \(\mathbb{E}_{x, d i r}^{(l)}\) where \(\operatorname{dir} \in\{\text{forward,backward}\}\)이라 하면</li>
      <li>\(\mathbb{E}_x^{(l+1)}=\sum_{\text {dir }}^{\{\text {forward,backward }\}} \mathcal{F}\left(\mathbb{E}_{y, \text { dir }}^{(l)}, \mathbb{E}_{x, d i r}^{(l)}\right)\)가 다음 layer의 input이 됨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/alg3.png" alt="그림1" /></p>

<h3 id="38-loss-function">3.8. Loss Function</h3>

<ul>
  <li>MSE : \(\mathcal{L}(Y, \hat{Y})=\frac{1}{\mid Y \mid} \sum_{i=1}^{\mid Y \mid}\left(y_{(i)}-\hat{y}_{(i)}\right)^2\)</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<h3 id="41-datasets">4.1. Datasets</h3>

<p><img src="/assets/img/timeseries/BiMamba+/table1.png" alt="그림1" /></p>

<h3 id="42-baseline-models">4.2. Baseline Models</h3>

<ul>
  <li>Autoformer (Wu et al. 2021)
    <ul>
      <li>series decomposition technique with Auto-Correlation mechanism</li>
    </ul>
  </li>
  <li>PatchTST (Nie et al. 2023)
    <ul>
      <li>patching and channel independent techniques</li>
    </ul>
  </li>
  <li>Crossformer (Zhang and Yan 2023)
    <ul>
      <li>PatchTST + Attention layer (for capture inter-series dependencies)</li>
    </ul>
  </li>
  <li>iTransformer (Liu et al. 2023)
    <ul>
      <li>inverts the modeling method of Transformer</li>
    </ul>
  </li>
  <li>DLinear (Zeng et al. 2023)
    <ul>
      <li>decomposes time series into two different components</li>
    </ul>
  </li>
  <li>TimesNet (Wu et al. 2022)
    <ul>
      <li>transforming the 1-D time series into a set of 2-D tensors</li>
    </ul>
  </li>
  <li>WITRAN (Jia et al. 2024)
    <ul>
      <li>RNN structure that process the univariate input sequence
        <ul>
          <li>in the 2-D space with a fixed scale</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>CrossGNN (Huang et al. 2024)
    <ul>
      <li>time series in a multi-scale way</li>
      <li>GNN to capture both cross-scale and cross-series dependencies</li>
    </ul>
  </li>
  <li>S-Mamba (Wang et al. 2024)
    <ul>
      <li>generates embeddings for each time series through a simple MLP layer</li>
      <li>and uses Mamba to extract inter-series dependencies</li>
    </ul>
  </li>
</ul>

<h3 id="43-experimental-settings">4.3. Experimental Settings</h3>

<ul>
  <li>\(L=96\) for all models on all datasets, \(H \in\{96,192,336,720\}\)</li>
  <li>\(S=\frac{1}{2} P\) and use patch length \(P=\frac{1}{4} L\)</li>
  <li>SRA decider, we set \(\lambda=0.6\)</li>
  <li>for <strong>Bi-Mamba+, PatchTST and Crossformer</strong> that use patching technique, we set \(D=128\) for Weather, Traffic, Electricity, Solar and \(D=64\) for ETT datasets,
    <ul>
      <li>while for <strong>S-Mamba and iTransformer</strong> that map the whole sequence to tokens, we set \(D=512\) for Weather, Traffic, Electricity, Solar and \(D=256\) for ETT datasets.</li>
    </ul>
  </li>
  <li>As for parameters within Mamba+ block은 Ahamed and Cheng 2024; Wang et al. 2024처럼</li>
  <li><strong>convolutional kernel size</strong> d_conv =2 and <strong>hidden state expansion</strong> expand =1 on all datasets.</li>
  <li><strong>hidden dimension</strong> d_state =16 for Weather, Electricity and Traffic and d_state =8 for ETT datasets.</li>
  <li><strong>encoder layer</strong> \(l \in\{1,2,3\}\),</li>
  <li><strong>learning rate</strong>는 \([5 \mathrm{e}-5,1 \mathrm{e}-4,2 \mathrm{e}-4,5 \mathrm{e}-4,1 \mathrm{e}-3, 2 \mathrm{e}-3,5 \mathrm{e}-3]\)</li>
</ul>

<h3 id="44-main-results">4.4. Main Results</h3>

<p><img src="/assets/img/timeseries/BiMamba+/table3.png" alt="그림1" /></p>

<h3 id="45-ablation-study">4.5. Ablation Study</h3>

<p><img src="/assets/img/timeseries/BiMamba+/table4.png" alt="그림1" /></p>

<ul>
  <li>(a) w/o SRA-I which use channel-independent strategy only</li>
  <li>(b) w/o SRA-M which use channelmixing strategy only</li>
  <li>(c) w/o Bi which use forward direction Mamba block only</li>
  <li>(d) w/o Residual that removes the residual connection</li>
  <li>(e) S-Mamba</li>
  <li>
    <p>(f) PatchTST used for the benchmark models</p>
  </li>
  <li><strong>filter threshold</strong> \(\lambda\)에 따른 tokenization strategy indicator</li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/fig3.png" alt="그림1" /></p>

<ul>
  <li>length of patches \(P\)에 따른 MSE</li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/fig4.png" alt="그림1" /></p>

<h3 id="46-model-efficiency">4.6. Model efficiency</h3>

<p><img src="/assets/img/timeseries/BiMamba+/fig7-1.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/BiMamba+/fig7-2.png" alt="그림1" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li><strong>Bi-Mamba+</strong></li>
  <li>adding forget gate in Mamba
    <ul>
      <li>to selectively combine the added new features with the forgotten historical features in a complementary manner,</li>
      <li>therefore preserving historical information in a longer range</li>
    </ul>
  </li>
  <li>dividing the time series into patches
    <ul>
      <li>for inter-series dependencies at a finer granularity</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2404.15772)]]></summary></entry><entry><title type="html">Is Mamba Effective for Time Series Forecasting? (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-11-07-s-mamba/" rel="alternate" type="text/html" title="Is Mamba Effective for Time Series Forecasting? (Arxiv 2024)" /><published>2024-11-07T00:00:00+09:00</published><updated>2024-11-12T12:47:21+09:00</updated><id>http://localhost:4000/timeseries/s-mamba</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-11-07-s-mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Time series forecasting (TSF)에서 Transformer는 quadratic complexity</li>
  <li><strong>Simple-Mamba (S-Mamba)</strong> :
    <ul>
      <li>Tokenize the time points of each variate autonomously via a linear layer</li>
      <li>Bi-directional Mamba layer is utilized to extract inter-variate correlations</li>
      <li>Feed-Forward Network is set to learn temporal dependencies</li>
      <li>Generation of forecast outcomes through a linear mapping layer</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>
    <p>Transformer-based</p>

    <ul>
      <li>quadratic computational complexity</li>
      <li>reduce the computational complexity \(\to\)  loss of information \(\to\) performance degradations</li>
    </ul>
  </li>
  <li>
    <p>Linear model</p>

    <ul>
      <li>solely on linear numerical calculations \(\to\) do not incorporate in-context information</li>
    </ul>
  </li>
  <li>
    <p>SSM</p>

    <ul>
      <li>convolutional calculation to capture sequence information</li>
      <li>eliminate hidden states (for parallel computing) \(\to\) near-linear complexity</li>
      <li>But unable to identify and filter content</li>
    </ul>
  </li>
  <li>
    <p>Mamba</p>

    <ul>
      <li>selective mechanism into SSM</li>
    </ul>
  </li>
  <li>
    <p><strong>Simple-Mamba (S-Mamba)</strong> :</p>

    <ul>
      <li>
        <ol>
          <li>time points of each variate are tokenized by a linear layer</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>Mamba VC (Inter-variate Correlation) Encoding layer encodes the VC</li>
        </ol>

        <ul>
          <li>by utilizing a bidirectional Mamba</li>
        </ul>
      </li>
      <li>
        <ol>
          <li>FeedForward Network (FFN) TD (Temporal Dependency) Encoding Layer extracts the TD</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>mapping layer is utilized to output the forecast results</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-preliminaries">3. Preliminaries</h2>

<h3 id="31-problem-statement">3.1. Problem Statement</h3>

<ul>
  <li>\(U_{\text {in }}=\left[u_1, u_2, \ldots, u_L\right] \in \mathbb{R}^{L \times V}\)를 보고 \(U_{\text {out }}=\left[u_{L+1}, u_{L+2}, \ldots, u_{L+T}\right] \in \mathbb{R}^{T \times V}\)를 예측
    <ul>
      <li>각각의 \(u_n=\left[p_1, p_2, \ldots, p_V\right]\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-state-space-models">3.2. State Space Models</h3>

<ul>
  <li>The continuous sequence is discretized by a step size \(\Delta\), and the discretized SSM model :
    <ul>
      <li>\(\begin{aligned}
h_t &amp; =\overline{\boldsymbol{A}} h_{t-1}+\overline{\boldsymbol{B}} x_t, \\
y_t &amp; =\boldsymbol{C} h_t,
\end{aligned}\)      where
        <ul>
          <li>\(\overline{\boldsymbol{A}}=\exp (\Delta \boldsymbol{A}) \text { and } \overline{\boldsymbol{B}}=(\Delta \boldsymbol{A})^{-1}(\exp (\Delta \boldsymbol{A})-I) \cdot \Delta \boldsymbol{B}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="33-mamba-block">3.3. Mamba Block</h3>

<p>: <strong>Data-dependent selection mechanism into the S4</strong> &amp; <strong>Incorporates hardware-aware parallel algorithms</strong></p>

<p><img src="/assets/img/timeseries/s-mamba/alg1.png" alt="그림1" /></p>

<ul>
  <li>
    <p>Mamba Layer</p>

    <ul>
      <li>
        <p>Input : \(X \in \mathbb{R}^{B \times V \times D}\)</p>
      </li>
      <li>
        <ol>
          <li>expands the hidden dimension to \(ED\) through linear projection \(\to x, z\)를 얻음</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>the projection using convolutional functions and a SiLU \(\to x'\)를 얻음</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>generates the state representation \(y\)</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>\(y\) is combined with a residual connection from \(z\) after activation,</li>
        </ol>

        <ul>
          <li>and the final output \(y_t\) at time step $t$ is obtained</li>
        </ul>
      </li>
      <li>
        <p>with state expansion factor \(N\),</p>

        <ul>
          <li>a size of convolutional kernel \(k\),</li>
          <li>and a block expansion factor \(E\)</li>
        </ul>
      </li>
      <li>The final output of the Mamba block is \(Y \in \mathbb{R}^{B \times V \times D}\).</li>
    </ul>
  </li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<ul>
  <li>1st layer : the Linear Tokenization Layer (tokenizes the time series with a linear layer)</li>
  <li>2nd layer :  the Mamba intervariate correlation (VC) Encoding layer (using a bidirectional Mamba block)</li>
  <li>3rd layer : the FFN Temporal Dependencies (TD) Encoding Layer (learns the temporal sequence information)
    <ul>
      <li>Feed-Forward Network : generates future series representations</li>
    </ul>
  </li>
  <li>4th layer : Projection Layer, is only mapping for forecasting</li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/alg2.png" alt="그림1" /></p>

<h3 id="41-linear-tokenization-layer">4.1. Linear Tokenization Layer</h3>

<ul>
  <li>\(U=\operatorname{Linear}\left(\operatorname{Batch}\left(U_{\text {in }}\right)\right), \quad\) where \(U\) is the output of this layer</li>
</ul>

<h3 id="42-mamba-vc-encoding-layer">4.2. Mamba VC Encoding Layer</h3>

<ul>
  <li>여기서는 유사한 trend를 보이는 변수들을 연결해서 VC를 찾고 싶음</li>
  <li>Transformer는 그냥 모든 변수들끼리 다 연결하니까 정확하긴 한데 변수 개수 따라 complexity 늘어남</li>
  <li>Mamba는 complexity는 near-linear이지만
    <ul>
      <li><strong>Selection mechanism이 uni-directional해서 앞쪽의 변수만 볼 수 있음</strong></li>
      <li>그래서 2개의 Mamba를 서로 다른 방향으로 흐르도록 놓음 (Bi-directional Mamba)</li>
      <li>\(\begin{aligned}&amp;\overrightarrow{\boldsymbol{Y}}=\overrightarrow{\operatorname{Mamba\operatorname {Block}}(\boldsymbol{U}),} \\
&amp; \overleftarrow{\boldsymbol{Y}}=\overleftarrow{\operatorname{Mamba} \operatorname{Block}}(\boldsymbol{U}) .
\end{aligned}\)이고 \(\boldsymbol{Y}=\overrightarrow{\boldsymbol{Y}}+\overleftarrow{\boldsymbol{Y}}\)로 Aggregate, with residual : \(\boldsymbol{U}^{\prime}=\boldsymbol{Y}+\boldsymbol{U}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-ffn-td-encoding-layer">4.3. FFN TD Encoding Layer</h3>

<ul>
  <li>
    <ol>
      <li>Normalization layer : enhance convergence and training stability</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>FFN layer : encodes observed time series (encodes TD by keeping the sequential relationships)</li>
    </ol>

    <ul>
      <li>decodes future series representations (adjust the future series representations)</li>
    </ul>
  </li>
</ul>

<h3 id="44-projection-layer">4.4. Projection Layer</h3>

<ul>
  <li>FFN TD Encoding layer의 output인 tokenized temporal information이
    <ul>
      <li>linear mapping을 통해서 reconstructed for predictive outcome</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-datasets-and-baselines">5.1. Datasets and Baselines</h3>

<p><img src="/assets/img/timeseries/s-mamba/table1.png" alt="그림1" /></p>

<ul>
  <li>SOTA와 비교 :
    <ul>
      <li>iTransformer: analyzes the time series information of each <strong>individual variate</strong> and then fuses the information of all variates.</li>
      <li>PatchTST: segments time series into <strong>subseries patches</strong> as input tokens and uses channel-independent shared embeddings and weights</li>
      <li>Crossformer: cross-attention mechanism that allows the model to <strong>interact with information between different time steps</strong></li>
      <li>FEDformer: a <strong>frequency-enhanced Transformer</strong> for utilizaing a sparse representation</li>
      <li>Autoformer: <strong>decomposition architecture</strong> that incorporates an auto-correlation mechanism</li>
      <li>RLinear: reversible normalization and channel independence into <strong>pure linear structure</strong></li>
      <li>TiDE: Multi-layer Perceptron (MLP) based encoderdecoder model</li>
      <li>DLinear: <strong>simple one-layer linear</strong> model with decomposition architecture</li>
      <li>TimesNet: a task-general backbone, <strong>transforms 1D time series into 2D tensors</strong></li>
    </ul>
  </li>
</ul>

<h3 id="52-overall-performance">5.2. Overall Performance</h3>

<p><img src="/assets/img/timeseries/s-mamba/table2.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/s-mamba/table3.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/s-mamba/table4.png" alt="그림1" /></p>

<ul>
  <li>S-Mamba가 traffic-related, Electricity, and Solar-Energy에서 성능이 좋음
    <ul>
      <li>변수들이 periodic한 데이터셋들.</li>
      <li>즉 period variates are more likely to contain learnable VC.</li>
      <li>Mamba VC Fusion Layer가 잘 잡은 것</li>
    </ul>
  </li>
  <li>ETT, and Exchange datasets에서는 성능이 매우 좋지는 않았음
    <ul>
      <li>변수 개수가 적은 데이터셋들 (predominantly of an aperiodic nature)</li>
      <li><strong>weak</strong> VCs between these variates 때문에 Mamba VC Encoding layer가 noise를 가져옴</li>
    </ul>
  </li>
  <li>Weather는 변수도 적고 aperiodic한데 왜 성능이 좋나
    <ul>
      <li>변수들의 Trend가 동시에 나타나는 도메인이라서 Mamba VC Encoding layer가 잘 작동</li>
      <li>Trend가 large sections로 나타나기 때문에 FFN이 이런 거 잘 잡음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig4.png" alt="그림1" /></p>

<h3 id="53-model-efficiency">5.3. Model Efficiency</h3>

<p><img src="/assets/img/timeseries/s-mamba/fig5.png" alt="그림1" /></p>

<h3 id="54-ablation-study">5.4. Ablation Study</h3>

<p><img src="/assets/img/timeseries/s-mamba/table5.png" alt="그림1" /></p>

<h3 id="55-can-variate-order-affect-the-performance-of-s-mamba">5.5. Can Variate Order Affect the Performance of S-Mamba?</h3>

<ul>
  <li>S-Mamba는 independent channel이라서 variates order 안중요했음
    <ul>
      <li>하지만 Mamba VC Encoding Layer는 variates order에 따라 initial bias 발생할 수 있음</li>
    </ul>
  </li>
  <li>그래서 Fourier transform해서 variates를 periodic and aperiodic groups으로 나누고
    <ul>
      <li>periodic variates =  reliable information / aperiodic variates = potential noise으로 가정</li>
    </ul>
  </li>
  <li>그래서 reliable information를 가지고 있는(그럴 것이라고 생각되는) periodic variates를 앞쪽에 배치</li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig6.png" alt="그림1" /></p>

<h3 id="56-can-mamba-outperform-advanced-transformers">5.6. Can Mamba Outperform Advanced Transformers?</h3>

<ul>
  <li>Transformer의 Encoder layer를 Mamba로 교체
    <ul>
      <li>Autoformer, Flashformer and Flowformer</li>
      <li>\(\to\)  Auto-M, FlashM and Flow-M 이라고 부르겠음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig7.png" alt="그림1" /></p>

<h3 id="57-can-mamba-help-benefit-from-increasing-lookback-length">5.7. Can Mamba Help Benefit from Increasing Lookback Length?</h3>

<ul>
  <li>Transformer-based model은:
    <ul>
      <li>lookback sequence length \(L\)이 늘어나도 성능이 비례해서 좋아지는 건 아님</li>
      <li><strong>sequential order를 신경쓰지 않아서 그렇다</strong></li>
    </ul>
  </li>
  <li>Mamba는:
    <ul>
      <li>certain sequential attributes가 잘 유지되는 편</li>
      <li>Mamba block을 Transformer-based model의 Encoder와 decoder 사이에 배치하면
        <ul>
          <li>Encoder layer의 output에 (decoder layer에 가기 전에)</li>
          <li>positional encoding처럼 어떤 정보를 추가해주는 역할을 하는 것</li>
        </ul>
      </li>
      <li>그것을 Reformer, Informer, and Transformer와 비교해서
        <ul>
          <li>Refor-M, Infor-M, and Trans-M라고 부르고 비교</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig8.png" alt="그림1" /></p>

<ul>
  <li>S-Mamba와 iTransformer 둘다 \(L\)이 길어지면 어느 정도 성능이 좋아지긴 함
    <ul>
      <li>하지만 이건 두 모델 모두 가지고 있는 FFN TD Encoding Layer 때문으로 보임</li>
    </ul>
  </li>
  <li>S-Mamba가 iTransformer와 비교해서 일관되게 성능이 좋아지는 편인데
    <ul>
      <li>이건 Mamba VC Encoding layer와 Transformer의 VC Encoding layer의 차이 !</li>
    </ul>
  </li>
</ul>

<h3 id="58-is-mamba-generalizable-in-tsf">5.8. Is Mamba Generalizable in TSF?</h3>

<ul>
  <li>Transformer는 generalization capabilities가 좋은 편이라서
    <ul>
      <li>iTransformer의 경우 40%의 변수만을 가지고 나머지 변수들은 masking해도</li>
      <li>성능이 큰 폭으로 나빠지지는 않음</li>
    </ul>
  </li>
  <li>Mamba의 경우에도 40%의 변수만 보고 나머지 변수들은 masking 했을 때
    <ul>
      <li>iTransformer에 크게 뒤쳐지지 않음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig9.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li><strong>Simple-Mamba(S-Mamba)</strong>
    <ul>
      <li>inter-variate correlation (VC) encoding은</li>
      <li>Transformer 대신 <strong>bi-directional</strong> Mamba block으로 하고</li>
      <li>(더 낮은 overhead로 VC를 파악)</li>
      <li>Temporal Dependencies (TD)는
        <ul>
          <li>Feed-Forward Network로 extract</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Mamba는 advanced-Transformer만큼의 <strong>stability</strong>도 있고
    <ul>
      <li><strong>generalization</strong> capabilities도 뛰어난 편</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2403.11144v3)]]></summary></entry><entry><title type="html">TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)</title><link href="http://localhost:4000/timeseries/2024-10-29-timemachine/" rel="alternate" type="text/html" title="TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)" /><published>2024-10-29T00:00:00+09:00</published><updated>2024-11-07T18:10:33+09:00</updated><id>http://localhost:4000/timeseries/timemachine</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-29-timemachine/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Long-term time-series forecasting(LTSF)에서 우리는 long-term dependencies를 capture하는데
    <ul>
      <li>linear scalability와 computational efficiency를 유지해야 함</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 TimeMachine은 Mamba를 활용하여
    <ul>
      <li>the unique properties of time series를 발견하여
        <ul>
          <li>multi-scales에서 salient contextual cues를 만들고</li>
        </ul>
      </li>
      <li>quadruple-Mamba architecture를 합쳐서
        <ul>
          <li>channel-mixing and channel-independence를 한 번에 통합</li>
        </ul>
      </li>
      <li>서로 다른 scales에서의 global / local contexts를 effective하게 selection할 수 있게 함</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>LTSF에서 Capturing long-term dependencies가 핵심</li>
  <li><strong>Linear model</strong> : DLinear, TiDE
    <ul>
      <li>may not well capture long-range correlations</li>
    </ul>
  </li>
  <li><strong>Transformer-based</strong> : iTransformer, PatchTST, Crossformer
    <ul>
      <li>suffer from the quadratic complexity</li>
    </ul>
  </li>
  <li><strong>state-space models (SSMs)</strong>
    <ul>
      <li>inferring over very long sequences</li>
      <li>context-aware selectivity</li>
      <li>LTSF에서도 활용될 수 있는가
        <ul>
          <li>highly content- and context-selective SSM이 최근에 많이 나오고 있고</li>
          <li>effectively representing the context in time series에 쓸 수 있을 것</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Transforemr-based approach에서는 each observation이나 sub-series, 아니면 time series를 token(patch)로 만드는데
    <ul>
      <li>SSM에서 이러한 접근을 그대로 쓰면 성능이 안나옴</li>
      <li><strong>그래서 salient contextual cues tailored to SSM을 extract하는 것이 먼저 !</strong></li>
    </ul>
  </li>
  <li>기존에는 channel-mixing way도 있고 (ex. Informer, FEDformer, and Autoformer, …)
    <ul>
      <li>channel-independence way도 있는데, (ex. PatchTST, TiDE, …)</li>
      <li>본 논문에서는 unified architecture : applicable to both scenarios!</li>
    </ul>
  </li>
  <li>그리고 Time series에는 downsampling해도 temporal relations가 유지된다는 특징이 있으니
    <ul>
      <li>모든 time points를 token으로 만드는 건 redundant하고, PatchTST처럼 patch를 사용하는 건 good</li>
      <li>하지만 pre-defined small patch는 fixed resolution에서의 context만 제공</li>
      <li>그러니 iTransformer처럼 whole look-back window를 token으로 만드는 것이 낫고</li>
      <li>하지만 iTransformer처럼 channel-independence에서는 select sub-token contents가 잘 안됨</li>
      <li>그러므로, SSM 쓰면 더 잘 될 것</li>
    </ul>
  </li>
  <li>그러니 본 논문에서는 TimeMachine을 제안
    <ul>
      <li>MTS를 2개의 scale에서 context-aware prediction하기 위해 SSM을 사용
        <ul>
          <li>high, low resolution이라는 2개의 scale마다 2개의 mamba를 사용.
            <ul>
              <li>하나는 global perspectives for the channel-mixing</li>
              <li>다른 하나는 both global and local perspectives for the channel-independence</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>이렇게 4개의 SSM modules를 사용해서 channel-independent, -dependent를 통합하고
        <ul>
          <li>즉 btw-channel correlation이 “있으면” 잡아내고 “없으면” independent 처럼</li>
          <li>다양한 scales에서 global and local contextual information을 효율적으로 selection</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li>Non-Transformer-based Supervised Approaches
    <ul>
      <li>Classical methods : ARIMA, VARMAX, GARCH, RNN, …</li>
      <li>MLP-based models : DLinear, TiDE, RLinear, …</li>
      <li>CNN-based : TimesNet, Scinet, …</li>
    </ul>
  </li>
  <li>Transformer-based Supervised Learning methods
    <ul>
      <li>iTransformer, PatchTST, Crossformer, FEDformer, stationary, Flowformer, and Autoformer</li>
      <li>time series를 token series로 만들고 self-attention</li>
      <li>하지만 quadratic time and memory complexity</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-method">3. Proposed Method</h2>

<ul>
  <li>input sequence \(\mathbf{x}=\left[x_1, \ldots, x_L\right]\)
    <ul>
      <li>\(x_t \in \mathcal{R}^M\) representing a vector of \(M\) channels at time point \(t\)</li>
    </ul>
  </li>
  <li><strong>Normalization</strong>
    <ul>
      <li>the original MTS \(\mathbf{x}\) into \(\mathbf{x}^0=\left[\mathbf{x}_1^{(0)}, \cdots, \mathbf{x}_L^{(0)}\right] \in \mathcal{R}^{M \times L}\), via \(\mathbf{x}^{(0)}=\operatorname{Normalize}(\mathbf{x})\).</li>
      <li>Here, Normalize \((\cdot)\) represents a normalization operation RevIN</li>
    </ul>
  </li>
  <li><strong>Channel Mixing vs. Channel Independence</strong>
    <ul>
      <li>PatchTST에서는 Channel Independence가 좋다고 하지만
        <ul>
          <li>그건 length에 비해 channels가 많지 않을 때 이야기고,</li>
          <li>channels가 많을 때에는 Channel Mixing이 더 낫다</li>
        </ul>
      </li>
      <li>TimeMachine은 “potentially” inter-channel correlation을 잡고
        <ul>
          <li>Channel Independence일 때에는 independence를 찾음</li>
        </ul>
      </li>
      <li>input의 shape은 BML, output은 BMT</li>
    </ul>
  </li>
  <li><strong>Embedded Representations</strong>
    <ul>
      <li>2-stage embedded representation</li>
      <li>\(\mathbf{x}^{(1)}=E_1\left(\mathbf{x}^{(0)}\right), \quad \mathbf{x}^{(2)}=E_2\left(D O\left(\mathbf{x}^{(1)}\right)\right)\), where
        <ul>
          <li>\(E_1: \mathbb{R}^{M \times L} \rightarrow \mathbb{R}^{M \times n_1}\) and \(E_2: \mathbb{R}^{M \times n_1} \rightarrow \mathbb{R}^{M \times n_2}\)은 MLP</li>
          <li>DO는 dropout, (MLP 쓰니까 overfitting 방지)</li>
        </ul>
      </li>
      <li>이렇게 input length에 상관없이 fixed-length tokens로 embedding</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timemachine/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p><strong>Integrated Quadruple Mambas</strong> (fig1 보면서 이해하면 좋음)</p>

    <ul>
      <li>
        <p>\(E_1, E_2\) 각각의 embedding level에서 2개의 mamba를 사용</p>

        <ul>
          <li>\(E_1\) level에서 사용되는 2개의 mamba의 input은 \(D O\left(\mathbf{x}^{(1)}\right)\)</li>
          <li>\(E_2\) level에서 사용되는 2개의 mamba의 input은 \(D O\left(\mathbf{x}^{(2)}\right)\)</li>
        </ul>
      </li>
      <li>
        <p>첫 번째 mamba block 안에서는 2개의 FC-layers가 linear projection하고</p>

        <ul>
          <li>하나만 1d causal conv와 SiLU activation 통과, 그리고 structured SSM으로 간다</li>
          <li>그 다음 남은 하나의 linear projection을 더하고 FC-layer를 한 번 더 태움</li>
          <li>이때 <strong>continuous-time SSM</strong>은 input sequence \(u(t)\)를 latente state \(h(t)\)를 통해 output \(v(t)\)로 보낸다.
            <ul>
              <li>즉 \(d h(t) / d t=A h(t)+B u(t), \quad v(t)=C h(t)\)
                <ul>
                  <li>\(h(t)\) is \(N\)-dimensional (\(N\)은 state expansion factor)</li>
                  <li>\(u(t)\) is \(D\)-dimensional (\(D\)는 dimension factor)</li>
                  <li>\(v(t)\)의 dimension도 \(D\)</li>
                  <li>\(A\), \(B\), \(C\)는 coefficient matrices of proper size</li>
                </ul>
              </li>
              <li><strong>여기서 \(A\), \(B\), \(C\), 그리고 hidden state를 time interval \(\Delta\)에 대한 함수로 놓음</strong></li>
              <li>이것이 모델을 input에 adaptive하게 context selectivity를 강화하는 방법
                <ul>
                  <li>즉 \(h_k=\bar{A} h_{k-1}+\bar{B} u_k, \quad v_k=C h_k\)</li>
                  <li>where \(h_k, u_k\), and \(v_k\) are respectively samples of \(h(t), u(t)\), and \(v(t)\) at time \(k \Delta\),</li>
                  <li>\(\bar{A}=\exp (\Delta A), \quad \bar{B}=(\Delta A)^{-1}(\exp (\Delta A)-I) \Delta B\).</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>(continuous 말고) <strong>SSM</strong>은 \(B\), \(C\), \(\Delta\)가 input에 따라 달라짐
            <ul>
              <li>\(B, C \leftarrow \operatorname{Linear}_N(u)\), \(\Delta \leftarrow \text{softplus}(parameter +Linear _D\left(\right. Linear \left.\left._1(u)\right)\right)\)</li>
              <li>coefficient matrices는 current token을 보고 정보를 selectively propagate하게 함</li>
              <li><strong>channel-mixing case</strong>에서는 각 univariate가 token(dim=\(n_2\))이 되고
                <ul>
                  <li><strong>Inner mambas</strong>에서는 \(BMn_2\)이 나오는데</li>
                  <li>Left / right inner mamba의 k번째 변수의 output은 \(v_{L, k}, v_{R, k} \in \mathcal{R}^{n_2}\)
                    <ul>
                      <li>둘을 더하고 embedding된 \(\mathbf{x}^{(2)}\)을 skip connection하면 \(\mathbf{x}^{(3)}=\mathbf{v}_L \bigoplus \mathbf{v}_R \bigoplus \mathbf{x}^{(2)}\) (Element-wise addition)</li>
                      <li>그 다음 linear mapping \(P_1: \mathbf{x}^{(3)} \rightarrow\mathbf{x}^{(4)} \in \mathcal{R}^{M \times n_1}\)</li>
                    </ul>
                  </li>
                  <li><strong>Outer mambas</strong>에서도 비슷하게
                    <ul>
                      <li>\(v_{L, k}^*, v_{R, k}^* \in \mathcal{R}^{n_1}\)구하고 \(\mathbf{x}^{(5)} \in \mathcal{R}^{M \times n_1}\)랑 해서 셋이 더함</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li><strong>channel-independence</strong>에서는 처음에 \(B M L \mapsto(B \times M) 1 L\) 이렇게 reshape을 해서 마치 match가 \(BM\)개이고 univariates인 것처럼 처리
                <ul>
                  <li>Outer든 inner이든
                    <ul>
                      <li>mamba 하나는 input dim =1, token length =\(n_1\) or \(n_2\),</li>
                      <li>다른 하나는 input dim =\(n_1\) or \(n_2\), token length =1</li>
                    </ul>
                  </li>
                  <li>이렇게 하면 global context and local context 동시에 학습 가능하고</li>
                  <li>fine and coarse scales with high- and low-resolution 각각의 context 추출</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Channel mixing은 변수 개수가 많을 때 하고 independence랑 switch하려면</p>

        <ul>
          <li>input sequence를 그냥 transposed하면 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Output Projection</p>

    <ul>
      <li>MLP 쓰고 \(P_1\) performs a mapping \(\mathcal{R}^{M \times n_2} \rightarrow \mathcal{R}^{M \times n_1}\), \(P_2\)는 \(\mathbb{R}^{M \times 2 n_1} \rightarrow \mathbb{R}^{M \times T}\)</li>
      <li>Residual connection도 fig1처럼 해주고</li>
      <li>Outer Mambas에서 나온 \(\mathbf{x}^{(5)}\)랑 Inner Mambas에서 나온 \(\mathbf{x}^{(4)}\)를 concat해서 사용하게 됨
        <ul>
          <li>즉 \(\mathbf{x}^{(6)}=\mathbf{x}^{(5)} \|\left(\mathbf{x}^{(4)} \bigoplus \mathbf{x}^{(1)}\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-result-analysis">4. Result Analysis</h2>

<h3 id="41-datasets">4.1. Datasets</h3>

<ul>
  <li>seven benchmark datasets extensively used for LTSF:
    <ul>
      <li>Weather, Traffic, Electricity, and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2)</li>
    </ul>
  </li>
</ul>

<h3 id="42-experimental-environment">4.2. Experimental Environment</h3>

<ul>
  <li>본 논문에서 제시하는 TimeMachine을 11 SOTA models와 비교 :
    <ul>
      <li>including iTransformer, PatchTST, DLinear, RLinear, Autoformer, Crossformer, TiDE, Scinet, TimesNet, FEDformer, and Stationary</li>
    </ul>
  </li>
</ul>

<h3 id="43-quantitative-results">4.3. Quantitative Results</h3>

<p><img src="/assets/img/timeseries/timemachine/fig2.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/timemachine/fig3.png" alt="그림1" /></p>

<h3 id="44-qualitative-result">4.4. Qualitative Result</h3>

<p><img src="/assets/img/timeseries/timemachine/table2.png" alt="그림1" /></p>

<h2 id="5--hyperparameter-sensitivity-analysis-and-ablation-study">5.  Hyperparameter Sensitivity Analysis and Ablation Study</h2>

<h3 id="51-effect-of-mlps-parameters-n1-n2">5.1. Effect of MLPs’ Parameters (n1, n2)</h3>

<ul>
  <li>MLP의 size인 \(n_1, n_2\)를 다양하게 해봤는데 별 차이 없음 (fig5)
    <ul>
      <li>MLP에 heavily dependent하지 않다는 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timemachine/fig5.png" alt="그림1" /></p>

<h3 id="52-sensitivity-of-dropouts">5.2. Sensitivity of Dropouts</h3>

<ul>
  <li>Dropout ratio 적당하게 0.7 사용</li>
</ul>

<h3 id="53-ablation-of-residual-connections">5.3. Ablation of Residual Connections</h3>

<ul>
  <li>Residual connections 쓰는 것이 좋더라</li>
</ul>

<h3 id="54-effects-of-mambas-local-convolutional-width">5.4. Effects of Mambas’ Local Convolutional Width</h3>

<ul>
  <li>각 Mamba 안에도 parameters가 있을거니까 local convolutional kernel widths를 2 and 4로실험 해봤더니 2가 낫더라</li>
</ul>

<h3 id="55-ablation-on-state-expansion-factor-of-mambas">5.5. Ablation on State Expansion Factor of Mambas</h3>

<p><img src="/assets/img/timeseries/timemachine/fig6.png" alt="그림1" /></p>

<ul>
  <li>State Expansion Factor를 8부터 256까지 해봤는데 256이 제일 좋아서 defualt로 설정</li>
</ul>

<h3 id="56-ablation-on-mamba-dimension-expansion-factor">5.6. Ablation on Mamba Dimension Expansion Factor</h3>

<ul>
  <li>dimension expansion factor (\(E\))도 있었는데, 크게하면 메모리는 많이 먹는데 성능 향상으로 이어지지는 않아서 그냥 1로 둔다</li>
</ul>

<h2 id="6-strengths-and-limitations">6. Strengths and Limitations</h2>

<ul>
  <li>memory efficiency and stable performance across varying look-back and prediction lengths !</li>
  <li>Weather에서 1등 못한게 limitation. (….?ㅋㅋ)</li>
</ul>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>LTSF with linear scalability and small memory footprints !</li>
  <li>integrated quadruple-Mamba architecture
    <ul>
      <li>to predict with rich global and local contextual cues at multiple scales</li>
      <li>\(\to\) unifies channel-mixing and channel-independence situations</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ECAI 2024](https://arxiv.org/abs/2403.09898)]]></summary></entry><entry><title type="html">Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Arxiv 2023)</title><link href="http://localhost:4000/timeseries/2024-10-28-mamba/" rel="alternate" type="text/html" title="Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Arxiv 2023)" /><published>2024-10-28T00:00:00+09:00</published><updated>2024-10-28T03:20:20+09:00</updated><id>http://localhost:4000/timeseries/mamba</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-28-mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>많은 subquadratic-time architectures (linear attention, gated convolution and recurrent models, and structured state space models (SSMs))가 Transformer의 연산 효율성을 해결하기 위해 제안되었지만
    <ul>
      <li>content-based reasoning에서는 여전히 약한 모습</li>
    </ul>
  </li>
  <li>그래서 본 논문에서 제시하는 Mamba는
    <ul>
      <li><strong>SSM parameters를 input의 함수 형태</strong>로 놓아서 모델이 selectively propagate or forget information 할 수 있도록 함</li>
      <li>그리고 recurrent 모드로 학습을 진행하게 되면 중간 Hidden State 크기가 매우 커질 수 있기 때문에
        <ul>
          <li><strong>hardware-aware parallel algorithm</strong>을 사용하여 hidden State를 메모리에 저장하지 않고 병렬적으로 scan 연산함</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>Selection Mechanism</strong>
    <ul>
      <li>parameterizing the SSM parameters based on the input
        <ul>
          <li>\(\to\) 필요한 정보만 기억하고 필요없는 정보 filter out</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Hardware-aware Algorithm</strong>
    <ul>
      <li>연산 커널을 결합하는 방식(커널 융합)으로 메모리 입출력 과정을 최적화하고 오버헤드를 줄임</li>
      <li>고속 메모리(SRAM)를 활용해 느린 GPU 메모리(HBM) 의존도를 줄여 연산 속도를 높이겠다는 것</li>
      <li>backpropagation 할 때에는 hidden state를 저장하지 않고 필요할 때마다 재계산함으로써 메모리 사용량을 최소화</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/mamba/fig1.png" alt="그림1" /></p>

<ul>
  <li><strong>Architecture</strong>
    <ul>
      <li>기존 SSM architectures와 Transformer의 MLP blocks을 합쳐서 Mamba를 만듬</li>
    </ul>
  </li>
</ul>

<h2 id="2-state-space-models">2. State Space Models</h2>

<ul>
  <li>Structured state space sequence models (S4)
    <ul>
      <li>inspired by a particular continuous system :
        <ul>
          <li>1-dimensional function or sequence $x(t) \in \mathbb{R} \mapsto y(t) \in \mathbb{R}$ through an implicit latent state $h(t) \in \mathbb{R}^N$.</li>
          <li>\(\begin{aligned} h^{\prime}(t) &amp; =A h(t)+B x(t) \\ y(t) &amp; =C h(t)\end{aligned}\) (1)</li>
          <li>4개의 parameters \((\Delta, A, B, C)\)로 정의됨 (아직 input의 함수 형태가 아님)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Discretization</strong>
    <ul>
      <li>첫번째 단계는 “continuous parameters” \((\Delta, A, B)\)를 “discrete parameters” \((\bar{A}, \bar{B})\)로 바꾸는 것
        <ul>
          <li>fixed formulas \(\overline{A}=f_A(\Delta, A)\) and \(\overline{B}=f_B(\Delta, A, B)\)를 사용</li>
          <li>\(\left(f_A, f_B\right)\)는 discretization rule</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Computation</strong>
    <ul>
      <li>\((\Delta, A, B, C) \mapsto(\bar{A}, \bar{B}, C)\) 변환이 끝났으면 그 다음에는 다음 두 가지 형태의 computation 가능
        <ul>
          <li>a linear recurrence :
            <ul>
              <li>\(\begin{aligned} h_t &amp; =\overline{A} h_{t-1}+\overline{B} x_t \\ y_t &amp; =C h_t\end{aligned}\) (2) 또는</li>
            </ul>
          </li>
          <li>a global convolution :
            <ul>
              <li>\(\begin{aligned} \bar{K} &amp; =\left(C \bar{B}, C \overline{A B}, \ldots, C \bar{A}^k \bar{B}, \ldots\right) \\ y &amp; =x * \bar{K}\end{aligned}\) (3)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Linear Time Invariance (LTI)</strong>
    <ul>
      <li>위 (1) (2) (3) 모델들은 model’s dynamics가 time-invariant</li>
      <li>하지만 본 논문에서는 이러한 LTI property가 근본적인 한계가 있음을 밝히고
        <ul>
          <li>LTI를 제거하면서도 efficiency bottlenecks를 극복함을 제시함</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Structure and Dimensions</strong>
    <ul>
      <li>\(A\) matrix를 사용하기 때문에 SSM인거고, 이때 \(A \in \mathbb{R}^{N \times N}, B \in \mathbb{R}^{N \times 1}, C \in \mathbb{R}^{1 \times N}\)</li>
      <li>total hidden state has dimension은: \(𝐷𝑁\) per input</li>
      <li>the sequence length requires \(𝑂(𝐵𝐿𝐷𝑁)\)</li>
    </ul>
  </li>
</ul>

<h2 id="3-selective-state-space-models">3. Selective State Space Models</h2>

<ul>
  <li>3.1절에서는 selection mechanism을 소개하고</li>
  <li>3.2절에서는 어떻게 selection mechanism이 SSM과 같이 쓰일 수 있는지 보고</li>
  <li>3.3절에서는 hardware-aware algorithm을 알아보고</li>
  <li>3.4절에서는 simple SSM을 attention이나 MLP없이 알아보고</li>
  <li>3.5절에서는 additional properties of selection mechanisms를 논의한다</li>
</ul>

<h3 id="31-motivation-selection-as-a-means-of-compression">3.1. Motivation: Selection as a Means of Compression</h3>

<ul>
  <li>sequence modeling의 본질적인 문제는 <strong>small state에 context를 압축</strong>하는 것
    <ul>
      <li>Trade off : 압축을 안하면 inefficient하고(Transformer) efficient하면 압축을 너무 많이 하고(RNN)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/mamba/fig2.png" alt="그림1" /></p>

<ul>
  <li>Synthetic task 2가지
    <ul>
      <li><strong>Selective Copying</strong>
        <ul>
          <li>필요한 tokens와 아닌 것들을 구별하기 위해 content-aware reasoning하는 task</li>
        </ul>
      </li>
      <li><strong>Induction Heads</strong>
        <ul>
          <li>다음에 뭐가 올지 추론하기 위해 context-aware reasoning하는 task</li>
        </ul>
      </li>
      <li>이 두 가지는 위에서 소개한 LTI mode로는 하기 어렵다</li>
    </ul>
  </li>
  <li>결국에는 efficient하려면 small state를 가져야 하는데, 그걸 “잘” 하려면 selectivity를 “잘” 해야 함</li>
</ul>

<h3 id="32--improving-ssms-with-selection">3.2.  Improving SSMs with Selection</h3>

<ul>
  <li>본 논문에서 소개하는 selection mechanism은 model의 parameters를 input-dependent하게 만드는 것
    <ul>
      <li>\(\Delta, B, C\)을 length dimension \(L\)로 만듬 (즉 time-invariant에서 time-varying으로)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/mamba/algorithm12.png" alt="그림1" /></p>

<h3 id="33-efficient-implementation-of-selective-ssms">3.3 Efficient Implementation of Selective SSMs</h3>

<ul>
  <li>Convolution이나 attention처럼 GPU-friendly하게 Selective SSM을 만들고 싶은 거고</li>
  <li>즉 시간에 따라 필요한 정보를 선택적으로 처리하겠다는 것. 그러면 더 빠르게 긴 시퀀스를 처리</li>
</ul>

<p><strong>3.3.1 Motivation of Prior Models</strong></p>

<ul>
  <li>paying speed and memory costs 없이 maximize hidden state dimension하고 싶음</li>
  <li>Recurrent mode는 hidden이 input보다 훨씬 커서 메모리 사용량이 많음
    <ul>
      <li>그래서 input의 shape (=output의 shape)과 같은 conv를 쓰겠다</li>
    </ul>
  </li>
  <li>기존 LTI는 데이터 특성을 잘 반영 못했지만 Mamba는 순환적 요소와 컨볼루션적 요소를 동시에 사용해 모델의 효율성을 극대화 !</li>
</ul>

<p><strong>3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion</strong></p>

<ul>
  <li>LTI의 한계를 극복하는 selection mechanism을 소개:</li>
  <li>문제는 (1) the sequential nature of recurrence, and (2) the large memory usage
    <ul>
      <li>(2) the large memory usage는 kernel fusion으로 해결
        <ul>
          <li>scan input \((\bar{A}, \bar{B})\) of size \((B, L, D, N)\)을 HBM에 저장하는 것이 아니라</li>
          <li>the SSM parameters \((\triangle, A, B, C)\)의 final output \((B, L, D)\)만 저장</li>
          <li>discretization이랑 recurrence는 SRAM에서 수행</li>
        </ul>
      </li>
      <li>(1) the sequential nature of recurrence는 recomputation으로 해결
        <ul>
          <li>intermediate states를 저장하지 않는데 이건 backpropagation에서 필요하니까</li>
          <li>그냥 다시 계산함 (recomputation)</li>
          <li>그 결과 FlashAttention과 유사한 memory efficiency</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="34-a-simplified-ssm-architecture">3.4 A Simplified SSM Architecture</h3>

<ul>
  <li>Mamba는 linear attention과 MLP를 결합해서 gated attention unit(GAP)처럼 만듬</li>
  <li>model dimension을 D에서 expansion factor E를 사용해서 늘려줌
    <ul>
      <li>대부분의 model parameters \(3ED^2\)개 \(2ED^2\)개가 input projection에, \(ED^2\)개가 output projection에 있음</li>
      <li>반면 SSM 안에는 parameters가 별로 없는데, Mamba는 이걸 반복해서 사용하기 때문에 효율적이다</li>
    </ul>
  </li>
</ul>

<h3 id="35-properties-of-selection-mechanisms">3.5 Properties of Selection Mechanisms</h3>

<ul>
  <li>The selection mechanism은 RNN이나 CNN에 쓸 수 있는 broader concept임</li>
</ul>

<p><strong>3.5.1 Connection to Gating Mechanisms</strong></p>

<ul>
  <li>SSM의 게이트 역할을 하는 \(\Delta\)가 RNN의 게이트와 유사하게 작동
    <ul>
      <li>입력된 정보 중 어떤 것을 유지하고 어떤 것을 버릴지 결정하는 역할인 점도 비슷</li>
      <li>When $N=1, A=-1, B=1, s_{\Delta}=\operatorname{Linear}(x)$, and $\tau_{\Delta}=$ softplus,</li>
      <li>\(\begin{aligned} &amp; g_t=\sigma\left(\operatorname{Linear}\left(x_t\right)\right) \\ &amp; h_t=\left(1-g_t\right) h_{t-1}+g_t x_t\end{aligned}\) .</li>
      <li>이런 식으로 \(g_t\)가 현재 입력 \(x_t\)가 얼마나 중요한지 표현하게 하고
        <ul>
          <li>\(g_t\)가 1에 가까울수록 \(x_t\)를 많이 반영, 0에 가까울수록 이전 state \(h_{t-1}\)을 많이 반영</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>3.5.2 Interpretation of Selection Mechanisms</strong></p>

<ul>
  <li><strong>Variable Spacing</strong>
    <ul>
      <li>Selectivity의 역할은 filtering out irrelevant noise tokens that may occur between inputs of interest</li>
    </ul>
  </li>
  <li><strong>Filtering Context</strong>
    <ul>
      <li>context가 길어진다고 성능이 좋아지는 것이 아님. 대부분의 모델이 너무 긴 sequence에서 불필요한 정보를 제거하지 못해서 성능 저하가 발생</li>
      <li>selective model은 state를 언제든 초기화 할 수 있으니 긴 sequence가 들어왔을 때 성능이 더 좋아지도록 작동</li>
    </ul>
  </li>
  <li><strong>Boundary Resetting</strong>
    <ul>
      <li>LTI는 sequence의 경계에서 정보가 섞이는 문제가 있었는데, selective SSM은 그런 문제 없음
        <ul>
          <li>언제든지 state를 초기화할 수 있으니 그냥 boundaries에서 초기화 하면 됨 (\(g_t=1\))</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Interpretation of \(\Delta\)</strong>
    <ul>
      <li>\(\Delta\)가 크면 다 잊고 현재 정보를 위주로 state를 만드는거고, 작으면 이전 state를 유지</li>
    </ul>
  </li>
  <li><strong>Interpretation of A</strong>
    <ul>
      <li>사실 \(\bar{A}=\exp (\Delta A)\)도 \(\Delta\)를 통해 만들어지니 크게 건드리지 말고 단순하게 둔다</li>
    </ul>
  </li>
  <li><strong>Interpretation of 𝑩 and 𝑪.</strong>
    <ul>
      <li>결국 Selectivity의 역할은 filtering out.</li>
      <li>B와 C는 입력을 상태로 전달할지, 상태를 출력으로 내보낼지를 결정</li>
      <li>모델이 state(context)를 더 세밀하게 제어할 수 있음</li>
    </ul>
  </li>
</ul>

<h3 id="36-additional-model-details">3.6 Additional Model Details</h3>

<p>pass</p>

<h2 id="4-empirical-evaluation">4. Empirical Evaluation</h2>

<h3 id="41-synthetic-tasks">4.1 Synthetic Tasks</h3>

<p><strong>4.1.1 Selective Copying</strong></p>

<p><strong>4.1.2 Induction Heads</strong></p>

<p><img src="/assets/img/timeseries/mamba/table12.png" alt="그림1" /></p>

<h3 id="42-language-modeling">4.2. Language Modeling</h3>

<p><img src="/assets/img/timeseries/mamba/table3.png" alt="그림1" /></p>

<h3 id="45-speed-and-memory-benchmarks">4.5 Speed and Memory Benchmarks</h3>

<p><img src="/assets/img/timeseries/mamba/fig8.png" alt="그림1" /></p>

<h3 id="46-model-ablations">4.6. Model Ablations</h3>

<p><img src="/assets/img/timeseries/mamba/table6.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/mamba/table78.png" alt="그림1" /></p>

<h2 id="5-discussion">5. Discussion</h2>

<p>Pass</p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>a selection mechanism to structured state space models
    <ul>
      <li>to perform context-dependent reasoning</li>
      <li>Without attention ! (simple attention-free architecture)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2312.00752)]]></summary></entry><entry><title type="html">T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)</title><link href="http://localhost:4000/timeseries/2024-10-27-T-PATCHGNN/" rel="alternate" type="text/html" title="T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)" /><published>2024-10-27T00:00:00+09:00</published><updated>2024-10-27T11:03:31+09:00</updated><id>http://localhost:4000/timeseries/T-PATCHGNN</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-27-T-PATCHGNN/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>transforms each univariate irregular time series into a series of transformable patches</li>
      <li>local semantics capture와, inter-time series correlation modeling는 하면서</li>
      <li>avoiding sequence <strong>length explosion</strong> in aligned IMTS (무슨 의미인지 1. introduction (3)에서 설명)</li>
    </ul>
  </li>
  <li>Time-adaptive graph neural networks으로 time-varying adaptive graphs를 학습해서
    <ul>
      <li>dynamic intertime series correlation를 표현</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Multivariate Time Series (IMTS)의 특징은 irregular sampling intervals and missing data</li>
  <li>Irregularity within the series and asynchrony 때문에 다루기 어려움
    <ul>
      <li>ODE로 풀려고 한 적은 있지만 numerical integration process으로 인해 computationally expensive</li>
    </ul>
  </li>
  <li>IMTS forecasting의 어려움에는 3가지 이유가 있음</li>
  <li>첫번째는 (1) irregularity in intra-time series dependency modeling
    <ul>
      <li><strong>varying time intervals</strong> between adjacent observations이 the consistent flow of time series data를 방해</li>
    </ul>
  </li>
  <li>두번째는 (2) asynchrony in intertime series correlation modeling
    <ul>
      <li><strong>misaligned at time</strong> due to irregular sampling or missing data.</li>
    </ul>
  </li>
  <li>가장 중요한 건 (3) sequence length explosion with the increase of variables
    <ul>
      <li>아래 fig1처럼 “단 하나의 변수라도 기록된 time stamp”는 모두 존재하는 걸로 해버리면, 변수 개수가 늘어남에 다라 time stamps의 수가 너무 많아지는 문제. (이러한 방법을 canonical pre-alignment representation이라고 부름)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TPatchGNN/fig1.png" alt="그림1" /></p>

<ul>
  <li>그래서 본 논문에서 제시하는 T-PATCHGNN의 장점은
    <ul>
      <li>첫째로 The independent patching process for each univariate irregular time series으로 representation에서 sequence length explosion의 risk를 없애고</li>
      <li>둘째로 local semantics를 잘 잡기 위해 putting each individual observation into patches with richer context</li>
      <li>셋째로 transformable patching 후에 IMTS is naturally aligned in a consistent patch-level temporal resolution</li>
    </ul>
  </li>
  <li>본 논문의 contribution은 :
    <ul>
      <li>New transformable patching method to transform each univariate irregular time series of IMTS into a series of variable-length yet time-aligned patches</li>
      <li>transformable patching outcomes을 바탕으로,  time-adaptive graph neural networks를 제안</li>
      <li>building a benchmark for IMTS forecasting evaluation</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2, Related Works</h2>

<h3 id="21-irregular-multivariate-time-series-forecasting">2.1. Irregular Multivariate Time Series Forecasting</h3>

<p>pass</p>

<h3 id="22-irregular-multivariate-time-series-representation">2.2. Irregular Multivariate Time Series Representation</h3>

<ul>
  <li>기존에는 time-aligned manner로 IMTS를 representation (pre-alignment representation method)
    <ul>
      <li>즉 하나의 변수라고 기록된 time stamp는 존재하는 걸로 생각하니</li>
      <li>sequence length that equals the number of all unique time stamps in IMTS</li>
      <li>예를 들어 변수 1은 1,3,5 시점에 기록되고 변수 2는 2,4,6 시점에 기록되면 unique time stamps의 개수는 6이 됨</li>
      <li>sequence length explosion problem 발생</li>
    </ul>
  </li>
</ul>

<h3 id="23-graph-neural-networks-for-multivariate-time-series">2.3. Graph Neural Networks for Multivariate Time Series</h3>

<ul>
  <li>
    <p>2018년 DCRNN, STGCN은 pre-defined graph structures를 사용해서 실제로 쓰기 어려웠고</p>
  </li>
  <li>2019년부터 data로부터 graph structures를 학습하는 방식을 사용
    <ul>
      <li>하지만 IMTS에서는 잘 작동을 안 함. mimisalignment at times으로 인해 inter-time series correlation modeling이 잘 안 됨</li>
    </ul>
  </li>
  <li>Raindrop(2021)[<a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">paper review</a>]
    <ul>
      <li>이 문제를 propagation the asynchronous observations at all the timestamps로 해결하려고 했지만  sequence length explosion problem을 피할 수 없음</li>
    </ul>
  </li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<h3 id="31-problem-definition">3.1. Problem Definition</h3>

<h3 id="definition-1">Definition 1</h3>

<ul>
  <li>Irregular Multivariate Time Series
    <ul>
      <li>\(\mathcal{O}=\left\{\mathbf{o}_{1: L_n}^n\right\}_{n=1}^N=\left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\), where</li>
      <li>\(N\)개의 변수가 있고 \(n\)번째 변수는 \(L_n\)개의 observations가 있고, \(n\)번째 변수의 \(i\)번째 변수의 값은 \(t_i^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="definition-2">Definition 2</h3>

<ul>
  <li>Forecasting Query \(q_j^n\)
    <ul>
      <li>\(j\)-th query on \(n\)-th variable to predict its corresponding value at a future time \(q_j^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="problem-1">Problem 1</h3>

<ul>
  <li>Irregular Multivariate Time Series Forecasting
    <ul>
      <li>IMTS \(\mathcal{O} =  \left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\)와 Forecasting query \(\mathcal{Q}=\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)가 있을 때,</li>
      <li>problem은 accurately forecast recorded values \(\hat{\mathcal{X}}=\left\{\left[\hat{x}_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\) in correspondence to the forecasting queries</li>
      <li>\(\mathcal{F}(\mathcal{O}, \mathcal{Q}) \longrightarrow \hat{\mathcal{X}}\)로 표현됨</li>
    </ul>
  </li>
</ul>

<h3 id="32-canonical-pre-alignment-representation-for-imts">3.2. Canonical Pre-Alignment Representation for IMTS</h3>

<ul>
  <li>2.2. Irregular Multivariate Time Series Representation 참고</li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<p><img src="/assets/img/timeseries/TPatchGNN/fig2.png" alt="그림1" /></p>

<h3 id="41-irregular-time-series-patching">4.1. Irregular Time Series Patching</h3>

<ul>
  <li>모든 univariate TS에 같은 patching operation을 하니까 변수 index 표기는 생략</li>
</ul>

<h3 id="411-transformable-patching">4.1.1. TRANSFORMABLE PATCHING</h3>

<ul>
  <li>Time series patching이 forecasting에 좋은 방법이라는 건 알려진 사실. benefits in :
    <ul>
      <li>capturing local semantic information,</li>
      <li>reducing computation and memory usage,</li>
      <li>modeling longer-range historical observations</li>
    </ul>
  </li>
  <li>일반적으로 time series patching은 하나의 patch에 같은 숫자의 observations가 있는데,
    <ul>
      <li>IMTS에서 time intervals는 다양하기 때문에 이러한 방식이 적절하지 않음</li>
    </ul>
  </li>
  <li>그래서 patch에 같은 개수의 observataions가 아니라, unified time horizon이 들어가도록 함
    <ul>
      <li>patch 안에 들어가는 observations의 개수는 다를 수 있지만, ex) 2시간인 건 동일하도록</li>
    </ul>
  </li>
  <li>patch는 \(\left[\mathbf{o}_{l_p: r_p}\right]_{p=1}^P\)로 표현되고 \(P\)</li>
</ul>

<h3 id="412-patch-encoding">4.1.2. PATCH ENCODING</h3>

<ul>
  <li><strong>Continuous time embedding</strong>
    <ul>
      <li>\(\phi(t)[d]=\left\{\begin{array}{lll}
\omega_0 \cdot t+\alpha_0, &amp; \text { if } &amp; d=0 \\
\sin \left(\omega_d \cdot t+\alpha_d\right), &amp; \text { if } &amp; 0&lt;d&lt;D_t
\end{array}\right.\).
        <ul>
          <li>where the \(\omega_d\) and \(\alpha_d\) are learnable parameters and \(D_t\) is embedding’s dimension</li>
        </ul>
      </li>
      <li>Concatenation하면 observations in the patch:
        <ul>
          <li>\(\mathbf{z}_{l_p: r_p}=\left[z_i\right]_{i=l_p}^{r_p}=\left[\phi\left(t_i\right) \| x_i\right]_{i=l_p}^{r_p}\).</li>
          <li>이건 하나의 patch에 대한 표현이 되는 것 !</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Transformable time-aware convolution</strong>
    <ul>
      <li>input sequence의 길이에 맞게 (adaptively), generated parameters와 transformable filter size를 사용</li>
      <li>\(\mathbf{f}_d=\left[\frac{\exp \left(\mathbf{F}_d\left(z_i\right)\right)}{\sum_{j=1}^{L_p} \exp \left(\mathbf{F}_d\left(z_j\right)\right)}\right]_{i=1}^{L_p}\)으로 표현됨
        <ul>
          <li>where \(L_p\) is the sequence length of patch \(\mathbf{z}_{l_p: r_p}, \mathbf{f}_d \in \mathbb{R}^{L_p \times D_{i n}}\) is the derived filter for \(d\)-th feature map, \(D_{i n}\) is dimension of inputs, and \(\mathbf{F}_d\) denotes the meta-filter that can be instantiated by learnable neural networks</li>
          <li>이건 filter의 parameters를 along the temporal dimension으로 normalizaing해서 consistent scaling 하겠다는 것</li>
        </ul>
      </li>
      <li>위 식으로 \(D-1\)개의 filters를 사용해서 <strong>latent patch embedding</strong> \(h_p^c \in \mathbb{R}^{D-1}\)를 얻음 :
        <ul>
          <li>\(h_p^c=\left[\sum_{i=1}^{L_p} \mathbf{f}_d[i]^{\top} \mathbf{z}_{l_p: r_p}[i]\right]_{d=1}^{D-1}\).</li>
          <li>이건  encoded transformable patches:
            <ul>
              <li>variable-length sequences에 따라 flexibility를 가지고</li>
              <li>parameterization for varying time intervals을 하면서</li>
              <li>additional learnable filter parameters 없이 더 긴 시퀀스를 처리할 수 있음</li>
            </ul>
          </li>
          <li>마지막으로 \(h_p=\left[h_p^c \| m_p\right]\) 이렇게 patch에 masking을 덧붙여주는데,
            <ul>
              <li>\(m_p\)는 이 patch 안에 observations가 하나 이상 있다~를 indicator로 표현</li>
            </ul>
          </li>
          <li>최종적으로 \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)를 얻는다.</li>
          <li>이건 \(P\)개의 patch를 \(D-1\)차원으로 표현하고 마지막에는 masking으로 indicator를 붙인 것</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-intra--and-inter-time-series-modeling">4.2. Intra- and Inter-Time Series Modeling</h3>

<ul>
  <li>이제 이  transformable patching을 irregular time series를 intra- and inter-time series modeling하는지 알아보자</li>
</ul>

<h3 id="421-transformer-to-model-sequential-patches">4.2.1. TRANSFORMER TO MODEL SEQUENTIAL PATCHES</h3>

<ul>
  <li>위에서 구한 \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)를 Transformer에 넣는다.</li>
  <li>먼저 positional encoding을 하고
    <ul>
      <li>\(\mathbf{x}_{1: P}^{t f, n}=\mathbf{h}_{1: P}^n+\mathbf{P E}_{1: P}\).</li>
    </ul>
  </li>
  <li>Q, K, V를 만들어서 MHA를 통과한다.
    <ul>
      <li>\(\mathbf{q}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^Q\) / \(\mathbf{k}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^K\) / \(\mathbf{v}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^V\) where \(\mathbf{W}_h^Q, \mathbf{W}_h^K, \mathbf{W}_h^V \in \mathbb{R}^{D \times(D / H)}\)</li>
      <li>\(\mathbf{h}_{1: P}^{t f, n}=\|_{h=1}^H \operatorname{Softmax}\left(\frac{\mathbf{q}_h^n \mathbf{k}_h^{n T}}{\sqrt{D / H}}\right) \mathbf{v}_h^n \in \mathbb{R}^{P \times D}\),</li>
    </ul>
  </li>
</ul>

<h3 id="422-time-varying-adaptive-graph-structure-learning">4.2.2. TIME-VARYING ADAPTIVE GRAPH STRUCTURE LEARNING</h3>

<ul>
  <li>한 변수를 예측하기 위해서 다른 변수의 정보는 매우 유용할 수가 있음</li>
  <li>하지만 IMTS에서는 misaligned at times으로 인해 correlation modeling이 어려움
    <ul>
      <li>그렇다고 Raindrop처럼 하기엔 e sequence length explosion problem이 발생</li>
    </ul>
  </li>
  <li>그래서 <strong>transformable patching</strong>으로 해결
    <ul>
      <li>patch를 observations의 개수가 아니라 시간 길이를 기준으로 끊다보니</li>
      <li>각 변수는 같은 숫자의 patches로 이루어지니까</li>
      <li>time-adaptive graph neural networks로 inter-time series correlation를 modeling할 수 있음</li>
    </ul>
  </li>
  <li>즉 IMTS의  dynamic correlations를 파악하기 위해서는
    <ul>
      <li>series of time-varying adaptive graphs를 학습하겠다는 것이고</li>
      <li>지금 문제는 variable embedding이 training에서는 update 가능하지만 inference에서는 static</li>
      <li>그러니 learnable \(\mathbf{E}_1^s, \mathbf{E}_2^s \in \mathbb{R}^{N \times D_g}\)를 사용해서</li>
      <li>우리가 지금까지 만들었던  time-varying patch embedding \(\mathbf{H}_p^{t f}=\left[\mathbf{h}_p^{t f, n}\right]_{n=1}^N \in \mathbb{R}^{N \times D}\)을
        <ul>
          <li>static variable embedding으로 만들면 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>그 gated adding operation은 다음과 같음
    <ul>
      <li>\(\begin{gathered}
\mathbf{E}_{p, k}=\mathbf{E}_k^s+g_{p, k} * \mathbf{E}_{p, k}^d, \\
\mathbf{E}_{p, k}^d=\mathbf{H}_p^{t f} \mathbf{W}_k^d, \\
g_{p, k}=\operatorname{ReLU}\left(\tanh \left(\left[\mathbf{H}_p^{t f} \| \mathbf{E}_k^s\right] \mathbf{W}_k^g\right)\right) \\
k=\{1,2\}
\end{gathered}\), where
        <ul>
          <li>\(\mathbf{W}_k^d \in \mathbb{R}^{D \times D_g}, \mathbf{W}_k^g \in \mathbb{R}^{\left(D+D_g\right) \times 1}\) are learnable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이제 time-varying adaptive graph structure를 다음과 같이 얻음 : \(\mathbf{A}_p=\operatorname{Softmax}\left(\operatorname{ReLU}\left(\mathbf{E}_{p, 1} \mathbf{E}_{p, 2}^T\right)\right)\)</li>
</ul>

<h3 id="423-gnns-to-model-inter-time-series-correlation">4.2.3. GNNS TO MODEL INTER-TIME SERIES CORRELATION</h3>

<ul>
  <li>다음으로 dynamic inter-time series correlation at a patch-level resolution을 얻음
    <ul>
      <li>\(\mathbf{H}_p=\operatorname{ReLU}\left(\sum_{m=0}^M\left(\mathbf{A}_p\right)^m \mathbf{H}_p^{t f} \mathbf{W}_m^{g n n}\right) \in \mathbb{R}^{N \times D}\).</li>
      <li>where $M$ is the number of layers for GNNs, and $\mathbf{W}_m^{g n n} \in$ $\mathbb{R}^{D \times D}$ are learnable parameters at $m$-th layer.</li>
    </ul>
  </li>
</ul>

<h3 id="43-imts-forecasti">4.3. IMTS Forecasti</h3>

<ul>
  <li>이제 final latent representation을 얻는다 :
    <ul>
      <li>\(\mathbf{H}=\text { Flatten }\left(\left[\mathbf{H}_p\right]_{p=1}^P\right) \mathbf{W}^f \in \mathbb{R}^{N \times D_o}\), where  \(\mathbf{W}^f \in \mathbb{R}^{P D \times D_o}\) are learnable parameters.</li>
      <li>각 변수마다 이 representation을 얻는다</li>
    </ul>
  </li>
  <li>n-번째 변수의 final latent representation \(\mathbf{H}^n \in \mathbf{H}\)과, forecasting query \(\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)를 가지고 MLP에 넣는다</li>
  <li>
    <p>\(\hat{x}_j^n=\operatorname{MLP}\left(\left[\mathbf{H}^n \| \phi\left(q_j^n\right)\right]\right)\).</p>
  </li>
  <li>모델은 각 변수의 예측의 MSE를 줄이는 방향으로 학습
    <ul>
      <li>\(\mathcal{L}=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2\).</li>
    </ul>
  </li>
</ul>

<h3 id="44-analysis-on-scalabil">4.4. Analysis on Scalabil</h3>

<ul>
  <li>The average sequence length : \(L_{t p}=L_{a v g} \leq L_{\max } \leq L_{c p r} \leq N \times L_{a v g}\), where
    <ul>
      <li>\(L_{\text {avg }}=\frac{1}{N} \sum_{n=1}^N L_n\).</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-experimental-setup">5.1. Experimental Setup</h3>

<ul>
  <li>Dataset :
    <ul>
      <li>PhysioNet, MIMIC, Human Activity, and USHCN</li>
      <li>training, validation, and test sets adhering to ratios of 60%, 20%, and 20%</li>
    </ul>
  </li>
  <li>Evaluation Metric :
    <ul>
      <li>\(\begin{aligned}
\text { MSE }&amp;=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2, \\\text { MAE }&amp;=
 \frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left|\hat{x}_j^n-x_j^n\right| .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h4 id="52-main-results">5.2. Main Results</h4>

<p><img src="/assets/img/timeseries/TPatchGNN/table1.png" alt="그림1" /></p>

<h3 id="53-ablation-study">5.3. Ablation Study</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table2.png" alt="그림1" /></p>

<h3 id="54-scalability-and-efficiency-analysis">5.4. Scalability and Efficiency Analysis</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table3.png" alt="그림1" /></p>

<h3 id="55-effect-of-patch-size">5.5. Effect of Patch Size</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/fig4.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>achieved the alignment between asynchronous IMTS
        <ul>
          <li>by transforming each univariate irregular time series into a series of transformable patches with varying observation counts but maintaining unified time horizon resolution.</li>
          <li>without a canonical pre-alignment representation process, preventing the aligned sequence length from explosively growing</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2024](https://openreview.net/pdf?id=UZlMXUGI6e)]]></summary></entry><entry><title type="html">Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-10-03-Mamba360/" rel="alternate" type="text/html" title="Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)" /><published>2024-10-03T00:00:00+09:00</published><updated>2024-10-03T20:01:49+09:00</updated><id>http://localhost:4000/timeseries/Mamba360</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-03-Mamba360/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Sequence modeling에서 RNN, LSTM을 사용했었음</li>
  <li>Transformer가 훌륭한 성능을 보여주었음
    <ul>
      <li>but \(O(N^2)\) complexity,inductive bias handling이 어려움</li>
    </ul>
  </li>
  <li>본 논문에서는 State Space Model (SSM)를 크게 3가지 카테고리로 분류
    <ul>
      <li>Gating architectures</li>
      <li>Structural architectures</li>
      <li>Recurrent architectures</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>RNN
    <ul>
      <li>look at only the last state and current input for predicting the next state</li>
      <li>gradient calculations being limited to the hidden state and current input</li>
      <li>exploding or vanishing gradient problem</li>
      <li>lack sufficient memory for long sequences</li>
    </ul>
  </li>
  <li>LSTM
    <ul>
      <li>complexity with their gating mechanisms</li>
      <li>exhibit challenges in transfer learning</li>
    </ul>
  </li>
  <li>Transformer
    <ul>
      <li>enable each token to interact with every other token in the input sequence</li>
      <li>but \(O(N^2)\) complexity</li>
    </ul>
  </li>
  <li>State Space Model (SSM)
    <ul>
      <li>Understanding of State Space Models (SSMs) : mathematical fundamentals</li>
      <li>Categorization and Recent Advances of SSMs : systematic categorization</li>
      <li>Application of SSMs Across Domains : utility in domains</li>
      <li>Performance Comparison of SSMs with Transformers : SSM과 Transformer 비교</li>
    </ul>
  </li>
</ul>

<h2 id="2-basics-of-state-space-model">2. Basics of State Space Model</h2>

<ul>
  <li>High-order를 first-order derivatives와 vector quantities로 representation</li>
  <li>Dynamics of  damped mass-spring system : \(m \frac{d^2 y(t)}{d t^2}+c \frac{d y(t)}{d t}+k y(t)=u(t)\)
    <ul>
      <li>\(u(t)\) : 질량에 작용하는 외부 힘</li>
      <li>\(y(t)\) : 수직 위치</li>
      <li>\(x(t)\) : 이 방정식을 1차 미분과 벡터 양으로 표현하기 위해 도입하는 벡터</li>
    </ul>
  </li>
</ul>

<h3 id="21-spring-mass-damper-system">2.1. Spring Mass-Damper system</h3>

<ul>
  <li>State Variables
    <ul>
      <li>\(x_1\) : equilibrium으로부터 질량의 위치</li>
      <li>\(\dot{x_1}\) : 질량의 속도</li>
    </ul>
  </li>
  <li>System Dynamics
    <ul>
      <li>뉴턴의 제 2법칙으로 표현하면 \(m \ddot{x}_1=-k x_1-c \dot{x}_1\)</li>
      <li>\(\ddot{x_1}\)는 질량의 가속도, \(-kx_1\)은 위치에 비례하는 스프링의 힘,</li>
      <li>\(c\dot{x_1}\)은 속도에 비례하는 damping force (운동 에너지 감쇠시키는 힘)</li>
    </ul>
  </li>
  <li>State-Space Formulation
    <ul>
      <li>State vector \(x \in \mathbb R^n\) : 시스템의 내부 상태 변수</li>
      <li>Input vector \(u\in \mathbb R^m\) : 시스템에 대한 제어 또는 외부 입력</li>
      <li>Output vector \(y \in \mathbb R^p\) : 관심 있는 측정 가능한 양</li>
      <li>System dynamics : 일차 미분 방정식으로 표현 \(\dot{\mathbf x}=\mathbf A \mathbf x+\mathbf B \mathbf u\)
        <ul>
          <li>\(\mathbf x=\left[x_1, \dot{x}_1\right]^T\)는 state vector, \(\mathbf u\)는 input,</li>
          <li>\(\mathbf A\in\mathbb R^{n \times n}\)는 dynamic matrix \(\mathbf A=\left[\begin{array}{cc}
0 &amp; 1 \\
-\frac{k}{m} &amp; -\frac{c}{m}
\end{array}\right]\)</li>
          <li>\(\mathbf B \in \mathbb R^{n \times m}\)은 input matrix \(\mathbf B=\left[\begin{array}{c}
0 \\
\frac{1}{m}
\end{array}\right] \dot{\mathbf x}\)</li>
          <li>Output equation : \(\mathbf{y}=\mathbf{C x}+\mathbf{D u}\)
            <ul>
              <li>\(\mathbf{C} \in \mathbb{R}^{p \times n}\)는 output or sensor matrix</li>
              <li>\(\mathbf{D} \in \mathbb{R}^{p \times m}\)는 feedthrough matrix</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-state-space-model">2.2. State Space Model</h3>

<ul>
  <li>Definition
    <ul>
      <li>Discrete-time dynamical system :
        <ul>
          <li>\(x(t+1)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t=0,1,2,\)…</li>
          <li>\(x(t) \in \mathbb{R}^n\) : t시점에서 state</li>
          <li>\(u(t) \in\mathbb{R}^p\) : control variables</li>
          <li>\(y(t) \in\mathbb{R}^k\)​ : specific outputs of interest</li>
        </ul>
      </li>
      <li>Continuous-time model
        <ul>
          <li>\(\frac{d}{d t} x(t)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t \geq 0\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/fig3.png" alt="그림1" /></p>

<ul>
  <li>Model Formulation
    <ul>
      <li>Complexity 때문에 Multi-head self-attention 대신 SSM 사용</li>
      <li>Continuous-time Latent State space는 linear ordinary differential equation으로 표현
        <ul>
          <li>\(\begin{aligned} \dot{x}(t) &amp; =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
y(t) &amp; =\boldsymbol{C} x(t)+\boldsymbol{D} u(t) \end{aligned}\),</li>
          <li>evolution parameter \(A \in \mathcal{R}^{N \times N}\)</li>
          <li>projection parameter \(B \in \mathcal{R}^{N \times 1} \text { and } C \in \mathcal{R}^{N \times 1}\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Discrete-time SSM
    <ul>
      <li>continuous parameters \(A, B, C\) 를 discrete하게 바꾸기 위해 time-scale parameter \(\Delta\) 사용</li>
      <li>즉 \(\bar{A}=f_A(\Delta, A), \bar{B}=f_B(\Delta, A, B)\)​</li>
      <li>\(\begin{array}{lll}
x_k=\overline{\boldsymbol{A}} x_{k-1}+\overline{\boldsymbol{B}} u_k &amp; \overline{\boldsymbol{A}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1}(\boldsymbol{I}+\Delta / 2 \cdot \boldsymbol{A}) &amp; \\
y_k=\overline{\boldsymbol{C}} x_k &amp; \overline{\boldsymbol{B}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1} \Delta \boldsymbol{B} &amp; \overline{\boldsymbol{C}}=\boldsymbol{C}\end{array}\)​.</li>
      <li>원래는 위처럼 생겼음</li>
    </ul>
  </li>
  <li>Convolutional Kernel Representation
    <ul>
      <li>하지만 위 식은 sequential nature 때문에 trainable하지 않음</li>
      <li>그래서 아래처럼 continuous convolution을 사용</li>
      <li>\(\begin{array}{lll}
x_0=\overline{\boldsymbol{B}} u_0 &amp; x_1=\overline{\boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{B}} u_1 &amp; x_2=\overline{\boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{B}} u_2 \\
y_0=\overline{\boldsymbol{C} B} u_0 &amp; y_1=\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{C} B} u_1 &amp; y_2=\overline{\boldsymbol{C} \boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{C} B} u_2
\end{array}\).</li>
      <li>vectorize하면 아래와 같음</li>
      <li>\(\begin{aligned}
y_k &amp; =\overline{\boldsymbol{C A}}^k \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C A}}^{k-1} \overline{\boldsymbol{B}} u_1+\cdots+\overline{\boldsymbol{C} \boldsymbol{A B}} u_{k-1}+\overline{\boldsymbol{C} \boldsymbol{B}} u_k \\
y &amp; =\overline{\boldsymbol{K}} * u \\
\overline{\boldsymbol{K}} \in \mathbb{R}^L: &amp; =\mathcal{K}_L(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}}):=\left(\overline{\boldsymbol{C}}^i \overline{\boldsymbol{B}}\right)_{i \in[L]}=\left(\overline{\boldsymbol{C B}}, \overline{\boldsymbol{C} \boldsymbol{A B}}, \ldots, \overline{\boldsymbol{C}}^{L-1} \overline{\boldsymbol{B}}\right) .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h2 id="3-recent-advances-in-state-space-models">3. Recent Advances in State Space Models</h2>

<ul>
  <li>Transformer의 limitations :
    <ul>
      <li>Computational Complexity</li>
      <li>Large Memory Requirements : for storing embeddings and intermediate actiavations</li>
      <li>Fixed Sequence Length : du to positional embeddings</li>
      <li>Attention Mechanism Scalability : quadratic scaling with input length</li>
      <li>Lack of Causality in Standard Attention : not inherently capture causality</li>
    </ul>
  </li>
  <li>SSM의 categorization : 어떻게 long sequence를 다룰 것인가
    <ul>
      <li>Structured SSMs : based on S4 and variants</li>
      <li>Recurrent SSMs : based on RNNs and variants</li>
      <li>Gated SSMs : leveraging gating techniques</li>
      <li>Miscellaneous SSMs : 기타 다양한 방법들</li>
    </ul>
  </li>
</ul>

<h3 id="31-structured-ssms">3.1. Structured SSMs</h3>

<ul>
  <li>
    <p>S4, HiPPO, H3, Liquid-S4 등…</p>
  </li>
  <li>
    <p>long-range dependency를 효율적으로 파악하기 위해 다음과 같은 방법 사용 :</p>

    <ul>
      <li>
        <p>polynomial projection operators</p>
      </li>
      <li>
        <p>multi-input multi-output systems</p>
      </li>
      <li>
        <p>and convolutional kernels</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="311-structured-state-space-sequence-s4">3.1.1. Structured State Space Sequence (S4)</h3>

<ul>
  <li>Higher-Order Polynomial Project Operator (HiPPO)
    <ul>
      <li>State and input transition matrices를 효율적으로 memorize</li>
    </ul>
  </li>
  <li>Diagonal Plus Low-Rank Parametrization</li>
  <li>SSM matrix (A)의 rank를 낮게 해서 diagonalizability and stability 보장</li>
  <li>Efficient (convolutional) Kernel Computation
    <ul>
      <li>FFT와 iFFT 사용해서 complexity를 \(𝑂(𝑁 log(𝑁))\)로 만듬</li>
    </ul>
  </li>
</ul>

<h3 id="312-high-order-polynomial-projection-operators-hippo">3.1.2. High-Order Polynomial Projection Operators (HiPPO)</h3>

<ul>
  <li>S4에 사용된 행렬의 수학적인 해석을 제공</li>
  <li>4가지의 변형을 사용하는데,
    <ul>
      <li>the truncated Fourier basis polynomial (Hippo-FouT)</li>
      <li>based on Lagurre polynomials(LagT)</li>
      <li>based on Legendre polynomials(LegT)</li>
      <li>based on Legendre polynomials with a sliding window(LegS)</li>
    </ul>
  </li>
</ul>

<h3 id="313-hungry-hungry-hippo-h3">3.1.3. Hungry Hungry HiPPO (H3)</h3>

<ul>
  <li>SSM에서의 2개의 challenges
    <ul>
      <li>첫째, difficulty in recalling earlier tokens
        <ul>
          <li>시퀀스 내에서 이전 토큰을 기억하는 데 어려움</li>
        </ul>
      </li>
      <li>둘째, difficult in comparing the tokens across different sequences
        <ul>
          <li>서로 다른 시퀀스에서 토큰을 비교하는 데 어려움</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>극복하기 위한 새로운 방법의 3가지 핵심 요소
    <ul>
      <li>Multiplicative Interactions가 있는 Stacked SSMs
        <ul>
          <li>stacking two SSMs with multiplicative interactions between their input and output projections</li>
        </ul>
      </li>
      <li>학습 효율성을 위한 FlashConv
        <ul>
          <li>FFT를 사용하여  training efficiency 향상</li>
        </ul>
      </li>
      <li>Scaling을 위한 State-Passing
        <ul>
          <li>effectively splits the input into the largest possible chunks that can fit</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="314-global-convolution">3.1.4. Global Convolution</h3>

<ul>
  <li>원래는 input만큼 긴 conv kernel을 hidden state matrix에 곱했는데 - 불안정함</li>
  <li>이 conv kernel을 parametrizing하는 방법을 제안</li>
  <li>일반적으로 conv kernel은 FFT를 사용하는데, 느릴 수가 있어서 IO-aware algorithm 사용</li>
</ul>

<h3 id="317-ldstack">3.1.7. LDStack</h3>

<ul>
  <li>RNN이 다중 입력 다중 출력(MIMO)  Linear Dynamical System(LDS)으로 표현될 수 있음</li>
  <li>이 때 Parallel scan이 사용됨</li>
  <li>즉  Single Input Multiple Outputs (SIMO) LDS를 합쳐서  MIMO LDS를 approximate
    <ul>
      <li>essential characteristics를 유지하면서도 계산은 simple해짐</li>
    </ul>
  </li>
  <li>LDS를 time-varying state space models로 볼 수 있음</li>
</ul>

<h3 id="318-s5">3.1.8 S5</h3>

<ul>
  <li>RNN을 다중 입력 다중 출력 선형 동적 시스템(LDS)으로 모델링한 LDStack을 state space models (SSMs)으로 확장</li>
  <li>LDStack과 달리, S5 계층은 여러 입력 및 출력을 동시에 처리</li>
</ul>

<h2 id="32-gated-ssms">3.2. Gated SSMs</h2>

<ul>
  <li>FFT 연산 최적화를 위해 gating units를 사용</li>
  <li>Toepliz NN은 position-encoded Toeplitz matrix로 token mixing</li>
  <li>Mamba는 gated MLP로 SSM의 compoutational inefficiency 극복하고자 함</li>
  <li>(무슨 말 ?) 더 읽어보자</li>
</ul>

<h3 id="323-toeplitz-neural-network-tnn">3.2.3. Toeplitz Neural Network (TNN)</h3>

<ul>
  <li>Transformer의 <strong>attention-mechanism</strong>과 <strong>positional embedding</strong>을 개선</li>
  <li>position-encoded Toeplitz matrix를 사용하여 token-pair 관계 파악
    <ul>
      <li>space-time complexity를 \(O(NlogN)\)으로 줄임</li>
      <li>Relative Position Encoder (RPE)로 상대적 위치를 생성해서 parameters가 input length에 독립적이게 함</li>
    </ul>
  </li>
</ul>

<h3 id="324-mamba">3.2.4. Mamba</h3>

<ul>
  <li>Transformer의 quadratic computational and memory complexity에 주목</li>
  <li>특히 SSM은  addressing tasks (selective copying, induction head)에서 비효율적이었음</li>
  <li>Mamba가 이 문제를 푸는 방법은 :
    <ul>
      <li>novel parametrization approach for SSMs based on input characteristics</li>
      <li>incorporating a simple selection mechanism</li>
      <li>efficient hardware-aware algorithm based on selective scan</li>
      <li>gated technique to reduce the dimensionality of global kernel operations</li>
      <li>combine gated MLP[93] with the SSM module</li>
    </ul>
  </li>
</ul>

<h2 id="4-applications-of-state-space-models">4. Applications of State Space Models</h2>

<h3 id="41-language-domain-long-sequence">4.1. Language Domain (long sequence)</h3>

<ul>
  <li>원래는 Transformer 많이 썼는데 \(O(N^2)\) quadratic complexity \(\to\) long sequence 불가능</li>
  <li>그래서  State Space Models (SSMs)이 등장
    <ul>
      <li>input data를 fixed-size latent state에 표현</li>
      <li>하지만 그러다보니 capability to retrieve and copy에서 trade-off</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table2.png" alt="그림1" /></p>

<h3 id="42-vision-domain">4.2. Vision domain</h3>

<ul>
  <li>Vision Mamba나 SiMBA와 같은 Vision-specific Mamba
    <ul>
      <li>utilize bidirectional and visual state space models</li>
    </ul>
  </li>
  <li>SiMBA
    <ul>
      <li>sequence length and channel dimensions이 꼭 perfect square dimensions이 아니어도 됨</li>
      <li>pyramid version of the transformer architecture (성능 향상)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table3.png" alt="그림1" /></p>

<h3 id="47-time-series-domain">4.7. Time Series Domain</h3>

<ul>
  <li>옛날에는 ARIMA 쓰다가 Transformer 등장하면서 variants가 많이 나옴
    <ul>
      <li>Informer, FEDFormer, PatchTST…</li>
      <li>하지만 여전히 attention complexity 때문에 long-range dependency 못잡음</li>
    </ul>
  </li>
  <li>그래서 SSM 모델인 Timemachine, SiMBA, MambaMix 등장</li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table11.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/Mamba360/table14.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>SSM은 3가지 범주로 분류 가능 (structured, gated, and recurrent)</li>
  <li>아직 Transformer가 더 잘하는 영역이 있긴 하지만 (맥락에서 정보를 검색하는 작업 등)
    <ul>
      <li>SiMBA는 트랜스포머와 Mamba 아키텍처를 결합해서 Time series에서 SOTA</li>
    </ul>
  </li>
  <li>SSM을 large network로 안정적으로 확하는 것이 아직 해결되지 않은 문제</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/abs/2404.16112)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2022) Raindrop</title><link href="http://localhost:4000/pytorch/2024-09-24-raindrop/" rel="alternate" type="text/html" title="(Code Review, ICLR 2022) Raindrop" /><published>2024-09-24T00:00:00+09:00</published><updated>2024-09-24T18:34:03+09:00</updated><id>http://localhost:4000/pytorch/raindrop</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-24-raindrop/"><![CDATA[<p><a href="https://arxiv.org/abs/2110.05357">(Paper) Graph-Guided Network for Irregularly Sampled Multivariate Time Series</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">(Paper Review, ICLR 2022) Raindrop</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig1.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig2.png" alt="사진1" /></p>

<ul>
  <li>먼저 터미널에 git clone과 requirements를 입력하여 install 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">python Raindrop.py</code>로 P19, P12, PAM 데이터셋에 대한 성능을 볼 수 있다.</li>
</ul>

<h2 id="2-raindroppy">2. Raindrop.py</h2>

<h3 id="21-data-preparing">2.1. Data Preparing</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig3.png" alt="사진1" />
<img src="/assets/img/pytorch/raindrop_code/fig4.png" alt="사진1" /></p>

<ul>
  <li>parser를 통해 aurgments를 만들고</li>
  <li>본 논문에서 제시하는 model은 irregular time series를 다룬다.
    <ul>
      <li>그러므로 <code class="language-plaintext highlighter-rouge">missing ratio</code>, 즉 feature를 masking하는 비율을 미리 결정해준다. (option)</li>
      <li>일단은 0(no missing)으로 두고 코드를 이해해보자</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig5.png" alt="사진1" /></p>

<ul>
  <li>사전에 정한 <code class="language-plaintext highlighter-rouge">missing ratio</code>를 사용한다.</li>
  <li>epoch 수와 learning rate도 미리 정한다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig6.png" alt="사진1" /></p>

<ul>
  <li>P19 데이터셋을 사용한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">d_static</code>과 <code class="language-plaintext highlighter-rouge">d_inp</code>는 시간에 따라 변하지 않는(정적) / 변하는(동적) 변수의 개수</li>
      <li><code class="language-plaintext highlighter-rouge">static_info</code>는 <code class="language-plaintext highlighter-rouge">d_static</code> 변수가 있는지 없는지 (bool)</li>
      <li><code class="language-plaintext highlighter-rouge">max_len</code>은, batch 내 샘플마다 시계열의 길이가 다른데 최대 길이
        <ul>
          <li>만약 <code class="language-plaintext highlighter-rouge">max_len</code>보다 짧다면 그 부분은 다 0으로 기록되어있다.</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">n_classes</code>는 샘플에 속하는 class의 개수</li>
    </ul>
  </li>
  <li>
    <p>다른 데이터셋을 사용한다면 위의 변수들은 달라질 수 있다.</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">d_ob</code>는 각 변수를 몇 차원으로 표현할지를 의미한다.</li>
  <li>그래서 <code class="language-plaintext highlighter-rouge">d_model</code>은 동적 변수의 개수인 <code class="language-plaintext highlighter-rouge">d_inp</code>에 <code class="language-plaintext highlighter-rouge">d_ob</code>를 곱한 값이 된다.</li>
  <li><code class="language-plaintext highlighter-rouge">nhid</code>는 FFN의 dimension인데 <code class="language-plaintext highlighter-rouge">d_model</code>의 2배를 사용</li>
  <li><code class="language-plaintext highlighter-rouge">nlayers</code>는 layer의 개수, <code class="language-plaintext highlighter-rouge">nhead</code>는 MHA(multi-head attention)에서 heads 개수이고 모두 2개를 사용</li>
  <li><code class="language-plaintext highlighter-rouge">dropout</code>은 TransformerEncoderLayer에서 사용하는 dropout ratio</li>
  <li><code class="language-plaintext highlighter-rouge">aggreg</code>는 나중에 각 배치마다, 각 시점을 vector로 표현할텐데 그걸 모든 시점에 대해 합칠 때 <strong>평균</strong>을 사용</li>
  <li><code class="language-plaintext highlighter-rouge">MAX</code>는 positional encoder에 들어가는 MAX parameter인데
    <ul>
      <li>막상 positional encoder 코드를 보면 <code class="language-plaintext highlighter-rouge">MAX</code>라는 변수를 사용하지 않으니 신경 안써도 된다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig7.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n_run</code>은 데이터셋에 대해 몇 번을 실험해서 기록할지를 의미한다.</li>
  <li><code class="language-plaintext highlighter-rouge">n_splits</code>는 데이터가 5등분 되어있어서 5를 사용한다.</li>
  <li>그리고 본 model을 평가하기 위한 성능 지표를 기록할 arrays를 만들어놓는다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig8.png" alt="사진1" /></p>

<ul>
  <li>그리고 불러온 데이터셋을 train(0.8) / valid(0.1) / test(0.1)로 나누고 label(y)도 따로 준비한다.</li>
  <li>P19 데이터셋의 경우 train에는 31042개의 샘플이 있다. (샘플은 한 명의 환자 정도로 생각할 수 있다.)
    <ul>
      <li>그리고 각 샘플은 <code class="language-plaintext highlighter-rouge">torch.size([# of timesetps,  # of features])</code>인 tensor이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig9.png" alt="사진1" /></p>

<ul>
  <li>T는 time steps의 수, F는 (동적) 변수의 개수이고, D는 (정적) 변수의 개수가 된다.
    <ul>
      <li>동적 변수는 <code class="language-plaintext highlighter-rouge">Ptrain</code>의 <code class="language-plaintext highlighter-rouge">arr</code>에, 정적 변수는 <code class="language-plaintext highlighter-rouge">Ptrain</code>의 <code class="language-plaintext highlighter-rouge">extended_static</code>에 따로 준비하고 있다.</li>
    </ul>
  </li>
  <li>그리고 normalization을 위해 모든 변수들의 평균과 표준편차를 얻는다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">getStats</code> 함수에는 사용하는데 특이사항 없으므로 skip</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig10.png" alt="사진1" /></p>

<ul>
  <li>각각의 shape은 아래와 같다.</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>의 동적 변수들의 개수가 34개였는데 68이 된 이유는 :
    <ul>
      <li>같은 크기의 Mask를 옆에 이어붙였기 때문이다.</li>
      <li>Mask는 <code class="language-plaintext highlighter-rouge">M = 1*(input_tensor &gt; 0) + 0*(input_tensor &lt;= 0)</code>이다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>은 31042개의 텐서인데, 각 tensor는 해당 샘플의 길이를 알려준다.
    <ul>
      <li>즉 만약 0번째 샘플의 길이가 40이라면, 0번째 tensor는<code class="language-plaintext highlighter-rouge">[1, 2, ..., 40, 0, 0, ...]</code>이다.</li>
      <li>일단 숫자는 <code class="language-plaintext highlighter-rouge">max_len</code>개인데 해당 샘플의 길이까지만 index를 기록하고 뒷부분은 zero padding</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">y_train</code>은 각 샘플의 정답 label이다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig11.png" alt="사진1" /></p>

<ul>
  <li>이제 <code class="language-plaintext highlighter-rouge">global_structure</code>를 정의하는데, 각각의 동적 변수가 상호작용하는지를 0, 1로 표현
    <ul>
      <li>adjacency matrix의 역할을 한다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">missing_ratio</code>가 존재했다면 몇몇 feature를 masking한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>이 <code class="language-plaintext highlighter-rouge">sample</code>이면 각 샘플(환자)마다 독립적으로 특성을 무작위 제거</li>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>이 <code class="language-plaintext highlighter-rouge">set</code>이면 미리 계산된 density scores를 사용하여 제거할 특성을 결정하고 모든 샘플에서 제거</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig12.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>의 shape을 <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps,  batch_size,  # of features(w/masking)])</code>으로,</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>의 shape을 <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps, batch_size])</code>로 setting</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig13.png" alt="사진1" /></p>

<ul>
  <li>앞서 소개한 parameters를 한 번 출력해보았다.</li>
  <li>지금은 masking ratio가 0이다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig14.png" alt="사진1" /></p>

<ul>
  <li>parameters의 descriptions는 위와 같다.</li>
</ul>

<h3 id="22-model-setting">2.2. Model setting</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig15.png" alt="사진1" /></p>

<ul>
  <li>이제 model, criterion, optimiazer, scheduler를 정의한다.</li>
  <li>model은 2d tensor로 표현된 샘플마다 classification하도록 설계되었으므로 CrossEntropyLoss를 사용</li>
  <li>아직 input을 model에 넣은 건 아님.
    <ul>
      <li>input이 model에 들어가면 어떤 과정을 거치는지는 아래 3. <code class="language-plaintext highlighter-rouge">models_rd.py</code>에서 보도록 한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig16.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">idx_0</code>은 <code class="language-plaintext highlighter-rouge">y</code>가 0인 samples의 index, <code class="language-plaintext highlighter-rouge">idx_1</code>은 반대</li>
  <li>label이 1인 샘플의 개수가 적은 unbalancing 문제를 해결하기 위해 3배로 늘림 (왜 <strong>3</strong>배인지는 모름)</li>
  <li>batch_size가 128인데 label이 0과 1인 samples를 절반씩 채울테니
    <ul>
      <li>n_batches는 개수가 더 적은 label 기준으로 모든 samples를 한 번씩 다 볼 수 있도록 설정했다.</li>
      <li>사실 label이 1인 samples를 3배 했으니 label이 1인 샘플을 3번씩 보는 꼴이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig17.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>이제 epoch를 시작하는데, label이 0인 샘플과 1인 샘플에서 무작위로 <code class="language-plaintext highlighter-rouge">batch_size/2</code>개씩 가져온다.</li>
  <li>사실 label이 1인 samples를 3배 했으니 여기서는 중복된 샘플이 나올 가능성이 있다.</li>
  <li>model에 들어갈 input tensors의 shape을 미리 확인해두자</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig19.png" alt="사진1" /></p>

<ul>
  <li>이제 model에 들어가고 통상적인 backpropagation을 거친다.</li>
  <li>model에 들어가면 어떤 일이 일어나는지 알아보자.</li>
</ul>

<h2 id="3-models_rdpy">3. models_rd.py</h2>

<h3 id="31-init">3.1. <strong>init</strong></h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>이 상당히 많지만 지금 다 알 필요는 없다.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>에서 사용할 때 다시 올라와서 보면 될 듯</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig20.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig21.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig22.png" alt="사진1" /></p>

<h2 id="32-forward">3.2. forward</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig23.png" alt="사진1" /></p>

<ul>
  <li>P19 데이터셋의 경우 input shape은 주황색 주석과 같다.</li>
  <li>src로 들어오는 P의 경우 34개의 변수였는데 같은 크기의 Mask를 옆에 이어붙인 것이니 다시 분리
    <ul>
      <li>각각을 missing_mask, src라고 부름</li>
    </ul>
  </li>
  <li>그 다음 34개의 변수를 <code class="language-plaintext highlighter-rouge">d_ob</code>(여기선 4)번 반복해서 src의 representation capacity를 키워주고
    <ul>
      <li>ReLu를 통과시켜서 non-linearity를 표현할 수 있게 한다.</li>
      <li>그 다음 dropout을 거친다.</li>
    </ul>
  </li>
  <li>결국 <code class="language-plaintext highlighter-rouge">h</code>는 src를 확장시키고 learnable weights와 ReLu를 곱해 모델이 학습할 수 있는 형태로 만든 것</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig24.png" alt="사진1" /></p>

<ul>
  <li>이제 batch에 있는 각 sample마다 mask를 만든다.</li>
  <li>sample에 값이 있으면 mask에는 False가 되고 값이 없으면 mask가 True가 된다.</li>
  <li>mask의 길이는 60으로 고정이지만 sample마다 길이가 다르기 때문에 어디까지 False이고 언제부터 True인지는 sample마다 다르다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig25.png" alt="사진1" /></p>

<ul>
  <li>다음으로 <code class="language-plaintext highlighter-rouge">global_structure</code>를 adjacency matrix로 사용한다.
    <ul>
      <li>shape은 동적 변수의 개수 <code class="language-plaintext highlighter-rouge">d_inp</code> x <code class="language-plaintext highlighter-rouge">d_inp</code>가 되므로 각 동적 변수를 연결 여부를 (0,1)로 표현한다.</li>
      <li>epoch가 진행되면서 바뀔 수도 있으니 대각성분은 항상 1로 update해준다.</li>
    </ul>
  </li>
  <li>그 다음 edge_index와 edge_weights를 미리 구해놓는다.
    <ul>
      <li>연결된 nodes의 index와 그 weights를 의미함</li>
    </ul>
  </li>
  <li>그 다음 batch에 있는 각 sample마다 (동적) 변수들의 global structure(edge)를 고려한 representation을 저장할 공간 <code class="language-plaintext highlighter-rouge">output</code>을 미리 만들어놓는다.
    <ul>
      <li>각 sample마다 <code class="language-plaintext highlighter-rouge">torch([# of time steps,  d_inp x d_ob])</code> shape의 tensor가 들어갈 예정이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig26.png" alt="사진1" /></p>

<ul>
  <li>이제 아까 만든 <code class="language-plaintext highlighter-rouge">h</code>를 <code class="language-plaintext highlighter-rouge">x</code>로 받아서 (<code class="language-plaintext highlighter-rouge">x=h</code>) 하나의 sample에 대한 <code class="language-plaintext highlighter-rouge">h</code>를 <code class="language-plaintext highlighter-rouge">stepdata</code> 가져온다</li>
  <li><code class="language-plaintext highlighter-rouge">p_t</code>는 각 timestep을 <code class="language-plaintext highlighter-rouge">d_pe = 16</code>차원 vector로 embedding한 것이다. (init 참고)</li>
  <li>이제 <code class="language-plaintext highlighter-rouge">stepdata</code>를 <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code>로 reshape한다.
    <ul>
      <li>왜냐하면 feature끼리 attention을 수행하기 때문에 각 feature를 하나의 vector로 만들 필요가 있기 때문</li>
    </ul>
  </li>
  <li>이제 각 feature를 vector로 만든 걸 <code class="language-plaintext highlighter-rouge">ob_propagation</code>으로 정의된 attention layer에 넣는다.
    <ul>
      <li>그러면 같은 shape <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code> tensor가 return되지만</li>
      <li>해당 sample의 각각의 features를 Observation Propagation을 거쳐 representation한 결과이다.</li>
      <li><code class="language-plaintext highlighter-rouge">Ob_propagation.py</code>에 있고, 코드를 따로 첨부하지는 않겠으나 아래와 같은 과정을 거친다.
        <ul>
          <li>1) Message Passing: node 간에 정보를 전달하는 mechanism 구현</li>
          <li>2) Attention Mechanism: 각 node가 이웃 node로부터 받는 메시지의 중요도를 학습</li>
          <li>3) Egde weights: graph의 edge에 weight를 적용하여 정보 전달의 강도를 조절</li>
          <li>4) Edge prune: 중요도가 낮은 edge를 제거하여 computation efficiency 높임</li>
          <li>5) Feature Transform: linear Transform과 activation ftn으로 node의 feature를 변환</li>
          <li>6) Aggregation: 이웃 node로부터 받은 메시지를 합침</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig27.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ob_propagation-layer</code>를 한 번 더 통과시키고 shape을 맞춰서 <code class="language-plaintext highlighter-rouge">output</code>의 sample index 자리에 넣는다.
    <ul>
      <li>그리고 alpha_all에는 그 attention weights를 넣는다.
        <ul>
          <li>34개의 features끼리의 attention이니 34\(\times\)34\(=\)1156개의 숫자가 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>모든 samples에 대해서 완료하여 <code class="language-plaintext highlighter-rouge">output</code>이 완성되면 distance를 구한다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig28.png" alt="사진1" /></p>

<ul>
  <li>다음으로 time embedding을 concat한다.</li>
  <li>이러면 shape이 <code class="language-plaintext highlighter-rouge">torch.size([60, 128, 152])</code>가 되는데, 각 sample마다(128), 하나의 시점을 152차원 vector로 표현한 것이다.
    <ul>
      <li>이 152는 (동적) 변수 34개를 34\(\times\)4 = 136차원으로 표현하고, time embedding 16차원을 붙인 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig29.png" alt="사진1" /></p>

<ul>
  <li>이걸 transformer encoder에 통과시키고</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig30.png" alt="사진1" /></p>

<ul>
  <li>aggregate 하는데, 이 때 모든 시점에 대해 평균을 내준다. (<code class="language-plaintext highlighter-rouge">aggreg == mean</code>)</li>
  <li>그러면 각 sample은 모든 시점과 모든 변수를 통합하여 152차원 벡터로 표현된 결과가 나온다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig31.png" alt="사진1" /></p>

<ul>
  <li>마지막으로 (정적) 변수를 embedding한 emb를 붙여서 2-layer MLP에 넣으면</li>
  <li>각 sample에 대한 classification이 완료된다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig32.png" alt="사진1" /></p>

<ul>
  <li>Training에 따른 validation set acccuracy가 출력된다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig33.png" alt="사진1" /></p>

<ul>
  <li>그리고 classification report가 출력된다.</li>
</ul>

<p>끝 !</p>

<ul>
  <li>참고로 나의 경우에는 <code class="language-plaintext highlighter-rouge">from torch_scatter import gather_csr, scatter, segment_csr</code>가 안되어서 아래와 같이 주석 처리하고
    <ul>
      <li>pytorch를 보고 함수를 직접 작성하여 사용하였다.</li>
      <li><a href="https://github.com/rusty1s/pytorch_scatter/blob/master/torch_scatter/scatter.py">pytorch_scatter 참고</a></li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># from torch_scatter import gather_csr, scatter, segment_csr
</span><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">+</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
            <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">(),</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">other</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">src</span>

<span class="k">def</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">index</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">dim_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">dim_size</span>
        <span class="k">elif</span> <span class="n">index</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">dim_size</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="n">index_dim</span> <span class="o">=</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">index_dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index_dim</span> <span class="o">+</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">index_dim</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">size</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">index_dim</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">count</span><span class="p">[</span><span class="n">count</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="p">.</span><span class="nf">is_floating_point</span><span class="p">():</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">true_divide_</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">div_</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">floor</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="nb">reduce</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">
    |

    .. image:: https://raw.githubusercontent.com/rusty1s/pytorch_scatter/
            master/docs/source/_figures/add.svg?sanitize=true
        :align: center
        :width: 400px

    |

    Reduces all values from the :attr:`src` tensor into :attr:`out` at the
    indices specified in the :attr:`index` tensor along a given axis
    :attr:`dim`.
    For each value in :attr:`src`, its output index is specified by its index
    in :attr:`src` for dimensions outside of :attr:`dim` and by the
    corresponding value in :attr:`index` for dimension :attr:`dim`.
    The applied reduction is defined via the :attr:`reduce` argument.

    Formally, if :attr:`src` and :attr:`index` are :math:`n`-dimensional
    tensors with size :math:`(x_0, ..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})`
    and :attr:`dim` = `i`, then :attr:`out` must be an :math:`n`-dimensional
    tensor with size :math:`(x_0, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})`.
    Moreover, the values of :attr:`index` must be between :math:`0` and
    :math:`y - 1`, although no specific ordering of indices is required.
    The :attr:`index` tensor supports broadcasting in case its dimensions do
    not match with :attr:`src`.

    For one-dimensional tensors with :obj:`reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, the operation
    computes

    .. math::
        \mathrm{out}_i = \mathrm{out}_i + \sum_j~\mathrm{src}_j

    where :math:`\sum_j` is over :math:`j` such that
    :math:`\mathrm{index}_j = i`.

    .. note::

        This operation is implemented via atomic operations on the GPU and is
        therefore **non-deterministic** since the order of parallel operations
        to the same value is undetermined.
        For floating-point variables, this results in a source of variance in
        the result.

    :param src: The source tensor.
    :param index: The indices of elements to scatter.
    :param dim: The axis along which to index. (default: :obj:`-1`)
    :param out: The destination tensor.
    :param dim_size: If :attr:`out` is not given, automatically create output
        with size :attr:`dim_size` at dimension :attr:`dim`.
        If :attr:`dim_size` is not given, a minimal sized output tensor
        according to :obj:`index.max() + 1` is returned.
    :param reduce: The reduce operation (:obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">mul</span><span class="sh">"</span><span class="s">`,
        :obj:`</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="s">` or :obj:`</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="s">`). (default: :obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`)

    :rtype: :class:`Tensor`

    .. code-block:: python

        from torch_scatter import scatter

        src = torch.randn(10, 6, 64)
        index = torch.tensor([0, 1, 0, 1, 2, 1])

        # Broadcasting in the first and last dim.
        out = scatter(src, index, dim=1, reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">)

        print(out.size())

    .. code-block::

        torch.Size([10, 3, 64])
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">sum</span><span class="sh">'</span> <span class="ow">or</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">add</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mul</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">min</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">max</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span>

</code></pre></div></div>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Raindrop github](https://github.com/mims-harvard/Raindrop)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2024) Pathformer</title><link href="http://localhost:4000/pytorch/2024-09-09-pathformer/" rel="alternate" type="text/html" title="(Code Review, ICLR 2024) Pathformer" /><published>2024-09-09T00:00:00+09:00</published><updated>2024-09-24T18:29:39+09:00</updated><id>http://localhost:4000/pytorch/pathformer</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-09-pathformer/"><![CDATA[<p><a href="https://openreview.net/pdf?id=lJkOCMP2aW">(Paper) Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-05-23-Pathformer">(Paper Review, ICLR 2024) Pathformer</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig1.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig2.png" alt="사진1" /></p>

<ul>
  <li>먼저 터미널에 git clone과 requirements를 입력하여 install 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">bash scripts/multivariate/ETTm2.sh</code>로 ETTm2 데이터셋을 예측할 수 있다.</li>
</ul>

<h2 id="2-sh">2. .sh</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig3.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ETTm2.sh</code> 파일에는 <code class="language-plaintext highlighter-rouge">run.py</code>를 실행하도록 되어있다.</li>
</ul>

<h2 id="3-runpy">3. run.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig4.png" alt="사진1" /></p>

<ul>
  <li>parser를 통해 arguments를 만든다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig5.png" alt="사진1" /></p>

<ul>
  <li>그리고 <code class="language-plaintext highlighter-rouge">Exp_Main</code>에 있는 train에 arguments를 넣어준다.</li>
</ul>

<h2 id="4-exp_mainpy">4. exp_main.py</h2>

<h3 id="41-train">4.1. train</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig6.png" alt="사진1" /></p>

<ul>
  <li>model, data, optimizer, criterion을 설정하는 간단한 함수들과, <code class="language-plaintext highlighter-rouge">vali</code>, <code class="language-plaintext highlighter-rouge">train</code>, <code class="language-plaintext highlighter-rouge">test</code>, <code class="language-plaintext highlighter-rouge">predict</code> 함수가 있다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig7.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Exp_main</code> : <code class="language-plaintext highlighter-rouge">train</code>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">_get_data</code>로 train, valid, test 데이터셋을 load</li>
      <li><code class="language-plaintext highlighter-rouge">sum(p.numel() for p in self.model.parameters())</code>는 parameters 개수</li>
      <li>time, early sipping, optimizer, criterion, learning rate scheduler 정의</li>
      <li><code class="language-plaintext highlighter-rouge">lr_scheduler.OneCycleLR</code>는 learning rate를 빠르게 최대 학습률까지 증가시켰다가 다시 감소시키면서 최적화 과정
        <ul>
          <li><code class="language-plaintext highlighter-rouge">optimizer</code> : 사용하는 optimizer</li>
          <li><code class="language-plaintext highlighter-rouge">steps_per_epoch</code> : 1 epoch가 몇 번의 update가 발생하는지 (mini-batch)</li>
          <li><code class="language-plaintext highlighter-rouge">pct_start</code> : learning rate가 증가하는 구간의 비율 / <code class="language-plaintext highlighter-rouge">epochs</code> : 전체 epoch 수</li>
          <li><code class="language-plaintext highlighter-rouge">max_lr</code> : 최대 learning rate</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig8.png" alt="사진1" /></p>

<ul>
  <li>each epoch에서는 train loder에서 batch 단위로 데이터를 받고</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig9.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">with torch.cuda.amp.autocast():</code>는 <code class="language-plaintext highlighter-rouge">float16</code>과 <code class="language-plaintext highlighter-rouge">float32</code>를 자동으로 캐스팅</li>
  <li>모델이 예측한 <code class="language-plaintext highlighter-rouge">outputs</code>와 정답 <code class="language-plaintext highlighter-rouge">batch_y</code>를 비교하여 loss 계산</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig10.png" alt="사진1" /></p>

<ul>
  <li>epoch에 걸린 시간과 loss를 출력하고 backward로 parameters를 update</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig11.png" alt="사진1" /></p>

<ul>
  <li>Validation set에 대한 loss로 early stopping 여부를 결정하고 학습이 종료되면 모델 저장</li>
  <li>vali 함수는 특이 사항 없으므로 pass</li>
</ul>

<h3 id="42-test">4.2. test</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig12.png" alt="사진1" /></p>

<ul>
  <li>test dataset과, 학습되어 저장된 model을 load한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig13.png" alt="사진1" /></p>

<ul>
  <li>train과 비슷하게 batch 단위로 모델에 넣어서 예측값을 얻는다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig14.png" alt="사진1" /></p>

<ul>
  <li>batch 20개마다 묶어서 visualizaiton을 한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig15.png" alt="사진1" /></p>

<ul>
  <li>최종적인 예측과 loss를 <code class="language-plaintext highlighter-rouge">results.txt</code>에 저장한다.</li>
</ul>

<h3 id="43-predict">4.3. predict</h3>

<p>pass</p>

<h2 id="5-modelspathformerpy">5. models/Pathformer.py</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">from models import PathFormer</code> 이므로 해당 경로로 가서 pathformer의 archtecture를 보자</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">forward</code>는 normalization \(\to\) <code class="language-plaintext highlighter-rouge">start_fc</code> \(\to\) for <code class="language-plaintext highlighter-rouge">layer</code> in <code class="language-plaintext highlighter-rouge">self.AMS_lists</code> \(\to\) de-normalization로 구성된다.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>에 들어온 x의 shape은 <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes])</code>이다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">seq_len</code>은 관측하는 과거 시점 수, <code class="language-plaintext highlighter-rouge">num_nodes</code>는 multivariate에서 variates 개수</li>
    </ul>
  </li>
  <li>x가 unsqueeze되어 normalization, <code class="language-plaintext highlighter-rouge">start_fc</code>를 통과하면 <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes, d_model])</code>이 된다. (아래 <code class="language-plaintext highlighter-rouge">__init__</code> 참고)</li>
  <li>이제 <code class="language-plaintext highlighter-rouge">AMS_list</code>의 <code class="language-plaintext highlighter-rouge">layers</code>를 통과하고 denormalization을 통과한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig17.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>을 보면 <code class="language-plaintext highlighter-rouge">self.AMS_lists</code>는 <code class="language-plaintext highlighter-rouge">layers.AMS</code>에서 import한다.</li>
  <li>AMS layer가 pathformer는 전부이니 살펴보자</li>
</ul>

<h2 id="6-layersamspy">6. Layers/AMS.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">self.seasonality_and_trend_decompose</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.noisy_top_k_gating</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.cv_squared</code></li>
  <li><code class="language-plaintext highlighter-rouge">SparseDispatcher</code>와 <code class="language-plaintext highlighter-rouge">SparseDispatcher.dispatch</code>, <code class="language-plaintext highlighter-rouge">SparseDispatcher.combine</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.experts</code></li>
  <li>각각에 대해서 하나씩 살펴보도록 한다.</li>
</ul>

<h3 id="61-selfseasonality_and_trend_decompose">6.1. self.seasonality_and_trend_decompose</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig19.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig20.png" alt="사진1" /></p>

<ul>
  <li>AMS class 안에서 정의된 함수</li>
  <li><strong>seasonality와 trend를 x에서 각각 계산</strong>하기 때문에 \(seasonal + trend = x\)가 아님
    <ul>
      <li>해당 함수의 결과는 x에 seasonality와 trend를 더한 결과이다.</li>
    </ul>
  </li>
  <li>처음에 <code class="language-plaintext highlighter-rouge">x = x[:, :, :, 0]</code>은 <code class="language-plaintext highlighter-rouge">d_model</code> 차원으로 표현된 x에서 첫 번째 dimension만 사용해서 decompose한다는 의미</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig21.png" alt="사진1" /></p>

<ul>
  <li>seasonality_model은 <code class="language-plaintext highlighter-rouge">FourierLayer</code>
    <ul>
      <li>푸리에 변환(fft) 후 amplitude가 높은 frequency \(k\)​​개를 inverse 푸리에 변환(extrapolate)</li>
    </ul>
  </li>
  <li>trend_model은 <code class="language-plaintext highlighter-rouge">series_decomp_multi</code>
    <ul>
      <li>다양한 크기의 kernel size로 moving average를 softmax</li>
    </ul>
  </li>
</ul>

<h3 id="62-selfnoisy_top_k_gating">6.2. self.noisy_top_k_gating</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig22.png" alt="사진1" />
<img src="/assets/img/pytorch/pathformer_code/fig23.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">start_linear.squeeze</code>와 <code class="language-plaintext highlighter-rouge">w_gate</code>로 <code class="language-plaintext highlighter-rouge">torch.Size([batch, seq_len, num_node])</code>가 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>가 된다.</li>
  <li>같은 크기 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>의 <code class="language-plaintext highlighter-rouge">logit</code>을 만들고 \(top-k\) logit을 <code class="language-plaintext highlighter-rouge">gates</code>에 넣는다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">scatter</code>는 특정 인덱스 위치에 값을 할당하는 함수이다.</li>
      <li><code class="language-plaintext highlighter-rouge">gate</code>의 shape은 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_experts])</code>가 되는데, 각 행(batch)에서 k개를 제외하고는 다 0이다.</li>
      <li>그리고 각 행(batch)마다 그 k개가 어떤 experts인지는 다르다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">load</code>는 각 expert가 배치 전체에서 얼마나 선택되었는지에 대한 비율을 의미한다.
    <ul>
      <li>shape은 <code class="language-plaintext highlighter-rouge">torch.Size([num_experts])</code>이다.</li>
    </ul>
  </li>
</ul>

<h3 id="63-selfcv_squared">6.3. self.cv_squared</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>다시 AMS.forward로 돌아오자</li>
  <li>각 expert마다 모든 배치에 대해 sum을 해서 <code class="language-plaintext highlighter-rouge">importance</code>를 계산하면 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자가 된다.</li>
  <li><code class="language-plaintext highlighter-rouge">cv_squared</code>를 통해 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자의 변동계수를 구해서 <code class="language-plaintext highlighter-rouge">balance_loss</code>를 구한다.
    <ul>
      <li>변동계수는 \(\frac{\sigma^2}{\mu^2}\)이다.</li>
      <li>이 값이 크면 특정 experts에 importance가 몰려있음을 의미한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig24.png" alt="사진1" /></p>

<h3 id="64-sparsedispatcher-어려움-주의">6.4. SparseDispatcher (*어려움 주의)</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig25.png" alt="사진1" />
<img src="/assets/img/pytorch/pathformer_code/fig26.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>에서 준비해놓는 것들이 많으니 하나하나 보도록 한다. \(k=2\), <code class="language-plaintext highlighter-rouge">num_experts</code>=4인 경우이다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig27.png" alt="사진1" /></p>
<ul>
  <li>각 행은 batch를 의미하기 때문에 행의 개수는 batch size (여기선 512)</li>
  <li>각 행에는 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자가 있고 그 중 \(k\)개만 non-negative, 나머지는 0</li>
  <li>첫 행에서 2, 3번째 숫자가 양수라는 것은, 첫번째 배치에서 2, 3번째 experts가 선택되었다는 것을 의미</li>
  <li>바로 아래에 있는 <code class="language-plaintext highlighter-rouge">torch.nonzero(gates)</code>에서도 그 사실을 알 수 있다.
    <ul>
      <li>첫 번째 배치에서는 index 1, 2인 experts가, 마지막 배치에서는 index 1, 2인 experts가 선택됨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig28.png" alt="사진1" /></p>
<ul>
  <li>이제 sort를 하는데 첫번째 열은 어차피 index라서 정렬되어있고
    <ul>
      <li>(두 번째 열이 정렬되면서 섞이기 때문에 두 번째 열의 숫자가 첫 번째 열의 배치 index와 상관 없게 된다)</li>
    </ul>
  </li>
  <li>그리고 <code class="language-plaintext highlighter-rouge">index_sorted_experts</code>는 정렬된 숫자가 몇 번째 index에 있던 숫자인지를 표시해준다.
    <ul>
      <li>(여기서부터 헷갈리기 시작함)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig29.png" alt="사진1" /></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">self._expert_index</code>는 각 배치에서 선택된 experts의 index를 정렬한 것이다.
    <ul>
      <li>각 배치마다 \(k\)개씩 있으니 총 batch_size \(\times k\)개의 숫자겠다.</li>
    </ul>
  </li>
  <li>그리고 그걸 다시 batch index로 되돌릴 수가 있을 것이다.
    <ul>
      <li>즉 <code class="language-plaintext highlighter-rouge">self._batch_index</code>가 1, 3, 6,…이라는 것은 expert 0이 선택되었던 batch가 1, 3, …이고</li>
      <li>그 다음 expert 1이 선택된 batch들이 몇 번째 batch인지 쭉 나열이 된다. (이걸 마지막 expert까지 반복)</li>
    </ul>
  </li>
  <li>마지막으로 <code class="language-plaintext highlighter-rouge">self._part_sizes</code>는 모든 batches 통틀어서 각 expert가 몇 번 선택되었는지를 의미한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig30.png" alt="사진1" /></p>
<ul>
  <li>이제 <code class="language-plaintext highlighter-rouge">gates_exp</code>는 expert 0이 선택되었던 batches를 쭉 나열하고, 그 다음에 expert 1이 선택되었던 batches를 쭉 나열하고… 마지막 expert가 선택되었던 batches까지 나열한 것이다.</li>
  <li>그리고 <code class="language-plaintext highlighter-rouge">self._nonzero_gates</code>는 expert \(i\) (\(i = 1, ...,\) <code class="language-plaintext highlighter-rouge">num_experts</code>)가 선택된 배치에서 expert \(i\)의 gates를 나열한 것이다.</li>
</ul>

<h3 id="641-sparsedispatcherdispatch">6.4.1. SparseDispatcher.dispatch</h3>

<ul>
  <li>이제 dispatch에서는 각 expert에 처리해야 할 batches를 할당한다.</li>
  <li>만약 지금처럼 inp의 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([512, 96, 7, 16])</code>, <code class="language-plaintext highlighter-rouge">self._batch_index</code>의 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([1024])</code>, 그리고 <code class="language-plaintext highlighter-rouge">self._part_sizes</code>가 <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>라고 가정하면:</li>
  <li><code class="language-plaintext highlighter-rouge">inp[self._batch_index]</code>에서는 inp 텐서에서 1024개의 샘플을 선택하여, 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([1024, 96, 7, 16])</code>인 새로운 텐서를 생성한다.</li>
  <li>그리고 첫 번째 차원(batch 차원, 1024개)을 <code class="language-plaintext highlighter-rouge">self._part_sizes</code> = <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>로 나눈다.</li>
  <li>결과는 각 expert에게 할당된 batches의 리스트이며, 각 텐서의 크기는:
    <ul>
      <li>첫 번째 expert: [262, 96, 7, 16]</li>
      <li>두 번째 expert: [348, 96, 7, 16]</li>
      <li>세 번째 expert: [249, 96, 7, 16]</li>
      <li>네 번째 expert: [165, 96, 7, 16]</li>
    </ul>
  </li>
  <li>이걸 리스트로 return한다.</li>
</ul>

<h3 id="642-sparsedispatchercombine">6.4.2. SparseDispatcher.combine</h3>

<ul>
  <li>이제 각각을 해당 expert에 통과시킨다.</li>
  <li>expert는 <code class="language-plaintext highlighter-rouge">TransformerLayer</code>이다. (Pathformer.py의 __init__참고)</li>
  <li>그리고 그 결과를 다시 combine한다.
    <ul>
      <li>그런데 위에서 combine 함수를 잘 보면 처음에 <code class="language-plaintext highlighter-rouge">.exp()</code>를 하고 다시 <code class="language-plaintext highlighter-rouge">.log()</code>를 해주는데,</li>
      <li><code class="language-plaintext highlighter-rouge">.exp()</code>에서 NaN이 나올 수가 있으니 주의하자.</li>
      <li>(벤치마크 데이터셋에서는 해당사항 없지만 내 프로젝트에서 사용하는 데이터에서는 발생했다.)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>이제 residual_connection만 적용해주면 끝난다.</li>
  <li>여기까지가 하나의 <code class="language-plaintext highlighter-rouge">AMS</code> layer이다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="사진1" /></p>

<ul>
  <li>여기서 for 안에 있는 layer가 AMS layer이다.</li>
  <li>
    <p>마지막으로 de-normalization을 하면 끝이다.</p>
  </li>
  <li>나머지는 위에서 이미 소개한 <code class="language-plaintext highlighter-rouge">3. run.py</code>와 <code class="language-plaintext highlighter-rouge">4. exp_main.py</code>가 전부이다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Pathformer github](https://github.com/decisionintelligence/pathformer)]]></summary></entry></feed>