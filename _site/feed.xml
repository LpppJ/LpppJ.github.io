<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-10-27T05:37:21+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)</title><link href="http://localhost:4000/timeseries/2024-10-27-TPatchGNN/" rel="alternate" type="text/html" title="T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)" /><published>2024-10-27T00:00:00+09:00</published><updated>2024-10-27T05:37:21+09:00</updated><id>http://localhost:4000/timeseries/TPatchGNN</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-27-TPatchGNN/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>transforms each univariate irregular time series into a series of transformable patches</li>
      <li>local semantics capture와, inter-time series correlation modeling는 하면서</li>
      <li>avoiding sequence <strong>length explosion</strong> in aligned IMTS (무슨 의미인지 1. introduction (3)에서 설명)</li>
    </ul>
  </li>
  <li>Time-adaptive graph neural networks으로 time-varying adaptive graphs를 학습해서
    <ul>
      <li>dynamic intertime series correlation를 표현</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Multivariate Time Series (IMTS)의 특징은 irregular sampling intervals and missing data</li>
  <li>Irregularity within the series and asynchrony 때문에 다루기 어려움
    <ul>
      <li>ODE로 풀려고 한 적은 있지만 numerical integration process으로 인해 computationally expensive</li>
    </ul>
  </li>
  <li>IMTS forecasting의 어려움에는 3가지 이유가 있음</li>
  <li>첫번째는 (1) irregularity in intra-time series dependency modeling
    <ul>
      <li><strong>varying time intervals</strong> between adjacent observations이 the consistent flow of time series data를 방해</li>
    </ul>
  </li>
  <li>두번째는 (2) asynchrony in intertime series correlation modeling
    <ul>
      <li><strong>misaligned at time</strong> due to irregular sampling or missing data.</li>
    </ul>
  </li>
  <li>가장 중요한 건 (3) sequence length explosion with the increase of variables
    <ul>
      <li>아래 fig1처럼 “단 하나의 변수라도 기록된 time stamp”는 모두 존재하는 걸로 해버리면, 변수 개수가 늘어남에 다라 time stamps의 수가 너무 많아지는 문제. (이러한 방법을 canonical pre-alignment representation이라고 부름)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TPatchGNN/fig1.png" alt="그림1" /></p>

<ul>
  <li>그래서 본 논문에서 제시하는 T-PATCHGNN의 장점은
    <ul>
      <li>첫째로 The independent patching process for each univariate irregular time series으로 representation에서 sequence length explosion의 risk를 없애고</li>
      <li>둘째로 local semantics를 잘 잡기 위해 putting each individual observation into patches with richer context</li>
      <li>셋째로 transformable patching 후에 IMTS is naturally aligned in a consistent patch-level temporal resolution</li>
    </ul>
  </li>
  <li>본 논문의 contribution은 :
    <ul>
      <li>New transformable patching method to transform each univariate irregular time series of IMTS into a series of variable-length yet time-aligned patches</li>
      <li>transformable patching outcomes을 바탕으로,  time-adaptive graph neural networks를 제안</li>
      <li>building a benchmark for IMTS forecasting evaluation</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2, Related Works</h2>

<h3 id="21-irregular-multivariate-time-series-forecasting">2.1. Irregular Multivariate Time Series Forecasting</h3>

<p>pass</p>

<h3 id="22-irregular-multivariate-time-series-representation">2.2. Irregular Multivariate Time Series Representation</h3>

<ul>
  <li>기존에는 time-aligned manner로 IMTS를 representation (pre-alignment representation method)
    <ul>
      <li>즉 하나의 변수라고 기록된 time stamp는 존재하는 걸로 생각하니</li>
      <li>sequence length that equals the number of all unique time stamps in IMTS</li>
      <li>예를 들어 변수 1은 1,3,5 시점에 기록되고 변수 2는 2,4,6 시점에 기록되면 unique time stamps의 개수는 6이 됨</li>
      <li>sequence length explosion problem 발생</li>
    </ul>
  </li>
</ul>

<h3 id="23-graph-neural-networks-for-multivariate-time-series">2.3. Graph Neural Networks for Multivariate Time Series</h3>

<ul>
  <li>
    <p>2018년 DCRNN, STGCN은 pre-defined graph structures를 사용해서 실제로 쓰기 어려웠고</p>
  </li>
  <li>2019년부터 data로부터 graph structures를 학습하는 방식을 사용
    <ul>
      <li>하지만 IMTS에서는 잘 작동을 안 함. mimisalignment at times으로 인해 inter-time series correlation modeling이 잘 안 됨</li>
    </ul>
  </li>
  <li>Raindrop(2021)[<a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">paper review</a>]
    <ul>
      <li>이 문제를 propagation the asynchronous observations at all the timestamps로 해결하려고 했지만  sequence length explosion problem을 피할 수 없음</li>
    </ul>
  </li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<h3 id="31-problem-definition">3.1. Problem Definition</h3>

<h3 id="definition-1">Definition 1</h3>

<ul>
  <li>Irregular Multivariate Time Series
    <ul>
      <li>\(\mathcal{O}=\left\{\mathbf{o}_{1: L_n}^n\right\}_{n=1}^N=\left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\), where</li>
      <li>\(N\)개의 변수가 있고 \(n\)번째 변수는 \(L_n\)개의 observations가 있고, \(n\)번째 변수의 \(i\)번째 변수의 값은 \(t_i^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="definition-2">Definition 2</h3>

<ul>
  <li>Forecasting Query \(q_j^n\)
    <ul>
      <li>\(j\)-th query on \(n\)-th variable to predict its corresponding value at a future time \(q_j^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="problem-1">Problem 1</h3>

<ul>
  <li>Irregular Multivariate Time Series Forecasting
    <ul>
      <li>IMTS \(\mathcal{O} =  \left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\)와 Forecasting query \(\mathcal{Q}=\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)가 있을 때,</li>
      <li>problem은 accurately forecast recorded values \(\hat{\mathcal{X}}=\left\{\left[\hat{x}_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\) in correspondence to the forecasting queries</li>
      <li>\(\mathcal{F}(\mathcal{O}, \mathcal{Q}) \longrightarrow \hat{\mathcal{X}}\)로 표현됨</li>
    </ul>
  </li>
</ul>

<h3 id="32-canonical-pre-alignment-representation-for-imts">3.2. Canonical Pre-Alignment Representation for IMTS</h3>

<ul>
  <li>2.2. Irregular Multivariate Time Series Representation 참고</li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<p><img src="/assets/img/timeseries/TPatchGNN/fig2.png" alt="그림1" /></p>

<h3 id="41-irregular-time-series-patching">4.1. Irregular Time Series Patching</h3>

<ul>
  <li>모든 univariate TS에 같은 patching operation을 하니까 변수 index 표기는 생략</li>
</ul>

<h3 id="411-transformable-patching">4.1.1. TRANSFORMABLE PATCHING</h3>

<ul>
  <li>Time series patching이 forecasting에 좋은 방법이라는 건 알려진 사실. benefits in :
    <ul>
      <li>capturing local semantic information,</li>
      <li>reducing computation and memory usage,</li>
      <li>modeling longer-range historical observations</li>
    </ul>
  </li>
  <li>일반적으로 time series patching은 하나의 patch에 같은 숫자의 observations가 있는데,
    <ul>
      <li>IMTS에서 time intervals는 다양하기 때문에 이러한 방식이 적절하지 않음</li>
    </ul>
  </li>
  <li>그래서 patch에 같은 개수의 observataions가 아니라, unified time horizon이 들어가도록 함
    <ul>
      <li>patch 안에 들어가는 observations의 개수는 다를 수 있지만, ex) 2시간인 건 동일하도록</li>
    </ul>
  </li>
  <li>patch는 \(\left[\mathbf{o}_{l_p: r_p}\right]_{p=1}^P\)로 표현되고 \(P\)</li>
</ul>

<h3 id="412-patch-encoding">4.1.2. PATCH ENCODING</h3>

<ul>
  <li><strong>Continuous time embedding</strong>
    <ul>
      <li>\(\phi(t)[d]=\left\{\begin{array}{lll}
\omega_0 \cdot t+\alpha_0, &amp; \text { if } &amp; d=0 \\
\sin \left(\omega_d \cdot t+\alpha_d\right), &amp; \text { if } &amp; 0&lt;d&lt;D_t
\end{array}\right.\).
        <ul>
          <li>where the \(\omega_d\) and \(\alpha_d\) are learnable parameters and \(D_t\) is embedding’s dimension</li>
        </ul>
      </li>
      <li>Concatenation하면 observations in the patch:
        <ul>
          <li>\(\mathbf{z}_{l_p: r_p}=\left[z_i\right]_{i=l_p}^{r_p}=\left[\phi\left(t_i\right) \| x_i\right]_{i=l_p}^{r_p}\).</li>
          <li>이건 하나의 patch에 대한 표현이 되는 것 !</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Transformable time-aware convolution</strong>
    <ul>
      <li>input sequence의 길이에 맞게 (adaptively), generated parameters와 transformable filter size를 사용</li>
      <li>\(\mathbf{f}_d=\left[\frac{\exp \left(\mathbf{F}_d\left(z_i\right)\right)}{\sum_{j=1}^{L_p} \exp \left(\mathbf{F}_d\left(z_j\right)\right)}\right]_{i=1}^{L_p}\)으로 표현됨
        <ul>
          <li>where \(L_p\) is the sequence length of patch \(\mathbf{z}_{l_p: r_p}, \mathbf{f}_d \in \mathbb{R}^{L_p \times D_{i n}}\) is the derived filter for \(d\)-th feature map, \(D_{i n}\) is dimension of inputs, and \(\mathbf{F}_d\) denotes the meta-filter that can be instantiated by learnable neural networks</li>
          <li>이건 filter의 parameters를 along the temporal dimension으로 normalizaing해서 consistent scaling 하겠다는 것</li>
        </ul>
      </li>
      <li>위 식으로 \(D-1\)개의 filters를 사용해서 <strong>latent patch embedding</strong> \(h_p^c \in \mathbb{R}^{D-1}\)를 얻음 :
        <ul>
          <li>\(h_p^c=\left[\sum_{i=1}^{L_p} \mathbf{f}_d[i]^{\top} \mathbf{z}_{l_p: r_p}[i]\right]_{d=1}^{D-1}\).</li>
          <li>이건  encoded transformable patches:
            <ul>
              <li>variable-length sequences에 따라 flexibility를 가지고</li>
              <li>parameterization for varying time intervals을 하면서</li>
              <li>additional learnable filter parameters 없이 더 긴 시퀀스를 처리할 수 있음</li>
            </ul>
          </li>
          <li>마지막으로 \(h_p=\left[h_p^c \| m_p\right]\) 이렇게 patch에 masking을 덧붙여주는데,
            <ul>
              <li>\(m_p\)는 이 patch 안에 observations가 하나 이상 있다~를 indicator로 표현</li>
            </ul>
          </li>
          <li>최종적으로 \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)를 얻는다.</li>
          <li>이건 \(P\)개의 patch를 \(D-1\)차원으로 표현하고 마지막에는 masking으로 indicator를 붙인 것</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-intra--and-inter-time-series-modeling">4.2. Intra- and Inter-Time Series Modeling</h3>

<ul>
  <li>이제 이  transformable patching을 irregular time series를 intra- and inter-time series modeling하는지 알아보자</li>
</ul>

<h3 id="421-transformer-to-model-sequential-patches">4.2.1. TRANSFORMER TO MODEL SEQUENTIAL PATCHES</h3>

<ul>
  <li>위에서 구한 \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)를 Transformer에 넣는다.</li>
  <li>먼저 positional encoding을 하고
    <ul>
      <li>\(\mathbf{x}_{1: P}^{t f, n}=\mathbf{h}_{1: P}^n+\mathbf{P E}_{1: P}\).</li>
    </ul>
  </li>
  <li>Q, K, V를 만들어서 MHA를 통과한다.
    <ul>
      <li>\(\mathbf{q}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^Q\) / \(\mathbf{k}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^K\) / \(\mathbf{v}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^V\) where \(\mathbf{W}_h^Q, \mathbf{W}_h^K, \mathbf{W}_h^V \in \mathbb{R}^{D \times(D / H)}\)</li>
      <li>\(\mathbf{h}_{1: P}^{t f, n}=\|_{h=1}^H \operatorname{Softmax}\left(\frac{\mathbf{q}_h^n \mathbf{k}_h^{n T}}{\sqrt{D / H}}\right) \mathbf{v}_h^n \in \mathbb{R}^{P \times D}\),</li>
    </ul>
  </li>
</ul>

<h3 id="422-time-varying-adaptive-graph-structure-learning">4.2.2. TIME-VARYING ADAPTIVE GRAPH STRUCTURE LEARNING</h3>

<ul>
  <li>한 변수를 예측하기 위해서 다른 변수의 정보는 매우 유용할 수가 있음</li>
  <li>하지만 IMTS에서는 misaligned at times으로 인해 correlation modeling이 어려움
    <ul>
      <li>그렇다고 Raindrop처럼 하기엔 e sequence length explosion problem이 발생</li>
    </ul>
  </li>
  <li>그래서 <strong>transformable patching</strong>으로 해결
    <ul>
      <li>patch를 observations의 개수가 아니라 시간 길이를 기준으로 끊다보니</li>
      <li>각 변수는 같은 숫자의 patches로 이루어지니까</li>
      <li>time-adaptive graph neural networks로 inter-time series correlation를 modeling할 수 있음</li>
    </ul>
  </li>
  <li>즉 IMTS의  dynamic correlations를 파악하기 위해서는
    <ul>
      <li>series of time-varying adaptive graphs를 학습하겠다는 것이고</li>
      <li>지금 문제는 variable embedding이 training에서는 update 가능하지만 inference에서는 static</li>
      <li>그러니 learnable \(\mathbf{E}_1^s, \mathbf{E}_2^s \in \mathbb{R}^{N \times D_g}\)를 사용해서</li>
      <li>우리가 지금까지 만들었던  time-varying patch embedding \(\mathbf{H}_p^{t f}=\left[\mathbf{h}_p^{t f, n}\right]_{n=1}^N \in \mathbb{R}^{N \times D}\)을
        <ul>
          <li>static variable embedding으로 만들면 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>그 gated adding operation은 다음과 같음
    <ul>
      <li>\(\begin{gathered}
\mathbf{E}_{p, k}=\mathbf{E}_k^s+g_{p, k} * \mathbf{E}_{p, k}^d, \\
\mathbf{E}_{p, k}^d=\mathbf{H}_p^{t f} \mathbf{W}_k^d, \\
g_{p, k}=\operatorname{ReLU}\left(\tanh \left(\left[\mathbf{H}_p^{t f} \| \mathbf{E}_k^s\right] \mathbf{W}_k^g\right)\right) \\
k=\{1,2\}
\end{gathered}\), where
        <ul>
          <li>\(\mathbf{W}_k^d \in \mathbb{R}^{D \times D_g}, \mathbf{W}_k^g \in \mathbb{R}^{\left(D+D_g\right) \times 1}\) are learnable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이제 time-varying adaptive graph structure를 다음과 같이 얻음 : \(\mathbf{A}_p=\operatorname{Softmax}\left(\operatorname{ReLU}\left(\mathbf{E}_{p, 1} \mathbf{E}_{p, 2}^T\right)\right)\)</li>
</ul>

<h3 id="423-gnns-to-model-inter-time-series-correlation">4.2.3. GNNS TO MODEL INTER-TIME SERIES CORRELATION</h3>

<ul>
  <li>다음으로 dynamic inter-time series correlation at a patch-level resolution을 얻음
    <ul>
      <li>\(\mathbf{H}_p=\operatorname{ReLU}\left(\sum_{m=0}^M\left(\mathbf{A}_p\right)^m \mathbf{H}_p^{t f} \mathbf{W}_m^{g n n}\right) \in \mathbb{R}^{N \times D}\).</li>
      <li>where $M$ is the number of layers for GNNs, and $\mathbf{W}_m^{g n n} \in$ $\mathbb{R}^{D \times D}$ are learnable parameters at $m$-th layer.</li>
    </ul>
  </li>
</ul>

<h3 id="43-imts-forecasti">4.3. IMTS Forecasti</h3>

<ul>
  <li>이제 final latent representation을 얻는다 :
    <ul>
      <li>\(\mathbf{H}=\text { Flatten }\left(\left[\mathbf{H}_p\right]_{p=1}^P\right) \mathbf{W}^f \in \mathbb{R}^{N \times D_o}\), where  \(\mathbf{W}^f \in \mathbb{R}^{P D \times D_o}\) are learnable parameters.</li>
      <li>각 변수마다 이 representation을 얻는다</li>
    </ul>
  </li>
  <li>n-번째 변수의 final latent representation \(\mathbf{H}^n \in \mathbf{H}\)과, forecasting query \(\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)를 가지고 MLP에 넣는다</li>
  <li>
    <p>\(\hat{x}_j^n=\operatorname{MLP}\left(\left[\mathbf{H}^n \| \phi\left(q_j^n\right)\right]\right)\).</p>
  </li>
  <li>모델은 각 변수의 예측의 MSE를 줄이는 방향으로 학습
    <ul>
      <li>\(\mathcal{L}=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2\).</li>
    </ul>
  </li>
</ul>

<h3 id="44-analysis-on-scalabil">4.4. Analysis on Scalabil</h3>

<ul>
  <li>The average sequence length : \(L_{t p}=L_{a v g} \leq L_{\max } \leq L_{c p r} \leq N \times L_{a v g}\), where
    <ul>
      <li>\(L_{\text {avg }}=\frac{1}{N} \sum_{n=1}^N L_n\).</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-experimental-setup">5.1. Experimental Setup</h3>

<ul>
  <li>Dataset :
    <ul>
      <li>PhysioNet, MIMIC, Human Activity, and USHCN</li>
      <li>training, validation, and test sets adhering to ratios of 60%, 20%, and 20%</li>
    </ul>
  </li>
  <li>Evaluation Metric :
    <ul>
      <li>\(\begin{aligned}
\text { MSE }&amp;=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2, \\\text { MAE }&amp;=
 \frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left|\hat{x}_j^n-x_j^n\right| .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h4 id="52-main-results">5.2. Main Results</h4>

<p><img src="/assets/img/timeseries/TPatchGNN/table1.png" alt="그림1" /></p>

<h3 id="53-ablation-study">5.3. Ablation Study</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table2.png" alt="그림1" /></p>

<h3 id="54-scalability-and-efficiency-analysis">5.4. Scalability and Efficiency Analysis</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table3.png" alt="그림1" /></p>

<h3 id="55-effect-of-patch-size">5.5. Effect of Patch Size</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/fig4.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>achieved the alignment between asynchronous IMTS
        <ul>
          <li>by transforming each univariate irregular time series into a series of transformable patches with varying observation counts but maintaining unified time horizon resolution.</li>
          <li>without a canonical pre-alignment representation process, preventing the aligned sequence length from explosively growing</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2024](https://openreview.net/pdf?id=UZlMXUGI6e)]]></summary></entry><entry><title type="html">Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-10-03-Mamba360/" rel="alternate" type="text/html" title="Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)" /><published>2024-10-03T00:00:00+09:00</published><updated>2024-10-03T20:01:49+09:00</updated><id>http://localhost:4000/timeseries/Mamba360</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-03-Mamba360/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Sequence modeling에서 RNN, LSTM을 사용했었음</li>
  <li>Transformer가 훌륭한 성능을 보여주었음
    <ul>
      <li>but \(O(N^2)\) complexity,inductive bias handling이 어려움</li>
    </ul>
  </li>
  <li>본 논문에서는 State Space Model (SSM)를 크게 3가지 카테고리로 분류
    <ul>
      <li>Gating architectures</li>
      <li>Structural architectures</li>
      <li>Recurrent architectures</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>RNN
    <ul>
      <li>look at only the last state and current input for predicting the next state</li>
      <li>gradient calculations being limited to the hidden state and current input</li>
      <li>exploding or vanishing gradient problem</li>
      <li>lack sufficient memory for long sequences</li>
    </ul>
  </li>
  <li>LSTM
    <ul>
      <li>complexity with their gating mechanisms</li>
      <li>exhibit challenges in transfer learning</li>
    </ul>
  </li>
  <li>Transformer
    <ul>
      <li>enable each token to interact with every other token in the input sequence</li>
      <li>but \(O(N^2)\) complexity</li>
    </ul>
  </li>
  <li>State Space Model (SSM)
    <ul>
      <li>Understanding of State Space Models (SSMs) : mathematical fundamentals</li>
      <li>Categorization and Recent Advances of SSMs : systematic categorization</li>
      <li>Application of SSMs Across Domains : utility in domains</li>
      <li>Performance Comparison of SSMs with Transformers : SSM과 Transformer 비교</li>
    </ul>
  </li>
</ul>

<h2 id="2-basics-of-state-space-model">2. Basics of State Space Model</h2>

<ul>
  <li>High-order를 first-order derivatives와 vector quantities로 representation</li>
  <li>Dynamics of  damped mass-spring system : \(m \frac{d^2 y(t)}{d t^2}+c \frac{d y(t)}{d t}+k y(t)=u(t)\)
    <ul>
      <li>\(u(t)\) : 질량에 작용하는 외부 힘</li>
      <li>\(y(t)\) : 수직 위치</li>
      <li>\(x(t)\) : 이 방정식을 1차 미분과 벡터 양으로 표현하기 위해 도입하는 벡터</li>
    </ul>
  </li>
</ul>

<h3 id="21-spring-mass-damper-system">2.1. Spring Mass-Damper system</h3>

<ul>
  <li>State Variables
    <ul>
      <li>\(x_1\) : equilibrium으로부터 질량의 위치</li>
      <li>\(\dot{x_1}\) : 질량의 속도</li>
    </ul>
  </li>
  <li>System Dynamics
    <ul>
      <li>뉴턴의 제 2법칙으로 표현하면 \(m \ddot{x}_1=-k x_1-c \dot{x}_1\)</li>
      <li>\(\ddot{x_1}\)는 질량의 가속도, \(-kx_1\)은 위치에 비례하는 스프링의 힘,</li>
      <li>\(c\dot{x_1}\)은 속도에 비례하는 damping force (운동 에너지 감쇠시키는 힘)</li>
    </ul>
  </li>
  <li>State-Space Formulation
    <ul>
      <li>State vector \(x \in \mathbb R^n\) : 시스템의 내부 상태 변수</li>
      <li>Input vector \(u\in \mathbb R^m\) : 시스템에 대한 제어 또는 외부 입력</li>
      <li>Output vector \(y \in \mathbb R^p\) : 관심 있는 측정 가능한 양</li>
      <li>System dynamics : 일차 미분 방정식으로 표현 \(\dot{\mathbf x}=\mathbf A \mathbf x+\mathbf B \mathbf u\)
        <ul>
          <li>\(\mathbf x=\left[x_1, \dot{x}_1\right]^T\)는 state vector, \(\mathbf u\)는 input,</li>
          <li>\(\mathbf A\in\mathbb R^{n \times n}\)는 dynamic matrix \(\mathbf A=\left[\begin{array}{cc}
0 &amp; 1 \\
-\frac{k}{m} &amp; -\frac{c}{m}
\end{array}\right]\)</li>
          <li>\(\mathbf B \in \mathbb R^{n \times m}\)은 input matrix \(\mathbf B=\left[\begin{array}{c}
0 \\
\frac{1}{m}
\end{array}\right] \dot{\mathbf x}\)</li>
          <li>Output equation : \(\mathbf{y}=\mathbf{C x}+\mathbf{D u}\)
            <ul>
              <li>\(\mathbf{C} \in \mathbb{R}^{p \times n}\)는 output or sensor matrix</li>
              <li>\(\mathbf{D} \in \mathbb{R}^{p \times m}\)는 feedthrough matrix</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-state-space-model">2.2. State Space Model</h3>

<ul>
  <li>Definition
    <ul>
      <li>Discrete-time dynamical system :
        <ul>
          <li>\(x(t+1)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t=0,1,2,\)…</li>
          <li>\(x(t) \in \mathbb{R}^n\) : t시점에서 state</li>
          <li>\(u(t) \in\mathbb{R}^p\) : control variables</li>
          <li>\(y(t) \in\mathbb{R}^k\)​ : specific outputs of interest</li>
        </ul>
      </li>
      <li>Continuous-time model
        <ul>
          <li>\(\frac{d}{d t} x(t)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t \geq 0\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/fig3.png" alt="그림1" /></p>

<ul>
  <li>Model Formulation
    <ul>
      <li>Complexity 때문에 Multi-head self-attention 대신 SSM 사용</li>
      <li>Continuous-time Latent State space는 linear ordinary differential equation으로 표현
        <ul>
          <li>\(\begin{aligned} \dot{x}(t) &amp; =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
y(t) &amp; =\boldsymbol{C} x(t)+\boldsymbol{D} u(t) \end{aligned}\),</li>
          <li>evolution parameter \(A \in \mathcal{R}^{N \times N}\)</li>
          <li>projection parameter \(B \in \mathcal{R}^{N \times 1} \text { and } C \in \mathcal{R}^{N \times 1}\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Discrete-time SSM
    <ul>
      <li>continuous parameters \(A, B, C\) 를 discrete하게 바꾸기 위해 time-scale parameter \(\Delta\) 사용</li>
      <li>즉 \(\bar{A}=f_A(\Delta, A), \bar{B}=f_B(\Delta, A, B)\)​</li>
      <li>\(\begin{array}{lll}
x_k=\overline{\boldsymbol{A}} x_{k-1}+\overline{\boldsymbol{B}} u_k &amp; \overline{\boldsymbol{A}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1}(\boldsymbol{I}+\Delta / 2 \cdot \boldsymbol{A}) &amp; \\
y_k=\overline{\boldsymbol{C}} x_k &amp; \overline{\boldsymbol{B}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1} \Delta \boldsymbol{B} &amp; \overline{\boldsymbol{C}}=\boldsymbol{C}\end{array}\)​.</li>
      <li>원래는 위처럼 생겼음</li>
    </ul>
  </li>
  <li>Convolutional Kernel Representation
    <ul>
      <li>하지만 위 식은 sequential nature 때문에 trainable하지 않음</li>
      <li>그래서 아래처럼 continuous convolution을 사용</li>
      <li>\(\begin{array}{lll}
x_0=\overline{\boldsymbol{B}} u_0 &amp; x_1=\overline{\boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{B}} u_1 &amp; x_2=\overline{\boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{B}} u_2 \\
y_0=\overline{\boldsymbol{C} B} u_0 &amp; y_1=\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{C} B} u_1 &amp; y_2=\overline{\boldsymbol{C} \boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{C} B} u_2
\end{array}\).</li>
      <li>vectorize하면 아래와 같음</li>
      <li>\(\begin{aligned}
y_k &amp; =\overline{\boldsymbol{C A}}^k \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C A}}^{k-1} \overline{\boldsymbol{B}} u_1+\cdots+\overline{\boldsymbol{C} \boldsymbol{A B}} u_{k-1}+\overline{\boldsymbol{C} \boldsymbol{B}} u_k \\
y &amp; =\overline{\boldsymbol{K}} * u \\
\overline{\boldsymbol{K}} \in \mathbb{R}^L: &amp; =\mathcal{K}_L(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}}):=\left(\overline{\boldsymbol{C}}^i \overline{\boldsymbol{B}}\right)_{i \in[L]}=\left(\overline{\boldsymbol{C B}}, \overline{\boldsymbol{C} \boldsymbol{A B}}, \ldots, \overline{\boldsymbol{C}}^{L-1} \overline{\boldsymbol{B}}\right) .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h2 id="3-recent-advances-in-state-space-models">3. Recent Advances in State Space Models</h2>

<ul>
  <li>Transformer의 limitations :
    <ul>
      <li>Computational Complexity</li>
      <li>Large Memory Requirements : for storing embeddings and intermediate actiavations</li>
      <li>Fixed Sequence Length : du to positional embeddings</li>
      <li>Attention Mechanism Scalability : quadratic scaling with input length</li>
      <li>Lack of Causality in Standard Attention : not inherently capture causality</li>
    </ul>
  </li>
  <li>SSM의 categorization : 어떻게 long sequence를 다룰 것인가
    <ul>
      <li>Structured SSMs : based on S4 and variants</li>
      <li>Recurrent SSMs : based on RNNs and variants</li>
      <li>Gated SSMs : leveraging gating techniques</li>
      <li>Miscellaneous SSMs : 기타 다양한 방법들</li>
    </ul>
  </li>
</ul>

<h3 id="31-structured-ssms">3.1. Structured SSMs</h3>

<ul>
  <li>
    <p>S4, HiPPO, H3, Liquid-S4 등…</p>
  </li>
  <li>
    <p>long-range dependency를 효율적으로 파악하기 위해 다음과 같은 방법 사용 :</p>

    <ul>
      <li>
        <p>polynomial projection operators</p>
      </li>
      <li>
        <p>multi-input multi-output systems</p>
      </li>
      <li>
        <p>and convolutional kernels</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="311-structured-state-space-sequence-s4">3.1.1. Structured State Space Sequence (S4)</h3>

<ul>
  <li>Higher-Order Polynomial Project Operator (HiPPO)
    <ul>
      <li>State and input transition matrices를 효율적으로 memorize</li>
    </ul>
  </li>
  <li>Diagonal Plus Low-Rank Parametrization</li>
  <li>SSM matrix (A)의 rank를 낮게 해서 diagonalizability and stability 보장</li>
  <li>Efficient (convolutional) Kernel Computation
    <ul>
      <li>FFT와 iFFT 사용해서 complexity를 \(𝑂(𝑁 log(𝑁))\)로 만듬</li>
    </ul>
  </li>
</ul>

<h3 id="312-high-order-polynomial-projection-operators-hippo">3.1.2. High-Order Polynomial Projection Operators (HiPPO)</h3>

<ul>
  <li>S4에 사용된 행렬의 수학적인 해석을 제공</li>
  <li>4가지의 변형을 사용하는데,
    <ul>
      <li>the truncated Fourier basis polynomial (Hippo-FouT)</li>
      <li>based on Lagurre polynomials(LagT)</li>
      <li>based on Legendre polynomials(LegT)</li>
      <li>based on Legendre polynomials with a sliding window(LegS)</li>
    </ul>
  </li>
</ul>

<h3 id="313-hungry-hungry-hippo-h3">3.1.3. Hungry Hungry HiPPO (H3)</h3>

<ul>
  <li>SSM에서의 2개의 challenges
    <ul>
      <li>첫째, difficulty in recalling earlier tokens
        <ul>
          <li>시퀀스 내에서 이전 토큰을 기억하는 데 어려움</li>
        </ul>
      </li>
      <li>둘째, difficult in comparing the tokens across different sequences
        <ul>
          <li>서로 다른 시퀀스에서 토큰을 비교하는 데 어려움</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>극복하기 위한 새로운 방법의 3가지 핵심 요소
    <ul>
      <li>Multiplicative Interactions가 있는 Stacked SSMs
        <ul>
          <li>stacking two SSMs with multiplicative interactions between their input and output projections</li>
        </ul>
      </li>
      <li>학습 효율성을 위한 FlashConv
        <ul>
          <li>FFT를 사용하여  training efficiency 향상</li>
        </ul>
      </li>
      <li>Scaling을 위한 State-Passing
        <ul>
          <li>effectively splits the input into the largest possible chunks that can fit</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="314-global-convolution">3.1.4. Global Convolution</h3>

<ul>
  <li>원래는 input만큼 긴 conv kernel을 hidden state matrix에 곱했는데 - 불안정함</li>
  <li>이 conv kernel을 parametrizing하는 방법을 제안</li>
  <li>일반적으로 conv kernel은 FFT를 사용하는데, 느릴 수가 있어서 IO-aware algorithm 사용</li>
</ul>

<h3 id="317-ldstack">3.1.7. LDStack</h3>

<ul>
  <li>RNN이 다중 입력 다중 출력(MIMO)  Linear Dynamical System(LDS)으로 표현될 수 있음</li>
  <li>이 때 Parallel scan이 사용됨</li>
  <li>즉  Single Input Multiple Outputs (SIMO) LDS를 합쳐서  MIMO LDS를 approximate
    <ul>
      <li>essential characteristics를 유지하면서도 계산은 simple해짐</li>
    </ul>
  </li>
  <li>LDS를 time-varying state space models로 볼 수 있음</li>
</ul>

<h3 id="318-s5">3.1.8 S5</h3>

<ul>
  <li>RNN을 다중 입력 다중 출력 선형 동적 시스템(LDS)으로 모델링한 LDStack을 state space models (SSMs)으로 확장</li>
  <li>LDStack과 달리, S5 계층은 여러 입력 및 출력을 동시에 처리</li>
</ul>

<h2 id="32-gated-ssms">3.2. Gated SSMs</h2>

<ul>
  <li>FFT 연산 최적화를 위해 gating units를 사용</li>
  <li>Toepliz NN은 position-encoded Toeplitz matrix로 token mixing</li>
  <li>Mamba는 gated MLP로 SSM의 compoutational inefficiency 극복하고자 함</li>
  <li>(무슨 말 ?) 더 읽어보자</li>
</ul>

<h3 id="323-toeplitz-neural-network-tnn">3.2.3. Toeplitz Neural Network (TNN)</h3>

<ul>
  <li>Transformer의 <strong>attention-mechanism</strong>과 <strong>positional embedding</strong>을 개선</li>
  <li>position-encoded Toeplitz matrix를 사용하여 token-pair 관계 파악
    <ul>
      <li>space-time complexity를 \(O(NlogN)\)으로 줄임</li>
      <li>Relative Position Encoder (RPE)로 상대적 위치를 생성해서 parameters가 input length에 독립적이게 함</li>
    </ul>
  </li>
</ul>

<h3 id="324-mamba">3.2.4. Mamba</h3>

<ul>
  <li>Transformer의 quadratic computational and memory complexity에 주목</li>
  <li>특히 SSM은  addressing tasks (selective copying, induction head)에서 비효율적이었음</li>
  <li>Mamba가 이 문제를 푸는 방법은 :
    <ul>
      <li>novel parametrization approach for SSMs based on input characteristics</li>
      <li>incorporating a simple selection mechanism</li>
      <li>efficient hardware-aware algorithm based on selective scan</li>
      <li>gated technique to reduce the dimensionality of global kernel operations</li>
      <li>combine gated MLP[93] with the SSM module</li>
    </ul>
  </li>
</ul>

<h2 id="4-applications-of-state-space-models">4. Applications of State Space Models</h2>

<h3 id="41-language-domain-long-sequence">4.1. Language Domain (long sequence)</h3>

<ul>
  <li>원래는 Transformer 많이 썼는데 \(O(N^2)\) quadratic complexity \(\to\) long sequence 불가능</li>
  <li>그래서  State Space Models (SSMs)이 등장
    <ul>
      <li>input data를 fixed-size latent state에 표현</li>
      <li>하지만 그러다보니 capability to retrieve and copy에서 trade-off</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table2.png" alt="그림1" /></p>

<h3 id="42-vision-domain">4.2. Vision domain</h3>

<ul>
  <li>Vision Mamba나 SiMBA와 같은 Vision-specific Mamba
    <ul>
      <li>utilize bidirectional and visual state space models</li>
    </ul>
  </li>
  <li>SiMBA
    <ul>
      <li>sequence length and channel dimensions이 꼭 perfect square dimensions이 아니어도 됨</li>
      <li>pyramid version of the transformer architecture (성능 향상)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table3.png" alt="그림1" /></p>

<h3 id="47-time-series-domain">4.7. Time Series Domain</h3>

<ul>
  <li>옛날에는 ARIMA 쓰다가 Transformer 등장하면서 variants가 많이 나옴
    <ul>
      <li>Informer, FEDFormer, PatchTST…</li>
      <li>하지만 여전히 attention complexity 때문에 long-range dependency 못잡음</li>
    </ul>
  </li>
  <li>그래서 SSM 모델인 Timemachine, SiMBA, MambaMix 등장</li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table11.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/Mamba360/table14.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>SSM은 3가지 범주로 분류 가능 (structured, gated, and recurrent)</li>
  <li>아직 Transformer가 더 잘하는 영역이 있긴 하지만 (맥락에서 정보를 검색하는 작업 등)
    <ul>
      <li>SiMBA는 트랜스포머와 Mamba 아키텍처를 결합해서 Time series에서 SOTA</li>
    </ul>
  </li>
  <li>SSM을 large network로 안정적으로 확하는 것이 아직 해결되지 않은 문제</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/abs/2404.16112)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2022) Raindrop</title><link href="http://localhost:4000/pytorch/2024-09-24-raindrop/" rel="alternate" type="text/html" title="(Code Review, ICLR 2022) Raindrop" /><published>2024-09-24T00:00:00+09:00</published><updated>2024-09-24T18:34:03+09:00</updated><id>http://localhost:4000/pytorch/raindrop</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-24-raindrop/"><![CDATA[<p><a href="https://arxiv.org/abs/2110.05357">(Paper) Graph-Guided Network for Irregularly Sampled Multivariate Time Series</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">(Paper Review, ICLR 2022) Raindrop</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig1.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig2.png" alt="사진1" /></p>

<ul>
  <li>먼저 터미널에 git clone과 requirements를 입력하여 install 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">python Raindrop.py</code>로 P19, P12, PAM 데이터셋에 대한 성능을 볼 수 있다.</li>
</ul>

<h2 id="2-raindroppy">2. Raindrop.py</h2>

<h3 id="21-data-preparing">2.1. Data Preparing</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig3.png" alt="사진1" />
<img src="/assets/img/pytorch/raindrop_code/fig4.png" alt="사진1" /></p>

<ul>
  <li>parser를 통해 aurgments를 만들고</li>
  <li>본 논문에서 제시하는 model은 irregular time series를 다룬다.
    <ul>
      <li>그러므로 <code class="language-plaintext highlighter-rouge">missing ratio</code>, 즉 feature를 masking하는 비율을 미리 결정해준다. (option)</li>
      <li>일단은 0(no missing)으로 두고 코드를 이해해보자</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig5.png" alt="사진1" /></p>

<ul>
  <li>사전에 정한 <code class="language-plaintext highlighter-rouge">missing ratio</code>를 사용한다.</li>
  <li>epoch 수와 learning rate도 미리 정한다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig6.png" alt="사진1" /></p>

<ul>
  <li>P19 데이터셋을 사용한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">d_static</code>과 <code class="language-plaintext highlighter-rouge">d_inp</code>는 시간에 따라 변하지 않는(정적) / 변하는(동적) 변수의 개수</li>
      <li><code class="language-plaintext highlighter-rouge">static_info</code>는 <code class="language-plaintext highlighter-rouge">d_static</code> 변수가 있는지 없는지 (bool)</li>
      <li><code class="language-plaintext highlighter-rouge">max_len</code>은, batch 내 샘플마다 시계열의 길이가 다른데 최대 길이
        <ul>
          <li>만약 <code class="language-plaintext highlighter-rouge">max_len</code>보다 짧다면 그 부분은 다 0으로 기록되어있다.</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">n_classes</code>는 샘플에 속하는 class의 개수</li>
    </ul>
  </li>
  <li>
    <p>다른 데이터셋을 사용한다면 위의 변수들은 달라질 수 있다.</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">d_ob</code>는 각 변수를 몇 차원으로 표현할지를 의미한다.</li>
  <li>그래서 <code class="language-plaintext highlighter-rouge">d_model</code>은 동적 변수의 개수인 <code class="language-plaintext highlighter-rouge">d_inp</code>에 <code class="language-plaintext highlighter-rouge">d_ob</code>를 곱한 값이 된다.</li>
  <li><code class="language-plaintext highlighter-rouge">nhid</code>는 FFN의 dimension인데 <code class="language-plaintext highlighter-rouge">d_model</code>의 2배를 사용</li>
  <li><code class="language-plaintext highlighter-rouge">nlayers</code>는 layer의 개수, <code class="language-plaintext highlighter-rouge">nhead</code>는 MHA(multi-head attention)에서 heads 개수이고 모두 2개를 사용</li>
  <li><code class="language-plaintext highlighter-rouge">dropout</code>은 TransformerEncoderLayer에서 사용하는 dropout ratio</li>
  <li><code class="language-plaintext highlighter-rouge">aggreg</code>는 나중에 각 배치마다, 각 시점을 vector로 표현할텐데 그걸 모든 시점에 대해 합칠 때 <strong>평균</strong>을 사용</li>
  <li><code class="language-plaintext highlighter-rouge">MAX</code>는 positional encoder에 들어가는 MAX parameter인데
    <ul>
      <li>막상 positional encoder 코드를 보면 <code class="language-plaintext highlighter-rouge">MAX</code>라는 변수를 사용하지 않으니 신경 안써도 된다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig7.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n_run</code>은 데이터셋에 대해 몇 번을 실험해서 기록할지를 의미한다.</li>
  <li><code class="language-plaintext highlighter-rouge">n_splits</code>는 데이터가 5등분 되어있어서 5를 사용한다.</li>
  <li>그리고 본 model을 평가하기 위한 성능 지표를 기록할 arrays를 만들어놓는다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig8.png" alt="사진1" /></p>

<ul>
  <li>그리고 불러온 데이터셋을 train(0.8) / valid(0.1) / test(0.1)로 나누고 label(y)도 따로 준비한다.</li>
  <li>P19 데이터셋의 경우 train에는 31042개의 샘플이 있다. (샘플은 한 명의 환자 정도로 생각할 수 있다.)
    <ul>
      <li>그리고 각 샘플은 <code class="language-plaintext highlighter-rouge">torch.size([# of timesetps,  # of features])</code>인 tensor이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig9.png" alt="사진1" /></p>

<ul>
  <li>T는 time steps의 수, F는 (동적) 변수의 개수이고, D는 (정적) 변수의 개수가 된다.
    <ul>
      <li>동적 변수는 <code class="language-plaintext highlighter-rouge">Ptrain</code>의 <code class="language-plaintext highlighter-rouge">arr</code>에, 정적 변수는 <code class="language-plaintext highlighter-rouge">Ptrain</code>의 <code class="language-plaintext highlighter-rouge">extended_static</code>에 따로 준비하고 있다.</li>
    </ul>
  </li>
  <li>그리고 normalization을 위해 모든 변수들의 평균과 표준편차를 얻는다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">getStats</code> 함수에는 사용하는데 특이사항 없으므로 skip</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig10.png" alt="사진1" /></p>

<ul>
  <li>각각의 shape은 아래와 같다.</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>의 동적 변수들의 개수가 34개였는데 68이 된 이유는 :
    <ul>
      <li>같은 크기의 Mask를 옆에 이어붙였기 때문이다.</li>
      <li>Mask는 <code class="language-plaintext highlighter-rouge">M = 1*(input_tensor &gt; 0) + 0*(input_tensor &lt;= 0)</code>이다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>은 31042개의 텐서인데, 각 tensor는 해당 샘플의 길이를 알려준다.
    <ul>
      <li>즉 만약 0번째 샘플의 길이가 40이라면, 0번째 tensor는<code class="language-plaintext highlighter-rouge">[1, 2, ..., 40, 0, 0, ...]</code>이다.</li>
      <li>일단 숫자는 <code class="language-plaintext highlighter-rouge">max_len</code>개인데 해당 샘플의 길이까지만 index를 기록하고 뒷부분은 zero padding</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">y_train</code>은 각 샘플의 정답 label이다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig11.png" alt="사진1" /></p>

<ul>
  <li>이제 <code class="language-plaintext highlighter-rouge">global_structure</code>를 정의하는데, 각각의 동적 변수가 상호작용하는지를 0, 1로 표현
    <ul>
      <li>adjacency matrix의 역할을 한다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">missing_ratio</code>가 존재했다면 몇몇 feature를 masking한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>이 <code class="language-plaintext highlighter-rouge">sample</code>이면 각 샘플(환자)마다 독립적으로 특성을 무작위 제거</li>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>이 <code class="language-plaintext highlighter-rouge">set</code>이면 미리 계산된 density scores를 사용하여 제거할 특성을 결정하고 모든 샘플에서 제거</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig12.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>의 shape을 <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps,  batch_size,  # of features(w/masking)])</code>으로,</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>의 shape을 <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps, batch_size])</code>로 setting</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig13.png" alt="사진1" /></p>

<ul>
  <li>앞서 소개한 parameters를 한 번 출력해보았다.</li>
  <li>지금은 masking ratio가 0이다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig14.png" alt="사진1" /></p>

<ul>
  <li>parameters의 descriptions는 위와 같다.</li>
</ul>

<h3 id="22-model-setting">2.2. Model setting</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig15.png" alt="사진1" /></p>

<ul>
  <li>이제 model, criterion, optimiazer, scheduler를 정의한다.</li>
  <li>model은 2d tensor로 표현된 샘플마다 classification하도록 설계되었으므로 CrossEntropyLoss를 사용</li>
  <li>아직 input을 model에 넣은 건 아님.
    <ul>
      <li>input이 model에 들어가면 어떤 과정을 거치는지는 아래 3. <code class="language-plaintext highlighter-rouge">models_rd.py</code>에서 보도록 한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig16.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">idx_0</code>은 <code class="language-plaintext highlighter-rouge">y</code>가 0인 samples의 index, <code class="language-plaintext highlighter-rouge">idx_1</code>은 반대</li>
  <li>label이 1인 샘플의 개수가 적은 unbalancing 문제를 해결하기 위해 3배로 늘림 (왜 <strong>3</strong>배인지는 모름)</li>
  <li>batch_size가 128인데 label이 0과 1인 samples를 절반씩 채울테니
    <ul>
      <li>n_batches는 개수가 더 적은 label 기준으로 모든 samples를 한 번씩 다 볼 수 있도록 설정했다.</li>
      <li>사실 label이 1인 samples를 3배 했으니 label이 1인 샘플을 3번씩 보는 꼴이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig17.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>이제 epoch를 시작하는데, label이 0인 샘플과 1인 샘플에서 무작위로 <code class="language-plaintext highlighter-rouge">batch_size/2</code>개씩 가져온다.</li>
  <li>사실 label이 1인 samples를 3배 했으니 여기서는 중복된 샘플이 나올 가능성이 있다.</li>
  <li>model에 들어갈 input tensors의 shape을 미리 확인해두자</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig19.png" alt="사진1" /></p>

<ul>
  <li>이제 model에 들어가고 통상적인 backpropagation을 거친다.</li>
  <li>model에 들어가면 어떤 일이 일어나는지 알아보자.</li>
</ul>

<h2 id="3-models_rdpy">3. models_rd.py</h2>

<h3 id="31-init">3.1. <strong>init</strong></h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>이 상당히 많지만 지금 다 알 필요는 없다.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>에서 사용할 때 다시 올라와서 보면 될 듯</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig20.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig21.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig22.png" alt="사진1" /></p>

<h2 id="32-forward">3.2. forward</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig23.png" alt="사진1" /></p>

<ul>
  <li>P19 데이터셋의 경우 input shape은 주황색 주석과 같다.</li>
  <li>src로 들어오는 P의 경우 34개의 변수였는데 같은 크기의 Mask를 옆에 이어붙인 것이니 다시 분리
    <ul>
      <li>각각을 missing_mask, src라고 부름</li>
    </ul>
  </li>
  <li>그 다음 34개의 변수를 <code class="language-plaintext highlighter-rouge">d_ob</code>(여기선 4)번 반복해서 src의 representation capacity를 키워주고
    <ul>
      <li>ReLu를 통과시켜서 non-linearity를 표현할 수 있게 한다.</li>
      <li>그 다음 dropout을 거친다.</li>
    </ul>
  </li>
  <li>결국 <code class="language-plaintext highlighter-rouge">h</code>는 src를 확장시키고 learnable weights와 ReLu를 곱해 모델이 학습할 수 있는 형태로 만든 것</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig24.png" alt="사진1" /></p>

<ul>
  <li>이제 batch에 있는 각 sample마다 mask를 만든다.</li>
  <li>sample에 값이 있으면 mask에는 False가 되고 값이 없으면 mask가 True가 된다.</li>
  <li>mask의 길이는 60으로 고정이지만 sample마다 길이가 다르기 때문에 어디까지 False이고 언제부터 True인지는 sample마다 다르다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig25.png" alt="사진1" /></p>

<ul>
  <li>다음으로 <code class="language-plaintext highlighter-rouge">global_structure</code>를 adjacency matrix로 사용한다.
    <ul>
      <li>shape은 동적 변수의 개수 <code class="language-plaintext highlighter-rouge">d_inp</code> x <code class="language-plaintext highlighter-rouge">d_inp</code>가 되므로 각 동적 변수를 연결 여부를 (0,1)로 표현한다.</li>
      <li>epoch가 진행되면서 바뀔 수도 있으니 대각성분은 항상 1로 update해준다.</li>
    </ul>
  </li>
  <li>그 다음 edge_index와 edge_weights를 미리 구해놓는다.
    <ul>
      <li>연결된 nodes의 index와 그 weights를 의미함</li>
    </ul>
  </li>
  <li>그 다음 batch에 있는 각 sample마다 (동적) 변수들의 global structure(edge)를 고려한 representation을 저장할 공간 <code class="language-plaintext highlighter-rouge">output</code>을 미리 만들어놓는다.
    <ul>
      <li>각 sample마다 <code class="language-plaintext highlighter-rouge">torch([# of time steps,  d_inp x d_ob])</code> shape의 tensor가 들어갈 예정이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig26.png" alt="사진1" /></p>

<ul>
  <li>이제 아까 만든 <code class="language-plaintext highlighter-rouge">h</code>를 <code class="language-plaintext highlighter-rouge">x</code>로 받아서 (<code class="language-plaintext highlighter-rouge">x=h</code>) 하나의 sample에 대한 <code class="language-plaintext highlighter-rouge">h</code>를 <code class="language-plaintext highlighter-rouge">stepdata</code> 가져온다</li>
  <li><code class="language-plaintext highlighter-rouge">p_t</code>는 각 timestep을 <code class="language-plaintext highlighter-rouge">d_pe = 16</code>차원 vector로 embedding한 것이다. (init 참고)</li>
  <li>이제 <code class="language-plaintext highlighter-rouge">stepdata</code>를 <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code>로 reshape한다.
    <ul>
      <li>왜냐하면 feature끼리 attention을 수행하기 때문에 각 feature를 하나의 vector로 만들 필요가 있기 때문</li>
    </ul>
  </li>
  <li>이제 각 feature를 vector로 만든 걸 <code class="language-plaintext highlighter-rouge">ob_propagation</code>으로 정의된 attention layer에 넣는다.
    <ul>
      <li>그러면 같은 shape <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code> tensor가 return되지만</li>
      <li>해당 sample의 각각의 features를 Observation Propagation을 거쳐 representation한 결과이다.</li>
      <li><code class="language-plaintext highlighter-rouge">Ob_propagation.py</code>에 있고, 코드를 따로 첨부하지는 않겠으나 아래와 같은 과정을 거친다.
        <ul>
          <li>1) Message Passing: node 간에 정보를 전달하는 mechanism 구현</li>
          <li>2) Attention Mechanism: 각 node가 이웃 node로부터 받는 메시지의 중요도를 학습</li>
          <li>3) Egde weights: graph의 edge에 weight를 적용하여 정보 전달의 강도를 조절</li>
          <li>4) Edge prune: 중요도가 낮은 edge를 제거하여 computation efficiency 높임</li>
          <li>5) Feature Transform: linear Transform과 activation ftn으로 node의 feature를 변환</li>
          <li>6) Aggregation: 이웃 node로부터 받은 메시지를 합침</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig27.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ob_propagation-layer</code>를 한 번 더 통과시키고 shape을 맞춰서 <code class="language-plaintext highlighter-rouge">output</code>의 sample index 자리에 넣는다.
    <ul>
      <li>그리고 alpha_all에는 그 attention weights를 넣는다.
        <ul>
          <li>34개의 features끼리의 attention이니 34\(\times\)34\(=\)1156개의 숫자가 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>모든 samples에 대해서 완료하여 <code class="language-plaintext highlighter-rouge">output</code>이 완성되면 distance를 구한다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig28.png" alt="사진1" /></p>

<ul>
  <li>다음으로 time embedding을 concat한다.</li>
  <li>이러면 shape이 <code class="language-plaintext highlighter-rouge">torch.size([60, 128, 152])</code>가 되는데, 각 sample마다(128), 하나의 시점을 152차원 vector로 표현한 것이다.
    <ul>
      <li>이 152는 (동적) 변수 34개를 34\(\times\)4 = 136차원으로 표현하고, time embedding 16차원을 붙인 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig29.png" alt="사진1" /></p>

<ul>
  <li>이걸 transformer encoder에 통과시키고</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig30.png" alt="사진1" /></p>

<ul>
  <li>aggregate 하는데, 이 때 모든 시점에 대해 평균을 내준다. (<code class="language-plaintext highlighter-rouge">aggreg == mean</code>)</li>
  <li>그러면 각 sample은 모든 시점과 모든 변수를 통합하여 152차원 벡터로 표현된 결과가 나온다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig31.png" alt="사진1" /></p>

<ul>
  <li>마지막으로 (정적) 변수를 embedding한 emb를 붙여서 2-layer MLP에 넣으면</li>
  <li>각 sample에 대한 classification이 완료된다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig32.png" alt="사진1" /></p>

<ul>
  <li>Training에 따른 validation set acccuracy가 출력된다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig33.png" alt="사진1" /></p>

<ul>
  <li>그리고 classification report가 출력된다.</li>
</ul>

<p>끝 !</p>

<ul>
  <li>참고로 나의 경우에는 <code class="language-plaintext highlighter-rouge">from torch_scatter import gather_csr, scatter, segment_csr</code>가 안되어서 아래와 같이 주석 처리하고
    <ul>
      <li>pytorch를 보고 함수를 직접 작성하여 사용하였다.</li>
      <li><a href="https://github.com/rusty1s/pytorch_scatter/blob/master/torch_scatter/scatter.py">pytorch_scatter 참고</a></li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># from torch_scatter import gather_csr, scatter, segment_csr
</span><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">+</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
            <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">(),</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">other</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">src</span>

<span class="k">def</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">index</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">dim_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">dim_size</span>
        <span class="k">elif</span> <span class="n">index</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">dim_size</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="n">index_dim</span> <span class="o">=</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">index_dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index_dim</span> <span class="o">+</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">index_dim</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">size</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">index_dim</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">count</span><span class="p">[</span><span class="n">count</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="p">.</span><span class="nf">is_floating_point</span><span class="p">():</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">true_divide_</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">div_</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">floor</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="nb">reduce</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">
    |

    .. image:: https://raw.githubusercontent.com/rusty1s/pytorch_scatter/
            master/docs/source/_figures/add.svg?sanitize=true
        :align: center
        :width: 400px

    |

    Reduces all values from the :attr:`src` tensor into :attr:`out` at the
    indices specified in the :attr:`index` tensor along a given axis
    :attr:`dim`.
    For each value in :attr:`src`, its output index is specified by its index
    in :attr:`src` for dimensions outside of :attr:`dim` and by the
    corresponding value in :attr:`index` for dimension :attr:`dim`.
    The applied reduction is defined via the :attr:`reduce` argument.

    Formally, if :attr:`src` and :attr:`index` are :math:`n`-dimensional
    tensors with size :math:`(x_0, ..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})`
    and :attr:`dim` = `i`, then :attr:`out` must be an :math:`n`-dimensional
    tensor with size :math:`(x_0, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})`.
    Moreover, the values of :attr:`index` must be between :math:`0` and
    :math:`y - 1`, although no specific ordering of indices is required.
    The :attr:`index` tensor supports broadcasting in case its dimensions do
    not match with :attr:`src`.

    For one-dimensional tensors with :obj:`reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, the operation
    computes

    .. math::
        \mathrm{out}_i = \mathrm{out}_i + \sum_j~\mathrm{src}_j

    where :math:`\sum_j` is over :math:`j` such that
    :math:`\mathrm{index}_j = i`.

    .. note::

        This operation is implemented via atomic operations on the GPU and is
        therefore **non-deterministic** since the order of parallel operations
        to the same value is undetermined.
        For floating-point variables, this results in a source of variance in
        the result.

    :param src: The source tensor.
    :param index: The indices of elements to scatter.
    :param dim: The axis along which to index. (default: :obj:`-1`)
    :param out: The destination tensor.
    :param dim_size: If :attr:`out` is not given, automatically create output
        with size :attr:`dim_size` at dimension :attr:`dim`.
        If :attr:`dim_size` is not given, a minimal sized output tensor
        according to :obj:`index.max() + 1` is returned.
    :param reduce: The reduce operation (:obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">mul</span><span class="sh">"</span><span class="s">`,
        :obj:`</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="s">` or :obj:`</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="s">`). (default: :obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`)

    :rtype: :class:`Tensor`

    .. code-block:: python

        from torch_scatter import scatter

        src = torch.randn(10, 6, 64)
        index = torch.tensor([0, 1, 0, 1, 2, 1])

        # Broadcasting in the first and last dim.
        out = scatter(src, index, dim=1, reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">)

        print(out.size())

    .. code-block::

        torch.Size([10, 3, 64])
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">sum</span><span class="sh">'</span> <span class="ow">or</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">add</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mul</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">min</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">max</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span>

</code></pre></div></div>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Raindrop github](https://github.com/mims-harvard/Raindrop)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2024) Pathformer</title><link href="http://localhost:4000/pytorch/2024-09-09-pathformer/" rel="alternate" type="text/html" title="(Code Review, ICLR 2024) Pathformer" /><published>2024-09-09T00:00:00+09:00</published><updated>2024-09-24T18:29:39+09:00</updated><id>http://localhost:4000/pytorch/pathformer</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-09-pathformer/"><![CDATA[<p><a href="https://openreview.net/pdf?id=lJkOCMP2aW">(Paper) Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-05-23-Pathformer">(Paper Review, ICLR 2024) Pathformer</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig1.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig2.png" alt="사진1" /></p>

<ul>
  <li>먼저 터미널에 git clone과 requirements를 입력하여 install 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">bash scripts/multivariate/ETTm2.sh</code>로 ETTm2 데이터셋을 예측할 수 있다.</li>
</ul>

<h2 id="2-sh">2. .sh</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig3.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ETTm2.sh</code> 파일에는 <code class="language-plaintext highlighter-rouge">run.py</code>를 실행하도록 되어있다.</li>
</ul>

<h2 id="3-runpy">3. run.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig4.png" alt="사진1" /></p>

<ul>
  <li>parser를 통해 arguments를 만든다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig5.png" alt="사진1" /></p>

<ul>
  <li>그리고 <code class="language-plaintext highlighter-rouge">Exp_Main</code>에 있는 train에 arguments를 넣어준다.</li>
</ul>

<h2 id="4-exp_mainpy">4. exp_main.py</h2>

<h3 id="41-train">4.1. train</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig6.png" alt="사진1" /></p>

<ul>
  <li>model, data, optimizer, criterion을 설정하는 간단한 함수들과, <code class="language-plaintext highlighter-rouge">vali</code>, <code class="language-plaintext highlighter-rouge">train</code>, <code class="language-plaintext highlighter-rouge">test</code>, <code class="language-plaintext highlighter-rouge">predict</code> 함수가 있다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig7.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Exp_main</code> : <code class="language-plaintext highlighter-rouge">train</code>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">_get_data</code>로 train, valid, test 데이터셋을 load</li>
      <li><code class="language-plaintext highlighter-rouge">sum(p.numel() for p in self.model.parameters())</code>는 parameters 개수</li>
      <li>time, early sipping, optimizer, criterion, learning rate scheduler 정의</li>
      <li><code class="language-plaintext highlighter-rouge">lr_scheduler.OneCycleLR</code>는 learning rate를 빠르게 최대 학습률까지 증가시켰다가 다시 감소시키면서 최적화 과정
        <ul>
          <li><code class="language-plaintext highlighter-rouge">optimizer</code> : 사용하는 optimizer</li>
          <li><code class="language-plaintext highlighter-rouge">steps_per_epoch</code> : 1 epoch가 몇 번의 update가 발생하는지 (mini-batch)</li>
          <li><code class="language-plaintext highlighter-rouge">pct_start</code> : learning rate가 증가하는 구간의 비율 / <code class="language-plaintext highlighter-rouge">epochs</code> : 전체 epoch 수</li>
          <li><code class="language-plaintext highlighter-rouge">max_lr</code> : 최대 learning rate</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig8.png" alt="사진1" /></p>

<ul>
  <li>each epoch에서는 train loder에서 batch 단위로 데이터를 받고</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig9.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">with torch.cuda.amp.autocast():</code>는 <code class="language-plaintext highlighter-rouge">float16</code>과 <code class="language-plaintext highlighter-rouge">float32</code>를 자동으로 캐스팅</li>
  <li>모델이 예측한 <code class="language-plaintext highlighter-rouge">outputs</code>와 정답 <code class="language-plaintext highlighter-rouge">batch_y</code>를 비교하여 loss 계산</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig10.png" alt="사진1" /></p>

<ul>
  <li>epoch에 걸린 시간과 loss를 출력하고 backward로 parameters를 update</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig11.png" alt="사진1" /></p>

<ul>
  <li>Validation set에 대한 loss로 early stopping 여부를 결정하고 학습이 종료되면 모델 저장</li>
  <li>vali 함수는 특이 사항 없으므로 pass</li>
</ul>

<h3 id="42-test">4.2. test</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig12.png" alt="사진1" /></p>

<ul>
  <li>test dataset과, 학습되어 저장된 model을 load한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig13.png" alt="사진1" /></p>

<ul>
  <li>train과 비슷하게 batch 단위로 모델에 넣어서 예측값을 얻는다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig14.png" alt="사진1" /></p>

<ul>
  <li>batch 20개마다 묶어서 visualizaiton을 한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig15.png" alt="사진1" /></p>

<ul>
  <li>최종적인 예측과 loss를 <code class="language-plaintext highlighter-rouge">results.txt</code>에 저장한다.</li>
</ul>

<h3 id="43-predict">4.3. predict</h3>

<p>pass</p>

<h2 id="5-modelspathformerpy">5. models/Pathformer.py</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">from models import PathFormer</code> 이므로 해당 경로로 가서 pathformer의 archtecture를 보자</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">forward</code>는 normalization \(\to\) <code class="language-plaintext highlighter-rouge">start_fc</code> \(\to\) for <code class="language-plaintext highlighter-rouge">layer</code> in <code class="language-plaintext highlighter-rouge">self.AMS_lists</code> \(\to\) de-normalization로 구성된다.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>에 들어온 x의 shape은 <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes])</code>이다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">seq_len</code>은 관측하는 과거 시점 수, <code class="language-plaintext highlighter-rouge">num_nodes</code>는 multivariate에서 variates 개수</li>
    </ul>
  </li>
  <li>x가 unsqueeze되어 normalization, <code class="language-plaintext highlighter-rouge">start_fc</code>를 통과하면 <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes, d_model])</code>이 된다. (아래 <code class="language-plaintext highlighter-rouge">__init__</code> 참고)</li>
  <li>이제 <code class="language-plaintext highlighter-rouge">AMS_list</code>의 <code class="language-plaintext highlighter-rouge">layers</code>를 통과하고 denormalization을 통과한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig17.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>을 보면 <code class="language-plaintext highlighter-rouge">self.AMS_lists</code>는 <code class="language-plaintext highlighter-rouge">layers.AMS</code>에서 import한다.</li>
  <li>AMS layer가 pathformer는 전부이니 살펴보자</li>
</ul>

<h2 id="6-layersamspy">6. Layers/AMS.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">self.seasonality_and_trend_decompose</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.noisy_top_k_gating</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.cv_squared</code></li>
  <li><code class="language-plaintext highlighter-rouge">SparseDispatcher</code>와 <code class="language-plaintext highlighter-rouge">SparseDispatcher.dispatch</code>, <code class="language-plaintext highlighter-rouge">SparseDispatcher.combine</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.experts</code></li>
  <li>각각에 대해서 하나씩 살펴보도록 한다.</li>
</ul>

<h3 id="61-selfseasonality_and_trend_decompose">6.1. self.seasonality_and_trend_decompose</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig19.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig20.png" alt="사진1" /></p>

<ul>
  <li>AMS class 안에서 정의된 함수</li>
  <li><strong>seasonality와 trend를 x에서 각각 계산</strong>하기 때문에 \(seasonal + trend = x\)가 아님
    <ul>
      <li>해당 함수의 결과는 x에 seasonality와 trend를 더한 결과이다.</li>
    </ul>
  </li>
  <li>처음에 <code class="language-plaintext highlighter-rouge">x = x[:, :, :, 0]</code>은 <code class="language-plaintext highlighter-rouge">d_model</code> 차원으로 표현된 x에서 첫 번째 dimension만 사용해서 decompose한다는 의미</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig21.png" alt="사진1" /></p>

<ul>
  <li>seasonality_model은 <code class="language-plaintext highlighter-rouge">FourierLayer</code>
    <ul>
      <li>푸리에 변환(fft) 후 amplitude가 높은 frequency \(k\)​​개를 inverse 푸리에 변환(extrapolate)</li>
    </ul>
  </li>
  <li>trend_model은 <code class="language-plaintext highlighter-rouge">series_decomp_multi</code>
    <ul>
      <li>다양한 크기의 kernel size로 moving average를 softmax</li>
    </ul>
  </li>
</ul>

<h3 id="62-selfnoisy_top_k_gating">6.2. self.noisy_top_k_gating</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig22.png" alt="사진1" />
<img src="/assets/img/pytorch/pathformer_code/fig23.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">start_linear.squeeze</code>와 <code class="language-plaintext highlighter-rouge">w_gate</code>로 <code class="language-plaintext highlighter-rouge">torch.Size([batch, seq_len, num_node])</code>가 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>가 된다.</li>
  <li>같은 크기 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>의 <code class="language-plaintext highlighter-rouge">logit</code>을 만들고 \(top-k\) logit을 <code class="language-plaintext highlighter-rouge">gates</code>에 넣는다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">scatter</code>는 특정 인덱스 위치에 값을 할당하는 함수이다.</li>
      <li><code class="language-plaintext highlighter-rouge">gate</code>의 shape은 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_experts])</code>가 되는데, 각 행(batch)에서 k개를 제외하고는 다 0이다.</li>
      <li>그리고 각 행(batch)마다 그 k개가 어떤 experts인지는 다르다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">load</code>는 각 expert가 배치 전체에서 얼마나 선택되었는지에 대한 비율을 의미한다.
    <ul>
      <li>shape은 <code class="language-plaintext highlighter-rouge">torch.Size([num_experts])</code>이다.</li>
    </ul>
  </li>
</ul>

<h3 id="63-selfcv_squared">6.3. self.cv_squared</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>다시 AMS.forward로 돌아오자</li>
  <li>각 expert마다 모든 배치에 대해 sum을 해서 <code class="language-plaintext highlighter-rouge">importance</code>를 계산하면 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자가 된다.</li>
  <li><code class="language-plaintext highlighter-rouge">cv_squared</code>를 통해 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자의 변동계수를 구해서 <code class="language-plaintext highlighter-rouge">balance_loss</code>를 구한다.
    <ul>
      <li>변동계수는 \(\frac{\sigma^2}{\mu^2}\)이다.</li>
      <li>이 값이 크면 특정 experts에 importance가 몰려있음을 의미한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig24.png" alt="사진1" /></p>

<h3 id="64-sparsedispatcher-어려움-주의">6.4. SparseDispatcher (*어려움 주의)</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig25.png" alt="사진1" />
<img src="/assets/img/pytorch/pathformer_code/fig26.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>에서 준비해놓는 것들이 많으니 하나하나 보도록 한다. \(k=2\), <code class="language-plaintext highlighter-rouge">num_experts</code>=4인 경우이다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig27.png" alt="사진1" /></p>
<ul>
  <li>각 행은 batch를 의미하기 때문에 행의 개수는 batch size (여기선 512)</li>
  <li>각 행에는 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자가 있고 그 중 \(k\)개만 non-negative, 나머지는 0</li>
  <li>첫 행에서 2, 3번째 숫자가 양수라는 것은, 첫번째 배치에서 2, 3번째 experts가 선택되었다는 것을 의미</li>
  <li>바로 아래에 있는 <code class="language-plaintext highlighter-rouge">torch.nonzero(gates)</code>에서도 그 사실을 알 수 있다.
    <ul>
      <li>첫 번째 배치에서는 index 1, 2인 experts가, 마지막 배치에서는 index 1, 2인 experts가 선택됨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig28.png" alt="사진1" /></p>
<ul>
  <li>이제 sort를 하는데 첫번째 열은 어차피 index라서 정렬되어있고
    <ul>
      <li>(두 번째 열이 정렬되면서 섞이기 때문에 두 번째 열의 숫자가 첫 번째 열의 배치 index와 상관 없게 된다)</li>
    </ul>
  </li>
  <li>그리고 <code class="language-plaintext highlighter-rouge">index_sorted_experts</code>는 정렬된 숫자가 몇 번째 index에 있던 숫자인지를 표시해준다.
    <ul>
      <li>(여기서부터 헷갈리기 시작함)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig29.png" alt="사진1" /></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">self._expert_index</code>는 각 배치에서 선택된 experts의 index를 정렬한 것이다.
    <ul>
      <li>각 배치마다 \(k\)개씩 있으니 총 batch_size \(\times k\)개의 숫자겠다.</li>
    </ul>
  </li>
  <li>그리고 그걸 다시 batch index로 되돌릴 수가 있을 것이다.
    <ul>
      <li>즉 <code class="language-plaintext highlighter-rouge">self._batch_index</code>가 1, 3, 6,…이라는 것은 expert 0이 선택되었던 batch가 1, 3, …이고</li>
      <li>그 다음 expert 1이 선택된 batch들이 몇 번째 batch인지 쭉 나열이 된다. (이걸 마지막 expert까지 반복)</li>
    </ul>
  </li>
  <li>마지막으로 <code class="language-plaintext highlighter-rouge">self._part_sizes</code>는 모든 batches 통틀어서 각 expert가 몇 번 선택되었는지를 의미한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig30.png" alt="사진1" /></p>
<ul>
  <li>이제 <code class="language-plaintext highlighter-rouge">gates_exp</code>는 expert 0이 선택되었던 batches를 쭉 나열하고, 그 다음에 expert 1이 선택되었던 batches를 쭉 나열하고… 마지막 expert가 선택되었던 batches까지 나열한 것이다.</li>
  <li>그리고 <code class="language-plaintext highlighter-rouge">self._nonzero_gates</code>는 expert \(i\) (\(i = 1, ...,\) <code class="language-plaintext highlighter-rouge">num_experts</code>)가 선택된 배치에서 expert \(i\)의 gates를 나열한 것이다.</li>
</ul>

<h3 id="641-sparsedispatcherdispatch">6.4.1. SparseDispatcher.dispatch</h3>

<ul>
  <li>이제 dispatch에서는 각 expert에 처리해야 할 batches를 할당한다.</li>
  <li>만약 지금처럼 inp의 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([512, 96, 7, 16])</code>, <code class="language-plaintext highlighter-rouge">self._batch_index</code>의 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([1024])</code>, 그리고 <code class="language-plaintext highlighter-rouge">self._part_sizes</code>가 <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>라고 가정하면:</li>
  <li><code class="language-plaintext highlighter-rouge">inp[self._batch_index]</code>에서는 inp 텐서에서 1024개의 샘플을 선택하여, 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([1024, 96, 7, 16])</code>인 새로운 텐서를 생성한다.</li>
  <li>그리고 첫 번째 차원(batch 차원, 1024개)을 <code class="language-plaintext highlighter-rouge">self._part_sizes</code> = <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>로 나눈다.</li>
  <li>결과는 각 expert에게 할당된 batches의 리스트이며, 각 텐서의 크기는:
    <ul>
      <li>첫 번째 expert: [262, 96, 7, 16]</li>
      <li>두 번째 expert: [348, 96, 7, 16]</li>
      <li>세 번째 expert: [249, 96, 7, 16]</li>
      <li>네 번째 expert: [165, 96, 7, 16]</li>
    </ul>
  </li>
  <li>이걸 리스트로 return한다.</li>
</ul>

<h3 id="642-sparsedispatchercombine">6.4.2. SparseDispatcher.combine</h3>

<ul>
  <li>이제 각각을 해당 expert에 통과시킨다.</li>
  <li>expert는 <code class="language-plaintext highlighter-rouge">TransformerLayer</code>이다. (Pathformer.py의 __init__참고)</li>
  <li>그리고 그 결과를 다시 combine한다.
    <ul>
      <li>그런데 위에서 combine 함수를 잘 보면 처음에 <code class="language-plaintext highlighter-rouge">.exp()</code>를 하고 다시 <code class="language-plaintext highlighter-rouge">.log()</code>를 해주는데,</li>
      <li><code class="language-plaintext highlighter-rouge">.exp()</code>에서 NaN이 나올 수가 있으니 주의하자.</li>
      <li>(벤치마크 데이터셋에서는 해당사항 없지만 내 프로젝트에서 사용하는 데이터에서는 발생했다.)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>이제 residual_connection만 적용해주면 끝난다.</li>
  <li>여기까지가 하나의 <code class="language-plaintext highlighter-rouge">AMS</code> layer이다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="사진1" /></p>

<ul>
  <li>여기서 for 안에 있는 layer가 AMS layer이다.</li>
  <li>
    <p>마지막으로 de-normalization을 하면 끝이다.</p>
  </li>
  <li>나머지는 위에서 이미 소개한 <code class="language-plaintext highlighter-rouge">3. run.py</code>와 <code class="language-plaintext highlighter-rouge">4. exp_main.py</code>가 전부이다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Pathformer github](https://github.com/decisionintelligence/pathformer)]]></summary></entry><entry><title type="html">Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIR’24 Best Paper)</title><link href="http://localhost:4000/timeseries/2024-09-03-SyNCRec/" rel="alternate" type="text/html" title="Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIR’24 Best Paper)" /><published>2024-09-03T00:00:00+09:00</published><updated>2024-09-03T15:04:04+09:00</updated><id>http://localhost:4000/timeseries/SyNCRec</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-09-03-SyNCRec/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Cross-Domain Sequential Recommendation (CDSR)은 multiple domain에서의 정보를 활용하여 Single-Domain Sequential Recommendation (SDSR)보다 좋은 성능을 보여주었음</li>
  <li>하지만 <strong>negative transfer</strong> : lack of relation btw domains은 성능 저하의 원인</li>
  <li>그래서 본 논문에서는
    <ol>
      <li>estimates the degree of <strong>negative transfer</strong> of each domain</li>
      <li>adaptively assigns it as a <strong>weight factor</strong> to the prediction loss
        <ul>
          <li>to control gradient flows through domains with significant negative transfer !</li>
        </ul>
      </li>
      <li>developed <strong>auxiliary loss</strong> that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis</li>
    </ol>
  </li>
  <li>이러한 CDSR과 SDSR의 cooperative learning은 collaborative dynamics between pacers and runners in a marathon와 유사함</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Single-Domain Sequential Recommendation (SDSR)
    <ul>
      <li>focuses on <strong>recommending the next item</strong> within a <strong>specific</strong> domain using <strong>only</strong> the <strong>single</strong>-domain sequence</li>
    </ul>
  </li>
  <li>Cross-Domain Sequential Recommendation (CDSR)
    <ul>
      <li><strong>predicts</strong> the <strong>next item</strong> a user will interact with, by leveraging their historical <strong>interaction</strong> sequences across <strong>multiple</strong> domains</li>
    </ul>
  </li>
  <li>둘의 차이는 결국 다른 domains의 정보를 활용하는지 여부</li>
  <li>CDSR은 성능 향상을 위해 다른 domains의 정보를 활용하지만 항상 성능이 향상되는 건 아님
    <ul>
      <li>만약 그것 때문에 성능이 더 안좋아진다면, 그건 <strong>negative transfer</strong>가 있었기 때문</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p>본 논문에서는 SyNCRec: Asymmetric Cooperative Network for Cross-Domain Sequential Recommendation을 제안</p>
  </li>
  <li>
    <ol>
      <li>assess the degree of <strong>negative transfer</strong> of each domain
        <ul>
          <li>by comparing the performance of CDSR and SDSR</li>
        </ul>
      </li>
      <li>adaptively assign this value as <strong>weight to the prediction loss</strong> corresponding to a specific domain
        <ul>
          <li>to reduces its flow in domains with significant negative transfer !</li>
        </ul>
      </li>
      <li>developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis
        <ul>
          <li>to exploit the effective correlation signals inherent in the representation pairs of SDSR and CDSR tasks within a specific domain</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>SDSR은 negative transfer를 줄이기 위한 pacer의 역할을 함
    <ul>
      <li>(마라톤에서 runner가 너무 빠르거나 느리게 하지 않게 해주는 pacer)</li>
    </ul>
  </li>
  <li>특히 CDSR이 SDSR보다 성능이 안좋았던 (=negative transfer가 발생한) 도메인에서 성능 향상됨</li>
  <li>이러한 방법으로 여러 개의 domain-specific models를 만들 필요가 없을 것을 기대함</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="21-single-domain-sequential-recommendation">2.1. Single-Domain Sequential Recommendation</h3>

<ul>
  <li>SDSR : temporal dynamics in user-item interactions를 디자인
    <ul>
      <li>GRU-based models : GRU4Rec, STAMP, NARM</li>
      <li>Attention-mechanism : SASRec, BERT4Rec, SINE, LightSANs</li>
      <li>Others : NextItNet(CNN), TransRec(Markov chain), …</li>
    </ul>
  </li>
</ul>

<h3 id="22-cross-domain-sequential-recommendation">2.2 Cross-Domain Sequential Recommendation</h3>

<ul>
  <li>CDSR : information from various other domains를 leverage
    <ul>
      <li>Matrix factorization : CMF, CLFM, …</li>
      <li>Multi-task learning : DTCDR, DeepAPF, BiTGCF, CAT-ART</li>
      <li>\(\pi-Net\) :  introduced gating mechanisms designed to transfer information from a single domain to another paired domain</li>
      <li>\(C^2DSR\) : employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations</li>
      <li>\(MIFN\) :  introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains</li>
      <li>\(MAN\) : designed group-prototype attention mechanisms to capture domainspecific and cross-domain relationships</li>
    </ul>
  </li>
  <li>However… 결국에는 모두 domain pair 끼리의 관계를 모델링
    <ul>
      <li>3개 이상의 domains의 관계를 파악할 때, domains이 엄청 많을 때에는 어려움</li>
      <li>그래서 CGRec에서 CDSR을 제안하면서 negative transfer 개념을 제안
        <ul>
          <li>high negative transfer를 가지는 domain에 panalty를 주는 방식</li>
          <li>하지만 여전히 SDSR보다 성능이 안좋은 domain이 꽤 있음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>그러므로 본 논문에서의 목표는 3개 이상의 <strong>모든</strong> 도메인에서 negative transfer를 <strong>효율적</strong>으로 줄이는 것</li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<ul>
  <li>Domains : \(\mathcal{D}=\{A, B, C, \ldots\}\) where \(\mid \mathcal{D}\mid  \geq 3\)
    <ul>
      <li>\(d \in \mathcal D\) 는 하나의 특정 도메인을 의미,</li>
      <li>\(V^d\)는 set of items specific to the domain \(d\), \(V\)는 total item set across all domains</li>
    </ul>
  </li>
</ul>

<h3 id="definition-1-single--and-cross-domain-sequential-recommendation">Definition 1. Single- and Cross-Domain Sequential Recommendation</h3>

<ul>
  <li>The single-domain sequences of domain \(d\) : \(X^d=\left[(\mathrm{SOS}), x_1^d, x_2^d, \ldots, x_{\mid X^d\mid -1}^d\right]\)​</li>
  <li>\(x_t^d\) :  interaction occurring at time \(t\)</li>
  <li>그러므로 cross-domain sequence는 \(X=\left(X^A, X^B, X^C, \ldots\right)\)로 표현할 수 있음</li>
  <li>예를 들어, \(X=\left[(\mathrm{SOS}), x_1^A, x_2^B, x_3^A, x_4^B, x_5^A, x_6^C, x_7^C\right]\)은 \(X^A=\left[(\mathrm{SOS}), x_1^A, x_3^A, x_5^A\right], X^B=\left[(\mathrm{SOS}), x_2^B, x_4^B\right], \text { and } X^C=[(\mathrm{SOS})\left., x_6^C, x_7^C\right]\)으로 split 가능</li>
  <li>SDSR은 하나의 domain 안에서 recommending, CDSR은 전체 도메인에서 recommending</li>
</ul>

<h3 id="definition-2-negative-transfer-gap-ntg">Definition 2. Negative Transfer Gap (NTG)</h3>

<ul>
  <li>\(\mathcal{L}_\pi^d\)는 domain \(d\)에서의 model \(\pi\)의 loss를 의미 (SDSR 또는 CDSR)</li>
  <li>그러므로 Negative transfer는 \(\phi_\pi(d) = \mathcal{L}_\pi^d\left(X^d\right)-\mathcal{L}_\pi^d(X)\)</li>
</ul>

<h3 id="problem-statement">Problem Statement</h3>

<ul>
  <li>historical cross-domain sequences \(X_{1:t}\)가 주어졌을 때, 목표는 다음 item \(x_{t+1}^d = \underset{x_{t+1}^d \in V^d}{\operatorname{argmax}} P\left(x_{t+1}^d \mid X_{1: t}\right)\)를 예측하는 것</li>
  <li>만약 \(\mid \mathcal{D}\mid\)개의 single-domain sequences (for SDSR)과 1개의 sequence (for CDSR)가 있다면
    <ul>
      <li>multi-tasking learning manner의 모델 하나는 \(\mid \mathcal{D}\mid +1\)개의 next item prediction tasks를 수행하는 것이다.</li>
    </ul>
  </li>
</ul>

<h2 id="4-model">4. Model</h2>

<p><img src="/assets/img/timeseries/SyNCRec/fig2.png" alt="그림1" /></p>

<h3 id="41-shared-embedding-layer">4.1. Shared Embedding Layer</h3>

<ul>
  <li>여기서는 <strong>initialized representations</strong> of items를 얻는다.
    <ul>
      <li>for \(\mid \mathcal{D}\mid\) single-domain sequences \(X^d\), and one cross-domain sequence \(X\)</li>
    </ul>
  </li>
  <li>Item embedding matrix \(M^d \in \mathbb R^{\mid V^d\mid \times r}\)​이고
    <ul>
      <li>\(\mid V^d\mid\)는 domain d의 items 개수, r은 embedding dimension</li>
    </ul>
  </li>
  <li>모든 domains에 대해 concat하면 \(M \in \mathbb R^{\mid V\mid \times r}\)
    <ul>
      <li>\(\mid V\mid\)는 모든 도메인에서 items 개수</li>
    </ul>
  </li>
  <li>여기서 최근 T개만을 사용 (T개보다 적다면 앞쪽에 padding으로 맞춰줌)
    <ul>
      <li>그러면 \(\mathbf{E}^d \in \mathbb{R}^{T \times r} \text { and } \mathbf{E} \in \mathbb{R}^{T \times r}\)를 얻음 (각각 Fig2(c-1), (c-2))</li>
      <li>\(\mid \mathcal D\mid\)개의 \(\mathbf{E}^d\)를 aggregation한 것이 \(\mathbf{E}^{\text {single }}\) (Fig2(c-1))</li>
      <li>참고로 \(\mathbf E, \mathbf E^d\)에는 learnable positional embedding 더해져있음</li>
      <li>\(t\)-th step에서의 \(\mathbf E, \mathbf E^d\)는 각각 \(\mathbf e, \mathbf e^d\)로 정의</li>
    </ul>
  </li>
</ul>

<h3 id="42-asymmetric-cooperative-network-with-mixture-of-sequential-experts-acmoe">4.2. Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMoE)</h3>

<ul>
  <li>Negative Transfer (NTG)는 <strong>loss of the SDSR</strong>과 <strong>the loss of CDSR</strong>의 차이로 정의
    <ul>
      <li>NTG가 작으면 다른 domains의 정보가 도움이 안되는 거고 크면 도움이 되는 것</li>
    </ul>
  </li>
  <li>그러므로 weight for the prediction loss in the domain로 사용할 수 있다
    <ul>
      <li>gradient flow를 작게 만들기 위해서다</li>
    </ul>
  </li>
  <li>Multi-gate Mixture of Sequential Experts (MoE) architecture를 사용하여 SDSR과 CDSR를 수행하고
    <ul>
      <li><strong>models</strong> relationships between different tasks and <strong>learns</strong> task-specific functionalities</li>
      <li>enabling it to effectively leverage shared representations</li>
    </ul>
  </li>
  <li>SDSR과 CDSR은 서로 간섭하지 않고, experts로는 Transformer를 사용</li>
</ul>

<h3 id="421-architecture">4.2.1. Architecture</h3>

<ul>
  <li>
    <p><strong>먼저 SDSR을 보자</strong></p>
  </li>
  <li>
    <p>shared embedding layer로부터 initialized representations of single- and cross-domain sequences,</p>

    <ul>
      <li>즉 \(\mathbf E, \mathbf E^d\)가 주어져있을 때, 각 expert는 many-to-many sequence learning을 수행</li>
    </ul>
  </li>
  <li>
    <p>domain \(d\)의 output : \(\begin{aligned}
&amp; \left(\mathbf{Y}^d\right)^{\text {single }}=h^d\left(f^d\left(\mathbf{E}^d\right)\right) \\
&amp; f^d\left(\mathbf{E}^d\right)=\sum_{k=1}^j g^d\left(\mathbf{E}^d\right)_k \mathrm{SG}\left(f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)\right)+\sum_{k=j+1}^K g^d\left(\mathbf{E}^d\right)_k f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)
\end{aligned}\)</p>

    <ul>
      <li>\(h^d\) : the tower network for domain \(d\)​ (Fig. 2(c-7))
        <ul>
          <li>feed-forward network with layer normalization</li>
        </ul>
      </li>
      <li>\(f^d\) : the multi-gated mixture of the sequential experts layer</li>
      <li>\(SG\)​ :  the stopgradient operation (Fig. 2(c-4))
        <ul>
          <li>forward pass에서는 identity function</li>
          <li>backward pass에서는 SG 안에 있는 것들의 gradient는 drop</li>
          <li>위 식에서는 \(j+1 \sim K\)번째 experts만 unique sequential pattern of single-domain sequences를 학습</li>
        </ul>
      </li>
      <li>\(f_{\text {TRM }}^k\)​ :  the 𝑘-th transformerbased sequential expert (Fig. 2(c-3))</li>
      <li>\(g^d\) :  gating network for domain \(d\) (Fig. 2(c-6))
        <ul>
          <li>\(g^d\left(\mathbf{E}^d\right)=\operatorname{softmax}\left(W_g^d \mathbf{E}^d\right)\) where \(W_g^d \in \mathbb{R}^{K \times d T}\) is trainable FC</li>
        </ul>
      </li>
      <li>The \(t\)-th element of \(\mathrm{Y}^{\text {single }}\)는 \(\left(y_t^d\right)^{\text {single }}\)</li>
    </ul>
  </li>
  <li>
    <p><strong>다음으로 CDSR을 보자</strong></p>
  </li>
  <li>
    <p>ACMoE module : \(\begin{aligned}
&amp; \mathbf{Y}^{\text {cross }}=h^{\text {cross }}\left(f^{\text {cross }}(\mathbf{E})\right) \\
&amp; f^{\text {cross }}(\mathbf{E})=\sum_{k=1}^j g^{\text {cross }}(\mathbf{E})_k f_{\mathrm{TRM}}^k(\mathbf{E})+\sum_{k=j+1}^K g^{\text {cross }}(\mathbf{E})_k \operatorname{SG}\left(f_{\mathrm{TRM}}^k(\mathbf{E})\right)
\end{aligned}\)</p>

    <ul>
      <li>\(h^{cross}\) :  the tower network (Fig. 2(c-9))</li>
      <li>\(f^{\text {cross }}\) : the multi-gated mixture of sequential experts layer for a cross-domain sequence</li>
      <li>\(SG\)는 \(j+1\sim K\)-th \(f^k_{TRM}\)에만 사용
        <ul>
          <li>그러면 \(1\sim j\)​번째 experts가cross-domain sequences에서 the distinct sequential patterns present를 학습</li>
        </ul>
      </li>
      <li>\(g^{\text {cross }}(\mathbf{E})=\operatorname{softmax}\left(W_a^{c r o s s} \mathbf{E}\right)\)​ : gating network for the crossdomain sequence (Fig. 2(c-8))</li>
    </ul>
  </li>
  <li>
    <p>\(\left(y_t^d\right)^{\text {single }} \text { and }\left(y_t\right)^{\text {cross }}\)는 two representations of different views for the same item</p>
  </li>
</ul>

<h3 id="422-transformer-experts">4.2.2. Transformer Experts</h3>

<ul>
  <li>각각의 Multi-head Self-Attention에 \(Z \in \mathbb{R}^{T \times r}\) 가 linear transformation
    <ul>
      <li>\(\to\) \(\text { queries } Q_i \in \mathbb{R}^{T \times r / p} \text {, keys } K_i \in \mathbb{R}^{T \times r / p} \text {, } \text { values } V_i \in \mathbb{R}^{T \times r / p}\)가 됨</li>
    </ul>
  </li>
  <li>
    <p>\(\begin{aligned}
&amp; \operatorname{Attn}\left(Q_i, K_i, V_i\right)=\operatorname{softmax}\left(\frac{Q_i K_i^{\top}}{\sqrt{r / p}}\right) V_i, Q_i=Z \mathrm{~W}_i^Q, K_i=Z \mathrm{~W}_i^K, V_i=Z \mathrm{~W}_i^V
\end{aligned}\) 거쳐 final output은 \(\mathbf{H} \in \mathbb{R}^{T \times r}\)</p>
  </li>
  <li>마지막으로 \(\operatorname{FFN}(\mathbf{H})=\left[\mathrm{FC}\left(\mathbf{H}_1\right)\left\\mid \mathrm{FC}\left(\mathbf{H}_2\right)\right\\mid , \ldots, \\mid  \mathrm{FC}\left(\mathbf{H}_T\right)\right]\)
    <ul>
      <li>where \(\mathrm{FC}\left(\mathbf{H}_t\right)=\operatorname{GELU}\left(\mathbf{H}_t \mathrm{~W}_1+b_1\right) \mathrm{W}_2+b_2\)</li>
      <li>\(\mathbf{H}_t\) : 𝑡-th representation of \(\mathbf{H}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-loss-correction-with-negative-transfer-gap-lc-ntg">4.3. Loss Correction with Negative Transfer Gap (LC-NTG)</h3>

<h3 id="431--single-domain-item-prediction">4.3.1.  Single-Domain Item Prediction</h3>

<ul>
  <li>Fig 2(e-1)</li>
  <li>single domoin sequence \(X_{1: t}^d\)가 주어졌을 때 다음 아이템 \(x_{t+1}^d\)를 예측하는 것은 pairwise ranking loss를 사용
    <ul>
      <li>즉 \(l_t^d=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}^d\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}^d\right)\right), \mathcal{L}_{\text {single }}^d=\sum_{t=1}^T l_t^d\)
        <ul>
          <li>where \(x^{d+}\) : ground-truth item paired with a negative item \(x^{d-}\) sampled froem Unif</li>
          <li>\(P\left(x_{t+1}^d=x^d \mid X_{1: t}^d\right)\) = \(\sigma\left(\left(y_t^d\right)^{\text {single }} \cdot M\left(x^d\right)\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="432-cross-domain-item-prediction">4.3.2. Cross-Domain Item Prediction</h3>

<ul>
  <li>CDSR \(l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t\)
    <ul>
      <li>where \(P\left(x_{t+1}^d=x^d \mid X_{1: t}\right) \text { is obtained by } \sigma\left(\left(y_t\right)^{\text {cross }} \cdot M\left(x^d\right)\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="433--calculating-the-negative-transfer-gap">4.3.3.  Calculating the Negative Transfer Gap</h3>

<ul>
  <li>이제 NTG를 구할 수 있다. \(\phi_\pi(d)=\sum_{t=1}^T\left(l_t^d-l_t\right)\)
    <ul>
      <li>where \(l_t^d\) and \(l_t\) are losses of the SDSR and CDSR tasks in time step \(t\) for the domain \(d\), respectively, calculated with our model \(\pi\)</li>
    </ul>
  </li>
  <li>\(\lambda=\left(\lambda_1, \lambda_2, \ldots, \lambda_{\mid \mathcal{D}\mid }\right)\)를 각 domain에서의 NTG라고 하면 \(\lambda_d \leftarrow \operatorname{softmax}\left(\alpha * \lambda_d+\beta * \phi_\pi(d) ; \delta\right)\)로 계산
    <ul>
      <li>where \(\alpha \text { and } \beta\) are learnable parameters</li>
    </ul>
  </li>
</ul>

<h3 id="434-loss-correction">4.3.4. Loss Correction</h3>

<ul>
  <li>NTG는 weight for the cross-domain item prediction loss로 활용됨
    <ul>
      <li>loss는 \(l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t\)</li>
    </ul>
  </li>
  <li>re-aggregate : multiplying the relative NTG for each domain separately
    <ul>
      <li>
\[\mathcal{L}_{\text {cross }}^{l c} = =\sum_{t=1}^T \sum_{d=1}^{\mid \mathcal{D}\mid } \lambda_d \log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right)\]
      </li>
    </ul>
  </li>
  <li>이렇게 하면 NTG가 발생하는 domain에서의 gradient flow를 줄이는 것</li>
</ul>

<h2 id="44-single-cross-mutual-information-maximization-sc-mim">4.4. Single-Cross Mutual Information Maximization (SC-MIM)</h2>

<ul>
  <li>SC-MIM: SDSR and CDSR tasks 사이의 정보를 잘 transfer하기 위한 방법
    <ul>
      <li>mutual information으로 두 tasks의 correlation signals를 파악</li>
      <li>mutual information: \(I(X, Y)=D_{K L}(p(X, Y) \\mid  p(X) p(Y))=\mathbb{E}_{p(X, Y)}\left[\log \frac{p(X, Y)}{p(X) p(Y)}\right]\)​</li>
    </ul>
  </li>
  <li>하지만 이 mutual information을 high-dimd에서 구하는 건 어렵기 때문에 lower bound로 InfoNCE를 사용
    <ul>
      <li>lower bound : \(I(X, Y) \geq \mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\mathbb{E}_{q(\hat{Y})}\left(\log \sum_{\hat{y} \in \hat{Y}} \exp \rho_\theta(x, \hat{y})\right)\right]+\log \mid \hat{Y}\mid\)
        <ul>
          <li>where \(x, y\)는 같은 input의 서로 다른 view points</li>
          <li>\(\rho_\theta\) 는 similarity function,</li>
        </ul>
      </li>
      <li>InfoNCE를 maximizing하는 것은 standard cross-entropy loss를 maximizing하는 것과 같음
        <ul>
          <li>: \($\mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\log \sum_{\hat{y} \in Y} \exp \rho_\theta(x, \hat{y})\right]\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>아무튼 돌아와서 우리는 \($\mathbf{Y}^{\text {single }}$ and $\mathbf{Y}^{\text {cross }}\)의 mutual information을 maximizing하고 싶음
    <ul>
      <li>그러므로 cross-domain representation \(\mathbf{Y}^{\text {ross }}\)를 domain별로 split해서 \((\mathbf{Y^d})^{\text {ross }}\) 구하고</li>
      <li>아래 식처럼 계산
        <ul>
          <li>: \(\begin{aligned} &amp; \mathcal{L}_{S C-M I M}^d=\rho\left(\left(\mathbf{Y}^d\right)^{\text {single }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)-\log \sum_{u-} \exp \left(\rho\left(\left(\mathbf{Y}^d\right)^{\text {single- }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)\right)\end{aligned}\)</li>
          <li>where \(u-\)는 other users in a training batch,</li>
          <li>\(\left(\mathbf{Y}^d\right)^{\text {single- }}\)는 subsequence of domain \(𝑑\) of user \(𝑢−\)​</li>
          <li>\(\rho(\cdot, \cdot)\)는 \(\rho(U, V)=\sigma\left(U^{\top} \cdot W^H \cdot V\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="45-model-training-and-evaluation">4.5. Model Training and Evaluation</h3>

<ul>
  <li>Total training loss : \(\mathcal{L}=\eta\left(\sum_{d=1}^{\mid \mathcal{D}\mid }\left(\mathcal{L}_{\text {single }}^d\right)+\mathcal{L}_{\text {cross }}^{l c}\right)+(1-\eta) \sum_{d=1}^{\mid \mathcal{D}\mid } \mathcal{L}_{S C-M I M}^d\)
    <ul>
      <li>where \(\eta\) is the harmonic factor</li>
      <li>evaluation할 때에는 cross-domain representation만 사용</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-dataset">5.1. Dataset</h3>

<h3 id="52-experimental-setting">5.2. Experimental Setting</h3>

<ul>
  <li>먼저 Amazon dataset과 Telco dataset에 대한 성능</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table23.png" alt="그림1" /></p>

<ul>
  <li>
    <p>Research Questions:</p>

    <ul>
      <li>
        <p>(RQ1): Does the performance of our model surpass the current stateof-the-art baselines in practical applications that involve more than three domains?</p>
      </li>
      <li>
        <p>(RQ2): Can our model effectively address the challenge of negative transfer across all domains in the CDSR task?</p>
      </li>
      <li>
        <p>(RQ3): What is the impact of various components of our model on its performance in CDSR tasks?</p>
      </li>
      <li>
        <p>(RQ4): How do variations in hyper-parameter settings influence the performance of our model?</p>
      </li>
      <li>
        <p>(RQ5): How does the model perform when deployed online ?</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="53-performance-evaluation-rq1">5.3. Performance Evaluation (RQ1)</h3>

<ul>
  <li>First, The effectiveness of our model can be observed.
    <ul>
      <li>다른 baseline models보다 성능이 뛰어남</li>
    </ul>
  </li>
  <li>Second, Integrating information from all domains simultaneously in a model can improve performance in each domain compared to modeling a pairwise domain-domain relationship.
    <ul>
      <li>본 논문에서 제시하는 방법을 사용할 경우에는 CDSR task에서 domain끼리의 정보를 결합해서 사용하는 것이 더 효율적이다.</li>
    </ul>
  </li>
</ul>

<h3 id="54-discussion-of-the-negative-transfer-rq2">5.4. Discussion of the negative transfer (RQ2)</h3>

<ul>
  <li>기존 baseline models는 SDSR보다 CDSR의 성능이 더 안좋았지만 본 논문에서 제시하는 모델은 그렇지 않다</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table4.png" alt="그림1" /></p>

<h3 id="55-discussion-of-model-variants-rq3">5.5 Discussion of Model Variants (RQ3)</h3>

<ul>
  <li>LC-NTG, SC-MIM, ACMoE 세 가지 components 모두 성능 향상을 위해 필요하다</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table5.png" alt="그림1" /></p>

<h3 id="56-hyperparameter-analysis-rq4">5.6. Hyperparameter Analysis (RQ4)</h3>

<p><img src="/assets/img/timeseries/SyNCRec/fig3.png" alt="그림1" /></p>

<h2 id="6-online-ab-test-rq5">6. Online A/B Test (RQ5)</h2>

<p>pass</p>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>Negative transfer를 다루는 CDSR framework를 제안
    <ul>
      <li>Negative transfer를 측정하고 prediction loss의 weight로 활용</li>
    </ul>
  </li>
  <li>SDSR and CDSR tasks의 정보를 교환시키는 Auxiliary loss 제안</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[SIGIR'24 Best Paper](https://arxiv.org/pdf/2407.11245)]]></summary></entry><entry><title type="html">MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-05-MG-TSD/" rel="alternate" type="text/html" title="MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)" /><published>2024-08-05T00:00:00+09:00</published><updated>2024-09-03T15:04:04+09:00</updated><id>http://localhost:4000/timeseries/MG-TSD</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-05-MG-TSD/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>어떻게 Diffusion model의 성능을 time series forecasting에 활용할 수 있는가</li>
  <li><strong>M</strong>ulti-<strong>G</strong>ranularity <strong>T</strong>ime <strong>S</strong>eries <strong>D</strong>iffusion <strong>(MG- TSD)</strong>
    <ul>
      <li>leveraging the inherent granularity levels</li>
      <li>intuition: diffusion step에서 점차 gaussian noise로 만드는 것을 fine \(\to\) coarse로 이해</li>
      <li>novel multi-granularity guidance diffusion loss function</li>
      <li>method to effectively utilize coarse-grained data across various granularity levels</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>최근에는 Time series predictive 목적으로 conditional generative model을 활용
    <ul>
      <li>처음에는 Auto-regressive 방식으로 하다가 CSDI도 했었음</li>
    </ul>
  </li>
  <li>하지만 문제는 Diffusion이 instability하다는 점
    <ul>
      <li>Image에서 diffusion은 다양한 이미지를 만들 수 있어서 장점이었는데</li>
      <li>시계열 예측 관점에서는 그것이 성능 하락의 원인이 될 수 있음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/MG-TSD/fig1.png" alt="그림1" /></p>

<ul>
  <li>Diffusion step에서 점차 gaussian noise로 만드는 것을 fine \(\to\) coarse로 이해한다면
    <ul>
      <li>Diffusion model이 <strong>labels을 the source of guidance</strong>로 필요로 하는 문제에서</li>
      <li>Time series의 fine feature가 그 labels as the source of guidance 역할을 할 수 있을 것</li>
    </ul>
  </li>
  <li>MG-TSD에서는 coarse-grained data를 denoising process 학습의 guide로 준다.
    <ul>
      <li>\(\to\) intermediate latent states에서의 constraints로 작용</li>
      <li>\(\to\) coarser feature는 더 빠르게 생성할 수 있기 때문에, 그만큼 finer feature recovery도 용이</li>
      <li>\(\to\) coarse-grained data의 trend와 pattern을 보존하는 sampling을 만듬</li>
      <li>\(\to\)​ reduces variability and results in high-quality predictions</li>
    </ul>
  </li>
</ul>

<h2 id="2-background">2. Background</h2>

<ul>
  <li>TimeGrad Model
    <ul>
      <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad Paper</a> <a href="https://lpppj.github.io/timeseries/2024-07-09-Timegrad">TimeGrad Review</a></li>
    </ul>
  </li>
  <li>\(\boldsymbol{X}^{(1)}=\left[\boldsymbol{x}_1^1, \ldots, \boldsymbol{x}_t^1, \ldots, \boldsymbol{x}_T^1\right]\) is the original observed data, where \(t \in[1, T]\) and \(\boldsymbol{x}_t \in \mathbb{R}^D\)
    <ul>
      <li>Mathematical expressions: \(q_{\mathcal{X}}\left(\boldsymbol{x}_{t_0: T}^1 \mid\left\{\boldsymbol{x}_{1: t_0-1}^1\right\}\right)=\prod_{t=t_0}^T q_{\mathcal{X}}\left(\boldsymbol{x}_t^1 \mid\left\{\boldsymbol{x}_{1: t-1}^1\right\}\right)\)</li>
    </ul>
  </li>
</ul>

<h2 id="3-method">3. Method</h2>

<h3 id="31-mg-tsd-model-architecture">3.1. MG-TSD Model Architecture</h3>

<p><img src="/assets/img/timeseries/MG-TSD/fig2.png" alt="그림2" /></p>

<h3 id="multi-granularity-data-generator">Multi-granularity Data Generator</h3>

<p>: for generating multi-granularity data from observations</p>

<ul>
  <li>historical sliding windows with different sizes를 통해 fine \(\to\) coase로 smoothing out</li>
  <li>즉 \(\boldsymbol{X}^{(g)}=f\left(\boldsymbol{X}^{(1)}, s^g\right)\) with pre-defined sliding window size \(s^g\)</li>
  <li>이 때 non-overlapping하게 window를 slicing하고, \(\boldsymbol{X}^{(g)}\)는 \(s^g\)번 복제해서 \([1, T]\)로 맞춤</li>
</ul>

<h3 id="temporal-process-module">Temporal Process Module</h3>

<p>: designed to capture the temporal dynamics of the multi-granularity time series data</p>

<ul>
  <li>각각의 granularity level \(g\)에서 GRU와 같은 방식으로 timestep \(t\)를 \(\mathbf{h}_t^g\)로 encoding</li>
</ul>

<h3 id="guided-diffusion-process-module">Guided Diffusion Process Module</h3>

<p>: designed to generate stable time series predictions at each timestep \(t\)</p>

<ul>
  <li>multi-granularity data를 활용하여 diffusion learning process의 guide로 제공</li>
</ul>

<h3 id="32-multi-granularity-guided-diffusion">3.2. Multi-Granularity Guided Diffusion</h3>

<p>: Guided Diffusion Process Module에 대한 details</p>

<h3 id="321-coarse-grained-guidance">3.2.1. Coarse-grained Guidance</h3>

<p>: the derivation of a heuristic guidance loss for the two- granularity case</p>

<ul>
  <li>consider two granularities at a fixed timestep \(t\)
    <ul>
      <li>: \(\text { finest-grained data } \boldsymbol{x}_t^{g_1}\left(g_1=1\right) \text { from } \boldsymbol{X}^{\left(g_1\right)}\) &amp; \(\text { coarse-grained data } \boldsymbol{x}_t^g \text { from } \boldsymbol{X}^{(g)}\)​</li>
    </ul>
  </li>
  <li>먼저 coarse-grained targets \(x^g\)를 intermediate diffusion step \(N_*^g \in[1, N-1]\)에 introduce
    <ul>
      <li>즉 objective function이 \(\log p_\theta\left(\boldsymbol{x}^g\right)\)</li>
    </ul>
  </li>
  <li>그러면 denoising process에서 recover된 coarser features는 실제 coarse-grained sample의 정보를 많이 가지고 있을테니
    <ul>
      <li>fine-grained feature를 recover하기도 쉬워질 것</li>
    </ul>
  </li>
  <li>\(\theta\)-parameterized: \(p_\theta\left(\boldsymbol{x}_{N_*^g}\right)=\int p_\theta\left(\boldsymbol{x}_{N_*^g: N}\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}=\int p\left(\boldsymbol{x}_N\right) \prod_{N_*^g+1}^N p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}\)​
    <ul>
      <li>where \(\boldsymbol{x}_N \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I}), p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right)=\mathcal{N}\left(\boldsymbol{x}_{n-1} ; \boldsymbol{\mu}_\theta\left(\boldsymbol{x}_n, n\right), \boldsymbol{\Sigma}_\theta\left(\boldsymbol{x}_n, n\right)\right)\)​</li>
    </ul>
  </li>
  <li>이건 \(N_*^g\)번째 diffusion step에서 \(N\)번째까지 총 \(N-N_*^g\) steps의 forward process이므로
    <ul>
      <li>the guidance objective: \(\log p_\theta\left(\boldsymbol{x}^g\right)=\log \int p_\theta\left(\boldsymbol{x}_{N_*^g}^g, \boldsymbol{x}_{N_*^g+1}^g, \ldots, \boldsymbol{x}_N^g\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}^g\)​</li>
    </ul>
  </li>
  <li>sample에 대한 loss 대신 noise에 대한 loss 사용
    <ul>
      <li>loss: \(\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}^g, n}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_n^g, n\right)\right\|^2\right]\)​</li>
      <li>where \(\boldsymbol{x}_n^g=\left(\prod_{i=N_{\boldsymbol{z}}^g}^n \alpha_i^1\right) \boldsymbol{x}^g+\sqrt{ } \mathbf{1}-\prod_{i=N^g}^n \alpha_i^1 \boldsymbol{\epsilon} \text { and } \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})\)</li>
    </ul>
  </li>
</ul>

<h3 id="322-multi-granularity-guidance">3.2.2. Multi-granularity Guidance</h3>

<ul>
  <li>
    <p>Multi-granularity Data Generator가 G개의 granularity levels마다 data 생성: \(\boldsymbol{X}^{(1)}, \boldsymbol{X}^{(2)}, \ldots, \boldsymbol{X}^{(G)}\)</p>
  </li>
  <li>Share ratio: \(r_g:=1-\left(N_*^g-1\right) / N\)
    <ul>
      <li>: the shared percentage of variance schedule between the gth granularity data and the finest-grained data</li>
      <li>ex. finest-grained data에서는 \(N_*^1=1 \text { and } r^1=1\)​
        <ul>
          <li>variance schedule for granularity \(g\) is, \(\alpha_n^g\left(N_*^g\right)= \begin{cases}1 &amp; \text { if } n=1, \ldots, N_*^g \\ \alpha_n^1 &amp; \text { if } n=N_*^g+1, \ldots, N\end{cases}\)​</li>
          <li>and \(\left\{\beta_n^g\right\}_{n=1}^N=\left\{1-\alpha_n^g\right\}_{n=1}^N\)​</li>
          <li>accordingly, \(a_n^g\left(N_*^g\right)=\prod_{k=1}^n \alpha_k^g \text {, and } b_n^g\left(N_*^g\right)=1-a_n^g\left(N_*^g\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이 때 \(N^g_*\)는 : represents the diffusion index for starting sharing the variance schedule across granularity level \(g \in\{1, \ldots, G\}\)</li>
  <li>이렇게 되면 larger coarser granularity level일수록 \(N^g_*\)가 커진다는 뜻
    <ul>
      <li>즉 coarser할수록 fine한 정보는 줄어들테니 이전 diffusion step과 차이가 크지 않을 것</li>
      <li>그러니까 \(N^g_*\)를 크게 해서 fine-grained feature를 생성할 steps를 많이 줌</li>
    </ul>
  </li>
  <li>Then the guidance loss function \(L^{(g)}(\theta)\) for \(g\)-th granularity \(x^g_{n,t}\) at timestep \(t\) and diffusion step \(n\),
    <ul>
      <li>can be expressed as: \(L^{(g)}(\theta)=\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, n} \|\left(\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon}, n, \mathbf{h}_{t-1}^g\right) \|_2^2\right.\)</li>
      <li>where \(\mathbf{h}_t^g=\mathrm{RNN}_\theta\left(\boldsymbol{x}_t^g, \mathbf{h}_{t-1}^g\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="training">Training</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm1.png" alt="그림41" /></p>

<ul>
  <li>최종적인 training objectives는 모든 granularities에서의 Loss의 weighted sum
    <ul>
      <li>: \(L^{\text {final }}=\omega^1 L^{(1)}(\theta)+L^{\text {guidance }}(\theta)=\sum_{q=1}^G \omega^g \mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, t}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_{n, t}^g, n, \mathbf{h}_{t-1}^g\right)\right\|^2\right]\)</li>
      <li>where \(\boldsymbol{x}_{n, t}^g=\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon} \text { and } \sum_{g=1}^G \omega^g=1\)</li>
      <li>이 때 denoising network의 parameters는 shared across all granularities</li>
    </ul>
  </li>
</ul>

<h3 id="inference">Inference</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm2.png" alt="그림42" /></p>

<ul>
  <li>우리의 목표는 특정한 prediction steps에 대한  finest-grained data에 대한 예측
    <ul>
      <li>\(t_0-1\) 시점까지 주어졌다면 아래 algorithm 2를 따라 \(t_0\)시점에 대한 데이터 생성,</li>
      <li>우리가 원하는 forecast horizon이 될 때까지 반복</li>
      <li>hidden states에 conditional inputs으로 무엇을 넣는지에 따라서 그에 해당하는 granularity levels로 샘플링</li>
    </ul>
  </li>
</ul>

<h3 id="selection-of-share-ratio">Selection of share ratio</h3>

<ul>
  <li>위에서는 share ratio \(r_g:=1-\left(N_*^g-1\right) / N\)를 heuristic하게 \(N^g_*\)에 따라 결정되도록 했음
    <ul>
      <li>Diffusion step \(N^g_*\)는 \(q\left(\boldsymbol{x}^g\right) \text { and } p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\)의 거리가 가장 작을 때로 설정 !</li>
      <li>: \(\to\) \(N_*^g:=\arg \min_n \mathcal{D}\left(q\left(\boldsymbol{x}^g\right), p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\right)\)​
        <ul>
          <li>\(\mathcal{D}\)는 두 분포의 거리를 측정하는 metric이 됨 (KL-divergence, …)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/MG-TSD/table12.png" alt="그림112" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig3.png" alt="그림3" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig4.png" alt="그림4" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Multi-Granularity Time Series Diffusion (MG-TSD)
    <ul>
      <li>leverages the inherent granularity levels within the data, as given targets at intermediate diffusion steps to guide the learning process of diffusion models</li>
      <li>to effectively utilize coarse-grained data across various granularity levels.</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=CZiY6OLktd)]]></summary></entry><entry><title type="html">Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/" rel="alternate" type="text/html" title="Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)" /><published>2024-08-04T00:00:00+09:00</published><updated>2024-08-04T18:52:22+09:00</updated><id>http://localhost:4000/timeseries/Diffusion-TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Diffusion-TS: uses an encoder-decoder transformer with disentangled temporal representations</li>
  <li>train the model to directly reconstruct the <strong>sample</strong> instead of the <strong>noise</strong> in each diffusion step</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Synthesizing realistic time series data는 데이터 공유가 개인정보 침해로 이어질 수 있는 사례에서의 솔루션</li>
  <li>지금까지 Diffusion을 활용한 time series generation은 대부분 task-agnostic generation
    <ul>
      <li>첫번째 문제는 RNN-based Autoregressive 방식: limited long-range performance due to error accumulation and slow inference speed</li>
      <li>두번째 문제는 diffusion process에서 noise를 추가할 때 시계열의 combinations of independent components(trend, seasonal, …)이 망가지는 문제 (특히 주기성이 뚜렷한 경우 interpretability가 부족 <a href="https://openreview.net/pdf?id=rdjeCNUS6TG">Liu et al., (2022)</a>)</li>
    </ul>
  </li>
  <li><strong>본 논문에서는 Transformer를 활용하여 trend와 seasonal을 non-autoregressive하게 생성</strong>
    <ul>
      <li>by imposing different forms of constraints on different representations.</li>
    </ul>
  </li>
  <li>For Reconstruct the <strong>samples</strong> rather than the <strong>noises</strong> in each diffusion step, Fourier-based loss 사용</li>
</ul>

<h2 id="2-problem-statement">2. Problem Statement</h2>

<ul>
  <li>N개로 이루어진 데이터셋 \(D A=\left\{X_{1: \tau}^i\right\}_{i=1}^N\)​
    <ul>
      <li>where \(X_{1: \tau}=\left(x_1, \ldots, x_\tau\right) \in \mathbb{R}^{\tau \times d}\)</li>
    </ul>
  </li>
  <li>목표는 Gaussian vectors \(Z_i=\left(z_1^i, \ldots, z_t^i\right) \in \mathbb{R}^{\tau \times d \times T}\)를 DA와 비슷한 \(\hat{X}_{1: \tau}^i=G\left(Z_i\right)\)로 바꾸는 Generator \(G\)를 학습하는 것</li>
  <li>Time series model은 trend와 여러 개의 seasonality로 구성 : \(x_j=\zeta_j+\sum_{i=1}^m s_{i, j}+e_j\)
    <ul>
      <li>where \(j=0,1, \ldots, \tau-1\)</li>
      <li>\(x_j\) : observed time series</li>
      <li>\(\zeta_j\): trend component</li>
      <li>\(s_{i,j}\): \(i\)-th seasonal component</li>
      <li>\(e_j\): remainder part (contatins the noise and some outliers at time t)</li>
    </ul>
  </li>
</ul>

<h2 id="3-diffusion-ts-interpretable-diffusion-for-time-series">3. Diffusion-TS: Interpretable Diffusion for Time Series</h2>

<ul>
  <li>이러한 interpretable decomposition architecture의 근거는 3가지
    <ul>
      <li>첫째, disentangled patterns in the diffusion model은 아직 연구되지 않음</li>
      <li>둘째, specific designs of architecture and objective 덕분에 interpretable</li>
      <li>셋째, explainable disentangled representations 덕분에 complex dynamics 파악</li>
    </ul>
  </li>
</ul>

<h3 id="31-diffusion-framework">3.1. Diffusion Framework</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig1.png" alt="그림1" /></p>

<ul>
  <li>Forward process
    <ul>
      <li>\(x_0 \sim q(x)\)에서 점점 noisy into Gaussian noise \(x_T \sim \mathcal{N}(0, \mathbf{I})\)</li>
      <li>Parameterization: \(q\left(x_t \mid x_{t-1}\right)=\mathcal{N}\left(x_t ; \sqrt{ } 1-\beta_t x_{t-1}, \beta_t \mathbf{I}\right) \text { with } \beta_t \in(0,1)\)</li>
    </ul>
  </li>
  <li>
    <p>Reverse process</p>

    <ul>
      <li>
        <p>반대로 \(p_\theta\left(x_{t-1} \mid x_t\right)=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)\)</p>
      </li>
      <li>
        <p>MSE: \(\mathcal{L}\left(x_0\right)=\sum_{t=1}^T \underset{q\left(x_t \mid x_0\right)}{\mathbb{E}}\left\|\mu\left(x_t, x_0\right)-\mu_\theta\left(x_t, t\right)\right\|^2\)</p>
        <ul>
          <li>where \(\mu\left(x_t, x_0\right) \text { is the mean of the posterior } q\left(x_{t-1} \mid x_0, x_t\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="32-decomposition-model-architecture">3.2. Decomposition Model Architecture</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig2.png" alt="그림2" /></p>

<ul>
  <li>Noisy sequence가 encoder 통과해서 decoder로 들어옴 (초록색)</li>
  <li>Decoder는 multilayer structure, 각 layer에는 <strong>Transformer Block</strong>, <strong>FFN</strong>, <strong>Trend and Fourier synthetic layer</strong>가 포함됨</li>
  <li>각 layer는 시계열의 각 component를 생성하는 역할
    <ul>
      <li>component에 해당하는 inductive bias를 각 layer에 반영해줌으로써 학습이 쉬워짐</li>
      <li>Trend representation captures the intrinsic trend which changes gradually and smoothly</li>
      <li>Seasonality representation illustrates the periodic patterns of the signal</li>
      <li>Error representation characterizes the remaining parts after removing trend and periodicity</li>
    </ul>
  </li>
  <li>\(w_{(\cdot)}^{i, t}\) where \(i \in 1, \ldots, D\)는 \(i\)번째 decoder block에서의 diffusion step \(t\)를 의미</li>
</ul>

<h3 id="trend-synthesis">Trend Synthesis</h3>

<ul>
  <li>smooth underlying mean of the data, which aims to model slow-varying behavior</li>
  <li>그러므로 Trend \(V_{t r}^t\)를 위해 Polynomial regressor 사용
    <ul>
      <li>\(V_{t r}^t=\sum_{i=1}^D\left(C \cdot \operatorname{Linear}\left(w_{t r}^{i, t}\right)+\mathcal{X}_{t r}^{i, t}\right)\) where \(C=\left[1, c, \ldots, c^p\right]\)</li>
      <li>\(\mathcal{X}_{t r}^{i, t}\)는 the mean value of the output of the \(i\)​-th decoder block</li>
      <li>\(C\)는 slow-varying poly space인데, matrix of powers of vector \(c=[0,1,2, \ldots, \tau-2, \tau-1]^T / \tau\)</li>
      <li>\(p\)는 small degree (e.g. \(p\)​=3) to model low frequency behavior</li>
    </ul>
  </li>
</ul>

<h3 id="seasonality--error-synthesis">Seasonality &amp; Error Synthesis</h3>

<ul>
  <li>이제 Trend, Seasonality, Error 모두 생각해보자.</li>
  <li><strong>결국 문제는 noisy input \(x_t\)에서 seasonal patterns를 구분해내는 것 !</strong></li>
  <li>푸리에 시리즈의 trigonometric representation of seasonal components를 기반으로 Fourier bases를 활용한 Fourier synthetic layers에서 seasonal component 파악</li>
</ul>

<p><img src="/assets/img/timeseries/Diffusion-TS/fomula456.png" alt="그림456" /></p>

<ul>
  <li>\(A_{i, t}^{(k)}, \Phi_{i, t}^{(k)}\) are the phase, amplitude of the \(k\)-th frequency after the DFT \(\mathcal F\) repectively</li>
  <li>\(f_k\)는 Fourier frequency of the corresponding index \(k\)</li>
  <li>결국 the Fourier synthetic layer는 진폭(amplitude)이 큰 frequency를 찾고, 그 frequency들만 IDFT.
    <ul>
      <li>그걸 seasonality로 본다. (Pathformer랑 같은 방식)</li>
    </ul>
  </li>
  <li>최종적으로 original signal: \(\hat{x}_0\left(x_t, t, \theta\right)=V_{t r}^t+\sum_{i=1}^D S_{i, t}+R\)​
    <ul>
      <li>\(R\): output of the last decoder block, which can be regarded as the sum of residual periodicity and other noise.</li>
    </ul>
  </li>
</ul>

<h3 id="33-fourier-based-traning-objective">3.3 Fourier-based Traning Objective</h3>

<ul>
  <li>\(\hat{x}_0\left(x_t, t, \theta\right)\)를 directly estimate
    <ul>
      <li>Reverse process: \(x_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \hat{x}_0\left(x_t, t, \theta\right)+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} x_t+\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t z_t\)</li>
      <li>where \(z_t \sim \mathcal{N}(0, \mathbf{I}), \alpha_t=1-\beta_t \text { and } \bar{\alpha}_t=\prod_{s=1}^t \alpha_s\)​</li>
    </ul>
  </li>
  <li>Reweighting strategy: \(\mathcal{L}_{\text {simple }}=\mathbb{E}_{t, x_0}\left[w_t\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2\right], \quad w_t=\frac{\lambda \alpha_t\left(1-\bar{\alpha}_t\right)}{\beta_t^2}\)​
    <ul>
      <li>where \(\lambda\) is constant (i.e. 0.01)</li>
      <li>즉 small t에서 down-weighted, 모델이 larger diffusion step에 집중하도록 만듬</li>
    </ul>
  </li>
  <li>Fourier-based loss term이 time serie reconstruction에서는 더 좋다 <a href="https://arxiv.org/pdf/2208.05836">Fons et al. (2022)</a>
    <ul>
      <li>: \(\mathcal{L}_\theta=\mathbb{E}_{t, x_0}\left[w_t\left[\lambda_1\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2+\lambda_2\left\|\mathcal{F} \mathcal{F} \mathcal{T}\left(x_0\right)-\mathcal{F F} \mathcal{T}\left(\hat{x}_0\left(x_t, t, \theta\right)\right)\right\|^2\right]\right]\)</li>
    </ul>
  </li>
</ul>

<h3 id="34-conditional-generation-for-time-series-applications">3.4. Conditional Generation for Time Series Applications</h3>

<ul>
  <li><strong>Conditional extensions of the Diffusion-TS</strong>, in which the modeled \(x_0\) is conditioned on targets \(y\)​</li>
  <li>목표는 pre-trained diffusion model과 the gradients of a classifier를 활용하여
    <ul>
      <li>Posterior \(p\left(x_{0: T} \mid y\right)=\prod_{t=1}^T p\left(x_{t-1} \mid x_t, y\right)\)에서 sampling하는 것</li>
    </ul>
  </li>
  <li>\(p\left(x_{t-1} \mid x_t, y\right) \propto p\left(x_{t-1} \mid x_t\right) p\left(y \mid x_{t-1}, x_t\right)\)이므로 bayse theorem을 통해 gradient update
    <ul>
      <li>Score function \(\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t, y\right)=\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t\right)+\nabla_{x_{t-1}} \log p\left(y \mid x_{t-1}\right)\)</li>
      <li>\(\log p\left(x_{t-1} \mid x_t\right)\)은 diffusion model에서 정의됨.</li>
      <li>\(\log p\left(y \mid x_{t-1}\right)\)는 classifier에서 parametrize되며, \(\nabla_{x_{t-1}} \log p\left(y \mid x_{0 \mid t-1}\right)\)로 근사됨</li>
    </ul>
  </li>
  <li>즉 classifier가 높은 likelihood를 가진 영역에서 sample이 생성되도록 하는 것
    <ul>
      <li>: \(\tilde{x}_0\left(x_t, t, \theta\right)=\hat{x}_0\left(x_t, t, \theta\right)+\eta \nabla_{x_t}\left(\left\|x_a-\hat{x}_a\left(x_t, t, \theta\right)\right\|_2^2+\gamma \log p\left(x_{t-1} \mid x_t\right)\right)\)</li>
      <li>where Conditional part \(x_a\), generative part \(x_b\)</li>
      <li>gradient term은 reconstruction-based guidance, \(\eta\)로 강도 조절</li>
    </ul>
  </li>
  <li>각 diffusion step에서 이 gradient update를 여러 번 반복하여 quality 높인다</li>
  <li>Replacing: \(\tilde{x}_a\left(x_t, t, \theta\right):=\sqrt{\bar{\alpha}_t} x_a+\sqrt{ } 1-\bar{\alpha}_t \epsilon\)을 통해, \(\tilde{x}_0\)를 사용한 sample \(x_{t-1}\)가 생성됨</li>
</ul>

<h2 id="4-empirical-evaluaiton">4. Empirical Evaluaiton</h2>

<h3 id="42-metrics">4.2. Metrics</h3>

<ul>
  <li>Discriminative score (Yoon et al., 2019): measures the similarity using a classification model to distinguish between the original and synthetic data as a supervised task;</li>
  <li>Predictive score (Yoon et al., 2019):  measures the usefulness of the synthesized data by training a post-hoc sequence model to predict next-step temporal vectors using the train-synthesis-and-test-real (TSTR) method;</li>
  <li>Context-Frechet Inception Distance (Context-FID) score ´ (Paul et al., 2022):  quantifies the quality of the synthetic time series samples by computing the difference between representations of time series that fit into the local context;</li>
  <li>Correlational score (Ni et al., 2020): uses the absolute error between cross correlation matrices by real data and synthetic data to assess the temporal dependency</li>
</ul>

<h3 id="43-interpretability-results">4.3. Interpretability Results</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig3.png" alt="그림3" /></p>

<ul>
  <li>the corrupted samples (shown in (a)) with 50 steps of noise added as input</li>
  <li>outputs the signals (shown in (c)) that try to restore the ground truth (shown in (b))</li>
  <li>with the aid of the decomposition of temporal trend (shown in (d)) and season &amp; error (shown in (e)).</li>
  <li>Result: As would be expected, the trend curve follows the overall shape of the signal, while the season &amp; error oscillates around zero !</li>
</ul>

<h3 id="44-unconditional-time-series-generation">4.4. Unconditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table1.png" alt="그림21" /></p>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig4.png" alt="그림4" /></p>

<h3 id="45-conditional-time-series-generation">4.5. Conditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig6.png" alt="그림6" /></p>

<h3 id="46-ablaction-study">4.6. Ablaction Study</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table2.png" alt="그림22" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Diffusion-TS, a DDPM-based method for general time series generation
    <ul>
      <li>TS-specific loss design and transformer-based deep decomposition architecture</li>
    </ul>
  </li>
  <li>Unconditional로 훈련된 model이 쉽게 conditional로 확장될 수 있음
    <ul>
      <li>by combining gradients into the sampling !</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/pdf/2403.01742)]]></summary></entry><entry><title type="html">Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting (ICLR 2022)</title><link href="http://localhost:4000/timeseries/2024-07-26-pyraformer/" rel="alternate" type="text/html" title="Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting (ICLR 2022)" /><published>2024-07-26T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/pyraformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-26-pyraformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Pyraformer: a flexible but parsimonious model that can capture a wide range of temporal dependencies, by exploring the multi-resolution representation of the time series
    <ul>
      <li>Pyramidal attention module (PAM)</li>
      <li>the inter-scale tree structure summarizes features at different resolutions</li>
      <li>the intra-scale neighboring connections model the temporal dependencies of different ranges</li>
      <li>the maximum length of the signal traversing path in Pyraformer is a constant with regard to the sequence length L (i.e. \(\mathcal{O}(1)\))</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>시계열 예측에서 Challenge는 powerful but parsimonious model</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/fig1.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/pyraformer/table1.png" alt="그림11" /></p>

<ul>
  <li>Pyraformer: to simultaneously capture temporal dependencies of different ranges in a compact multi-resolution fashion</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-method">3. Method</h2>

<h3 id="31-pyramidal-attention-module-pam">3.1. Pyramidal Attention Module (PAM)</h3>

<ul>
  <li>The inter-scale connections form a C-ary tree, in which each parent has C children.
    <ul>
      <li>the nodes at coarser scales can be regarded as the daily, weekly, and even monthly features of the time series</li>
    </ul>
  </li>
  <li>\(\to\) The pyramidal graph offers a multi-resolution representation of the original time series !
    <ul>
      <li>long-range dependencies 파악이 쉬워짐. 그냥 이웃 노드 연결하기만 하면 되니까 (intra-scale)</li>
    </ul>
  </li>
  <li>Original Attention mechanism
    <ul>
      <li>input \(X\), output \(Y\)</li>
      <li>Query \({Q}={X} {W}_Q\), key \({K}={X} {W}_K\), value \({V}={X} {W}_V\)</li>
      <li>where \({W}_Q, {W}_K, {W}_V \in \mathbb{R}^{L \times D_K}\)</li>
      <li>Then, \({y}_i=\sum_{\ell=1}^L \frac{\exp \left({q}_i {k}_{\ell}^T / \sqrt{D_K}\right) {v}_{\ell}}{\sum_{\ell=1}^L \exp \left({q}_i {k}_{\ell}^T / \sqrt{D_K}\right)}\)</li>
      <li>time and space complexity \(\mathcal{O}(L^2)\)</li>
    </ul>
  </li>
  <li>Pyramidal Attention Module (PAM)</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/fomula2.png" alt="그림21" /></p>

<p><img src="/assets/img/timeseries/pyraformer/myfig1.png" alt="그림31" /></p>

<ul>
  <li>Then, \({y}_i=\sum_{\ell \in \mathbb{N}_{\ell}^{(s)}} \frac{\exp \left({q}_i {k}_{\ell}^T / \sqrt{d_K}\right) {v}_{\ell}}{\sum_{\ell \in \mathbb{N}_l^{(s)}} \exp \left({q}_i {k}_{\ell}^T / \sqrt{d_K}\right)}\)</li>
  <li>모든 시점끼리 attention을 하지 않고 conv filter로 nodes를 만들고 이웃 노드끼리 attention !</li>
</ul>

<h3 id="32-coarser-saleㄴ-construvtion-module-cscm">3.2. Coarser-saleㄴ Construvtion Module (CSCM)</h3>

<p><img src="/assets/img/timeseries/pyraformer/fig3.png" alt="그림3" /></p>

<ul>
  <li>PAM이 작동할 수 있도록 pyramidal 구조를 initialize하는 역할</li>
</ul>

<h3 id="33-prediction-module">3.3. Prediction Module</h3>

<ul>
  <li>input embedding 할 때에 예측하고자 하는 길이만큼 붙여서 CSCM, PAM을 통과하면</li>
  <li>예측 시점에 대한 representation을 얻을 수 있음</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/myfig3.png" alt="그림33" /></p>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/pyraformer/table2.png" alt="그림12" /></p>

<p><img src="/assets/img/timeseries/pyraformer/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/pyraformer/fig4.png" alt="그림14" /></p>

<h2 id="5-conclusion-and-outlook">5. Conclusion and Outlook</h2>

<ul>
  <li>Pyraformer: a novel model based on pyramidal attention
    <ul>
      <li>effectively describe both short and long temporal dependencies with low time and space complexity</li>
      <li>CSCM to construct a C-ary tree, and then design the PAM to pass messages in both the inter-scale and the intra-scale fashion</li>
      <li>Pyraformer can achieve the theoretical \(\mathcal{O}(L)\) complexity and \(\mathcal{O}(1)\) maximum signal traversing path length (L: input sequence length)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2022](https://arxiv.org/pdf/2012.07436)]]></summary></entry><entry><title type="html">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)</title><link href="http://localhost:4000/timeseries/2024-07-26-Informer/" rel="alternate" type="text/html" title="Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)" /><published>2024-07-26T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/Informer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-26-Informer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer: quadratic time complexity, high memory usage, and in- herent limitation of the encoder-decoder architecture</li>
  <li><strong>Informer</strong> : efficient transformer-based model for LSTF !
    <ul>
      <li><em>ProbSparse</em> self-attention mechanism
        <ul>
          <li>\(\mathcal{O}(L \log L)\) in time complexity and memory usage</li>
        </ul>
      </li>
      <li>The self-attention distilling
        <ul>
          <li>highlights dominating attention by halving cascading layer input</li>
          <li>and efficiently handles extreme long input sequences</li>
        </ul>
      </li>
      <li>The generative style decoder
        <ul>
          <li>predicts the long time-series sequences at one forward operation, rather than a step-by-step way</li>
          <li>improves the inference speed of long-sequence predictions</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>The major challenge for LSTF is to enhance the <strong>prediction capacity to meet the increasingly long sequence</strong> demand</li>
  <li><em>can we improve Transformer models to be computation, memory, and architecture efficient, as well as maintaining higher prediction capacity?</em></li>
  <li>Vanila Transformer의 limitation 3
    <ul>
      <li>The quadratic computation of self-attention \(\mathcal{O}\left(L^2\right)\)</li>
      <li>The memory bottleneck in stacking layers for long inputs \(\mathcal{O}\left(J \cdot L^2\right)\)</li>
      <li>The speed plunge in predicting long outputs</li>
    </ul>
  </li>
</ul>

<h2 id="2-preliminary">2. Preliminary</h2>

<p>pass</p>

<h2 id="3-methodology">3. Methodology</h2>

<p><img src="/assets/img/timeseries/Informer/fig2.png" alt="그림1" /></p>

<h3 id="query-sparsity-measurement">Query Sparsity Measurement</h3>

<ul>
  <li>Based on KL divergence
    <ul>
      <li>: \(K L(q \| p)=\ln \sum_{l=1}^{L_K} e^{\mathbf{q}_i \mathbf{k}_l^{\top} / \sqrt{d}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \mathbf{q}_i \mathbf{k}_j^{\top} / \sqrt{d}-\ln L_K\)​</li>
    </ul>
  </li>
  <li>\(i\)-th query’s sparsity measurement
    <ul>
      <li>: \(M\left(\mathbf{q}_i, \mathbf{K}\right)=\ln \sum_{j=1}^{L_K} e^{\frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}\)</li>
      <li>즉 query별로 key들과의 attention이 uniform distribution과 얼마나 다른지를 측정</li>
    </ul>
  </li>
</ul>

<h3 id="probsparse-self-attention"><em>ProbSparse</em> Self-attention</h3>

<ul>
  <li>\(\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Softmax}\left(\frac{\overline{\mathbf{Q}} \mathbf{K}^{\top}}{\sqrt{d}}\right) \mathbf{V}\), where \(\overline{\mathbf{Q}}\) only contains the Top-u queries under the sparsity measurement \(M(\mathbf{q}, \mathbf{K})\)​</li>
  <li>Query 중에서 특정 key에 대해서 높은 attention을 가지는 query도 있지만 (Active) 모든 key에 대해서 비슷한 attention을 가지는 query도 있음 (Lazy)
    <ul>
      <li>굳이 모든 query를 다 볼 필요는 없다. Active = useful query이고 Lazy = trivial query</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Informer/myfig3.png" alt="그림2" /></p>

<p><img src="/assets/img/timeseries/Informer/myfig4.png" alt="그림4" /></p>

<ul>
  <li>하지만 모든 query들 중 query가 active한지 알기 위해서는 또 모든 keys와 attention을 계산해봐야 할 것 같지만,
    <ul>
      <li>그렇지 않고 keys를 sampling해서 몇 개만 가져와서 모든 query들과 attention을 계산해도 된다. (증명 : lemma1)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Informer/lemma1.png" alt="그림6" /></p>

<ul>
  <li>그렇게 찾은 useful query들만 가지고, 이제는 모든 keys와 attention을 계산한다</li>
</ul>

<h3 id="encoder-allowing-for-processing-longer-sequential-inputs-under-the-memory-usage-limitation">Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation</h3>

<p><img src="/assets/img/timeseries/Informer/fig3.png" alt="그림7" /></p>

<ul>
  <li>We use the distilling operation to privilege the superior ones with dominating features and make a focused self-attention fea- ture map in the next layer.</li>
  <li>Distilling procedure : \(\mathbf{X}_{j+1}^t=\operatorname{MaxPool}\left(\operatorname{ELU}\left(\operatorname{Conv1d}\left(\left[\mathbf{X}_j^t\right]_{\mathrm{AB}}\right)\right)\right)\)</li>
</ul>

<h3 id="decoder-generating-long-sequential-outputs-through-one-forward-procedure">Decoder: Generating Long Sequential Outputs Through One Forward Procedure</h3>

<ul>
  <li>
    <p>Transformer의 Masked-attention과 Encoder-Decoder Attention 대신 <strong>generative inference</strong></p>

    <ul>
      <li>
        <p>Decoder의 input : \(\mathbf{X}_{\mathrm{de}}^t=\operatorname{Concat}\left(\mathbf{X}_{\text {token }}^t, \mathbf{X}_{\mathbf{0}}^t\right) \in \mathbb{R}^{\left(L_{\text {token }}+L_y\right) \times d_{\text {model }}}\)</p>
      </li>
      <li>
        <p>Start token을 사용하는 대신, Target 직전 시점의 값 몇개를 start token으로 주고, 우리가 원하는 길이의 예측값을 한 번에 decoding</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/Informer/table1.png" alt="그림9" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Informer
    <ul>
      <li><em>ProbSparse</em> self- attention mechanism</li>
      <li>Distilling operation : to handle the challenges of quadratic time complexity and quadratic mem- ory usage in vanilla Transformer</li>
      <li>generative decoder alleviates : alleviates the limitation of tra- ditional encoder-decoder architecture</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2021](https://arxiv.org/pdf/2012.07436)]]></summary></entry><entry><title type="html">Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)</title><link href="http://localhost:4000/timeseries/2024-07-15-timediff/" rel="alternate" type="text/html" title="Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)" /><published>2024-07-15T00:00:00+09:00</published><updated>2024-07-23T18:18:28+09:00</updated><id>http://localhost:4000/timeseries/timediff</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-15-timediff/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>TimeDiff : non-autoregressive diffusion model, w/ two novel conditioning mechanisms
    <ul>
      <li>future mixup : future prediction의 ground-truth의 일부를 conditioning하는 것을 허용</li>
      <li>autoregressive initialization : time series의 basic pattern (short term trends 등)을 모델 initialization에 사용</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Diffusion model (iterative denoising)은 이미지 생성에서 뛰어난 quality
    <ul>
      <li>하지만 time series prediction을 위해 어떻게 쓸지에 대한 연구는 아직</li>
      <li>time series는 <strong>complex dynamics, nonlinear patterns, long-temporal dependencies</strong></li>
    </ul>
  </li>
  <li>기존 diffusion model들은 decoding strategy에 따라 구분됨
    <ul>
      <li><strong>Autoregressive</strong> : future prediction이 one by one으로 generated (ex. Timegrad)
        <ul>
          <li>하지만 error accumulation 때문에 long range prediction 성능이 떨어지고</li>
          <li>하나씩 예측하다보니 inference가 느리다는 단점이 있음</li>
        </ul>
      </li>
      <li><strong>Non-autoregressive</strong> : CSDI, SSSD처럼 denoising networks에 intermediate layers를 conditioning으로 넣고 the denoising objective에 inductive bias를 introduce
        <ul>
          <li>하지만 long-range prediction performance는 Fedformer, NBeats보다 떨어짐</li>
          <li>왜냐하면 conditioning 전략이 image, textf를 위한 것이지 time series를 위한 것이 아니기 때문</li>
          <li>inductive bias를 위해 denoising objective를 사용하는 것만으로는 lookback window에서 유용한 정보를 알아내기 어렵다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>본 논문에서는 long time series prediction을 위한 conditional non-autoregressive diffusion model인 TimeDiff 제안
    <ul>
      <li>CSDI, SSSD와 다르게 conditioning module에 time series를 위한 additional inductive bias 도입
        <ul>
          <li><strong>future mixup</strong>: randomly reveals parts of the ground-truth future pre- dictions during training</li>
          <li><strong>autoregressive initialization</strong>: better initializes the model with basic components in the time series</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-preliminaries">2. Preliminaries</h2>

<h3 id="21-diffusion-models">2.1. Diffusion Models</h3>

<p>pass</p>

<h3 id="22-conditional-ddpms-for-time-series-prediction">2.2. Conditional DDPMs for Time Series Prediction</h3>

<ul>
  <li>\(\mathbf{x}_{-L+1: 0}^0 \in \mathbb{R}^{d \times L}\)를 보고 \(\mathbf{x}_{1: H}^0 \in \mathbb{R}^{d \times H}\)를 예측하는 문제</li>
  <li>\(p_\theta\left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}\right)=p_\theta\left(\mathbf{x}_{1: H}^K\right) \prod_{k=1}^K p_\theta\left(\mathbf{x}_{1: H}^{k-1} \mid \mathbf{x}_{1: H}^k, \mathbf{c}\right)\),
    <ul>
      <li>where \(\mathbf{x}_{1: H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)</li>
    </ul>
  </li>
  <li>아직 efficient denoising network \(\mu_{\theta}\)와 conditioning network \(\mathcal F\) in time series diffusion models를 어떻게 디자인할 것인지 명확하지 않음</li>
  <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad (ICML 2021)</a>
    <ul>
      <li>autoregressive manner :
 \(\begin{aligned}
p_\theta &amp; \left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\right) \\
&amp; =\prod_{t=1}^H p_\theta\left(\mathbf{x}_t^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right) \\
&amp; =\prod^H p_\theta\left(\mathbf{x}_t^K\right) \prod^K p_\theta\left(\mathbf{x}_t^{k-1} \mid \mathbf{x}_t^k, \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right)
\end{aligned}\)</li>
      <li>training objectives : \(\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_t^k, k \mid \mathbf{h}_t\right)\right\|^2\right]\)</li>
      <li>autoregressive decoding 때문에 error accumulation이 발생하고 inference가 느리고 부정확함</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/2107.03502">CSDI(NeurIPS 2021)</a>
    <ul>
      <li>time series \(\mathbf{x}_{-L+1: H}^0\) 전체를 한 번에 diffusing and denoising</li>
      <li>binary mask \(\mathbf{m} \in\{0,1\}^{d \times(L+H)}\)를 사용하여 self-supervised strategy 제안</li>
      <li>training objectives : \(\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_{\text {target }}^k, k \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{\text {observed }}^k\right)\right)\right\|^2\right]\)</li>
    </ul>
  </li>
  <li>하지만 CSDI의 한계는
    <ul>
      <li>Denoising networks가 2개의 transformers를 사용해서 complexity가 높다.</li>
      <li>conditioning에 사용되는 masking은 vision의 inpainting이랑 비슷한데
        <ul>
          <li><a href="https://arxiv.org/abs/2201.09865">(Lugmayr et al., 2022)</a>에서는 이 방식이 masking과 observed사이의 부조화 발생한다고 밝힘</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/2208.09399">SSSD (TMLR 2022)</a>
    <ul>
      <li>Transfermer를 structured state space model로 대체</li>
      <li>하지만 여전히 non-autoregressive strategy이라서 boundary disharmony가 발생할 수 있음</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-model">3. Proposed Model</h2>

<ul>
  <li>Conditional Diffusion의 conditioning은 semantic similarities across modalities 파악에 중점</li>
  <li>하지만 현실에서의 non-stationary time series는 complex temporal dependencies 파악이 중요함</li>
</ul>

<h3 id="31-forward-diffusion-process">3.1. Forward Diffusion Process</h3>

<ul>
  <li>Forward process : \(\mathbf{x}_{1: H}^k=\sqrt{\bar{\alpha}_k} \mathbf{x}_{1: H}^0+\sqrt{1-\bar{\alpha}_k} \epsilon\)
    <ul>
      <li>where \(\epsilon\) is sampled from \(\mathcal{N}(0, \mathbf{I})\) with the same size as \(\mathbf{x}_{1: H}^0\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-conditioning-the-backward-denoising-process">3.2. Conditioning the Backward Denoising Process</h3>

<ul>
  <li>Illustration</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/fig1.png" alt="그림1" /></p>

<h4 id="321-future-mixup">3.2.1. FUTURE MIXUP</h4>

<ul>
  <li>먼저 <em>future mixup</em>으로 \(\mathbf{z}_{\text {mix }}=\mathbf{m}^k \odot \mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)+\left(1-\mathbf{m}^k\right) \odot \mathbf{x}_{1: H}^0\)를 만든다.
    <ul>
      <li>past information’s mapping \(\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)과 the future ground-truth \(\mathbf{x}_{1: H}^0\)를 combine</li>
      <li>training에서 적용되는 것이고, inference에서는 \(\mathbf{z}_{\text {mix }}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)</li>
    </ul>
  </li>
</ul>

<h4 id="322-autoregressive-model">3.2.2. AUTOREGRESSIVE MODEL</h4>

<ul>
  <li>Non-autoregressive models는 masked와 observed의 경계에서 disharmony
    <ul>
      <li>그래서 linear autoregressive (AR) model \(\mathcal{M}_{a r}\) 사용. \(\mathbf{z}_{a r}=\sum_{i=-L+1}^0 \mathbf{W}_i \odot \mathbf{X}_i^0+\mathbf{B}\)
        <ul>
          <li>\(\mathbf{X}_i^0 \in \mathbb{R}^{d \times H}\) is a matrix containing \(H\) copies of \(\mathbf{x}_i^0\),</li>
          <li>\(\mathbf{W}_i\) s \(\in \mathbb{R}^{d \times H}, \mathbf{B} \in \mathbb{R}^{d \times H}\) are trainable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>complex nonlinear time series는 approximate 못하는 건 사실이지만
    <ul>
      <li>simple patterns (short-term trends) 정도는 잘 잡으니까</li>
      <li>그리고 one by one으로 하는 것이 아니라 \(\mathbf{z}_{a r}\)의 모든 columns는 동시에 계산됨</li>
    </ul>
  </li>
</ul>

<h4 id="33-denoising-network">3.3. Denoising Network</h4>

<ul>
  <li>먼저 the transformer’s sinusoidal position embedding으로 the diffusion-step embedding \(\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}\)​ 얻음
    <ul>
      <li>즉 \(\begin{aligned}
k_{\text {embedding }}= &amp; {\left[\sin \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \sin \left(10^{\frac{w \times 4}{w-1}} t\right),\right.} \\
&amp; \left.\cos \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \cos \left(10^{\frac{w \times 4}{w-1}} t\right)\right],
\end{aligned}\)​
        <ul>
          <li>where \(w=\frac{d^{\prime}}{2}\)</li>
        </ul>
      </li>
      <li>그리고 \(\mathbf{p}^k=\operatorname{SiLU}\left(\mathrm{FC}\left(\operatorname{SiLU}\left(\mathrm{FC}\left(k_{\text {embedding }}\right)\right)\right)\right) \in \mathbb{R}^{d^{\prime} \times 1}\)</li>
    </ul>
  </li>
  <li>그 다음 \(\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}\)는 diffused input \(\mathbf{x}_{1: H}^k\)의 embedding \(\mathbf{z}_1^k \in \mathbb{R}^{d^{\prime} \times H}\)에 합쳐짐 (\(2 d^{\prime} \times H\)이 됨)
    <ul>
      <li>\(\mathbf{z}_1^k\)는 여러 개의 convolution layers로 이루어진 input projection block을 통과시켜 얻음</li>
    </ul>
  </li>
  <li>그 다음 multilayer convolution-based <strong>encoder</strong> 통과하면 \(\mathbf{z}_2^k \in \mathbb{R}^{d^{\prime \prime} \times H}\)로 representation</li>
  <li>그 다음 \(\mathbf{c}\)와 \(\mathbf{z}_2^k\)를 fuse해서 \(\left(2 d+d^{\prime \prime}\right) \times H\)로 만들고
    <ul>
      <li>multiple convolution layers <strong>decoder</strong>에 넣어서 \(\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right)\in \mathbb{R^{d \times H}}\)로 만듬 (\(\mathbf{x}_{1: H}^k\)와 같은 size)</li>
    </ul>
  </li>
  <li>마지막으로 \(\mu_{\mathbf{x}}\left(\mathbf{x}_\theta\right)=\frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}^k, k \mid \mathbf{c}\right)\)을 통해 denoised output \(\begin{aligned}
\hat{\mathbf{x}}_{1: H}^{k-1}= &amp; \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
+\sigma_k \epsilon
\end{aligned}\)얻음</li>
  <li>흔히 아는 Diffusion에서는 noise \(\epsilon_\theta\left(\mathbf{x}_{1: H}^k, k\right)\)를 예측하지만, time series에서는  highly irregular noisy components라서 데이터 \(\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k\right)\)를 예측</li>
</ul>

<h3 id="34-training">3.4. Training</h3>

<ul>
  <li>Pesudo code</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/code.png" alt="그림3" /></p>

<ul>
  <li>각각의 \(\mathbf{x}_{1: H}^0\)에 대해 batch of diffusion steps \(k\)’s를 sampling하고
    <ul>
      <li>conditioned variant of loss를 minimize: \(\min _\theta \mathcal{L}(\theta)=\min _\theta \mathbb{E}_{\mathbf{x}_{1 . H}^0, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), k} \mathcal{L}_k(\theta)\)</li>
    </ul>
  </li>
</ul>

<h3 id="35-inference">3.5. Inference</h3>

<ul>
  <li>Pesudo code</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/code.png" alt="그림4" /></p>

<ul>
  <li>먼저 noise vector \(\mathbf{x}_{1 \cdot H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \in \mathbb{R}^{d \times H}\)를 생성하고</li>
  <li>\(\begin{aligned}
\hat{\mathbf{x}}_{1: H}^{k-1}= &amp; \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
+\sigma_k \epsilon
\end{aligned}\)을 반복 (\(k=1\) 까지)
    <ul>
      <li>when \(k=1\), \(\epsilon=0\)이므로 \(\hat{\mathbf{x}}_{1: H}^0\)를 final prediction으로 얻을 수 있음</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/timediff/table2.png" alt="그림12" /></p>

<p><img src="/assets/img/timeseries/timediff/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/timediff/fig2.png" alt="그림2" /></p>

<h3 id="43-ablation-study">4.3. Ablation study</h3>

<ul>
  <li><strong>The Effectiveness of Future mixup</strong></li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table4.png" alt="그림14" /></p>

<ul>
  <li>
    <p>특히 ETTh1 데이터셋에서 future mixup을 안썼을 때 성능이 많이 떨어진다.</p>
  </li>
  <li>
    <p><strong>The Mixup Strategies in Future mixup</strong></p>
    <ul>
      <li>Hard mixup : The sampled values in \(\mathbf{m}^k\) are binarized by a threshold \(\tau \in (0,1)\)</li>
      <li>Segment mixup : The mask  \(\mathbf{m}^k\), Each masked segment has a length following the geometric distribution with a mean of 3. This is then followed by an unmasked segment with mean length \(3(1 − \tau)/\tau\)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table5.png" alt="그림15" /></p>

<ul>
  <li><strong>Predicting \(\mathbf{x}_\theta\) vs Predicting \(\epsilon_\theta\)</strong></li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table6.png" alt="그림16" /></p>

<h3 id="44-integration-into-existing-diffusion-models">4.4. Integration into Existing Diffusion Models</h3>

<p><img src="/assets/img/timeseries/timediff/table7.png" alt="그림17" /></p>

<h3 id="45-inference-efficiency">4.5. Inference Efficiency</h3>

<p><img src="/assets/img/timeseries/timediff/table8.png" alt="그림18" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Timediff : diffusion model for time series prediction,
    <ul>
      <li>1)future mixup과 2)autoregressive initialization이라는 conditioning mechanisms으로</li>
      <li>conditioning network에 useful inductive bias를 추가</li>
      <li>한계점으로 변수의 개수가 많을 때 multivariate dependencies를 학습하기 어렵다
        <ul>
          <li>graph 사용 ?</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2023](https://arxiv.org/pdf/2306.05043)]]></summary></entry></feed>