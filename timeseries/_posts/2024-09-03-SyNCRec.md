---
layout: post
related_posts:
  _
title: 
description: >
  [SIGIR'24 Best Paper](https://arxiv.org/pdf/2407.11245)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIR'24 Best Paper)

## Abstract

- Cross-Domain Sequential Recommendation (CDSR)ì€ multiple domainì—ì„œì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ Single-Domain Sequential Recommendation (SDSR)ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ
- í•˜ì§€ë§Œ **negative transfer** : lack of relation btw domainsì€ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸
- ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ”
  1. estimates the degree of **negative transfer** of each domain
  2. adaptively assigns it as a **weight factor** to the prediction loss
     - to control gradient flows through domains with significant negative transfer !
  3. developed **auxiliary loss** that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis

- ì´ëŸ¬í•œ CDSRê³¼ SDSRì˜ cooperative learningì€ collaborative dynamics between pacers and runners in a marathonì™€ ìœ ì‚¬í•¨

## 1. Introduction

- Single-Domain Sequential Recommendation (SDSR)
  - focuses on **recommending the next item** within a **specific** domain using **only** the **single**-domain sequence
- Cross-Domain Sequential Recommendation (CDSR)
  - **predicts** the **next item** a user will interact with, by leveraging their historical **interaction** sequences across **multiple** domains
- ë‘˜ì˜ ì°¨ì´ëŠ” ê²°êµ­ ë‹¤ë¥¸ domainsì˜ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ”ì§€ ì—¬ë¶€
- CDSRì€ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë‹¤ë¥¸ domainsì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì§€ë§Œ í•­ìƒ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê±´ ì•„ë‹˜
  - ë§Œì•½ ê·¸ê²ƒ ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë” ì•ˆì¢‹ì•„ì§„ë‹¤ë©´, ê·¸ê±´ **negative transfer**ê°€ ìˆì—ˆê¸° ë•Œë¬¸

![ê·¸ë¦¼1](/assets/img/timeseries/SyNCRec/fig1.png)

- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” SyNCRec: Asymmetric Cooperative Network for Cross-Domain Sequential Recommendationì„ ì œì•ˆ

- 1. assess the degree of **negative transfer** of each domain
     - by comparing the performance of CDSR and SDSR
  2. adaptively assign this value as **weight to the prediction loss** corresponding to a specific domain
     - to reduces its flow in domains with significant negative transfer !

  3. developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis
     - to exploit the effective correlation signals inherent in the representation pairs of SDSR and CDSR tasks within a specific domain

- SDSRì€ negative transferë¥¼ ì¤„ì´ê¸° ìœ„í•œ pacerì˜ ì—­í• ì„ í•¨
  - (ë§ˆë¼í†¤ì—ì„œ runnerê°€ ë„ˆë¬´ ë¹ ë¥´ê±°ë‚˜ ëŠë¦¬ê²Œ í•˜ì§€ ì•Šê²Œ í•´ì£¼ëŠ” pacer)
- íŠ¹íˆ CDSRì´ SDSRë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì•˜ë˜ (=negative transferê°€ ë°œìƒí•œ) ë„ë©”ì¸ì—ì„œ ì„±ëŠ¥ í–¥ìƒë¨
- ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ domain-specific modelsë¥¼ ë§Œë“¤ í•„ìš”ê°€ ì—†ì„ ê²ƒì„ ê¸°ëŒ€í•¨

## 2. Related Work

### 2.1. Single-Domain Sequential Recommendation

- SDSR : temporal dynamics in user-item interactionsë¥¼ ë””ìì¸
  - GRU-based models : GRU4Rec, STAMP, NARM
  - Attention-mechanism : SASRec, BERT4Rec, SINE, LightSANs
  - Others : NextItNet(CNN), TransRec(Markov chain), ...

### 2.2 Cross-Domain Sequential Recommendation

- CDSR : information from various other domainsë¥¼ leverage
  - Matrix factorization : CMF, CLFM, ...
  - Multi-task learning : DTCDR, DeepAPF, BiTGCF, CAT-ART
  - $$\pi-Net$$ :  introduced gating mechanisms designed to transfer information from a single domain to another paired domain
  - $$C^2DSR$$ : employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations
  - $$MIFN$$ :  introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains
  - $$MAN$$ : designed group-prototype attention mechanisms to capture domainspecific and cross-domain relationships
- However... ê²°êµ­ì—ëŠ” ëª¨ë‘ domain pair ë¼ë¦¬ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§
  - 3ê°œ ì´ìƒì˜ domainsì˜ ê´€ê³„ë¥¼ íŒŒì•…í•  ë•Œ, domainsì´ ì—„ì²­ ë§ì„ ë•Œì—ëŠ” ì–´ë ¤ì›€
  - ê·¸ë˜ì„œ CGRecì—ì„œ CDSRì„ ì œì•ˆí•˜ë©´ì„œ negative transfer ê°œë…ì„ ì œì•ˆ
    - high negative transferë¥¼ ê°€ì§€ëŠ” domainì— panaltyë¥¼ ì£¼ëŠ” ë°©ì‹
    - í•˜ì§€ë§Œ ì—¬ì „íˆ SDSRë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì€ domainì´ ê½¤ ìˆìŒ
- ê·¸ëŸ¬ë¯€ë¡œ ë³¸ ë…¼ë¬¸ì—ì„œì˜ ëª©í‘œëŠ” 3ê°œ ì´ìƒì˜ **ëª¨ë“ ** ë„ë©”ì¸ì—ì„œ negative transferë¥¼ **íš¨ìœ¨ì **ìœ¼ë¡œ ì¤„ì´ëŠ” ê²ƒ

## 3. Preliminary

- Domains : $$\mathcal{D}=\{A, B, C, \ldots\}$$ where $$\mid \mathcal{D}\mid  \geq 3$$
  - $$d \in \mathcal D$$ ëŠ” í•˜ë‚˜ì˜ íŠ¹ì • ë„ë©”ì¸ì„ ì˜ë¯¸,
  - $$V^d$$ëŠ” set of items specific to the domain $$d$$, $$V$$ëŠ” total item set across all domains

### Definition 1. Single- and Cross-Domain Sequential Recommendation

-  The single-domain sequences of domain $$d$$ : $$X^d=\left[(\mathrm{SOS}), x_1^d, x_2^d, \ldots, x_{\mid X^d\mid -1}^d\right]$$â€‹
  - $$x_t^d$$ :  interaction occurring at time $$t$$
- ê·¸ëŸ¬ë¯€ë¡œ cross-domain sequenceëŠ” $$X=\left(X^A, X^B, X^C, \ldots\right)$$ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ
- ì˜ˆë¥¼ ë“¤ì–´, $$X=\left[(\mathrm{SOS}), x_1^A, x_2^B, x_3^A, x_4^B, x_5^A, x_6^C, x_7^C\right]$$ì€ $$
  X^A=\left[(\mathrm{SOS}), x_1^A, x_3^A, x_5^A\right], X^B=\left[(\mathrm{SOS}), x_2^B, x_4^B\right], \text { and } X^C=[(\mathrm{SOS})\left., x_6^C, x_7^C\right]$$ìœ¼ë¡œ split ê°€ëŠ¥
- SDSRì€ í•˜ë‚˜ì˜ domain ì•ˆì—ì„œ recommending, CDSRì€ ì „ì²´ ë„ë©”ì¸ì—ì„œ recommending

### Definition 2. Negative Transfer Gap (NTG)

- $$\mathcal{L}_\pi^d$$ëŠ” domain $$d$$ì—ì„œì˜ model $$\pi$$ì˜ lossë¥¼ ì˜ë¯¸ (SDSR ë˜ëŠ” CDSR)
- ê·¸ëŸ¬ë¯€ë¡œ Negative transferëŠ” $$\phi_\pi(d) = \mathcal{L}_\pi^d\left(X^d\right)-\mathcal{L}_\pi^d(X)$$

### Problem Statement

- historical cross-domain sequences $$X_{1:t}$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ëª©í‘œëŠ” ë‹¤ìŒ item $$x_{t+1}^d = \underset{x_{t+1}^d \in V^d}{\operatorname{argmax}} P\left(x_{t+1}^d \mid X_{1: t}\right)$$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
- ë§Œì•½ $$\mid \mathcal{D}\mid $$ê°œì˜ single-domain sequences (for SDSR)ê³¼ 1ê°œì˜ sequence (for CDSR)ê°€ ìˆë‹¤ë©´
  - multi-tasking learning mannerì˜ ëª¨ë¸ í•˜ë‚˜ëŠ” $$\mid \mathcal{D}\mid +1$$ê°œì˜ next item prediction tasksë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.

## 4. Model

![ê·¸ë¦¼1](/assets/img/timeseries/SyNCRec/fig2.png)

### 4.1. Shared Embedding Layer

- ì—¬ê¸°ì„œëŠ” **initialized representations** of itemsë¥¼ ì–»ëŠ”ë‹¤.
  - for $$\mid \mathcal{D}\mid $$ single-domain sequences $$X^d$$, and one cross-domain sequence $$X$$
- Item embedding matrix $$M^d \in \mathbb R^{\mid V^d\mid \times r}$$â€‹ì´ê³ 
  - $$\mid V^d\mid $$ëŠ” domain dì˜ items ê°œìˆ˜, rì€ embedding dimension
- ëª¨ë“  domainsì— ëŒ€í•´ concatí•˜ë©´ $$M \in \mathbb R^{\mid V\mid \times r}$$
  - $$\mid V\mid $$ëŠ” ëª¨ë“  ë„ë©”ì¸ì—ì„œ items ê°œìˆ˜
- ì—¬ê¸°ì„œ ìµœê·¼ Tê°œë§Œì„ ì‚¬ìš© (Tê°œë³´ë‹¤ ì ë‹¤ë©´ ì•ìª½ì— paddingìœ¼ë¡œ ë§ì¶°ì¤Œ)
  - ê·¸ëŸ¬ë©´ $$\mathbf{E}^d \in \mathbb{R}^{T \times r} \text { and } \mathbf{E} \in \mathbb{R}^{T \times r}$$ë¥¼ ì–»ìŒ (ê°ê° Fig2(c-1), (c-2))
  - $$\mid \mathcal D\mid $$ê°œì˜ $$\mathbf{E}^d$$ë¥¼ aggregationí•œ ê²ƒì´ $$\mathbf{E}^{\text {single }}$$ (Fig2(c-1))
  - ì°¸ê³ ë¡œ $$\mathbf E, \mathbf E^d$$ì—ëŠ” learnable positional embedding ë”í•´ì ¸ìˆìŒ
  - $$t$$-th stepì—ì„œì˜ $$\mathbf E, \mathbf E^d$$ëŠ” ê°ê° $$\mathbf e, \mathbf e^d$$ë¡œ ì •ì˜

### 4.2. Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMoE)

- Negative Transfer (NTG)ëŠ” **loss of the SDSR**ê³¼ **the loss of CDSR**ì˜ ì°¨ì´ë¡œ ì •ì˜
  - NTGê°€ ì‘ìœ¼ë©´ ë‹¤ë¥¸ domainsì˜ ì •ë³´ê°€ ë„ì›€ì´ ì•ˆë˜ëŠ” ê±°ê³  í¬ë©´ ë„ì›€ì´ ë˜ëŠ” ê²ƒ
- ê·¸ëŸ¬ë¯€ë¡œ weight for the prediction loss in the domainë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤
  - gradient flowë¥¼ ì‘ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œë‹¤
- Multi-gate Mixture of Sequential Experts (MoE) architectureë¥¼ ì‚¬ìš©í•˜ì—¬ SDSRê³¼ CDSRë¥¼ ìˆ˜í–‰í•˜ê³ 
  - **models** relationships between different tasks and **learns** task-specific functionalities
  - enabling it to effectively leverage shared representations

- SDSRê³¼ CDSRì€ ì„œë¡œ ê°„ì„­í•˜ì§€ ì•Šê³ , expertsë¡œëŠ” Transformerë¥¼ ì‚¬ìš©

### 4.2.1. Architecture

- **ë¨¼ì € SDSRì„ ë³´ì**

- shared embedding layerë¡œë¶€í„° initialized representations of single- and cross-domain sequences,

  - ì¦‰ $$\mathbf E, \mathbf E^d$$ê°€ ì£¼ì–´ì ¸ìˆì„ ë•Œ, ê° expertëŠ” many-to-many sequence learningì„ ìˆ˜í–‰

- domain $$d$$ì˜ output : $$\begin{aligned}
  & \left(\mathbf{Y}^d\right)^{\text {single }}=h^d\left(f^d\left(\mathbf{E}^d\right)\right) \\
  & f^d\left(\mathbf{E}^d\right)=\sum_{k=1}^j g^d\left(\mathbf{E}^d\right)_k \mathrm{SG}\left(f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)\right)+\sum_{k=j+1}^K g^d\left(\mathbf{E}^d\right)_k f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)
  \end{aligned}$$

  - $$h^d$$ : the tower network for domain $$d$$â€‹ (Fig. 2(c-7))
    - feed-forward network with layer normalization
  - $$f^d$$ : the multi-gated mixture of the sequential experts layer
  - $$SG$$â€‹ :  the stopgradient operation (Fig. 2(c-4))
    - forward passì—ì„œëŠ” identity function
    - backward passì—ì„œëŠ” SG ì•ˆì— ìˆëŠ” ê²ƒë“¤ì˜ gradientëŠ” drop
    - ìœ„ ì‹ì—ì„œëŠ” $$j+1 \sim K$$ë²ˆì§¸ expertsë§Œ unique sequential pattern of single-domain sequencesë¥¼ í•™ìŠµ
  - $$f_{\text {TRM }}^k$$â€‹ :  the ğ‘˜-th transformerbased sequential expert (Fig. 2(c-3))
  - $$g^d$$ :  gating network for domain $$d$$ (Fig. 2(c-6))
    - $$g^d\left(\mathbf{E}^d\right)=\operatorname{softmax}\left(W_g^d \mathbf{E}^d\right)$$ where $$W_g^d \in \mathbb{R}^{K \times d T}$$ is trainable FC
  - The $$t$$-th element of $$\mathrm{Y}^{\text {single }}$$ëŠ” $$\left(y_t^d\right)^{\text {single }}$$

- **ë‹¤ìŒìœ¼ë¡œ CDSRì„ ë³´ì**

- ACMoE module : $$\begin{aligned}
  & \mathbf{Y}^{\text {cross }}=h^{\text {cross }}\left(f^{\text {cross }}(\mathbf{E})\right) \\
  & f^{\text {cross }}(\mathbf{E})=\sum_{k=1}^j g^{\text {cross }}(\mathbf{E})_k f_{\mathrm{TRM}}^k(\mathbf{E})+\sum_{k=j+1}^K g^{\text {cross }}(\mathbf{E})_k \operatorname{SG}\left(f_{\mathrm{TRM}}^k(\mathbf{E})\right)
  \end{aligned}$$

  - $$h^{cross}$$ :  the tower network (Fig. 2(c-9))
  - $$f^{\text {cross }}$$ : the multi-gated mixture of sequential experts layer for a cross-domain sequence
  - $$SG$$ëŠ” $$j+1\sim K$$-th $$f^k_{TRM}$$ì—ë§Œ ì‚¬ìš©
    - ê·¸ëŸ¬ë©´ $$1\sim j$$â€‹ë²ˆì§¸ expertsê°€cross-domain sequencesì—ì„œ the distinct sequential patterns presentë¥¼ í•™ìŠµ

  - $$g^{\text {cross }}(\mathbf{E})=\operatorname{softmax}\left(W_a^{c r o s s} \mathbf{E}\right)$$â€‹ : gating network for the crossdomain sequence (Fig. 2(c-8))

- $$\left(y_t^d\right)^{\text {single }} \text { and }\left(y_t\right)^{\text {cross }}$$ëŠ” two representations of different views for the same item

### 4.2.2. Transformer Experts

- ê°ê°ì˜ Multi-head Self-Attentionì— $$Z \in \mathbb{R}^{T \times r}$$ ê°€ linear transformation
  - $$\to$$ $$\text { queries } Q_i \in \mathbb{R}^{T \times r / p} \text {, keys } K_i \in \mathbb{R}^{T \times r / p} \text {, } \text { values } V_i \in \mathbb{R}^{T \times r / p}$$ê°€ ë¨
- $$\begin{aligned}
  & \operatorname{Attn}\left(Q_i, K_i, V_i\right)=\operatorname{softmax}\left(\frac{Q_i K_i^{\top}}{\sqrt{r / p}}\right) V_i, Q_i=Z \mathrm{~W}_i^Q, K_i=Z \mathrm{~W}_i^K, V_i=Z \mathrm{~W}_i^V
  \end{aligned}$$ ê±°ì³ final outputì€ $$\mathbf{H} \in \mathbb{R}^{T \times r}$$ 

- ë§ˆì§€ë§‰ìœ¼ë¡œ $$\operatorname{FFN}(\mathbf{H})=\left[\mathrm{FC}\left(\mathbf{H}_1\right)\left\\mid \mathrm{FC}\left(\mathbf{H}_2\right)\right\\mid , \ldots, \\mid  \mathrm{FC}\left(\mathbf{H}_T\right)\right]$$
  - where $$\mathrm{FC}\left(\mathbf{H}_t\right)=\operatorname{GELU}\left(\mathbf{H}_t \mathrm{~W}_1+b_1\right) \mathrm{W}_2+b_2$$
  - $$\mathbf{H}_t$$ : ğ‘¡-th representation of $$\mathbf{H}$$

### 4.3. Loss Correction with Negative Transfer Gap (LC-NTG)

### 4.3.1.  Single-Domain Item Prediction

- Fig 2(e-1)
- single domoin sequence $$X_{1: t}^d$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ ì•„ì´í…œ $$x_{t+1}^d$$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ pairwise ranking lossë¥¼ ì‚¬ìš©
  - ì¦‰ $$l_t^d=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}^d\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}^d\right)\right), \mathcal{L}_{\text {single }}^d=\sum_{t=1}^T l_t^d$$
    - where $$x^{d+}$$ : ground-truth item paired with a negative item $$x^{d-}$$ sampled froem Unif
    - $$P\left(x_{t+1}^d=x^d \mid X_{1: t}^d\right)$$ = $$\sigma\left(\left(y_t^d\right)^{\text {single }} \cdot M\left(x^d\right)\right)$$

### 4.3.2. Cross-Domain Item Prediction

- CDSR $$l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t$$
  - where $$P\left(x_{t+1}^d=x^d \mid X_{1: t}\right) \text { is obtained by } \sigma\left(\left(y_t\right)^{\text {cross }} \cdot M\left(x^d\right)\right)$$

### 4.3.3.  Calculating the Negative Transfer Gap

- ì´ì œ NTGë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. $$\phi_\pi(d)=\sum_{t=1}^T\left(l_t^d-l_t\right)$$
  - where $$l_t^d$$ and $$l_t$$ are losses of the SDSR and CDSR tasks in time step $$t$$ for the domain $$d$$, respectively, calculated with our model $$\pi$$
- $$\lambda=\left(\lambda_1, \lambda_2, \ldots, \lambda_{\mid \mathcal{D}\mid }\right)$$ë¥¼ ê° domainì—ì„œì˜ NTGë¼ê³  í•˜ë©´ $$\lambda_d \leftarrow \operatorname{softmax}\left(\alpha * \lambda_d+\beta * \phi_\pi(d) ; \delta\right)$$ë¡œ ê³„ì‚°
  - where $$\alpha \text { and } \beta$$ are learnable parameters

### 4.3.4. Loss Correction

- NTGëŠ” weight for the cross-domain item prediction lossë¡œ í™œìš©ë¨ 
  - lossëŠ” $$l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t$$
- re-aggregate : multiplying the relative NTG for each domain separately 
  - $$\mathcal{L}_{\text {cross }}^{l c} = =\sum_{t=1}^T \sum_{d=1}^{\mid \mathcal{D}\mid } \lambda_d \log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right)$$

- ì´ë ‡ê²Œ í•˜ë©´ NTGê°€ ë°œìƒí•˜ëŠ” domainì—ì„œì˜ gradient flowë¥¼ ì¤„ì´ëŠ” ê²ƒ

## 4.4. Single-Cross Mutual Information Maximization (SC-MIM)

- SC-MIM: SDSR and CDSR tasks ì‚¬ì´ì˜ ì •ë³´ë¥¼ ì˜ transferí•˜ê¸° ìœ„í•œ ë°©ë²•
  - mutual informationìœ¼ë¡œ ë‘ tasksì˜ correlation signalsë¥¼ íŒŒì•…
  - mutual information: $$I(X, Y)=D_{K L}(p(X, Y) \\mid  p(X) p(Y))=\mathbb{E}_{p(X, Y)}\left[\log \frac{p(X, Y)}{p(X) p(Y)}\right]$$â€‹
- í•˜ì§€ë§Œ ì´ mutual informationì„ high-dimdì—ì„œ êµ¬í•˜ëŠ” ê±´ ì–´ë µê¸° ë•Œë¬¸ì— lower boundë¡œ InfoNCEë¥¼ ì‚¬ìš©
  - lower bound : $$I(X, Y) \geq \mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\mathbb{E}_{q(\hat{Y})}\left(\log \sum_{\hat{y} \in \hat{Y}} \exp \rho_\theta(x, \hat{y})\right)\right]+\log \mid \hat{Y}\mid $$
    - where $$x, y$$ëŠ” ê°™ì€ inputì˜ ì„œë¡œ ë‹¤ë¥¸ view points
    - $$\rho_\theta$$ ëŠ” similarity function,
  - InfoNCEë¥¼ maximizingí•˜ëŠ” ê²ƒì€ standard cross-entropy lossë¥¼ maximizingí•˜ëŠ” ê²ƒê³¼ ê°™ìŒ
    - : $$$\mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\log \sum_{\hat{y} \in Y} \exp \rho_\theta(x, \hat{y})\right]$$

- ì•„ë¬´íŠ¼ ëŒì•„ì™€ì„œ ìš°ë¦¬ëŠ” $$$\mathbf{Y}^{\text {single }}$ and $\mathbf{Y}^{\text {cross }}$$ì˜ mutual informationì„ maximizingí•˜ê³  ì‹¶ìŒ
  - ê·¸ëŸ¬ë¯€ë¡œ cross-domain representation $$\mathbf{Y}^{\text {ross }}$$ë¥¼ domainë³„ë¡œ splití•´ì„œ $$(\mathbf{Y^d})^{\text {ross }}$$ êµ¬í•˜ê³ 
  - ì•„ë˜ ì‹ì²˜ëŸ¼ ê³„ì‚°
    - : $$\begin{aligned} & \mathcal{L}_{S C-M I M}^d=\rho\left(\left(\mathbf{Y}^d\right)^{\text {single }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)-\log \sum_{u-} \exp \left(\rho\left(\left(\mathbf{Y}^d\right)^{\text {single- }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)\right)\end{aligned}$$
    - where $$u-$$ëŠ” other users in a training batch,
    - $$\left(\mathbf{Y}^d\right)^{\text {single- }}$$ëŠ” subsequence of domain $$ğ‘‘$$ of user $$ğ‘¢âˆ’$$â€‹ 
    - $$\rho(\cdot, \cdot)$$ëŠ” $$\rho(U, V)=\sigma\left(U^{\top} \cdot W^H \cdot V\right)$$

### 4.5. Model Training and Evaluation

- Total training loss : $$\mathcal{L}=\eta\left(\sum_{d=1}^{\mid \mathcal{D}\mid }\left(\mathcal{L}_{\text {single }}^d\right)+\mathcal{L}_{\text {cross }}^{l c}\right)+(1-\eta) \sum_{d=1}^{\mid \mathcal{D}\mid } \mathcal{L}_{S C-M I M}^d$$
  - where $$\eta$$ is the harmonic factor
  - evaluationí•  ë•Œì—ëŠ” cross-domain representationë§Œ ì‚¬ìš©

## 5. Experiments

### 5.1. Dataset

### 5.2. Experimental Setting

- ë¨¼ì € Amazon datasetê³¼ Telco datasetì— ëŒ€í•œ ì„±ëŠ¥

![ê·¸ë¦¼1](/assets/img/timeseries/SyNCRec/table23.png)

- Research Questions:

  - (RQ1): Does the performance of our model surpass the current stateof-the-art baselines in practical applications that involve more than three domains?

  - (RQ2): Can our model effectively address the challenge of negative transfer across all domains in the CDSR task?

  - (RQ3): What is the impact of various components of our model on its performance in CDSR tasks?

  - (RQ4): How do variations in hyper-parameter settings influence the performance of our model?

  - (RQ5): How does the model perform when deployed online ?

### 5.3. Performance Evaluation (RQ1)

- First, The effectiveness of our model can be observed.
  - ë‹¤ë¥¸ baseline modelsë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨
- Second, Integrating information from all domains simultaneously in a model can improve performance in each domain compared to modeling a pairwise domain-domain relationship.
  - ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” CDSR taskì—ì„œ domainë¼ë¦¬ì˜ ì •ë³´ë¥¼ ê²°í•©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë‹¤.

### 5.4. Discussion of the negative transfer (RQ2)

- ê¸°ì¡´ baseline modelsëŠ” SDSRë³´ë‹¤ CDSRì˜ ì„±ëŠ¥ì´ ë” ì•ˆì¢‹ì•˜ì§€ë§Œ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ëª¨ë¸ì€ ê·¸ë ‡ì§€ ì•Šë‹¤

![ê·¸ë¦¼1](/assets/img/timeseries/SyNCRec/table4.png)

### 5.5 Discussion of Model Variants (RQ3)

- LC-NTG, SC-MIM, ACMoE ì„¸ ê°€ì§€ components ëª¨ë‘ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ í•„ìš”í•˜ë‹¤

![ê·¸ë¦¼1](/assets/img/timeseries/SyNCRec/table5.png)

### 5.6. Hyperparameter Analysis (RQ4)

![ê·¸ë¦¼1](/assets/img/timeseries/SyNCRec/fig3.png)

## 6. Online A/B Test (RQ5)

pass

## 7. Conclusion

- Negative transferë¥¼ ë‹¤ë£¨ëŠ” CDSR frameworkë¥¼ ì œì•ˆ
  - Negative transferë¥¼ ì¸¡ì •í•˜ê³  prediction lossì˜ weightë¡œ í™œìš©
- SDSR and CDSR tasksì˜ ì •ë³´ë¥¼ êµí™˜ì‹œí‚¤ëŠ” Auxiliary loss ì œì•ˆ