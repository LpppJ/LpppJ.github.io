<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-11-07T18:09:04+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">Is Mamba Effective for Time Series Forecasting? (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-11-07-s-mamba/" rel="alternate" type="text/html" title="Is Mamba Effective for Time Series Forecasting? (Arxiv 2024)" /><published>2024-11-07T00:00:00+09:00</published><updated>2024-11-07T18:08:15+09:00</updated><id>http://localhost:4000/timeseries/s-mamba</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-11-07-s-mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Time series forecasting (TSF)ì—ì„œ TransformerëŠ” quadratic complexity</li>
  <li><strong>Simple-Mamba (S-Mamba)</strong> :
    <ul>
      <li>Tokenize the time points of each variate autonomously via a linear layer</li>
      <li>Bi-directional Mamba layer is utilized to extract inter-variate correlations</li>
      <li>Feed-Forward Network is set to learn temporal dependencies</li>
      <li>Generation of forecast outcomes through a linear mapping layer</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>
    <p>Transformer-based</p>

    <ul>
      <li>quadratic computational complexity</li>
      <li>reduce the computational complexity \(\to\)  loss of information \(\to\) performance degradations</li>
    </ul>
  </li>
  <li>
    <p>Linear model</p>

    <ul>
      <li>solely on linear numerical calculations \(\to\) do not incorporate in-context information</li>
    </ul>
  </li>
  <li>
    <p>SSM</p>

    <ul>
      <li>convolutional calculation to capture sequence information</li>
      <li>eliminate hidden states (for parallel computing) \(\to\) near-linear complexity</li>
      <li>But unable to identify and filter content</li>
    </ul>
  </li>
  <li>
    <p>Mamba</p>

    <ul>
      <li>selective mechanism into SSM</li>
    </ul>
  </li>
  <li>
    <p><strong>Simple-Mamba (S-Mamba)</strong> :</p>

    <ul>
      <li>
        <ol>
          <li>time points of each variate are tokenized by a linear layer</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>Mamba VC (Inter-variate Correlation) Encoding layer encodes the VC</li>
        </ol>

        <ul>
          <li>by utilizing a bidirectional Mamba</li>
        </ul>
      </li>
      <li>
        <ol>
          <li>FeedForward Network (FFN) TD (Temporal Dependency) Encoding Layer extracts the TD</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>mapping layer is utilized to output the forecast results</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-preliminaries">3. Preliminaries</h2>

<h3 id="31-problem-statement">3.1. Problem Statement</h3>

<ul>
  <li>\(U_{\text {in }}=\left[u_1, u_2, \ldots, u_L\right] \in \mathbb{R}^{L \times V}\)ë¥¼ ë³´ê³  \(U_{\text {out }}=\left[u_{L+1}, u_{L+2}, \ldots, u_{L+T}\right] \in \mathbb{R}^{T \times V}\)ë¥¼ ì˜ˆì¸¡
    <ul>
      <li>ê°ê°ì˜ \(u_n=\left[p_1, p_2, \ldots, p_V\right]\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-state-space-models">3.2. State Space Models</h3>

<ul>
  <li>The continuous sequence is discretized by a step size \(\Delta\), and the discretized SSM model :
    <ul>
      <li>\(\begin{aligned}
h_t &amp; =\overline{\boldsymbol{A}} h_{t-1}+\overline{\boldsymbol{B}} x_t, \\
y_t &amp; =\boldsymbol{C} h_t,
\end{aligned}\)      where
        <ul>
          <li>\(\overline{\boldsymbol{A}}=\exp (\Delta \boldsymbol{A}) \text { and } \overline{\boldsymbol{B}}=(\Delta \boldsymbol{A})^{-1}(\exp (\Delta \boldsymbol{A})-I) \cdot \Delta \boldsymbol{B}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="33-mamba-block">3.3. Mamba Block</h3>

<p>: <strong>Data-dependent selection mechanism into the S4</strong> &amp; <strong>Incorporates hardware-aware parallel algorithms</strong></p>

<p><img src="/assets/img/timeseries/s-mamba/alg1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>
    <p>Mamba Layer</p>

    <ul>
      <li>
        <p>Input : \(X \in \mathbb{R}^{B \times V \times D}\)</p>
      </li>
      <li>
        <ol>
          <li>expands the hidden dimension to \(ED\) through linear projection \(\to x, z\)ë¥¼ ì–»ìŒ</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>the projection using convolutional functions and a SiLU \(\to x'\)ë¥¼ ì–»ìŒ</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>generates the state representation \(y\)</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>\(y\) is combined with a residual connection from \(z\) after activation,</li>
        </ol>

        <ul>
          <li>and the final output $y_t$ at time step $t$ is obtained</li>
        </ul>
      </li>
      <li>
        <p>with state expansion factor \(N\),</p>

        <ul>
          <li>a size of convolutional kernel \(k\),</li>
          <li>and a block expansion factor \(E\)</li>
        </ul>
      </li>
      <li>The final output of the Mamba block is \(Y \in \mathbb{R}^{B \times V \times D}\).</li>
    </ul>
  </li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<ul>
  <li>1st layer : the Linear Tokenization Layer (tokenizes the time series with a linear layer)</li>
  <li>2nd layer :  the Mamba intervariate correlation (VC) Encoding layer (using a bidirectional Mamba block)</li>
  <li>3rd layer : the FFN Temporal Dependencies (TD) Encoding Layer (learns the temporal sequence information)
    <ul>
      <li>Feed-Forward Network : generates future series representations</li>
    </ul>
  </li>
  <li>4th layer : Projection Layer, is only mapping for forecasting</li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/alg2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="41-linear-tokenization-layer">4.1. Linear Tokenization Layer</h3>

<ul>
  <li>\(U=\operatorname{Linear}\left(\operatorname{Batch}\left(U_{\text {in }}\right)\right), \quad\) where \(U\) is the output of this layer</li>
</ul>

<h3 id="42-mamba-vc-encoding-layer">4.2. Mamba VC Encoding Layer</h3>

<ul>
  <li>ì—¬ê¸°ì„œëŠ” ìœ ì‚¬í•œ trendë¥¼ ë³´ì´ëŠ” ë³€ìˆ˜ë“¤ì„ ì—°ê²°í•´ì„œ VCë¥¼ ì°¾ê³  ì‹¶ìŒ</li>
  <li>TransformerëŠ” ê·¸ëƒ¥ ëª¨ë“  ë³€ìˆ˜ë“¤ë¼ë¦¬ ë‹¤ ì—°ê²°í•˜ë‹ˆê¹Œ ì •í™•í•˜ê¸´ í•œë° ë³€ìˆ˜ ê°œìˆ˜ ë”°ë¼ complexity ëŠ˜ì–´ë‚¨</li>
  <li>MambaëŠ” complexityëŠ” near-linearì´ì§€ë§Œ
    <ul>
      <li><strong>Selection mechanismì´ uni-directionalí•´ì„œ ì•ìª½ì˜ ë³€ìˆ˜ë§Œ ë³¼ ìˆ˜ ìˆìŒ</strong></li>
      <li>ê·¸ë˜ì„œ 2ê°œì˜ Mambaë¥¼ ì„œë¡œ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ íë¥´ë„ë¡ ë†“ìŒ (Bi-directional Mamba)</li>
      <li>\(\begin{aligned}&amp;\overrightarrow{\boldsymbol{Y}}=\overrightarrow{\operatorname{Mamba\operatorname {Block}}(\boldsymbol{U}),} \\
&amp; \overleftarrow{\boldsymbol{Y}}=\overleftarrow{\operatorname{Mamba} \operatorname{Block}}(\boldsymbol{U}) .
\end{aligned}\)ì´ê³  \(\boldsymbol{Y}=\overrightarrow{\boldsymbol{Y}}+\overleftarrow{\boldsymbol{Y}}\)ë¡œ Aggregate, with residual : \(\boldsymbol{U}^{\prime}=\boldsymbol{Y}+\boldsymbol{U}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-ffn-td-encoding-layer">4.3. FFN TD Encoding Layer</h3>

<ul>
  <li>
    <ol>
      <li>Normalization layer : enhance convergence and training stability</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>FFN layer : encodes observed time series (encodes TD by keeping the sequential relationships)</li>
    </ol>

    <ul>
      <li>decodes future series representations (adjust the future series representations)</li>
    </ul>
  </li>
</ul>

<h3 id="44-projection-layer">4.4. Projection Layer</h3>

<ul>
  <li>FFN TD Encoding layerì˜ outputì¸ tokenized temporal informationì´
    <ul>
      <li>linear mappingì„ í†µí•´ì„œ reconstructed for predictive outcome</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-datasets-and-baselines">5.1. Datasets and Baselines</h3>

<p><img src="/assets/img/timeseries/s-mamba/table1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>SOTAì™€ ë¹„êµ :
    <ul>
      <li>iTransformer: analyzes the time series information of each <strong>individual variate</strong> and then fuses the information of all variates.</li>
      <li>PatchTST: segments time series into <strong>subseries patches</strong> as input tokens and uses channel-independent shared embeddings and weights</li>
      <li>Crossformer: cross-attention mechanism that allows the model to <strong>interact with information between different time steps</strong></li>
      <li>FEDformer: a <strong>frequency-enhanced Transformer</strong> for utilizaing a sparse representation</li>
      <li>Autoformer: <strong>decomposition architecture</strong> that incorporates an auto-correlation mechanism</li>
      <li>RLinear: reversible normalization and channel independence into <strong>pure linear structure</strong></li>
      <li>TiDE: Multi-layer Perceptron (MLP) based encoderdecoder model</li>
      <li>DLinear: <strong>simple one-layer linear</strong> model with decomposition architecture</li>
      <li>TimesNet: a task-general backbone, <strong>transforms 1D time series into 2D tensors</strong></li>
    </ul>
  </li>
</ul>

<h3 id="52-overall-performance">5.2. Overall Performance</h3>

<p><img src="/assets/img/timeseries/s-mamba/table2.png" alt="ê·¸ë¦¼1" /></p>

<p><img src="/assets/img/timeseries/s-mamba/table3.png" alt="ê·¸ë¦¼1" /></p>

<p><img src="/assets/img/timeseries/s-mamba/table4.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>S-Mambaê°€ traffic-related, Electricity, and Solar-Energyì—ì„œ ì„±ëŠ¥ì´ ì¢‹ìŒ
    <ul>
      <li>ë³€ìˆ˜ë“¤ì´ periodicí•œ ë°ì´í„°ì…‹ë“¤.</li>
      <li>ì¦‰ period variates are more likely to contain learnable VC.</li>
      <li>Mamba VC Fusion Layerê°€ ì˜ ì¡ì€ ê²ƒ</li>
    </ul>
  </li>
  <li>ETT, and Exchange datasetsì—ì„œëŠ” ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì§€ëŠ” ì•Šì•˜ìŒ
    <ul>
      <li>ë³€ìˆ˜ ê°œìˆ˜ê°€ ì ì€ ë°ì´í„°ì…‹ë“¤ (predominantly of an aperiodic nature)</li>
      <li><strong>weak</strong> VCs between these variates ë•Œë¬¸ì— Mamba VC Encoding layerê°€ noiseë¥¼ ê°€ì ¸ì˜´</li>
    </ul>
  </li>
  <li>WeatherëŠ” ë³€ìˆ˜ë„ ì ê³  aperiodicí•œë° ì™œ ì„±ëŠ¥ì´ ì¢‹ë‚˜
    <ul>
      <li>ë³€ìˆ˜ë“¤ì˜ Trendê°€ ë™ì‹œì— ë‚˜íƒ€ë‚˜ëŠ” ë„ë©”ì¸ì´ë¼ì„œ Mamba VC Encoding layerê°€ ì˜ ì‘ë™</li>
      <li>Trendê°€ large sectionsë¡œ ë‚˜íƒ€ë‚˜ê¸° ë•Œë¬¸ì— FFNì´ ì´ëŸ° ê±° ì˜ ì¡ìŒ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig4.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="53-model-efficiency">5.3. Model Efficiency</h3>

<p><img src="/assets/img/timeseries/s-mamba/fig5.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="54-ablation-study">5.4. Ablation Study</h3>

<p><img src="/assets/img/timeseries/s-mamba/table5.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="55-can-variate-order-affect-the-performance-of-s-mamba">5.5. Can Variate Order Affect the Performance of S-Mamba?</h3>

<ul>
  <li>S-MambaëŠ” independent channelì´ë¼ì„œ variates order ì•ˆì¤‘ìš”í–ˆìŒ
    <ul>
      <li>í•˜ì§€ë§Œ Mamba VC Encoding LayerëŠ” variates orderì— ë”°ë¼ initial bias ë°œìƒí•  ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ Fourier transformí•´ì„œ variatesë¥¼ periodic and aperiodic groupsìœ¼ë¡œ ë‚˜ëˆ„ê³ 
    <ul>
      <li>periodic variates =  reliable information / aperiodic variates = potential noiseìœ¼ë¡œ ê°€ì •</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ reliable informationë¥¼ ê°€ì§€ê³  ìˆëŠ”(ê·¸ëŸ´ ê²ƒì´ë¼ê³  ìƒê°ë˜ëŠ”) periodic variatesë¥¼ ì•ìª½ì— ë°°ì¹˜</li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig6.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="56-can-mamba-outperform-advanced-transformers">5.6. Can Mamba Outperform Advanced Transformers?</h3>

<ul>
  <li>Transformerì˜ Encoder layerë¥¼ Mambaë¡œ êµì²´
    <ul>
      <li>Autoformer, Flashformer and Flowformer</li>
      <li>\(\to\)  Auto-M, FlashM and Flow-M ì´ë¼ê³  ë¶€ë¥´ê² ìŒ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig7.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="57-can-mamba-help-benefit-from-increasing-lookback-length">5.7. Can Mamba Help Benefit from Increasing Lookback Length?</h3>

<ul>
  <li>Transformer-based modelì€:
    <ul>
      <li>lookback sequence length \(L\)ì´ ëŠ˜ì–´ë‚˜ë„ ì„±ëŠ¥ì´ ë¹„ë¡€í•´ì„œ ì¢‹ì•„ì§€ëŠ” ê±´ ì•„ë‹˜</li>
      <li><strong>sequential orderë¥¼ ì‹ ê²½ì“°ì§€ ì•Šì•„ì„œ ê·¸ë ‡ë‹¤</strong></li>
    </ul>
  </li>
  <li>MambaëŠ”:
    <ul>
      <li>certain sequential attributesê°€ ì˜ ìœ ì§€ë˜ëŠ” í¸</li>
      <li>Mamba blockì„ Transformer-based modelì˜ Encoderì™€ decoder ì‚¬ì´ì— ë°°ì¹˜í•˜ë©´
        <ul>
          <li>Encoder layerì˜ outputì— (decoder layerì— ê°€ê¸° ì „ì—)</li>
          <li>positional encodingì²˜ëŸ¼ ì–´ë–¤ ì •ë³´ë¥¼ ì¶”ê°€í•´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ” ê²ƒ</li>
        </ul>
      </li>
      <li>ê·¸ê²ƒì„ Reformer, Informer, and Transformerì™€ ë¹„êµí•´ì„œ
        <ul>
          <li>Refor-M, Infor-M, and Trans-Më¼ê³  ë¶€ë¥´ê³  ë¹„êµ</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig8.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>S-Mambaì™€ iTransformer ë‘˜ë‹¤ \(L\)ì´ ê¸¸ì–´ì§€ë©´ ì–´ëŠ ì •ë„ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ê¸´ í•¨
    <ul>
      <li>í•˜ì§€ë§Œ ì´ê±´ ë‘ ëª¨ë¸ ëª¨ë‘ ê°€ì§€ê³  ìˆëŠ” FFN TD Encoding Layer ë•Œë¬¸ìœ¼ë¡œ ë³´ì„</li>
    </ul>
  </li>
  <li>S-Mambaê°€ iTransformerì™€ ë¹„êµí•´ì„œ ì¼ê´€ë˜ê²Œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” í¸ì¸ë°
    <ul>
      <li>ì´ê±´ Mamba VC Encoding layerì™€ Transformerì˜ VC Encoding layerì˜ ì°¨ì´ !</li>
    </ul>
  </li>
</ul>

<h3 id="58-is-mamba-generalizable-in-tsf">5.8. Is Mamba Generalizable in TSF?</h3>

<ul>
  <li>TransformerëŠ” generalization capabilitiesê°€ ì¢‹ì€ í¸ì´ë¼ì„œ
    <ul>
      <li>iTransformerì˜ ê²½ìš° 40%ì˜ ë³€ìˆ˜ë§Œì„ ê°€ì§€ê³  ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ì€ maskingí•´ë„</li>
      <li>ì„±ëŠ¥ì´ í° í­ìœ¼ë¡œ ë‚˜ë¹ ì§€ì§€ëŠ” ì•ŠìŒ</li>
    </ul>
  </li>
  <li>Mambaì˜ ê²½ìš°ì—ë„ 40%ì˜ ë³€ìˆ˜ë§Œ ë³´ê³  ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ì€ masking í–ˆì„ ë•Œ
    <ul>
      <li>iTransformerì— í¬ê²Œ ë’¤ì³ì§€ì§€ ì•ŠìŒ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/s-mamba/fig9.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li><strong>Simple-Mamba(S-Mamba)</strong>
    <ul>
      <li>inter-variate correlation (VC) encodingì€</li>
      <li>Transformer ëŒ€ì‹  <strong>bi-directional</strong> Mamba blockìœ¼ë¡œ í•˜ê³ </li>
      <li>(ë” ë‚®ì€ overheadë¡œ VCë¥¼ íŒŒì•…)</li>
      <li>Temporal Dependencies (TD)ëŠ”
        <ul>
          <li>Feed-Forward Networkë¡œ extract</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>MambaëŠ” advanced-Transformerë§Œí¼ì˜ <strong>stability</strong>ë„ ìˆê³ 
    <ul>
      <li><strong>generalization</strong> capabilitiesë„ ë›°ì–´ë‚œ í¸</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2403.11144v3)]]></summary></entry><entry><title type="html">TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)</title><link href="http://localhost:4000/timeseries/2024-10-29-timemachine/" rel="alternate" type="text/html" title="TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)" /><published>2024-10-29T00:00:00+09:00</published><updated>2024-10-29T21:25:07+09:00</updated><id>http://localhost:4000/timeseries/timemachine</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-29-timemachine/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Long-term time-series forecasting(LTSF)ì—ì„œ ìš°ë¦¬ëŠ” long-term dependenciesë¥¼ captureí•˜ëŠ”ë°
    <ul>
      <li>linear scalabilityì™€ computational efficiencyë¥¼ ìœ ì§€í•´ì•¼ í•¨</li>
    </ul>
  </li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” TimeMachineì€ Mambaë¥¼ í™œìš©í•˜ì—¬
    <ul>
      <li>the unique properties of time seriesë¥¼ ë°œê²¬í•˜ì—¬
        <ul>
          <li>multi-scalesì—ì„œ salient contextual cuesë¥¼ ë§Œë“¤ê³ </li>
        </ul>
      </li>
      <li>quadruple-Mamba architectureë¥¼ í•©ì³ì„œ
        <ul>
          <li>channel-mixing and channel-independenceë¥¼ í•œ ë²ˆì— í†µí•©</li>
        </ul>
      </li>
      <li>ì„œë¡œ ë‹¤ë¥¸ scalesì—ì„œì˜ global / local contextsë¥¼ effectiveí•˜ê²Œ selectioní•  ìˆ˜ ìˆê²Œ í•¨</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>LTSFì—ì„œ Capturing long-term dependenciesê°€ í•µì‹¬</li>
  <li><strong>Linear model</strong> : DLinear, TiDE
    <ul>
      <li>may not well capture long-range correlations</li>
    </ul>
  </li>
  <li><strong>Transformer-based</strong> : iTransformer, PatchTST, Crossformer
    <ul>
      <li>suffer from the quadratic complexity</li>
    </ul>
  </li>
  <li><strong>state-space models (SSMs)</strong>
    <ul>
      <li>inferring over very long sequences</li>
      <li>context-aware selectivity</li>
      <li>LTSFì—ì„œë„ í™œìš©ë  ìˆ˜ ìˆëŠ”ê°€
        <ul>
          <li>highly content- and context-selective SSMì´ ìµœê·¼ì— ë§ì´ ë‚˜ì˜¤ê³  ìˆê³ </li>
          <li>effectively representing the context in time seriesì— ì“¸ ìˆ˜ ìˆì„ ê²ƒ</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Transforemr-based approachì—ì„œëŠ” each observationì´ë‚˜ sub-series, ì•„ë‹ˆë©´ time seriesë¥¼ token(patch)ë¡œ ë§Œë“œëŠ”ë°
    <ul>
      <li>SSMì—ì„œ ì´ëŸ¬í•œ ì ‘ê·¼ì„ ê·¸ëŒ€ë¡œ ì“°ë©´ ì„±ëŠ¥ì´ ì•ˆë‚˜ì˜´</li>
      <li><strong>ê·¸ë˜ì„œ salient contextual cues tailored to SSMì„ extractí•˜ëŠ” ê²ƒì´ ë¨¼ì € !</strong></li>
    </ul>
  </li>
  <li>ê¸°ì¡´ì—ëŠ” channel-mixing wayë„ ìˆê³  (ex. Informer, FEDformer, and Autoformer, â€¦)
    <ul>
      <li>channel-independence wayë„ ìˆëŠ”ë°, (ex. PatchTST, TiDE, â€¦)</li>
      <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” unified architecture : applicable to both scenarios!</li>
    </ul>
  </li>
  <li>ê·¸ë¦¬ê³  Time seriesì—ëŠ” downsamplingí•´ë„ temporal relationsê°€ ìœ ì§€ëœë‹¤ëŠ” íŠ¹ì§•ì´ ìˆìœ¼ë‹ˆ
    <ul>
      <li>ëª¨ë“  time pointsë¥¼ tokenìœ¼ë¡œ ë§Œë“œëŠ” ê±´ redundantí•˜ê³ , PatchTSTì²˜ëŸ¼ patchë¥¼ ì‚¬ìš©í•˜ëŠ” ê±´ good</li>
      <li>í•˜ì§€ë§Œ pre-defined small patchëŠ” fixed resolutionì—ì„œì˜ contextë§Œ ì œê³µ</li>
      <li>ê·¸ëŸ¬ë‹ˆ iTransformerì²˜ëŸ¼ whole look-back windowë¥¼ tokenìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ ë‚«ê³ </li>
      <li>í•˜ì§€ë§Œ iTransformerì²˜ëŸ¼ channel-independenceì—ì„œëŠ” select sub-token contentsê°€ ì˜ ì•ˆë¨</li>
      <li>ê·¸ëŸ¬ë¯€ë¡œ, SSM ì“°ë©´ ë” ì˜ ë  ê²ƒ</li>
    </ul>
  </li>
  <li>ê·¸ëŸ¬ë‹ˆ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” TimeMachineì„ ì œì•ˆ
    <ul>
      <li>MTSë¥¼ 2ê°œì˜ scaleì—ì„œ context-aware predictioní•˜ê¸° ìœ„í•´ SSMì„ ì‚¬ìš©
        <ul>
          <li>high, low resolutionì´ë¼ëŠ” 2ê°œì˜ scaleë§ˆë‹¤ 2ê°œì˜ mambaë¥¼ ì‚¬ìš©.
            <ul>
              <li>í•˜ë‚˜ëŠ” global perspectives for the channel-mixing</li>
              <li>ë‹¤ë¥¸ í•˜ë‚˜ëŠ” both global and local perspectives for the channel-independence</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>ì´ë ‡ê²Œ 4ê°œì˜ SSM modulesë¥¼ ì‚¬ìš©í•´ì„œ channel-independent, -dependentë¥¼ í†µí•©í•˜ê³ 
        <ul>
          <li>ì¦‰ btw-channel correlationì´ â€œìˆìœ¼ë©´â€ ì¡ì•„ë‚´ê³  â€œì—†ìœ¼ë©´â€ independent ì²˜ëŸ¼</li>
          <li>ë‹¤ì–‘í•œ scalesì—ì„œ global and local contextual informationì„ íš¨ìœ¨ì ìœ¼ë¡œ selection</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li>Non-Transformer-based Supervised Approaches
    <ul>
      <li>Classical methods : ARIMA, VARMAX, GARCH, RNN, â€¦</li>
      <li>MLP-based models : DLinear, TiDE, RLinear, â€¦</li>
      <li>CNN-based : TimesNet, Scinet, â€¦</li>
    </ul>
  </li>
  <li>Transformer-based Supervised Learning methods
    <ul>
      <li>iTransformer, PatchTST, Crossformer, FEDformer, stationary, Flowformer, and Autoformer</li>
      <li>time seriesë¥¼ token seriesë¡œ ë§Œë“¤ê³  self-attention</li>
      <li>í•˜ì§€ë§Œ quadratic time and memory complexity</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-method">3. Proposed Method</h2>

<ul>
  <li>input sequence \(\mathbf{x}=\left[x_1, \ldots, x_L\right]\)
    <ul>
      <li>\(x_t \in \mathcal{R}^M\) representing a vector of \(M\) channels at time point \(t\)</li>
    </ul>
  </li>
  <li><strong>Normalization</strong>
    <ul>
      <li>the original MTS \(\mathbf{x}\) into \(\mathbf{x}^0=\left[\mathbf{x}_1^{(0)}, \cdots, \mathbf{x}_L^{(0)}\right] \in \mathcal{R}^{M \times L}\), via \(\mathbf{x}^{(0)}=\operatorname{Normalize}(\mathbf{x})\).</li>
      <li>Here, Normalize \((\cdot)\) represents a normalization operation RevIN</li>
    </ul>
  </li>
  <li><strong>Channel Mixing vs. Channel Independence</strong>
    <ul>
      <li>PatchTSTì—ì„œëŠ” Channel Independenceê°€ ì¢‹ë‹¤ê³  í•˜ì§€ë§Œ
        <ul>
          <li>ê·¸ê±´ lengthì— ë¹„í•´ channelsê°€ ë§ì§€ ì•Šì„ ë•Œ ì´ì•¼ê¸°ê³ ,</li>
          <li>channelsê°€ ë§ì„ ë•Œì—ëŠ” Channel Mixingì´ ë” ë‚«ë‹¤</li>
        </ul>
      </li>
      <li>TimeMachineì€ â€œpotentiallyâ€ inter-channel correlationì„ ì¡ê³ 
        <ul>
          <li>Channel Independenceì¼ ë•Œì—ëŠ” independenceë¥¼ ì°¾ìŒ</li>
        </ul>
      </li>
      <li>inputì˜ shapeì€ BML, outputì€ BMT</li>
    </ul>
  </li>
  <li><strong>Embedded Representations</strong>
    <ul>
      <li>2-stage embedded representation</li>
      <li>\(\mathbf{x}^{(1)}=E_1\left(\mathbf{x}^{(0)}\right), \quad \mathbf{x}^{(2)}=E_2\left(D O\left(\mathbf{x}^{(1)}\right)\right)\), where
        <ul>
          <li>\(E_1: \mathbb{R}^{M \times L} \rightarrow \mathbb{R}^{M \times n_1}\) and \(E_2: \mathbb{R}^{M \times n_1} \rightarrow \mathbb{R}^{M \times n_2}\)ì€ MLP</li>
          <li>DOëŠ” dropout, (MLP ì“°ë‹ˆê¹Œ overfitting ë°©ì§€)</li>
        </ul>
      </li>
      <li>ì´ë ‡ê²Œ input lengthì— ìƒê´€ì—†ì´ fixed-length tokensë¡œ embedding</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timemachine/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>
    <p><strong>Integrated Quadruple Mambas</strong> (fig1 ë³´ë©´ì„œ ì´í•´í•˜ë©´ ì¢‹ìŒ)</p>

    <ul>
      <li>
        <p>\(E_1, E_2\) ê°ê°ì˜ embedding levelì—ì„œ 2ê°œì˜ mambaë¥¼ ì‚¬ìš©</p>

        <ul>
          <li>\(E_1\) levelì—ì„œ ì‚¬ìš©ë˜ëŠ” 2ê°œì˜ mambaì˜ inputì€ \(D O\left(\mathbf{x}^{(1)}\right)\)</li>
          <li>\(E_2\) levelì—ì„œ ì‚¬ìš©ë˜ëŠ” 2ê°œì˜ mambaì˜ inputì€ \(D O\left(\mathbf{x}^{(2)}\right)\)</li>
        </ul>
      </li>
      <li>
        <p>ì²« ë²ˆì§¸ mamba block ì•ˆì—ì„œëŠ” 2ê°œì˜ FC-layersê°€ linear projectioní•˜ê³ </p>

        <ul>
          <li>í•˜ë‚˜ë§Œ 1d causal convì™€ SiLU activation í†µê³¼, ê·¸ë¦¬ê³  structured SSMìœ¼ë¡œ ê°„ë‹¤</li>
          <li>ê·¸ ë‹¤ìŒ ë‚¨ì€ í•˜ë‚˜ì˜ linear projectionì„ ë”í•˜ê³  FC-layerë¥¼ í•œ ë²ˆ ë” íƒœì›€</li>
          <li>ì´ë•Œ <strong>continuous-time SSM</strong>ì€ input sequence \(u(t)\)ë¥¼ latente state \(h(t)\)ë¥¼ í†µí•´ output \(v(t)\)ë¡œ ë³´ë‚¸ë‹¤.
            <ul>
              <li>ì¦‰ \(d h(t) / d t=A h(t)+B u(t), \quad v(t)=C h(t)\)
                <ul>
                  <li>\(h(t)\) is \(N\)-dimensional (\(N\)ì€ state expansion factor)</li>
                  <li>\(u(t)\) is \(D\)-dimensional (\(D\)ëŠ” dimension factor)</li>
                  <li>\(v(t)\)ì˜ dimensionë„ \(D\)</li>
                  <li>\(A\), \(B\), \(C\)ëŠ” coefficient matrices of proper size</li>
                </ul>
              </li>
              <li><strong>ì—¬ê¸°ì„œ \(A\), \(B\), \(C\), ê·¸ë¦¬ê³  hidden stateë¥¼ time interval \(\Delta\)ì— ëŒ€í•œ í•¨ìˆ˜ë¡œ ë†“ìŒ</strong></li>
              <li>ì´ê²ƒì´ ëª¨ë¸ì„ inputì— adaptiveí•˜ê²Œ context selectivityë¥¼ ê°•í™”í•˜ëŠ” ë°©ë²•
                <ul>
                  <li>ì¦‰ \(h_k=\bar{A} h_{k-1}+\bar{B} u_k, \quad v_k=C h_k\)</li>
                  <li>where \(h_k, u_k\), and \(v_k\) are respectively samples of \(h(t), u(t)\), and \(v(t)\) at time \(k \Delta\),</li>
                  <li>\(\bar{A}=\exp (\Delta A), \quad \bar{B}=(\Delta A)^{-1}(\exp (\Delta A)-I) \Delta B\).</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>(continuous ë§ê³ ) <strong>SSM</strong>ì€ \(B\), \(C\), \(\Delta\)ê°€ inputì— ë”°ë¼ ë‹¬ë¼ì§
            <ul>
              <li>\(B, C \leftarrow \operatorname{Linear}_N(u)\), \(\Delta \leftarrow \text{softplus}(parameter +Linear _D\left(\right. Linear \left.\left._1(u)\right)\right)\)</li>
              <li>coefficient matricesëŠ” current tokenì„ ë³´ê³  ì •ë³´ë¥¼ selectively propagateí•˜ê²Œ í•¨</li>
              <li><strong>channel-mixing case</strong>ì—ì„œëŠ” ê° univariateê°€ token(dim=\(n_2\))ì´ ë˜ê³ 
                <ul>
                  <li><strong>Inner mambas</strong>ì—ì„œëŠ” \(BMn_2\)ì´ ë‚˜ì˜¤ëŠ”ë°</li>
                  <li>Left / right inner mambaì˜ kë²ˆì§¸ ë³€ìˆ˜ì˜ outputì€ \(v_{L, k}, v_{R, k} \in \mathcal{R}^{n_2}\)
                    <ul>
                      <li>ë‘˜ì„ ë”í•˜ê³  embeddingëœ \(\mathbf{x}^{(2)}\)ì„ skip connectioní•˜ë©´ \(\mathbf{x}^{(3)}=\mathbf{v}_L \bigoplus \mathbf{v}_R \bigoplus \mathbf{x}^{(2)}\) (Element-wise addition)</li>
                      <li>ê·¸ ë‹¤ìŒ linear mapping \(P_1: \mathbf{x}^{(3)} \rightarrow\mathbf{x}^{(4)} \in \mathcal{R}^{M \times n_1}\)</li>
                    </ul>
                  </li>
                  <li><strong>Outer mambas</strong>ì—ì„œë„ ë¹„ìŠ·í•˜ê²Œ
                    <ul>
                      <li>\(v_{L, k}^*, v_{R, k}^* \in \mathcal{R}^{n_1}\)êµ¬í•˜ê³  \(\mathbf{x}^{(5)} \in \mathcal{R}^{M \times n_1}\)ë‘ í•´ì„œ ì…‹ì´ ë”í•¨</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li><strong>channel-independence</strong>ì—ì„œëŠ” ì²˜ìŒì— \(B M L \mapsto(B \times M) 1 L\) ì´ë ‡ê²Œ reshapeì„ í•´ì„œ ë§ˆì¹˜ matchê°€ \(BM\)ê°œì´ê³  univariatesì¸ ê²ƒì²˜ëŸ¼ ì²˜ë¦¬
                <ul>
                  <li>Outerë“  innerì´ë“ 
                    <ul>
                      <li>mamba í•˜ë‚˜ëŠ” input dim =1, token length =\(n_1\) or \(n_2\),</li>
                      <li>ë‹¤ë¥¸ í•˜ë‚˜ëŠ” input dim =\(n_1\) or \(n_2\), token length =1</li>
                    </ul>
                  </li>
                  <li>ì´ë ‡ê²Œ í•˜ë©´ global context and local context ë™ì‹œì— í•™ìŠµ ê°€ëŠ¥í•˜ê³ </li>
                  <li>fine and coarse scales with high- and low-resolution ê°ê°ì˜ context ì¶”ì¶œ</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Channel mixingì€ ë³€ìˆ˜ ê°œìˆ˜ê°€ ë§ì„ ë•Œ í•˜ê³  independenceë‘ switchí•˜ë ¤ë©´</p>

        <ul>
          <li>input sequenceë¥¼ ê·¸ëƒ¥ transposedí•˜ë©´ ë¨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Output Projection</p>

    <ul>
      <li>MLP ì“°ê³  \(P_1\) performs a mapping \(\mathcal{R}^{M \times n_2} \rightarrow \mathcal{R}^{M \times n_1}\), \(P_2\)ëŠ” \(\mathbb{R}^{M \times 2 n_1} \rightarrow \mathbb{R}^{M \times T}\)</li>
      <li>Residual connectionë„ fig1ì²˜ëŸ¼ í•´ì£¼ê³ </li>
      <li>Outer Mambasì—ì„œ ë‚˜ì˜¨ \(\mathbf{x}^{(5)}\)ë‘ Inner Mambasì—ì„œ ë‚˜ì˜¨ \(\mathbf{x}^{(4)}\)ë¥¼ concatí•´ì„œ ì‚¬ìš©í•˜ê²Œ ë¨
        <ul>
          <li>ì¦‰ \(\mathbf{x}^{(6)}=\mathbf{x}^{(5)} \|\left(\mathbf{x}^{(4)} \bigoplus \mathbf{x}^{(1)}\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-result-analysis">4. Result Analysis</h2>

<h3 id="41-datasets">4.1. Datasets</h3>

<ul>
  <li>seven benchmark datasets extensively used for LTSF:
    <ul>
      <li>Weather, Traffic, Electricity, and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2)</li>
    </ul>
  </li>
</ul>

<h3 id="42-experimental-environment">4.2. Experimental Environment</h3>

<ul>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” TimeMachineì„ 11 SOTA modelsì™€ ë¹„êµ :
    <ul>
      <li>including iTransformer, PatchTST, DLinear, RLinear, Autoformer, Crossformer, TiDE, Scinet, TimesNet, FEDformer, and Stationary</li>
    </ul>
  </li>
</ul>

<h3 id="43-quantitative-results">4.3. Quantitative Results</h3>

<p><img src="/assets/img/timeseries/timemachine/fig2.png" alt="ê·¸ë¦¼1" /></p>

<p><img src="/assets/img/timeseries/timemachine/fig3.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="44-qualitative-result">4.4. Qualitative Result</h3>

<p><img src="/assets/img/timeseries/timemachine/table2.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="5--hyperparameter-sensitivity-analysis-and-ablation-study">5.  Hyperparameter Sensitivity Analysis and Ablation Study</h2>

<h3 id="51-effect-of-mlps-parameters-n1-n2">5.1. Effect of MLPsâ€™ Parameters (n1, n2)</h3>

<ul>
  <li>MLPì˜ sizeì¸ \(n_1, n_2\)ë¥¼ ë‹¤ì–‘í•˜ê²Œ í•´ë´¤ëŠ”ë° ë³„ ì°¨ì´ ì—†ìŒ (fig5)
    <ul>
      <li>MLPì— heavily dependentí•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timemachine/fig5.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="52-sensitivity-of-dropouts">5.2. Sensitivity of Dropouts</h3>

<ul>
  <li>Dropout ratio ì ë‹¹í•˜ê²Œ 0.7 ì‚¬ìš©</li>
</ul>

<h3 id="53-ablation-of-residual-connections">5.3. Ablation of Residual Connections</h3>

<ul>
  <li>Residual connections ì“°ëŠ” ê²ƒì´ ì¢‹ë”ë¼</li>
</ul>

<h3 id="54-effects-of-mambas-local-convolutional-width">5.4. Effects of Mambasâ€™ Local Convolutional Width</h3>

<ul>
  <li>ê° Mamba ì•ˆì—ë„ parametersê°€ ìˆì„ê±°ë‹ˆê¹Œ local convolutional kernel widthsë¥¼ 2 and 4ë¡œì‹¤í—˜ í•´ë´¤ë”ë‹ˆ 2ê°€ ë‚«ë”ë¼</li>
</ul>

<h3 id="55-ablation-on-state-expansion-factor-of-mambas">5.5. Ablation on State Expansion Factor of Mambas</h3>

<p><img src="/assets/img/timeseries/timemachine/fig6.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>State Expansion Factorë¥¼ 8ë¶€í„° 256ê¹Œì§€ í•´ë´¤ëŠ”ë° 256ì´ ì œì¼ ì¢‹ì•„ì„œ defualtë¡œ ì„¤ì •</li>
</ul>

<h3 id="56-ablation-on-mamba-dimension-expansion-factor">5.6. Ablation on Mamba Dimension Expansion Factor</h3>

<ul>
  <li>dimension expansion factor (\(E\))ë„ ìˆì—ˆëŠ”ë°, í¬ê²Œí•˜ë©´ ë©”ëª¨ë¦¬ëŠ” ë§ì´ ë¨¹ëŠ”ë° ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì´ì–´ì§€ì§€ëŠ” ì•Šì•„ì„œ ê·¸ëƒ¥ 1ë¡œ ë‘”ë‹¤</li>
</ul>

<h2 id="6-strengths-and-limitations">6. Strengths and Limitations</h2>

<ul>
  <li>memory efficiency and stable performance across varying look-back and prediction lengths !</li>
  <li>Weatherì—ì„œ 1ë“± ëª»í•œê²Œ limitation. (â€¦.?ã…‹ã…‹)</li>
</ul>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>LTSF with linear scalability and small memory footprints !</li>
  <li>integrated quadruple-Mamba architecture
    <ul>
      <li>to predict with rich global and local contextual cues at multiple scales</li>
      <li>\(\to\) unifies channel-mixing and channel-independence situations</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ECAI 2024](https://arxiv.org/abs/2403.09898)]]></summary></entry><entry><title type="html">Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Arxiv 2023)</title><link href="http://localhost:4000/timeseries/2024-10-28-mamba/" rel="alternate" type="text/html" title="Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Arxiv 2023)" /><published>2024-10-28T00:00:00+09:00</published><updated>2024-10-28T03:20:20+09:00</updated><id>http://localhost:4000/timeseries/mamba</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-28-mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>ë§ì€ subquadratic-time architectures (linear attention, gated convolution and recurrent models, and structured state space models (SSMs))ê°€ Transformerì˜ ì—°ì‚° íš¨ìœ¨ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆë˜ì—ˆì§€ë§Œ
    <ul>
      <li>content-based reasoningì—ì„œëŠ” ì—¬ì „íˆ ì•½í•œ ëª¨ìŠµ</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” MambaëŠ”
    <ul>
      <li><strong>SSM parametersë¥¼ inputì˜ í•¨ìˆ˜ í˜•íƒœ</strong>ë¡œ ë†“ì•„ì„œ ëª¨ë¸ì´ selectively propagate or forget information í•  ìˆ˜ ìˆë„ë¡ í•¨</li>
      <li>ê·¸ë¦¬ê³  recurrent ëª¨ë“œë¡œ í•™ìŠµì„ ì§„í–‰í•˜ê²Œ ë˜ë©´ ì¤‘ê°„ Hidden State í¬ê¸°ê°€ ë§¤ìš° ì»¤ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—
        <ul>
          <li><strong>hardware-aware parallel algorithm</strong>ì„ ì‚¬ìš©í•˜ì—¬ hidden Stateë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì§€ ì•Šê³  ë³‘ë ¬ì ìœ¼ë¡œ scan ì—°ì‚°í•¨</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>Selection Mechanism</strong>
    <ul>
      <li>parameterizing the SSM parameters based on the input
        <ul>
          <li>\(\to\) í•„ìš”í•œ ì •ë³´ë§Œ ê¸°ì–µí•˜ê³  í•„ìš”ì—†ëŠ” ì •ë³´ filter out</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Hardware-aware Algorithm</strong>
    <ul>
      <li>ì—°ì‚° ì»¤ë„ì„ ê²°í•©í•˜ëŠ” ë°©ì‹(ì»¤ë„ ìœµí•©)ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì…ì¶œë ¥ ê³¼ì •ì„ ìµœì í™”í•˜ê³  ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì„</li>
      <li>ê³ ì† ë©”ëª¨ë¦¬(SRAM)ë¥¼ í™œìš©í•´ ëŠë¦° GPU ë©”ëª¨ë¦¬(HBM) ì˜ì¡´ë„ë¥¼ ì¤„ì—¬ ì—°ì‚° ì†ë„ë¥¼ ë†’ì´ê² ë‹¤ëŠ” ê²ƒ</li>
      <li>backpropagation í•  ë•Œì—ëŠ” hidden stateë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  í•„ìš”í•  ë•Œë§ˆë‹¤ ì¬ê³„ì‚°í•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/mamba/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li><strong>Architecture</strong>
    <ul>
      <li>ê¸°ì¡´ SSM architecturesì™€ Transformerì˜ MLP blocksì„ í•©ì³ì„œ Mambaë¥¼ ë§Œë“¬</li>
    </ul>
  </li>
</ul>

<h2 id="2-state-space-models">2. State Space Models</h2>

<ul>
  <li>Structured state space sequence models (S4)
    <ul>
      <li>inspired by a particular continuous system :
        <ul>
          <li>1-dimensional function or sequence $x(t) \in \mathbb{R} \mapsto y(t) \in \mathbb{R}$ through an implicit latent state $h(t) \in \mathbb{R}^N$.</li>
          <li>\(\begin{aligned} h^{\prime}(t) &amp; =A h(t)+B x(t) \\ y(t) &amp; =C h(t)\end{aligned}\) (1)</li>
          <li>4ê°œì˜ parameters \((\Delta, A, B, C)\)ë¡œ ì •ì˜ë¨ (ì•„ì§ inputì˜ í•¨ìˆ˜ í˜•íƒœê°€ ì•„ë‹˜)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Discretization</strong>
    <ul>
      <li>ì²«ë²ˆì§¸ ë‹¨ê³„ëŠ” â€œcontinuous parametersâ€ \((\Delta, A, B)\)ë¥¼ â€œdiscrete parametersâ€ \((\bar{A}, \bar{B})\)ë¡œ ë°”ê¾¸ëŠ” ê²ƒ
        <ul>
          <li>fixed formulas \(\overline{A}=f_A(\Delta, A)\) and \(\overline{B}=f_B(\Delta, A, B)\)ë¥¼ ì‚¬ìš©</li>
          <li>\(\left(f_A, f_B\right)\)ëŠ” discretization rule</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Computation</strong>
    <ul>
      <li>\((\Delta, A, B, C) \mapsto(\bar{A}, \bar{B}, C)\) ë³€í™˜ì´ ëë‚¬ìœ¼ë©´ ê·¸ ë‹¤ìŒì—ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ í˜•íƒœì˜ computation ê°€ëŠ¥
        <ul>
          <li>a linear recurrence :
            <ul>
              <li>\(\begin{aligned} h_t &amp; =\overline{A} h_{t-1}+\overline{B} x_t \\ y_t &amp; =C h_t\end{aligned}\) (2) ë˜ëŠ”</li>
            </ul>
          </li>
          <li>a global convolution :
            <ul>
              <li>\(\begin{aligned} \bar{K} &amp; =\left(C \bar{B}, C \overline{A B}, \ldots, C \bar{A}^k \bar{B}, \ldots\right) \\ y &amp; =x * \bar{K}\end{aligned}\) (3)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Linear Time Invariance (LTI)</strong>
    <ul>
      <li>ìœ„ (1) (2) (3) ëª¨ë¸ë“¤ì€ modelâ€™s dynamicsê°€ time-invariant</li>
      <li>í•˜ì§€ë§Œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ LTI propertyê°€ ê·¼ë³¸ì ì¸ í•œê³„ê°€ ìˆìŒì„ ë°íˆê³ 
        <ul>
          <li>LTIë¥¼ ì œê±°í•˜ë©´ì„œë„ efficiency bottlenecksë¥¼ ê·¹ë³µí•¨ì„ ì œì‹œí•¨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Structure and Dimensions</strong>
    <ul>
      <li>\(A\) matrixë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— SSMì¸ê±°ê³ , ì´ë•Œ \(A \in \mathbb{R}^{N \times N}, B \in \mathbb{R}^{N \times 1}, C \in \mathbb{R}^{1 \times N}\)</li>
      <li>total hidden state has dimensionì€: \(ğ·ğ‘\) per input</li>
      <li>the sequence length requires \(ğ‘‚(ğµğ¿ğ·ğ‘)\)</li>
    </ul>
  </li>
</ul>

<h2 id="3-selective-state-space-models">3. Selective State Space Models</h2>

<ul>
  <li>3.1ì ˆì—ì„œëŠ” selection mechanismì„ ì†Œê°œí•˜ê³ </li>
  <li>3.2ì ˆì—ì„œëŠ” ì–´ë–»ê²Œ selection mechanismì´ SSMê³¼ ê°™ì´ ì“°ì¼ ìˆ˜ ìˆëŠ”ì§€ ë³´ê³ </li>
  <li>3.3ì ˆì—ì„œëŠ” hardware-aware algorithmì„ ì•Œì•„ë³´ê³ </li>
  <li>3.4ì ˆì—ì„œëŠ” simple SSMì„ attentionì´ë‚˜ MLPì—†ì´ ì•Œì•„ë³´ê³ </li>
  <li>3.5ì ˆì—ì„œëŠ” additional properties of selection mechanismsë¥¼ ë…¼ì˜í•œë‹¤</li>
</ul>

<h3 id="31-motivation-selection-as-a-means-of-compression">3.1. Motivation: Selection as a Means of Compression</h3>

<ul>
  <li>sequence modelingì˜ ë³¸ì§ˆì ì¸ ë¬¸ì œëŠ” <strong>small stateì— contextë¥¼ ì••ì¶•</strong>í•˜ëŠ” ê²ƒ
    <ul>
      <li>Trade off : ì••ì¶•ì„ ì•ˆí•˜ë©´ inefficientí•˜ê³ (Transformer) efficientí•˜ë©´ ì••ì¶•ì„ ë„ˆë¬´ ë§ì´ í•˜ê³ (RNN)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/mamba/fig2.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>Synthetic task 2ê°€ì§€
    <ul>
      <li><strong>Selective Copying</strong>
        <ul>
          <li>í•„ìš”í•œ tokensì™€ ì•„ë‹Œ ê²ƒë“¤ì„ êµ¬ë³„í•˜ê¸° ìœ„í•´ content-aware reasoningí•˜ëŠ” task</li>
        </ul>
      </li>
      <li><strong>Induction Heads</strong>
        <ul>
          <li>ë‹¤ìŒì— ë­ê°€ ì˜¬ì§€ ì¶”ë¡ í•˜ê¸° ìœ„í•´ context-aware reasoningí•˜ëŠ” task</li>
        </ul>
      </li>
      <li>ì´ ë‘ ê°€ì§€ëŠ” ìœ„ì—ì„œ ì†Œê°œí•œ LTI modeë¡œëŠ” í•˜ê¸° ì–´ë µë‹¤</li>
    </ul>
  </li>
  <li>ê²°êµ­ì—ëŠ” efficientí•˜ë ¤ë©´ small stateë¥¼ ê°€ì ¸ì•¼ í•˜ëŠ”ë°, ê·¸ê±¸ â€œì˜â€ í•˜ë ¤ë©´ selectivityë¥¼ â€œì˜â€ í•´ì•¼ í•¨</li>
</ul>

<h3 id="32--improving-ssms-with-selection">3.2.  Improving SSMs with Selection</h3>

<ul>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” selection mechanismì€ modelì˜ parametersë¥¼ input-dependentí•˜ê²Œ ë§Œë“œëŠ” ê²ƒ
    <ul>
      <li>\(\Delta, B, C\)ì„ length dimension \(L\)ë¡œ ë§Œë“¬ (ì¦‰ time-invariantì—ì„œ time-varyingìœ¼ë¡œ)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/mamba/algorithm12.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="33-efficient-implementation-of-selective-ssms">3.3 Efficient Implementation of Selective SSMs</h3>

<ul>
  <li>Convolutionì´ë‚˜ attentionì²˜ëŸ¼ GPU-friendlyí•˜ê²Œ Selective SSMì„ ë§Œë“¤ê³  ì‹¶ì€ ê±°ê³ </li>
  <li>ì¦‰ ì‹œê°„ì— ë”°ë¼ í•„ìš”í•œ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê² ë‹¤ëŠ” ê²ƒ. ê·¸ëŸ¬ë©´ ë” ë¹ ë¥´ê²Œ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬</li>
</ul>

<p><strong>3.3.1 Motivation of Prior Models</strong></p>

<ul>
  <li>paying speed and memory costs ì—†ì´ maximize hidden state dimensioní•˜ê³  ì‹¶ìŒ</li>
  <li>Recurrent modeëŠ” hiddenì´ inputë³´ë‹¤ í›¨ì”¬ ì»¤ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ìŒ
    <ul>
      <li>ê·¸ë˜ì„œ inputì˜ shape (=outputì˜ shape)ê³¼ ê°™ì€ convë¥¼ ì“°ê² ë‹¤</li>
    </ul>
  </li>
  <li>ê¸°ì¡´ LTIëŠ” ë°ì´í„° íŠ¹ì„±ì„ ì˜ ë°˜ì˜ ëª»í–ˆì§€ë§Œ MambaëŠ” ìˆœí™˜ì  ìš”ì†Œì™€ ì»¨ë³¼ë£¨ì…˜ì  ìš”ì†Œë¥¼ ë™ì‹œì— ì‚¬ìš©í•´ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™” !</li>
</ul>

<p><strong>3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion</strong></p>

<ul>
  <li>LTIì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” selection mechanismì„ ì†Œê°œ:</li>
  <li>ë¬¸ì œëŠ” (1) the sequential nature of recurrence, and (2) the large memory usage
    <ul>
      <li>(2) the large memory usageëŠ” kernel fusionìœ¼ë¡œ í•´ê²°
        <ul>
          <li>scan input \((\bar{A}, \bar{B})\) of size \((B, L, D, N)\)ì„ HBMì— ì €ì¥í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼</li>
          <li>the SSM parameters \((\triangle, A, B, C)\)ì˜ final output \((B, L, D)\)ë§Œ ì €ì¥</li>
          <li>discretizationì´ë‘ recurrenceëŠ” SRAMì—ì„œ ìˆ˜í–‰</li>
        </ul>
      </li>
      <li>(1) the sequential nature of recurrenceëŠ” recomputationìœ¼ë¡œ í•´ê²°
        <ul>
          <li>intermediate statesë¥¼ ì €ì¥í•˜ì§€ ì•ŠëŠ”ë° ì´ê±´ backpropagationì—ì„œ í•„ìš”í•˜ë‹ˆê¹Œ</li>
          <li>ê·¸ëƒ¥ ë‹¤ì‹œ ê³„ì‚°í•¨ (recomputation)</li>
          <li>ê·¸ ê²°ê³¼ FlashAttentionê³¼ ìœ ì‚¬í•œ memory efficiency</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="34-a-simplified-ssm-architecture">3.4 A Simplified SSM Architecture</h3>

<ul>
  <li>MambaëŠ” linear attentionê³¼ MLPë¥¼ ê²°í•©í•´ì„œ gated attention unit(GAP)ì²˜ëŸ¼ ë§Œë“¬</li>
  <li>model dimensionì„ Dì—ì„œ expansion factor Eë¥¼ ì‚¬ìš©í•´ì„œ ëŠ˜ë ¤ì¤Œ
    <ul>
      <li>ëŒ€ë¶€ë¶„ì˜ model parameters \(3ED^2\)ê°œ \(2ED^2\)ê°œê°€ input projectionì—, \(ED^2\)ê°œê°€ output projectionì— ìˆìŒ</li>
      <li>ë°˜ë©´ SSM ì•ˆì—ëŠ” parametersê°€ ë³„ë¡œ ì—†ëŠ”ë°, MambaëŠ” ì´ê±¸ ë°˜ë³µí•´ì„œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ì´ë‹¤</li>
    </ul>
  </li>
</ul>

<h3 id="35-properties-of-selection-mechanisms">3.5 Properties of Selection Mechanisms</h3>

<ul>
  <li>The selection mechanismì€ RNNì´ë‚˜ CNNì— ì“¸ ìˆ˜ ìˆëŠ” broader conceptì„</li>
</ul>

<p><strong>3.5.1 Connection to Gating Mechanisms</strong></p>

<ul>
  <li>SSMì˜ ê²Œì´íŠ¸ ì—­í• ì„ í•˜ëŠ” \(\Delta\)ê°€ RNNì˜ ê²Œì´íŠ¸ì™€ ìœ ì‚¬í•˜ê²Œ ì‘ë™
    <ul>
      <li>ì…ë ¥ëœ ì •ë³´ ì¤‘ ì–´ë–¤ ê²ƒì„ ìœ ì§€í•˜ê³  ì–´ë–¤ ê²ƒì„ ë²„ë¦´ì§€ ê²°ì •í•˜ëŠ” ì—­í• ì¸ ì ë„ ë¹„ìŠ·</li>
      <li>When $N=1, A=-1, B=1, s_{\Delta}=\operatorname{Linear}(x)$, and $\tau_{\Delta}=$ softplus,</li>
      <li>\(\begin{aligned} &amp; g_t=\sigma\left(\operatorname{Linear}\left(x_t\right)\right) \\ &amp; h_t=\left(1-g_t\right) h_{t-1}+g_t x_t\end{aligned}\) .</li>
      <li>ì´ëŸ° ì‹ìœ¼ë¡œ \(g_t\)ê°€ í˜„ì¬ ì…ë ¥ \(x_t\)ê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ í‘œí˜„í•˜ê²Œ í•˜ê³ 
        <ul>
          <li>\(g_t\)ê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ \(x_t\)ë¥¼ ë§ì´ ë°˜ì˜, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì´ì „ state \(h_{t-1}\)ì„ ë§ì´ ë°˜ì˜</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>3.5.2 Interpretation of Selection Mechanisms</strong></p>

<ul>
  <li><strong>Variable Spacing</strong>
    <ul>
      <li>Selectivityì˜ ì—­í• ì€ filtering out irrelevant noise tokens that may occur between inputs of interest</li>
    </ul>
  </li>
  <li><strong>Filtering Context</strong>
    <ul>
      <li>contextê°€ ê¸¸ì–´ì§„ë‹¤ê³  ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒì´ ì•„ë‹˜. ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ ë„ˆë¬´ ê¸´ sequenceì—ì„œ ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ì œê±°í•˜ì§€ ëª»í•´ì„œ ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒ</li>
      <li>selective modelì€ stateë¥¼ ì–¸ì œë“  ì´ˆê¸°í™” í•  ìˆ˜ ìˆìœ¼ë‹ˆ ê¸´ sequenceê°€ ë“¤ì–´ì™”ì„ ë•Œ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§€ë„ë¡ ì‘ë™</li>
    </ul>
  </li>
  <li><strong>Boundary Resetting</strong>
    <ul>
      <li>LTIëŠ” sequenceì˜ ê²½ê³„ì—ì„œ ì •ë³´ê°€ ì„ì´ëŠ” ë¬¸ì œê°€ ìˆì—ˆëŠ”ë°, selective SSMì€ ê·¸ëŸ° ë¬¸ì œ ì—†ìŒ
        <ul>
          <li>ì–¸ì œë“ ì§€ stateë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìœ¼ë‹ˆ ê·¸ëƒ¥ boundariesì—ì„œ ì´ˆê¸°í™” í•˜ë©´ ë¨ (\(g_t=1\))</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Interpretation of \(\Delta\)</strong>
    <ul>
      <li>\(\Delta\)ê°€ í¬ë©´ ë‹¤ ìŠê³  í˜„ì¬ ì •ë³´ë¥¼ ìœ„ì£¼ë¡œ stateë¥¼ ë§Œë“œëŠ”ê±°ê³ , ì‘ìœ¼ë©´ ì´ì „ stateë¥¼ ìœ ì§€</li>
    </ul>
  </li>
  <li><strong>Interpretation of A</strong>
    <ul>
      <li>ì‚¬ì‹¤ \(\bar{A}=\exp (\Delta A)\)ë„ \(\Delta\)ë¥¼ í†µí•´ ë§Œë“¤ì–´ì§€ë‹ˆ í¬ê²Œ ê±´ë“œë¦¬ì§€ ë§ê³  ë‹¨ìˆœí•˜ê²Œ ë‘”ë‹¤</li>
    </ul>
  </li>
  <li><strong>Interpretation of ğ‘© and ğ‘ª.</strong>
    <ul>
      <li>ê²°êµ­ Selectivityì˜ ì—­í• ì€ filtering out.</li>
      <li>Bì™€ CëŠ” ì…ë ¥ì„ ìƒíƒœë¡œ ì „ë‹¬í• ì§€, ìƒíƒœë¥¼ ì¶œë ¥ìœ¼ë¡œ ë‚´ë³´ë‚¼ì§€ë¥¼ ê²°ì •</li>
      <li>ëª¨ë¸ì´ state(context)ë¥¼ ë” ì„¸ë°€í•˜ê²Œ ì œì–´í•  ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<h3 id="36-additional-model-details">3.6 Additional Model Details</h3>

<p>pass</p>

<h2 id="4-empirical-evaluation">4. Empirical Evaluation</h2>

<h3 id="41-synthetic-tasks">4.1 Synthetic Tasks</h3>

<p><strong>4.1.1 Selective Copying</strong></p>

<p><strong>4.1.2 Induction Heads</strong></p>

<p><img src="/assets/img/timeseries/mamba/table12.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="42-language-modeling">4.2. Language Modeling</h3>

<p><img src="/assets/img/timeseries/mamba/table3.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="45-speed-and-memory-benchmarks">4.5 Speed and Memory Benchmarks</h3>

<p><img src="/assets/img/timeseries/mamba/fig8.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="46-model-ablations">4.6. Model Ablations</h3>

<p><img src="/assets/img/timeseries/mamba/table6.png" alt="ê·¸ë¦¼1" /></p>

<p><img src="/assets/img/timeseries/mamba/table78.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="5-discussion">5. Discussion</h2>

<p>Pass</p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>a selection mechanism to structured state space models
    <ul>
      <li>to perform context-dependent reasoning</li>
      <li>Without attention ! (simple attention-free architecture)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2312.00752)]]></summary></entry><entry><title type="html">T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)</title><link href="http://localhost:4000/timeseries/2024-10-27-T-PATCHGNN/" rel="alternate" type="text/html" title="T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)" /><published>2024-10-27T00:00:00+09:00</published><updated>2024-10-27T11:03:31+09:00</updated><id>http://localhost:4000/timeseries/T-PATCHGNN</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-27-T-PATCHGNN/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>transforms each univariate irregular time series into a series of transformable patches</li>
      <li>local semantics captureì™€, inter-time series correlation modelingëŠ” í•˜ë©´ì„œ</li>
      <li>avoiding sequence <strong>length explosion</strong> in aligned IMTS (ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€ 1. introduction (3)ì—ì„œ ì„¤ëª…)</li>
    </ul>
  </li>
  <li>Time-adaptive graph neural networksìœ¼ë¡œ time-varying adaptive graphsë¥¼ í•™ìŠµí•´ì„œ
    <ul>
      <li>dynamic intertime series correlationë¥¼ í‘œí˜„</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Multivariate Time Series (IMTS)ì˜ íŠ¹ì§•ì€ irregular sampling intervals and missing data</li>
  <li>Irregularity within the series and asynchrony ë•Œë¬¸ì— ë‹¤ë£¨ê¸° ì–´ë ¤ì›€
    <ul>
      <li>ODEë¡œ í’€ë ¤ê³  í•œ ì ì€ ìˆì§€ë§Œ numerical integration processìœ¼ë¡œ ì¸í•´ computationally expensive</li>
    </ul>
  </li>
  <li>IMTS forecastingì˜ ì–´ë ¤ì›€ì—ëŠ” 3ê°€ì§€ ì´ìœ ê°€ ìˆìŒ</li>
  <li>ì²«ë²ˆì§¸ëŠ” (1) irregularity in intra-time series dependency modeling
    <ul>
      <li><strong>varying time intervals</strong> between adjacent observationsì´ the consistent flow of time series dataë¥¼ ë°©í•´</li>
    </ul>
  </li>
  <li>ë‘ë²ˆì§¸ëŠ” (2) asynchrony in intertime series correlation modeling
    <ul>
      <li><strong>misaligned at time</strong> due to irregular sampling or missing data.</li>
    </ul>
  </li>
  <li>ê°€ì¥ ì¤‘ìš”í•œ ê±´ (3) sequence length explosion with the increase of variables
    <ul>
      <li>ì•„ë˜ fig1ì²˜ëŸ¼ â€œë‹¨ í•˜ë‚˜ì˜ ë³€ìˆ˜ë¼ë„ ê¸°ë¡ëœ time stampâ€ëŠ” ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê±¸ë¡œ í•´ë²„ë¦¬ë©´, ë³€ìˆ˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚¨ì— ë‹¤ë¼ time stampsì˜ ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„ì§€ëŠ” ë¬¸ì œ. (ì´ëŸ¬í•œ ë°©ë²•ì„ canonical pre-alignment representationì´ë¼ê³  ë¶€ë¦„)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TPatchGNN/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” T-PATCHGNNì˜ ì¥ì ì€
    <ul>
      <li>ì²«ì§¸ë¡œ The independent patching process for each univariate irregular time seriesìœ¼ë¡œ representationì—ì„œ sequence length explosionì˜ riskë¥¼ ì—†ì• ê³ </li>
      <li>ë‘˜ì§¸ë¡œ local semanticsë¥¼ ì˜ ì¡ê¸° ìœ„í•´ putting each individual observation into patches with richer context</li>
      <li>ì…‹ì§¸ë¡œ transformable patching í›„ì— IMTS is naturally aligned in a consistent patch-level temporal resolution</li>
    </ul>
  </li>
  <li>ë³¸ ë…¼ë¬¸ì˜ contributionì€ :
    <ul>
      <li>New transformable patching method to transform each univariate irregular time series of IMTS into a series of variable-length yet time-aligned patches</li>
      <li>transformable patching outcomesì„ ë°”íƒ•ìœ¼ë¡œ,  time-adaptive graph neural networksë¥¼ ì œì•ˆ</li>
      <li>building a benchmark for IMTS forecasting evaluation</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2, Related Works</h2>

<h3 id="21-irregular-multivariate-time-series-forecasting">2.1. Irregular Multivariate Time Series Forecasting</h3>

<p>pass</p>

<h3 id="22-irregular-multivariate-time-series-representation">2.2. Irregular Multivariate Time Series Representation</h3>

<ul>
  <li>ê¸°ì¡´ì—ëŠ” time-aligned mannerë¡œ IMTSë¥¼ representation (pre-alignment representation method)
    <ul>
      <li>ì¦‰ í•˜ë‚˜ì˜ ë³€ìˆ˜ë¼ê³  ê¸°ë¡ëœ time stampëŠ” ì¡´ì¬í•˜ëŠ” ê±¸ë¡œ ìƒê°í•˜ë‹ˆ</li>
      <li>sequence length that equals the number of all unique time stamps in IMTS</li>
      <li>ì˜ˆë¥¼ ë“¤ì–´ ë³€ìˆ˜ 1ì€ 1,3,5 ì‹œì ì— ê¸°ë¡ë˜ê³  ë³€ìˆ˜ 2ëŠ” 2,4,6 ì‹œì ì— ê¸°ë¡ë˜ë©´ unique time stampsì˜ ê°œìˆ˜ëŠ” 6ì´ ë¨</li>
      <li>sequence length explosion problem ë°œìƒ</li>
    </ul>
  </li>
</ul>

<h3 id="23-graph-neural-networks-for-multivariate-time-series">2.3. Graph Neural Networks for Multivariate Time Series</h3>

<ul>
  <li>
    <p>2018ë…„ DCRNN, STGCNì€ pre-defined graph structuresë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤ì œë¡œ ì“°ê¸° ì–´ë ¤ì› ê³ </p>
  </li>
  <li>2019ë…„ë¶€í„° dataë¡œë¶€í„° graph structuresë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©
    <ul>
      <li>í•˜ì§€ë§Œ IMTSì—ì„œëŠ” ì˜ ì‘ë™ì„ ì•ˆ í•¨. mimisalignment at timesìœ¼ë¡œ ì¸í•´ inter-time series correlation modelingì´ ì˜ ì•ˆ ë¨</li>
    </ul>
  </li>
  <li>Raindrop(2021)[<a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">paper review</a>]
    <ul>
      <li>ì´ ë¬¸ì œë¥¼ propagation the asynchronous observations at all the timestampsë¡œ í•´ê²°í•˜ë ¤ê³  í–ˆì§€ë§Œ  sequence length explosion problemì„ í”¼í•  ìˆ˜ ì—†ìŒ</li>
    </ul>
  </li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<h3 id="31-problem-definition">3.1. Problem Definition</h3>

<h3 id="definition-1">Definition 1</h3>

<ul>
  <li>Irregular Multivariate Time Series
    <ul>
      <li>\(\mathcal{O}=\left\{\mathbf{o}_{1: L_n}^n\right\}_{n=1}^N=\left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\), where</li>
      <li>\(N\)ê°œì˜ ë³€ìˆ˜ê°€ ìˆê³  \(n\)ë²ˆì§¸ ë³€ìˆ˜ëŠ” \(L_n\)ê°œì˜ observationsê°€ ìˆê³ , \(n\)ë²ˆì§¸ ë³€ìˆ˜ì˜ \(i\)ë²ˆì§¸ ë³€ìˆ˜ì˜ ê°’ì€ \(t_i^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="definition-2">Definition 2</h3>

<ul>
  <li>Forecasting Query \(q_j^n\)
    <ul>
      <li>\(j\)-th query on \(n\)-th variable to predict its corresponding value at a future time \(q_j^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="problem-1">Problem 1</h3>

<ul>
  <li>Irregular Multivariate Time Series Forecasting
    <ul>
      <li>IMTS \(\mathcal{O} =  \left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\)ì™€ Forecasting query \(\mathcal{Q}=\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)ê°€ ìˆì„ ë•Œ,</li>
      <li>problemì€ accurately forecast recorded values \(\hat{\mathcal{X}}=\left\{\left[\hat{x}_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\) in correspondence to the forecasting queries</li>
      <li>\(\mathcal{F}(\mathcal{O}, \mathcal{Q}) \longrightarrow \hat{\mathcal{X}}\)ë¡œ í‘œí˜„ë¨</li>
    </ul>
  </li>
</ul>

<h3 id="32-canonical-pre-alignment-representation-for-imts">3.2. Canonical Pre-Alignment Representation for IMTS</h3>

<ul>
  <li>2.2. Irregular Multivariate Time Series Representation ì°¸ê³ </li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<p><img src="/assets/img/timeseries/TPatchGNN/fig2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="41-irregular-time-series-patching">4.1. Irregular Time Series Patching</h3>

<ul>
  <li>ëª¨ë“  univariate TSì— ê°™ì€ patching operationì„ í•˜ë‹ˆê¹Œ ë³€ìˆ˜ index í‘œê¸°ëŠ” ìƒëµ</li>
</ul>

<h3 id="411-transformable-patching">4.1.1. TRANSFORMABLE PATCHING</h3>

<ul>
  <li>Time series patchingì´ forecastingì— ì¢‹ì€ ë°©ë²•ì´ë¼ëŠ” ê±´ ì•Œë ¤ì§„ ì‚¬ì‹¤. benefits in :
    <ul>
      <li>capturing local semantic information,</li>
      <li>reducing computation and memory usage,</li>
      <li>modeling longer-range historical observations</li>
    </ul>
  </li>
  <li>ì¼ë°˜ì ìœ¼ë¡œ time series patchingì€ í•˜ë‚˜ì˜ patchì— ê°™ì€ ìˆ«ìì˜ observationsê°€ ìˆëŠ”ë°,
    <ul>
      <li>IMTSì—ì„œ time intervalsëŠ” ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë°©ì‹ì´ ì ì ˆí•˜ì§€ ì•ŠìŒ</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ patchì— ê°™ì€ ê°œìˆ˜ì˜ observataionsê°€ ì•„ë‹ˆë¼, unified time horizonì´ ë“¤ì–´ê°€ë„ë¡ í•¨
    <ul>
      <li>patch ì•ˆì— ë“¤ì–´ê°€ëŠ” observationsì˜ ê°œìˆ˜ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ, ex) 2ì‹œê°„ì¸ ê±´ ë™ì¼í•˜ë„ë¡</li>
    </ul>
  </li>
  <li>patchëŠ” \(\left[\mathbf{o}_{l_p: r_p}\right]_{p=1}^P\)ë¡œ í‘œí˜„ë˜ê³  \(P\)</li>
</ul>

<h3 id="412-patch-encoding">4.1.2. PATCH ENCODING</h3>

<ul>
  <li><strong>Continuous time embedding</strong>
    <ul>
      <li>\(\phi(t)[d]=\left\{\begin{array}{lll}
\omega_0 \cdot t+\alpha_0, &amp; \text { if } &amp; d=0 \\
\sin \left(\omega_d \cdot t+\alpha_d\right), &amp; \text { if } &amp; 0&lt;d&lt;D_t
\end{array}\right.\).
        <ul>
          <li>where the \(\omega_d\) and \(\alpha_d\) are learnable parameters and \(D_t\) is embeddingâ€™s dimension</li>
        </ul>
      </li>
      <li>Concatenationí•˜ë©´ observations in the patch:
        <ul>
          <li>\(\mathbf{z}_{l_p: r_p}=\left[z_i\right]_{i=l_p}^{r_p}=\left[\phi\left(t_i\right) \| x_i\right]_{i=l_p}^{r_p}\).</li>
          <li>ì´ê±´ í•˜ë‚˜ì˜ patchì— ëŒ€í•œ í‘œí˜„ì´ ë˜ëŠ” ê²ƒ !</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Transformable time-aware convolution</strong>
    <ul>
      <li>input sequenceì˜ ê¸¸ì´ì— ë§ê²Œ (adaptively), generated parametersì™€ transformable filter sizeë¥¼ ì‚¬ìš©</li>
      <li>\(\mathbf{f}_d=\left[\frac{\exp \left(\mathbf{F}_d\left(z_i\right)\right)}{\sum_{j=1}^{L_p} \exp \left(\mathbf{F}_d\left(z_j\right)\right)}\right]_{i=1}^{L_p}\)ìœ¼ë¡œ í‘œí˜„ë¨
        <ul>
          <li>where \(L_p\) is the sequence length of patch \(\mathbf{z}_{l_p: r_p}, \mathbf{f}_d \in \mathbb{R}^{L_p \times D_{i n}}\) is the derived filter for \(d\)-th feature map, \(D_{i n}\) is dimension of inputs, and \(\mathbf{F}_d\) denotes the meta-filter that can be instantiated by learnable neural networks</li>
          <li>ì´ê±´ filterì˜ parametersë¥¼ along the temporal dimensionìœ¼ë¡œ normalizaingí•´ì„œ consistent scaling í•˜ê² ë‹¤ëŠ” ê²ƒ</li>
        </ul>
      </li>
      <li>ìœ„ ì‹ìœ¼ë¡œ \(D-1\)ê°œì˜ filtersë¥¼ ì‚¬ìš©í•´ì„œ <strong>latent patch embedding</strong> \(h_p^c \in \mathbb{R}^{D-1}\)ë¥¼ ì–»ìŒ :
        <ul>
          <li>\(h_p^c=\left[\sum_{i=1}^{L_p} \mathbf{f}_d[i]^{\top} \mathbf{z}_{l_p: r_p}[i]\right]_{d=1}^{D-1}\).</li>
          <li>ì´ê±´  encoded transformable patches:
            <ul>
              <li>variable-length sequencesì— ë”°ë¼ flexibilityë¥¼ ê°€ì§€ê³ </li>
              <li>parameterization for varying time intervalsì„ í•˜ë©´ì„œ</li>
              <li>additional learnable filter parameters ì—†ì´ ë” ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ</li>
            </ul>
          </li>
          <li>ë§ˆì§€ë§‰ìœ¼ë¡œ \(h_p=\left[h_p^c \| m_p\right]\) ì´ë ‡ê²Œ patchì— maskingì„ ë§ë¶™ì—¬ì£¼ëŠ”ë°,
            <ul>
              <li>\(m_p\)ëŠ” ì´ patch ì•ˆì— observationsê°€ í•˜ë‚˜ ì´ìƒ ìˆë‹¤~ë¥¼ indicatorë¡œ í‘œí˜„</li>
            </ul>
          </li>
          <li>ìµœì¢…ì ìœ¼ë¡œ \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)ë¥¼ ì–»ëŠ”ë‹¤.</li>
          <li>ì´ê±´ \(P\)ê°œì˜ patchë¥¼ \(D-1\)ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•˜ê³  ë§ˆì§€ë§‰ì—ëŠ” maskingìœ¼ë¡œ indicatorë¥¼ ë¶™ì¸ ê²ƒ</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-intra--and-inter-time-series-modeling">4.2. Intra- and Inter-Time Series Modeling</h3>

<ul>
  <li>ì´ì œ ì´  transformable patchingì„ irregular time seriesë¥¼ intra- and inter-time series modelingí•˜ëŠ”ì§€ ì•Œì•„ë³´ì</li>
</ul>

<h3 id="421-transformer-to-model-sequential-patches">4.2.1. TRANSFORMER TO MODEL SEQUENTIAL PATCHES</h3>

<ul>
  <li>ìœ„ì—ì„œ êµ¬í•œ \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)ë¥¼ Transformerì— ë„£ëŠ”ë‹¤.</li>
  <li>ë¨¼ì € positional encodingì„ í•˜ê³ 
    <ul>
      <li>\(\mathbf{x}_{1: P}^{t f, n}=\mathbf{h}_{1: P}^n+\mathbf{P E}_{1: P}\).</li>
    </ul>
  </li>
  <li>Q, K, Vë¥¼ ë§Œë“¤ì–´ì„œ MHAë¥¼ í†µê³¼í•œë‹¤.
    <ul>
      <li>\(\mathbf{q}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^Q\) / \(\mathbf{k}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^K\) / \(\mathbf{v}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^V\) where \(\mathbf{W}_h^Q, \mathbf{W}_h^K, \mathbf{W}_h^V \in \mathbb{R}^{D \times(D / H)}\)</li>
      <li>\(\mathbf{h}_{1: P}^{t f, n}=\|_{h=1}^H \operatorname{Softmax}\left(\frac{\mathbf{q}_h^n \mathbf{k}_h^{n T}}{\sqrt{D / H}}\right) \mathbf{v}_h^n \in \mathbb{R}^{P \times D}\),</li>
    </ul>
  </li>
</ul>

<h3 id="422-time-varying-adaptive-graph-structure-learning">4.2.2. TIME-VARYING ADAPTIVE GRAPH STRUCTURE LEARNING</h3>

<ul>
  <li>í•œ ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ë‹¤ë¥¸ ë³€ìˆ˜ì˜ ì •ë³´ëŠ” ë§¤ìš° ìœ ìš©í•  ìˆ˜ê°€ ìˆìŒ</li>
  <li>í•˜ì§€ë§Œ IMTSì—ì„œëŠ” misaligned at timesìœ¼ë¡œ ì¸í•´ correlation modelingì´ ì–´ë ¤ì›€
    <ul>
      <li>ê·¸ë ‡ë‹¤ê³  Raindropì²˜ëŸ¼ í•˜ê¸°ì—” e sequence length explosion problemì´ ë°œìƒ</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ <strong>transformable patching</strong>ìœ¼ë¡œ í•´ê²°
    <ul>
      <li>patchë¥¼ observationsì˜ ê°œìˆ˜ê°€ ì•„ë‹ˆë¼ ì‹œê°„ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŠë‹¤ë³´ë‹ˆ</li>
      <li>ê° ë³€ìˆ˜ëŠ” ê°™ì€ ìˆ«ìì˜ patchesë¡œ ì´ë£¨ì–´ì§€ë‹ˆê¹Œ</li>
      <li>time-adaptive graph neural networksë¡œ inter-time series correlationë¥¼ modelingí•  ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
  <li>ì¦‰ IMTSì˜  dynamic correlationsë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œëŠ”
    <ul>
      <li>series of time-varying adaptive graphsë¥¼ í•™ìŠµí•˜ê² ë‹¤ëŠ” ê²ƒì´ê³ </li>
      <li>ì§€ê¸ˆ ë¬¸ì œëŠ” variable embeddingì´ trainingì—ì„œëŠ” update ê°€ëŠ¥í•˜ì§€ë§Œ inferenceì—ì„œëŠ” static</li>
      <li>ê·¸ëŸ¬ë‹ˆ learnable \(\mathbf{E}_1^s, \mathbf{E}_2^s \in \mathbb{R}^{N \times D_g}\)ë¥¼ ì‚¬ìš©í•´ì„œ</li>
      <li>ìš°ë¦¬ê°€ ì§€ê¸ˆê¹Œì§€ ë§Œë“¤ì—ˆë˜  time-varying patch embedding \(\mathbf{H}_p^{t f}=\left[\mathbf{h}_p^{t f, n}\right]_{n=1}^N \in \mathbb{R}^{N \times D}\)ì„
        <ul>
          <li>static variable embeddingìœ¼ë¡œ ë§Œë“¤ë©´ ë¨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ê·¸ gated adding operationì€ ë‹¤ìŒê³¼ ê°™ìŒ
    <ul>
      <li>\(\begin{gathered}
\mathbf{E}_{p, k}=\mathbf{E}_k^s+g_{p, k} * \mathbf{E}_{p, k}^d, \\
\mathbf{E}_{p, k}^d=\mathbf{H}_p^{t f} \mathbf{W}_k^d, \\
g_{p, k}=\operatorname{ReLU}\left(\tanh \left(\left[\mathbf{H}_p^{t f} \| \mathbf{E}_k^s\right] \mathbf{W}_k^g\right)\right) \\
k=\{1,2\}
\end{gathered}\), where
        <ul>
          <li>\(\mathbf{W}_k^d \in \mathbb{R}^{D \times D_g}, \mathbf{W}_k^g \in \mathbb{R}^{\left(D+D_g\right) \times 1}\) are learnable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ì´ì œ time-varying adaptive graph structureë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì–»ìŒ : \(\mathbf{A}_p=\operatorname{Softmax}\left(\operatorname{ReLU}\left(\mathbf{E}_{p, 1} \mathbf{E}_{p, 2}^T\right)\right)\)</li>
</ul>

<h3 id="423-gnns-to-model-inter-time-series-correlation">4.2.3. GNNS TO MODEL INTER-TIME SERIES CORRELATION</h3>

<ul>
  <li>ë‹¤ìŒìœ¼ë¡œ dynamic inter-time series correlation at a patch-level resolutionì„ ì–»ìŒ
    <ul>
      <li>\(\mathbf{H}_p=\operatorname{ReLU}\left(\sum_{m=0}^M\left(\mathbf{A}_p\right)^m \mathbf{H}_p^{t f} \mathbf{W}_m^{g n n}\right) \in \mathbb{R}^{N \times D}\).</li>
      <li>where $M$ is the number of layers for GNNs, and $\mathbf{W}_m^{g n n} \in$ $\mathbb{R}^{D \times D}$ are learnable parameters at $m$-th layer.</li>
    </ul>
  </li>
</ul>

<h3 id="43-imts-forecasti">4.3. IMTS Forecasti</h3>

<ul>
  <li>ì´ì œ final latent representationì„ ì–»ëŠ”ë‹¤ :
    <ul>
      <li>\(\mathbf{H}=\text { Flatten }\left(\left[\mathbf{H}_p\right]_{p=1}^P\right) \mathbf{W}^f \in \mathbb{R}^{N \times D_o}\), where  \(\mathbf{W}^f \in \mathbb{R}^{P D \times D_o}\) are learnable parameters.</li>
      <li>ê° ë³€ìˆ˜ë§ˆë‹¤ ì´ representationì„ ì–»ëŠ”ë‹¤</li>
    </ul>
  </li>
  <li>n-ë²ˆì§¸ ë³€ìˆ˜ì˜ final latent representation \(\mathbf{H}^n \in \mathbf{H}\)ê³¼, forecasting query \(\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)ë¥¼ ê°€ì§€ê³  MLPì— ë„£ëŠ”ë‹¤</li>
  <li>
    <p>\(\hat{x}_j^n=\operatorname{MLP}\left(\left[\mathbf{H}^n \| \phi\left(q_j^n\right)\right]\right)\).</p>
  </li>
  <li>ëª¨ë¸ì€ ê° ë³€ìˆ˜ì˜ ì˜ˆì¸¡ì˜ MSEë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
    <ul>
      <li>\(\mathcal{L}=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2\).</li>
    </ul>
  </li>
</ul>

<h3 id="44-analysis-on-scalabil">4.4. Analysis on Scalabil</h3>

<ul>
  <li>The average sequence length : \(L_{t p}=L_{a v g} \leq L_{\max } \leq L_{c p r} \leq N \times L_{a v g}\), where
    <ul>
      <li>\(L_{\text {avg }}=\frac{1}{N} \sum_{n=1}^N L_n\).</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-experimental-setup">5.1. Experimental Setup</h3>

<ul>
  <li>Dataset :
    <ul>
      <li>PhysioNet, MIMIC, Human Activity, and USHCN</li>
      <li>training, validation, and test sets adhering to ratios of 60%, 20%, and 20%</li>
    </ul>
  </li>
  <li>Evaluation Metric :
    <ul>
      <li>\(\begin{aligned}
\text { MSE }&amp;=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2, \\\text { MAE }&amp;=
 \frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left|\hat{x}_j^n-x_j^n\right| .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h4 id="52-main-results">5.2. Main Results</h4>

<p><img src="/assets/img/timeseries/TPatchGNN/table1.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="53-ablation-study">5.3. Ablation Study</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="54-scalability-and-efficiency-analysis">5.4. Scalability and Efficiency Analysis</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table3.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="55-effect-of-patch-size">5.5. Effect of Patch Size</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/fig4.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>achieved the alignment between asynchronous IMTS
        <ul>
          <li>by transforming each univariate irregular time series into a series of transformable patches with varying observation counts but maintaining unified time horizon resolution.</li>
          <li>without a canonical pre-alignment representation process, preventing the aligned sequence length from explosively growing</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2024](https://openreview.net/pdf?id=UZlMXUGI6e)]]></summary></entry><entry><title type="html">Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-10-03-Mamba360/" rel="alternate" type="text/html" title="Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)" /><published>2024-10-03T00:00:00+09:00</published><updated>2024-10-03T20:01:49+09:00</updated><id>http://localhost:4000/timeseries/Mamba360</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-03-Mamba360/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Sequence modelingì—ì„œ RNN, LSTMì„ ì‚¬ìš©í–ˆì—ˆìŒ</li>
  <li>Transformerê°€ í›Œë¥­í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ
    <ul>
      <li>but \(O(N^2)\) complexity,inductive bias handlingì´ ì–´ë ¤ì›€</li>
    </ul>
  </li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” State Space Model (SSM)ë¥¼ í¬ê²Œ 3ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
    <ul>
      <li>Gating architectures</li>
      <li>Structural architectures</li>
      <li>Recurrent architectures</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>RNN
    <ul>
      <li>look at only the last state and current input for predicting the next state</li>
      <li>gradient calculations being limited to the hidden state and current input</li>
      <li>exploding or vanishing gradient problem</li>
      <li>lack sufficient memory for long sequences</li>
    </ul>
  </li>
  <li>LSTM
    <ul>
      <li>complexity with their gating mechanisms</li>
      <li>exhibit challenges in transfer learning</li>
    </ul>
  </li>
  <li>Transformer
    <ul>
      <li>enable each token to interact with every other token in the input sequence</li>
      <li>but \(O(N^2)\) complexity</li>
    </ul>
  </li>
  <li>State Space Model (SSM)
    <ul>
      <li>Understanding of State Space Models (SSMs) : mathematical fundamentals</li>
      <li>Categorization and Recent Advances of SSMs : systematic categorization</li>
      <li>Application of SSMs Across Domains : utility in domains</li>
      <li>Performance Comparison of SSMs with Transformers : SSMê³¼ Transformer ë¹„êµ</li>
    </ul>
  </li>
</ul>

<h2 id="2-basics-of-state-space-model">2. Basics of State Space Model</h2>

<ul>
  <li>High-orderë¥¼ first-order derivativesì™€ vector quantitiesë¡œ representation</li>
  <li>Dynamics of  damped mass-spring system : \(m \frac{d^2 y(t)}{d t^2}+c \frac{d y(t)}{d t}+k y(t)=u(t)\)
    <ul>
      <li>\(u(t)\) : ì§ˆëŸ‰ì— ì‘ìš©í•˜ëŠ” ì™¸ë¶€ í˜</li>
      <li>\(y(t)\) : ìˆ˜ì§ ìœ„ì¹˜</li>
      <li>\(x(t)\) : ì´ ë°©ì •ì‹ì„ 1ì°¨ ë¯¸ë¶„ê³¼ ë²¡í„° ì–‘ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ ë„ì…í•˜ëŠ” ë²¡í„°</li>
    </ul>
  </li>
</ul>

<h3 id="21-spring-mass-damper-system">2.1. Spring Mass-Damper system</h3>

<ul>
  <li>State Variables
    <ul>
      <li>\(x_1\) : equilibriumìœ¼ë¡œë¶€í„° ì§ˆëŸ‰ì˜ ìœ„ì¹˜</li>
      <li>\(\dot{x_1}\) : ì§ˆëŸ‰ì˜ ì†ë„</li>
    </ul>
  </li>
  <li>System Dynamics
    <ul>
      <li>ë‰´í„´ì˜ ì œ 2ë²•ì¹™ìœ¼ë¡œ í‘œí˜„í•˜ë©´ \(m \ddot{x}_1=-k x_1-c \dot{x}_1\)</li>
      <li>\(\ddot{x_1}\)ëŠ” ì§ˆëŸ‰ì˜ ê°€ì†ë„, \(-kx_1\)ì€ ìœ„ì¹˜ì— ë¹„ë¡€í•˜ëŠ” ìŠ¤í”„ë§ì˜ í˜,</li>
      <li>\(c\dot{x_1}\)ì€ ì†ë„ì— ë¹„ë¡€í•˜ëŠ” damping force (ìš´ë™ ì—ë„ˆì§€ ê°ì‡ ì‹œí‚¤ëŠ” í˜)</li>
    </ul>
  </li>
  <li>State-Space Formulation
    <ul>
      <li>State vector \(x \in \mathbb R^n\) : ì‹œìŠ¤í…œì˜ ë‚´ë¶€ ìƒíƒœ ë³€ìˆ˜</li>
      <li>Input vector \(u\in \mathbb R^m\) : ì‹œìŠ¤í…œì— ëŒ€í•œ ì œì–´ ë˜ëŠ” ì™¸ë¶€ ì…ë ¥</li>
      <li>Output vector \(y \in \mathbb R^p\) : ê´€ì‹¬ ìˆëŠ” ì¸¡ì • ê°€ëŠ¥í•œ ì–‘</li>
      <li>System dynamics : ì¼ì°¨ ë¯¸ë¶„ ë°©ì •ì‹ìœ¼ë¡œ í‘œí˜„ \(\dot{\mathbf x}=\mathbf A \mathbf x+\mathbf B \mathbf u\)
        <ul>
          <li>\(\mathbf x=\left[x_1, \dot{x}_1\right]^T\)ëŠ” state vector, \(\mathbf u\)ëŠ” input,</li>
          <li>\(\mathbf A\in\mathbb R^{n \times n}\)ëŠ” dynamic matrix \(\mathbf A=\left[\begin{array}{cc}
0 &amp; 1 \\
-\frac{k}{m} &amp; -\frac{c}{m}
\end{array}\right]\)</li>
          <li>\(\mathbf B \in \mathbb R^{n \times m}\)ì€ input matrix \(\mathbf B=\left[\begin{array}{c}
0 \\
\frac{1}{m}
\end{array}\right] \dot{\mathbf x}\)</li>
          <li>Output equation : \(\mathbf{y}=\mathbf{C x}+\mathbf{D u}\)
            <ul>
              <li>\(\mathbf{C} \in \mathbb{R}^{p \times n}\)ëŠ” output or sensor matrix</li>
              <li>\(\mathbf{D} \in \mathbb{R}^{p \times m}\)ëŠ” feedthrough matrix</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-state-space-model">2.2. State Space Model</h3>

<ul>
  <li>Definition
    <ul>
      <li>Discrete-time dynamical system :
        <ul>
          <li>\(x(t+1)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t=0,1,2,\)â€¦</li>
          <li>\(x(t) \in \mathbb{R}^n\) : tì‹œì ì—ì„œ state</li>
          <li>\(u(t) \in\mathbb{R}^p\) : control variables</li>
          <li>\(y(t) \in\mathbb{R}^k\)â€‹ : specific outputs of interest</li>
        </ul>
      </li>
      <li>Continuous-time model
        <ul>
          <li>\(\frac{d}{d t} x(t)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t \geq 0\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/fig3.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>Model Formulation
    <ul>
      <li>Complexity ë•Œë¬¸ì— Multi-head self-attention ëŒ€ì‹  SSM ì‚¬ìš©</li>
      <li>Continuous-time Latent State spaceëŠ” linear ordinary differential equationìœ¼ë¡œ í‘œí˜„
        <ul>
          <li>\(\begin{aligned} \dot{x}(t) &amp; =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
y(t) &amp; =\boldsymbol{C} x(t)+\boldsymbol{D} u(t) \end{aligned}\),</li>
          <li>evolution parameter \(A \in \mathcal{R}^{N \times N}\)</li>
          <li>projection parameter \(B \in \mathcal{R}^{N \times 1} \text { and } C \in \mathcal{R}^{N \times 1}\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Discrete-time SSM
    <ul>
      <li>continuous parameters \(A, B, C\) ë¥¼ discreteí•˜ê²Œ ë°”ê¾¸ê¸° ìœ„í•´ time-scale parameter \(\Delta\) ì‚¬ìš©</li>
      <li>ì¦‰ \(\bar{A}=f_A(\Delta, A), \bar{B}=f_B(\Delta, A, B)\)â€‹</li>
      <li>\(\begin{array}{lll}
x_k=\overline{\boldsymbol{A}} x_{k-1}+\overline{\boldsymbol{B}} u_k &amp; \overline{\boldsymbol{A}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1}(\boldsymbol{I}+\Delta / 2 \cdot \boldsymbol{A}) &amp; \\
y_k=\overline{\boldsymbol{C}} x_k &amp; \overline{\boldsymbol{B}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1} \Delta \boldsymbol{B} &amp; \overline{\boldsymbol{C}}=\boldsymbol{C}\end{array}\)â€‹.</li>
      <li>ì›ë˜ëŠ” ìœ„ì²˜ëŸ¼ ìƒê²¼ìŒ</li>
    </ul>
  </li>
  <li>Convolutional Kernel Representation
    <ul>
      <li>í•˜ì§€ë§Œ ìœ„ ì‹ì€ sequential nature ë•Œë¬¸ì— trainableí•˜ì§€ ì•ŠìŒ</li>
      <li>ê·¸ë˜ì„œ ì•„ë˜ì²˜ëŸ¼ continuous convolutionì„ ì‚¬ìš©</li>
      <li>\(\begin{array}{lll}
x_0=\overline{\boldsymbol{B}} u_0 &amp; x_1=\overline{\boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{B}} u_1 &amp; x_2=\overline{\boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{B}} u_2 \\
y_0=\overline{\boldsymbol{C} B} u_0 &amp; y_1=\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{C} B} u_1 &amp; y_2=\overline{\boldsymbol{C} \boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{C} B} u_2
\end{array}\).</li>
      <li>vectorizeí•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ</li>
      <li>\(\begin{aligned}
y_k &amp; =\overline{\boldsymbol{C A}}^k \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C A}}^{k-1} \overline{\boldsymbol{B}} u_1+\cdots+\overline{\boldsymbol{C} \boldsymbol{A B}} u_{k-1}+\overline{\boldsymbol{C} \boldsymbol{B}} u_k \\
y &amp; =\overline{\boldsymbol{K}} * u \\
\overline{\boldsymbol{K}} \in \mathbb{R}^L: &amp; =\mathcal{K}_L(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}}):=\left(\overline{\boldsymbol{C}}^i \overline{\boldsymbol{B}}\right)_{i \in[L]}=\left(\overline{\boldsymbol{C B}}, \overline{\boldsymbol{C} \boldsymbol{A B}}, \ldots, \overline{\boldsymbol{C}}^{L-1} \overline{\boldsymbol{B}}\right) .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h2 id="3-recent-advances-in-state-space-models">3. Recent Advances in State Space Models</h2>

<ul>
  <li>Transformerì˜ limitations :
    <ul>
      <li>Computational Complexity</li>
      <li>Large Memory Requirements : for storing embeddings and intermediate actiavations</li>
      <li>Fixed Sequence Length : du to positional embeddings</li>
      <li>Attention Mechanism Scalability : quadratic scaling with input length</li>
      <li>Lack of Causality in Standard Attention : not inherently capture causality</li>
    </ul>
  </li>
  <li>SSMì˜ categorization : ì–´ë–»ê²Œ long sequenceë¥¼ ë‹¤ë£° ê²ƒì¸ê°€
    <ul>
      <li>Structured SSMs : based on S4 and variants</li>
      <li>Recurrent SSMs : based on RNNs and variants</li>
      <li>Gated SSMs : leveraging gating techniques</li>
      <li>Miscellaneous SSMs : ê¸°íƒ€ ë‹¤ì–‘í•œ ë°©ë²•ë“¤</li>
    </ul>
  </li>
</ul>

<h3 id="31-structured-ssms">3.1. Structured SSMs</h3>

<ul>
  <li>
    <p>S4, HiPPO, H3, Liquid-S4 ë“±â€¦</p>
  </li>
  <li>
    <p>long-range dependencyë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²• ì‚¬ìš© :</p>

    <ul>
      <li>
        <p>polynomial projection operators</p>
      </li>
      <li>
        <p>multi-input multi-output systems</p>
      </li>
      <li>
        <p>and convolutional kernels</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="311-structured-state-space-sequence-s4">3.1.1. Structured State Space Sequence (S4)</h3>

<ul>
  <li>Higher-Order Polynomial Project Operator (HiPPO)
    <ul>
      <li>State and input transition matricesë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ memorize</li>
    </ul>
  </li>
  <li>Diagonal Plus Low-Rank Parametrization</li>
  <li>SSM matrix (A)ì˜ rankë¥¼ ë‚®ê²Œ í•´ì„œ diagonalizability and stability ë³´ì¥</li>
  <li>Efficient (convolutional) Kernel Computation
    <ul>
      <li>FFTì™€ iFFT ì‚¬ìš©í•´ì„œ complexityë¥¼ \(ğ‘‚(ğ‘ log(ğ‘))\)ë¡œ ë§Œë“¬</li>
    </ul>
  </li>
</ul>

<h3 id="312-high-order-polynomial-projection-operators-hippo">3.1.2. High-Order Polynomial Projection Operators (HiPPO)</h3>

<ul>
  <li>S4ì— ì‚¬ìš©ëœ í–‰ë ¬ì˜ ìˆ˜í•™ì ì¸ í•´ì„ì„ ì œê³µ</li>
  <li>4ê°€ì§€ì˜ ë³€í˜•ì„ ì‚¬ìš©í•˜ëŠ”ë°,
    <ul>
      <li>the truncated Fourier basis polynomial (Hippo-FouT)</li>
      <li>based on Lagurre polynomials(LagT)</li>
      <li>based on Legendre polynomials(LegT)</li>
      <li>based on Legendre polynomials with a sliding window(LegS)</li>
    </ul>
  </li>
</ul>

<h3 id="313-hungry-hungry-hippo-h3">3.1.3. Hungry Hungry HiPPO (H3)</h3>

<ul>
  <li>SSMì—ì„œì˜ 2ê°œì˜ challenges
    <ul>
      <li>ì²«ì§¸, difficulty in recalling earlier tokens
        <ul>
          <li>ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ì´ì „ í† í°ì„ ê¸°ì–µí•˜ëŠ” ë° ì–´ë ¤ì›€</li>
        </ul>
      </li>
      <li>ë‘˜ì§¸, difficult in comparing the tokens across different sequences
        <ul>
          <li>ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ì—ì„œ í† í°ì„ ë¹„êµí•˜ëŠ” ë° ì–´ë ¤ì›€</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ê·¹ë³µí•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì˜ 3ê°€ì§€ í•µì‹¬ ìš”ì†Œ
    <ul>
      <li>Multiplicative Interactionsê°€ ìˆëŠ” Stacked SSMs
        <ul>
          <li>stacking two SSMs with multiplicative interactions between their input and output projections</li>
        </ul>
      </li>
      <li>í•™ìŠµ íš¨ìœ¨ì„±ì„ ìœ„í•œ FlashConv
        <ul>
          <li>FFTë¥¼ ì‚¬ìš©í•˜ì—¬  training efficiency í–¥ìƒ</li>
        </ul>
      </li>
      <li>Scalingì„ ìœ„í•œ State-Passing
        <ul>
          <li>effectively splits the input into the largest possible chunks that can fit</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="314-global-convolution">3.1.4. Global Convolution</h3>

<ul>
  <li>ì›ë˜ëŠ” inputë§Œí¼ ê¸´ conv kernelì„ hidden state matrixì— ê³±í–ˆëŠ”ë° - ë¶ˆì•ˆì •í•¨</li>
  <li>ì´ conv kernelì„ parametrizingí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆ</li>
  <li>ì¼ë°˜ì ìœ¼ë¡œ conv kernelì€ FFTë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ëŠë¦´ ìˆ˜ê°€ ìˆì–´ì„œ IO-aware algorithm ì‚¬ìš©</li>
</ul>

<h3 id="317-ldstack">3.1.7. LDStack</h3>

<ul>
  <li>RNNì´ ë‹¤ì¤‘ ì…ë ¥ ë‹¤ì¤‘ ì¶œë ¥(MIMO)  Linear Dynamical System(LDS)ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆìŒ</li>
  <li>ì´ ë•Œ Parallel scanì´ ì‚¬ìš©ë¨</li>
  <li>ì¦‰  Single Input Multiple Outputs (SIMO) LDSë¥¼ í•©ì³ì„œ  MIMO LDSë¥¼ approximate
    <ul>
      <li>essential characteristicsë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ê³„ì‚°ì€ simpleí•´ì§</li>
    </ul>
  </li>
  <li>LDSë¥¼ time-varying state space modelsë¡œ ë³¼ ìˆ˜ ìˆìŒ</li>
</ul>

<h3 id="318-s5">3.1.8 S5</h3>

<ul>
  <li>RNNì„ ë‹¤ì¤‘ ì…ë ¥ ë‹¤ì¤‘ ì¶œë ¥ ì„ í˜• ë™ì  ì‹œìŠ¤í…œ(LDS)ìœ¼ë¡œ ëª¨ë¸ë§í•œ LDStackì„ state space models (SSMs)ìœ¼ë¡œ í™•ì¥</li>
  <li>LDStackê³¼ ë‹¬ë¦¬, S5 ê³„ì¸µì€ ì—¬ëŸ¬ ì…ë ¥ ë° ì¶œë ¥ì„ ë™ì‹œì— ì²˜ë¦¬</li>
</ul>

<h2 id="32-gated-ssms">3.2. Gated SSMs</h2>

<ul>
  <li>FFT ì—°ì‚° ìµœì í™”ë¥¼ ìœ„í•´ gating unitsë¥¼ ì‚¬ìš©</li>
  <li>Toepliz NNì€ position-encoded Toeplitz matrixë¡œ token mixing</li>
  <li>MambaëŠ” gated MLPë¡œ SSMì˜ compoutational inefficiency ê·¹ë³µí•˜ê³ ì í•¨</li>
  <li>(ë¬´ìŠ¨ ë§ ?) ë” ì½ì–´ë³´ì</li>
</ul>

<h3 id="323-toeplitz-neural-network-tnn">3.2.3. Toeplitz Neural Network (TNN)</h3>

<ul>
  <li>Transformerì˜ <strong>attention-mechanism</strong>ê³¼ <strong>positional embedding</strong>ì„ ê°œì„ </li>
  <li>position-encoded Toeplitz matrixë¥¼ ì‚¬ìš©í•˜ì—¬ token-pair ê´€ê³„ íŒŒì•…
    <ul>
      <li>space-time complexityë¥¼ \(O(NlogN)\)ìœ¼ë¡œ ì¤„ì„</li>
      <li>Relative Position Encoder (RPE)ë¡œ ìƒëŒ€ì  ìœ„ì¹˜ë¥¼ ìƒì„±í•´ì„œ parametersê°€ input lengthì— ë…ë¦½ì ì´ê²Œ í•¨</li>
    </ul>
  </li>
</ul>

<h3 id="324-mamba">3.2.4. Mamba</h3>

<ul>
  <li>Transformerì˜ quadratic computational and memory complexityì— ì£¼ëª©</li>
  <li>íŠ¹íˆ SSMì€  addressing tasks (selective copying, induction head)ì—ì„œ ë¹„íš¨ìœ¨ì ì´ì—ˆìŒ</li>
  <li>Mambaê°€ ì´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì€ :
    <ul>
      <li>novel parametrization approach for SSMs based on input characteristics</li>
      <li>incorporating a simple selection mechanism</li>
      <li>efficient hardware-aware algorithm based on selective scan</li>
      <li>gated technique to reduce the dimensionality of global kernel operations</li>
      <li>combine gated MLP[93] with the SSM module</li>
    </ul>
  </li>
</ul>

<h2 id="4-applications-of-state-space-models">4. Applications of State Space Models</h2>

<h3 id="41-language-domain-long-sequence">4.1. Language Domain (long sequence)</h3>

<ul>
  <li>ì›ë˜ëŠ” Transformer ë§ì´ ì¼ëŠ”ë° \(O(N^2)\) quadratic complexity \(\to\) long sequence ë¶ˆê°€ëŠ¥</li>
  <li>ê·¸ë˜ì„œ  State Space Models (SSMs)ì´ ë“±ì¥
    <ul>
      <li>input dataë¥¼ fixed-size latent stateì— í‘œí˜„</li>
      <li>í•˜ì§€ë§Œ ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ capability to retrieve and copyì—ì„œ trade-off</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="42-vision-domain">4.2. Vision domain</h3>

<ul>
  <li>Vision Mambaë‚˜ SiMBAì™€ ê°™ì€ Vision-specific Mamba
    <ul>
      <li>utilize bidirectional and visual state space models</li>
    </ul>
  </li>
  <li>SiMBA
    <ul>
      <li>sequence length and channel dimensionsì´ ê¼­ perfect square dimensionsì´ ì•„ë‹ˆì–´ë„ ë¨</li>
      <li>pyramid version of the transformer architecture (ì„±ëŠ¥ í–¥ìƒ)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table3.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="47-time-series-domain">4.7. Time Series Domain</h3>

<ul>
  <li>ì˜›ë‚ ì—ëŠ” ARIMA ì“°ë‹¤ê°€ Transformer ë“±ì¥í•˜ë©´ì„œ variantsê°€ ë§ì´ ë‚˜ì˜´
    <ul>
      <li>Informer, FEDFormer, PatchTSTâ€¦</li>
      <li>í•˜ì§€ë§Œ ì—¬ì „íˆ attention complexity ë•Œë¬¸ì— long-range dependency ëª»ì¡ìŒ</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ SSM ëª¨ë¸ì¸ Timemachine, SiMBA, MambaMix ë“±ì¥</li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table11.png" alt="ê·¸ë¦¼1" /></p>

<p><img src="/assets/img/timeseries/Mamba360/table14.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>SSMì€ 3ê°€ì§€ ë²”ì£¼ë¡œ ë¶„ë¥˜ ê°€ëŠ¥ (structured, gated, and recurrent)</li>
  <li>ì•„ì§ Transformerê°€ ë” ì˜í•˜ëŠ” ì˜ì—­ì´ ìˆê¸´ í•˜ì§€ë§Œ (ë§¥ë½ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‘ì—… ë“±)
    <ul>
      <li>SiMBAëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì™€ Mamba ì•„í‚¤í…ì²˜ë¥¼ ê²°í•©í•´ì„œ Time seriesì—ì„œ SOTA</li>
    </ul>
  </li>
  <li>SSMì„ large networkë¡œ ì•ˆì •ì ìœ¼ë¡œ í™•í•˜ëŠ” ê²ƒì´ ì•„ì§ í•´ê²°ë˜ì§€ ì•Šì€ ë¬¸ì œ</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/abs/2404.16112)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2022) Raindrop</title><link href="http://localhost:4000/pytorch/2024-09-24-raindrop/" rel="alternate" type="text/html" title="(Code Review, ICLR 2022) Raindrop" /><published>2024-09-24T00:00:00+09:00</published><updated>2024-09-24T18:34:03+09:00</updated><id>http://localhost:4000/pytorch/raindrop</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-24-raindrop/"><![CDATA[<p><a href="https://arxiv.org/abs/2110.05357">(Paper) Graph-Guided Network for Irregularly Sampled Multivariate Time Series</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">(Paper Review, ICLR 2022) Raindrop</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig1.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig2.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë¨¼ì € í„°ë¯¸ë„ì— git cloneê³¼ requirementsë¥¼ ì…ë ¥í•˜ì—¬ install í•œë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">python Raindrop.py</code>ë¡œ P19, P12, PAM ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ì„ ë³¼ ìˆ˜ ìˆë‹¤.</li>
</ul>

<h2 id="2-raindroppy">2. Raindrop.py</h2>

<h3 id="21-data-preparing">2.1. Data Preparing</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig3.png" alt="ì‚¬ì§„1" />
<img src="/assets/img/pytorch/raindrop_code/fig4.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>parserë¥¼ í†µí•´ aurgmentsë¥¼ ë§Œë“¤ê³ </li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” modelì€ irregular time seriesë¥¼ ë‹¤ë£¬ë‹¤.
    <ul>
      <li>ê·¸ëŸ¬ë¯€ë¡œ <code class="language-plaintext highlighter-rouge">missing ratio</code>, ì¦‰ featureë¥¼ maskingí•˜ëŠ” ë¹„ìœ¨ì„ ë¯¸ë¦¬ ê²°ì •í•´ì¤€ë‹¤. (option)</li>
      <li>ì¼ë‹¨ì€ 0(no missing)ìœ¼ë¡œ ë‘ê³  ì½”ë“œë¥¼ ì´í•´í•´ë³´ì</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig5.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì‚¬ì „ì— ì •í•œ <code class="language-plaintext highlighter-rouge">missing ratio</code>ë¥¼ ì‚¬ìš©í•œë‹¤.</li>
  <li>epoch ìˆ˜ì™€ learning rateë„ ë¯¸ë¦¬ ì •í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig6.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>P19 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">d_static</code>ê³¼ <code class="language-plaintext highlighter-rouge">d_inp</code>ëŠ” ì‹œê°„ì— ë”°ë¼ ë³€í•˜ì§€ ì•ŠëŠ”(ì •ì ) / ë³€í•˜ëŠ”(ë™ì ) ë³€ìˆ˜ì˜ ê°œìˆ˜</li>
      <li><code class="language-plaintext highlighter-rouge">static_info</code>ëŠ” <code class="language-plaintext highlighter-rouge">d_static</code> ë³€ìˆ˜ê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ (bool)</li>
      <li><code class="language-plaintext highlighter-rouge">max_len</code>ì€, batch ë‚´ ìƒ˜í”Œë§ˆë‹¤ ì‹œê³„ì—´ì˜ ê¸¸ì´ê°€ ë‹¤ë¥¸ë° ìµœëŒ€ ê¸¸ì´
        <ul>
          <li>ë§Œì•½ <code class="language-plaintext highlighter-rouge">max_len</code>ë³´ë‹¤ ì§§ë‹¤ë©´ ê·¸ ë¶€ë¶„ì€ ë‹¤ 0ìœ¼ë¡œ ê¸°ë¡ë˜ì–´ìˆë‹¤.</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">n_classes</code>ëŠ” ìƒ˜í”Œì— ì†í•˜ëŠ” classì˜ ê°œìˆ˜</li>
    </ul>
  </li>
  <li>
    <p>ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤ë©´ ìœ„ì˜ ë³€ìˆ˜ë“¤ì€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">d_ob</code>ëŠ” ê° ë³€ìˆ˜ë¥¼ ëª‡ ì°¨ì›ìœ¼ë¡œ í‘œí˜„í• ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.</li>
  <li>ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">d_model</code>ì€ ë™ì  ë³€ìˆ˜ì˜ ê°œìˆ˜ì¸ <code class="language-plaintext highlighter-rouge">d_inp</code>ì— <code class="language-plaintext highlighter-rouge">d_ob</code>ë¥¼ ê³±í•œ ê°’ì´ ëœë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">nhid</code>ëŠ” FFNì˜ dimensionì¸ë° <code class="language-plaintext highlighter-rouge">d_model</code>ì˜ 2ë°°ë¥¼ ì‚¬ìš©</li>
  <li><code class="language-plaintext highlighter-rouge">nlayers</code>ëŠ” layerì˜ ê°œìˆ˜, <code class="language-plaintext highlighter-rouge">nhead</code>ëŠ” MHA(multi-head attention)ì—ì„œ heads ê°œìˆ˜ì´ê³  ëª¨ë‘ 2ê°œë¥¼ ì‚¬ìš©</li>
  <li><code class="language-plaintext highlighter-rouge">dropout</code>ì€ TransformerEncoderLayerì—ì„œ ì‚¬ìš©í•˜ëŠ” dropout ratio</li>
  <li><code class="language-plaintext highlighter-rouge">aggreg</code>ëŠ” ë‚˜ì¤‘ì— ê° ë°°ì¹˜ë§ˆë‹¤, ê° ì‹œì ì„ vectorë¡œ í‘œí˜„í• í…ë° ê·¸ê±¸ ëª¨ë“  ì‹œì ì— ëŒ€í•´ í•©ì¹  ë•Œ <strong>í‰ê· </strong>ì„ ì‚¬ìš©</li>
  <li><code class="language-plaintext highlighter-rouge">MAX</code>ëŠ” positional encoderì— ë“¤ì–´ê°€ëŠ” MAX parameterì¸ë°
    <ul>
      <li>ë§‰ìƒ positional encoder ì½”ë“œë¥¼ ë³´ë©´ <code class="language-plaintext highlighter-rouge">MAX</code>ë¼ëŠ” ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë‹ˆ ì‹ ê²½ ì•ˆì¨ë„ ëœë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig7.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n_run</code>ì€ ë°ì´í„°ì…‹ì— ëŒ€í•´ ëª‡ ë²ˆì„ ì‹¤í—˜í•´ì„œ ê¸°ë¡í• ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">n_splits</code>ëŠ” ë°ì´í„°ê°€ 5ë“±ë¶„ ë˜ì–´ìˆì–´ì„œ 5ë¥¼ ì‚¬ìš©í•œë‹¤.</li>
  <li>ê·¸ë¦¬ê³  ë³¸ modelì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì„±ëŠ¥ ì§€í‘œë¥¼ ê¸°ë¡í•  arraysë¥¼ ë§Œë“¤ì–´ë†“ëŠ”ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig8.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ê·¸ë¦¬ê³  ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ì…‹ì„ train(0.8) / valid(0.1) / test(0.1)ë¡œ ë‚˜ëˆ„ê³  label(y)ë„ ë”°ë¡œ ì¤€ë¹„í•œë‹¤.</li>
  <li>P19 ë°ì´í„°ì…‹ì˜ ê²½ìš° trainì—ëŠ” 31042ê°œì˜ ìƒ˜í”Œì´ ìˆë‹¤. (ìƒ˜í”Œì€ í•œ ëª…ì˜ í™˜ì ì •ë„ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.)
    <ul>
      <li>ê·¸ë¦¬ê³  ê° ìƒ˜í”Œì€ <code class="language-plaintext highlighter-rouge">torch.size([# of timesetps,  # of features])</code>ì¸ tensorì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig9.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>TëŠ” time stepsì˜ ìˆ˜, FëŠ” (ë™ì ) ë³€ìˆ˜ì˜ ê°œìˆ˜ì´ê³ , DëŠ” (ì •ì ) ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ëœë‹¤.
    <ul>
      <li>ë™ì  ë³€ìˆ˜ëŠ” <code class="language-plaintext highlighter-rouge">Ptrain</code>ì˜ <code class="language-plaintext highlighter-rouge">arr</code>ì—, ì •ì  ë³€ìˆ˜ëŠ” <code class="language-plaintext highlighter-rouge">Ptrain</code>ì˜ <code class="language-plaintext highlighter-rouge">extended_static</code>ì— ë”°ë¡œ ì¤€ë¹„í•˜ê³  ìˆë‹¤.</li>
    </ul>
  </li>
  <li>ê·¸ë¦¬ê³  normalizationì„ ìœ„í•´ ëª¨ë“  ë³€ìˆ˜ë“¤ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì–»ëŠ”ë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">getStats</code> í•¨ìˆ˜ì—ëŠ” ì‚¬ìš©í•˜ëŠ”ë° íŠ¹ì´ì‚¬í•­ ì—†ìœ¼ë¯€ë¡œ skip</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig10.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ê°ê°ì˜ shapeì€ ì•„ë˜ì™€ ê°™ë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>ì˜ ë™ì  ë³€ìˆ˜ë“¤ì˜ ê°œìˆ˜ê°€ 34ê°œì˜€ëŠ”ë° 68ì´ ëœ ì´ìœ ëŠ” :
    <ul>
      <li>ê°™ì€ í¬ê¸°ì˜ Maskë¥¼ ì˜†ì— ì´ì–´ë¶™ì˜€ê¸° ë•Œë¬¸ì´ë‹¤.</li>
      <li>MaskëŠ” <code class="language-plaintext highlighter-rouge">M = 1*(input_tensor &gt; 0) + 0*(input_tensor &lt;= 0)</code>ì´ë‹¤.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>ì€ 31042ê°œì˜ í…ì„œì¸ë°, ê° tensorëŠ” í•´ë‹¹ ìƒ˜í”Œì˜ ê¸¸ì´ë¥¼ ì•Œë ¤ì¤€ë‹¤.
    <ul>
      <li>ì¦‰ ë§Œì•½ 0ë²ˆì§¸ ìƒ˜í”Œì˜ ê¸¸ì´ê°€ 40ì´ë¼ë©´, 0ë²ˆì§¸ tensorëŠ”<code class="language-plaintext highlighter-rouge">[1, 2, ..., 40, 0, 0, ...]</code>ì´ë‹¤.</li>
      <li>ì¼ë‹¨ ìˆ«ìëŠ” <code class="language-plaintext highlighter-rouge">max_len</code>ê°œì¸ë° í•´ë‹¹ ìƒ˜í”Œì˜ ê¸¸ì´ê¹Œì§€ë§Œ indexë¥¼ ê¸°ë¡í•˜ê³  ë’·ë¶€ë¶„ì€ zero padding</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">y_train</code>ì€ ê° ìƒ˜í”Œì˜ ì •ë‹µ labelì´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig11.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ <code class="language-plaintext highlighter-rouge">global_structure</code>ë¥¼ ì •ì˜í•˜ëŠ”ë°, ê°ê°ì˜ ë™ì  ë³€ìˆ˜ê°€ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€ë¥¼ 0, 1ë¡œ í‘œí˜„
    <ul>
      <li>adjacency matrixì˜ ì—­í• ì„ í•œë‹¤.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">missing_ratio</code>ê°€ ì¡´ì¬í–ˆë‹¤ë©´ ëª‡ëª‡ featureë¥¼ maskingí•œë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>ì´ <code class="language-plaintext highlighter-rouge">sample</code>ì´ë©´ ê° ìƒ˜í”Œ(í™˜ì)ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ íŠ¹ì„±ì„ ë¬´ì‘ìœ„ ì œê±°</li>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>ì´ <code class="language-plaintext highlighter-rouge">set</code>ì´ë©´ ë¯¸ë¦¬ ê³„ì‚°ëœ density scoresë¥¼ ì‚¬ìš©í•˜ì—¬ ì œê±°í•  íŠ¹ì„±ì„ ê²°ì •í•˜ê³  ëª¨ë“  ìƒ˜í”Œì—ì„œ ì œê±°</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig12.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>ì˜ shapeì„ <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps,  batch_size,  # of features(w/masking)])</code>ìœ¼ë¡œ,</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>ì˜ shapeì„ <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps, batch_size])</code>ë¡œ setting</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig13.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì•ì„œ ì†Œê°œí•œ parametersë¥¼ í•œ ë²ˆ ì¶œë ¥í•´ë³´ì•˜ë‹¤.</li>
  <li>ì§€ê¸ˆì€ masking ratioê°€ 0ì´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig14.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>parametersì˜ descriptionsëŠ” ìœ„ì™€ ê°™ë‹¤.</li>
</ul>

<h3 id="22-model-setting">2.2. Model setting</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig15.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ model, criterion, optimiazer, schedulerë¥¼ ì •ì˜í•œë‹¤.</li>
  <li>modelì€ 2d tensorë¡œ í‘œí˜„ëœ ìƒ˜í”Œë§ˆë‹¤ classificationí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë¯€ë¡œ CrossEntropyLossë¥¼ ì‚¬ìš©</li>
  <li>ì•„ì§ inputì„ modelì— ë„£ì€ ê±´ ì•„ë‹˜.
    <ul>
      <li>inputì´ modelì— ë“¤ì–´ê°€ë©´ ì–´ë–¤ ê³¼ì •ì„ ê±°ì¹˜ëŠ”ì§€ëŠ” ì•„ë˜ 3. <code class="language-plaintext highlighter-rouge">models_rd.py</code>ì—ì„œ ë³´ë„ë¡ í•œë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig16.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">idx_0</code>ì€ <code class="language-plaintext highlighter-rouge">y</code>ê°€ 0ì¸ samplesì˜ index, <code class="language-plaintext highlighter-rouge">idx_1</code>ì€ ë°˜ëŒ€</li>
  <li>labelì´ 1ì¸ ìƒ˜í”Œì˜ ê°œìˆ˜ê°€ ì ì€ unbalancing ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 3ë°°ë¡œ ëŠ˜ë¦¼ (ì™œ <strong>3</strong>ë°°ì¸ì§€ëŠ” ëª¨ë¦„)</li>
  <li>batch_sizeê°€ 128ì¸ë° labelì´ 0ê³¼ 1ì¸ samplesë¥¼ ì ˆë°˜ì”© ì±„ìš¸í…Œë‹ˆ
    <ul>
      <li>n_batchesëŠ” ê°œìˆ˜ê°€ ë” ì ì€ label ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  samplesë¥¼ í•œ ë²ˆì”© ë‹¤ ë³¼ ìˆ˜ ìˆë„ë¡ ì„¤ì •í–ˆë‹¤.</li>
      <li>ì‚¬ì‹¤ labelì´ 1ì¸ samplesë¥¼ 3ë°° í–ˆìœ¼ë‹ˆ labelì´ 1ì¸ ìƒ˜í”Œì„ 3ë²ˆì”© ë³´ëŠ” ê¼´ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig17.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig18.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ epochë¥¼ ì‹œì‘í•˜ëŠ”ë°, labelì´ 0ì¸ ìƒ˜í”Œê³¼ 1ì¸ ìƒ˜í”Œì—ì„œ ë¬´ì‘ìœ„ë¡œ <code class="language-plaintext highlighter-rouge">batch_size/2</code>ê°œì”© ê°€ì ¸ì˜¨ë‹¤.</li>
  <li>ì‚¬ì‹¤ labelì´ 1ì¸ samplesë¥¼ 3ë°° í–ˆìœ¼ë‹ˆ ì—¬ê¸°ì„œëŠ” ì¤‘ë³µëœ ìƒ˜í”Œì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ìˆë‹¤.</li>
  <li>modelì— ë“¤ì–´ê°ˆ input tensorsì˜ shapeì„ ë¯¸ë¦¬ í™•ì¸í•´ë‘ì</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig19.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ modelì— ë“¤ì–´ê°€ê³  í†µìƒì ì¸ backpropagationì„ ê±°ì¹œë‹¤.</li>
  <li>modelì— ë“¤ì–´ê°€ë©´ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì•Œì•„ë³´ì.</li>
</ul>

<h2 id="3-models_rdpy">3. models_rd.py</h2>

<h3 id="31-init">3.1. <strong>init</strong></h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>ì´ ìƒë‹¹íˆ ë§ì§€ë§Œ ì§€ê¸ˆ ë‹¤ ì•Œ í•„ìš”ëŠ” ì—†ë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>ì—ì„œ ì‚¬ìš©í•  ë•Œ ë‹¤ì‹œ ì˜¬ë¼ì™€ì„œ ë³´ë©´ ë  ë“¯</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig20.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig21.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig22.png" alt="ì‚¬ì§„1" /></p>

<h2 id="32-forward">3.2. forward</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig23.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>P19 ë°ì´í„°ì…‹ì˜ ê²½ìš° input shapeì€ ì£¼í™©ìƒ‰ ì£¼ì„ê³¼ ê°™ë‹¤.</li>
  <li>srcë¡œ ë“¤ì–´ì˜¤ëŠ” Pì˜ ê²½ìš° 34ê°œì˜ ë³€ìˆ˜ì˜€ëŠ”ë° ê°™ì€ í¬ê¸°ì˜ Maskë¥¼ ì˜†ì— ì´ì–´ë¶™ì¸ ê²ƒì´ë‹ˆ ë‹¤ì‹œ ë¶„ë¦¬
    <ul>
      <li>ê°ê°ì„ missing_mask, srcë¼ê³  ë¶€ë¦„</li>
    </ul>
  </li>
  <li>ê·¸ ë‹¤ìŒ 34ê°œì˜ ë³€ìˆ˜ë¥¼ <code class="language-plaintext highlighter-rouge">d_ob</code>(ì—¬ê¸°ì„  4)ë²ˆ ë°˜ë³µí•´ì„œ srcì˜ representation capacityë¥¼ í‚¤ì›Œì£¼ê³ 
    <ul>
      <li>ReLuë¥¼ í†µê³¼ì‹œì¼œì„œ non-linearityë¥¼ í‘œí˜„í•  ìˆ˜ ìˆê²Œ í•œë‹¤.</li>
      <li>ê·¸ ë‹¤ìŒ dropoutì„ ê±°ì¹œë‹¤.</li>
    </ul>
  </li>
  <li>ê²°êµ­ <code class="language-plaintext highlighter-rouge">h</code>ëŠ” srcë¥¼ í™•ì¥ì‹œí‚¤ê³  learnable weightsì™€ ReLuë¥¼ ê³±í•´ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë§Œë“  ê²ƒ</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig24.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ batchì— ìˆëŠ” ê° sampleë§ˆë‹¤ maskë¥¼ ë§Œë“ ë‹¤.</li>
  <li>sampleì— ê°’ì´ ìˆìœ¼ë©´ maskì—ëŠ” Falseê°€ ë˜ê³  ê°’ì´ ì—†ìœ¼ë©´ maskê°€ Trueê°€ ëœë‹¤.</li>
  <li>maskì˜ ê¸¸ì´ëŠ” 60ìœ¼ë¡œ ê³ ì •ì´ì§€ë§Œ sampleë§ˆë‹¤ ê¸¸ì´ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì–´ë””ê¹Œì§€ Falseì´ê³  ì–¸ì œë¶€í„° Trueì¸ì§€ëŠ” sampleë§ˆë‹¤ ë‹¤ë¥´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig25.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë‹¤ìŒìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">global_structure</code>ë¥¼ adjacency matrixë¡œ ì‚¬ìš©í•œë‹¤.
    <ul>
      <li>shapeì€ ë™ì  ë³€ìˆ˜ì˜ ê°œìˆ˜ <code class="language-plaintext highlighter-rouge">d_inp</code> x <code class="language-plaintext highlighter-rouge">d_inp</code>ê°€ ë˜ë¯€ë¡œ ê° ë™ì  ë³€ìˆ˜ë¥¼ ì—°ê²° ì—¬ë¶€ë¥¼ (0,1)ë¡œ í‘œí˜„í•œë‹¤.</li>
      <li>epochê°€ ì§„í–‰ë˜ë©´ì„œ ë°”ë€” ìˆ˜ë„ ìˆìœ¼ë‹ˆ ëŒ€ê°ì„±ë¶„ì€ í•­ìƒ 1ë¡œ updateí•´ì¤€ë‹¤.</li>
    </ul>
  </li>
  <li>ê·¸ ë‹¤ìŒ edge_indexì™€ edge_weightsë¥¼ ë¯¸ë¦¬ êµ¬í•´ë†“ëŠ”ë‹¤.
    <ul>
      <li>ì—°ê²°ëœ nodesì˜ indexì™€ ê·¸ weightsë¥¼ ì˜ë¯¸í•¨</li>
    </ul>
  </li>
  <li>ê·¸ ë‹¤ìŒ batchì— ìˆëŠ” ê° sampleë§ˆë‹¤ (ë™ì ) ë³€ìˆ˜ë“¤ì˜ global structure(edge)ë¥¼ ê³ ë ¤í•œ representationì„ ì €ì¥í•  ê³µê°„ <code class="language-plaintext highlighter-rouge">output</code>ì„ ë¯¸ë¦¬ ë§Œë“¤ì–´ë†“ëŠ”ë‹¤.
    <ul>
      <li>ê° sampleë§ˆë‹¤ <code class="language-plaintext highlighter-rouge">torch([# of time steps,  d_inp x d_ob])</code> shapeì˜ tensorê°€ ë“¤ì–´ê°ˆ ì˜ˆì •ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig26.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ ì•„ê¹Œ ë§Œë“  <code class="language-plaintext highlighter-rouge">h</code>ë¥¼ <code class="language-plaintext highlighter-rouge">x</code>ë¡œ ë°›ì•„ì„œ (<code class="language-plaintext highlighter-rouge">x=h</code>) í•˜ë‚˜ì˜ sampleì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">h</code>ë¥¼ <code class="language-plaintext highlighter-rouge">stepdata</code> ê°€ì ¸ì˜¨ë‹¤</li>
  <li><code class="language-plaintext highlighter-rouge">p_t</code>ëŠ” ê° timestepì„ <code class="language-plaintext highlighter-rouge">d_pe = 16</code>ì°¨ì› vectorë¡œ embeddingí•œ ê²ƒì´ë‹¤. (init ì°¸ê³ )</li>
  <li>ì´ì œ <code class="language-plaintext highlighter-rouge">stepdata</code>ë¥¼ <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code>ë¡œ reshapeí•œë‹¤.
    <ul>
      <li>ì™œëƒí•˜ë©´ featureë¼ë¦¬ attentionì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ê° featureë¥¼ í•˜ë‚˜ì˜ vectorë¡œ ë§Œë“¤ í•„ìš”ê°€ ìˆê¸° ë•Œë¬¸</li>
    </ul>
  </li>
  <li>ì´ì œ ê° featureë¥¼ vectorë¡œ ë§Œë“  ê±¸ <code class="language-plaintext highlighter-rouge">ob_propagation</code>ìœ¼ë¡œ ì •ì˜ëœ attention layerì— ë„£ëŠ”ë‹¤.
    <ul>
      <li>ê·¸ëŸ¬ë©´ ê°™ì€ shape <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code> tensorê°€ returnë˜ì§€ë§Œ</li>
      <li>í•´ë‹¹ sampleì˜ ê°ê°ì˜ featuresë¥¼ Observation Propagationì„ ê±°ì³ representationí•œ ê²°ê³¼ì´ë‹¤.</li>
      <li><code class="language-plaintext highlighter-rouge">Ob_propagation.py</code>ì— ìˆê³ , ì½”ë“œë¥¼ ë”°ë¡œ ì²¨ë¶€í•˜ì§€ëŠ” ì•Šê² ìœ¼ë‚˜ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ì„ ê±°ì¹œë‹¤.
        <ul>
          <li>1) Message Passing: node ê°„ì— ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” mechanism êµ¬í˜„</li>
          <li>2) Attention Mechanism: ê° nodeê°€ ì´ì›ƒ nodeë¡œë¶€í„° ë°›ëŠ” ë©”ì‹œì§€ì˜ ì¤‘ìš”ë„ë¥¼ í•™ìŠµ</li>
          <li>3) Egde weights: graphì˜ edgeì— weightë¥¼ ì ìš©í•˜ì—¬ ì •ë³´ ì „ë‹¬ì˜ ê°•ë„ë¥¼ ì¡°ì ˆ</li>
          <li>4) Edge prune: ì¤‘ìš”ë„ê°€ ë‚®ì€ edgeë¥¼ ì œê±°í•˜ì—¬ computation efficiency ë†’ì„</li>
          <li>5) Feature Transform: linear Transformê³¼ activation ftnìœ¼ë¡œ nodeì˜ featureë¥¼ ë³€í™˜</li>
          <li>6) Aggregation: ì´ì›ƒ nodeë¡œë¶€í„° ë°›ì€ ë©”ì‹œì§€ë¥¼ í•©ì¹¨</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig27.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ob_propagation-layer</code>ë¥¼ í•œ ë²ˆ ë” í†µê³¼ì‹œí‚¤ê³  shapeì„ ë§ì¶°ì„œ <code class="language-plaintext highlighter-rouge">output</code>ì˜ sample index ìë¦¬ì— ë„£ëŠ”ë‹¤.
    <ul>
      <li>ê·¸ë¦¬ê³  alpha_allì—ëŠ” ê·¸ attention weightsë¥¼ ë„£ëŠ”ë‹¤.
        <ul>
          <li>34ê°œì˜ featuresë¼ë¦¬ì˜ attentionì´ë‹ˆ 34\(\times\)34\(=\)1156ê°œì˜ ìˆ«ìê°€ ëœë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ëª¨ë“  samplesì— ëŒ€í•´ì„œ ì™„ë£Œí•˜ì—¬ <code class="language-plaintext highlighter-rouge">output</code>ì´ ì™„ì„±ë˜ë©´ distanceë¥¼ êµ¬í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig28.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë‹¤ìŒìœ¼ë¡œ time embeddingì„ concatí•œë‹¤.</li>
  <li>ì´ëŸ¬ë©´ shapeì´ <code class="language-plaintext highlighter-rouge">torch.size([60, 128, 152])</code>ê°€ ë˜ëŠ”ë°, ê° sampleë§ˆë‹¤(128), í•˜ë‚˜ì˜ ì‹œì ì„ 152ì°¨ì› vectorë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤.
    <ul>
      <li>ì´ 152ëŠ” (ë™ì ) ë³€ìˆ˜ 34ê°œë¥¼ 34\(\times\)4 = 136ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•˜ê³ , time embedding 16ì°¨ì›ì„ ë¶™ì¸ ê²ƒ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig29.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ê±¸ transformer encoderì— í†µê³¼ì‹œí‚¤ê³ </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig30.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>aggregate í•˜ëŠ”ë°, ì´ ë•Œ ëª¨ë“  ì‹œì ì— ëŒ€í•´ í‰ê· ì„ ë‚´ì¤€ë‹¤. (<code class="language-plaintext highlighter-rouge">aggreg == mean</code>)</li>
  <li>ê·¸ëŸ¬ë©´ ê° sampleì€ ëª¨ë“  ì‹œì ê³¼ ëª¨ë“  ë³€ìˆ˜ë¥¼ í†µí•©í•˜ì—¬ 152ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ëœ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig31.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë§ˆì§€ë§‰ìœ¼ë¡œ (ì •ì ) ë³€ìˆ˜ë¥¼ embeddingí•œ embë¥¼ ë¶™ì—¬ì„œ 2-layer MLPì— ë„£ìœ¼ë©´</li>
  <li>ê° sampleì— ëŒ€í•œ classificationì´ ì™„ë£Œëœë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig32.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>Trainingì— ë”°ë¥¸ validation set acccuracyê°€ ì¶œë ¥ëœë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig33.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ê·¸ë¦¬ê³  classification reportê°€ ì¶œë ¥ëœë‹¤.</li>
</ul>

<p>ë !</p>

<ul>
  <li>ì°¸ê³ ë¡œ ë‚˜ì˜ ê²½ìš°ì—ëŠ” <code class="language-plaintext highlighter-rouge">from torch_scatter import gather_csr, scatter, segment_csr</code>ê°€ ì•ˆë˜ì–´ì„œ ì•„ë˜ì™€ ê°™ì´ ì£¼ì„ ì²˜ë¦¬í•˜ê³ 
    <ul>
      <li>pytorchë¥¼ ë³´ê³  í•¨ìˆ˜ë¥¼ ì§ì ‘ ì‘ì„±í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ë‹¤.</li>
      <li><a href="https://github.com/rusty1s/pytorch_scatter/blob/master/torch_scatter/scatter.py">pytorch_scatter ì°¸ê³ </a></li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># from torch_scatter import gather_csr, scatter, segment_csr
</span><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">+</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
            <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">(),</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">other</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">src</span>

<span class="k">def</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">index</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">dim_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">dim_size</span>
        <span class="k">elif</span> <span class="n">index</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">dim_size</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="n">index_dim</span> <span class="o">=</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">index_dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index_dim</span> <span class="o">+</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">index_dim</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">size</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">index_dim</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">count</span><span class="p">[</span><span class="n">count</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="p">.</span><span class="nf">is_floating_point</span><span class="p">():</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">true_divide_</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">div_</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">floor</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="nb">reduce</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">
    |

    .. image:: https://raw.githubusercontent.com/rusty1s/pytorch_scatter/
            master/docs/source/_figures/add.svg?sanitize=true
        :align: center
        :width: 400px

    |

    Reduces all values from the :attr:`src` tensor into :attr:`out` at the
    indices specified in the :attr:`index` tensor along a given axis
    :attr:`dim`.
    For each value in :attr:`src`, its output index is specified by its index
    in :attr:`src` for dimensions outside of :attr:`dim` and by the
    corresponding value in :attr:`index` for dimension :attr:`dim`.
    The applied reduction is defined via the :attr:`reduce` argument.

    Formally, if :attr:`src` and :attr:`index` are :math:`n`-dimensional
    tensors with size :math:`(x_0, ..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})`
    and :attr:`dim` = `i`, then :attr:`out` must be an :math:`n`-dimensional
    tensor with size :math:`(x_0, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})`.
    Moreover, the values of :attr:`index` must be between :math:`0` and
    :math:`y - 1`, although no specific ordering of indices is required.
    The :attr:`index` tensor supports broadcasting in case its dimensions do
    not match with :attr:`src`.

    For one-dimensional tensors with :obj:`reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, the operation
    computes

    .. math::
        \mathrm{out}_i = \mathrm{out}_i + \sum_j~\mathrm{src}_j

    where :math:`\sum_j` is over :math:`j` such that
    :math:`\mathrm{index}_j = i`.

    .. note::

        This operation is implemented via atomic operations on the GPU and is
        therefore **non-deterministic** since the order of parallel operations
        to the same value is undetermined.
        For floating-point variables, this results in a source of variance in
        the result.

    :param src: The source tensor.
    :param index: The indices of elements to scatter.
    :param dim: The axis along which to index. (default: :obj:`-1`)
    :param out: The destination tensor.
    :param dim_size: If :attr:`out` is not given, automatically create output
        with size :attr:`dim_size` at dimension :attr:`dim`.
        If :attr:`dim_size` is not given, a minimal sized output tensor
        according to :obj:`index.max() + 1` is returned.
    :param reduce: The reduce operation (:obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">mul</span><span class="sh">"</span><span class="s">`,
        :obj:`</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="s">` or :obj:`</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="s">`). (default: :obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`)

    :rtype: :class:`Tensor`

    .. code-block:: python

        from torch_scatter import scatter

        src = torch.randn(10, 6, 64)
        index = torch.tensor([0, 1, 0, 1, 2, 1])

        # Broadcasting in the first and last dim.
        out = scatter(src, index, dim=1, reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">)

        print(out.size())

    .. code-block::

        torch.Size([10, 3, 64])
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">sum</span><span class="sh">'</span> <span class="ow">or</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">add</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mul</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">min</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">max</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span>

</code></pre></div></div>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Raindrop github](https://github.com/mims-harvard/Raindrop)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2024) Pathformer</title><link href="http://localhost:4000/pytorch/2024-09-09-pathformer/" rel="alternate" type="text/html" title="(Code Review, ICLR 2024) Pathformer" /><published>2024-09-09T00:00:00+09:00</published><updated>2024-09-24T18:29:39+09:00</updated><id>http://localhost:4000/pytorch/pathformer</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-09-pathformer/"><![CDATA[<p><a href="https://openreview.net/pdf?id=lJkOCMP2aW">(Paper) Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-05-23-Pathformer">(Paper Review, ICLR 2024) Pathformer</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig1.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig2.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë¨¼ì € í„°ë¯¸ë„ì— git cloneê³¼ requirementsë¥¼ ì…ë ¥í•˜ì—¬ install í•œë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">bash scripts/multivariate/ETTm2.sh</code>ë¡œ ETTm2 ë°ì´í„°ì…‹ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h2 id="2-sh">2. .sh</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig3.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ETTm2.sh</code> íŒŒì¼ì—ëŠ” <code class="language-plaintext highlighter-rouge">run.py</code>ë¥¼ ì‹¤í–‰í•˜ë„ë¡ ë˜ì–´ìˆë‹¤.</li>
</ul>

<h2 id="3-runpy">3. run.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig4.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>parserë¥¼ í†µí•´ argumentsë¥¼ ë§Œë“ ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig5.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">Exp_Main</code>ì— ìˆëŠ” trainì— argumentsë¥¼ ë„£ì–´ì¤€ë‹¤.</li>
</ul>

<h2 id="4-exp_mainpy">4. exp_main.py</h2>

<h3 id="41-train">4.1. train</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig6.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>model, data, optimizer, criterionì„ ì„¤ì •í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë“¤ê³¼, <code class="language-plaintext highlighter-rouge">vali</code>, <code class="language-plaintext highlighter-rouge">train</code>, <code class="language-plaintext highlighter-rouge">test</code>, <code class="language-plaintext highlighter-rouge">predict</code> í•¨ìˆ˜ê°€ ìˆë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig7.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Exp_main</code> : <code class="language-plaintext highlighter-rouge">train</code>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">_get_data</code>ë¡œ train, valid, test ë°ì´í„°ì…‹ì„ load</li>
      <li><code class="language-plaintext highlighter-rouge">sum(p.numel() for p in self.model.parameters())</code>ëŠ” parameters ê°œìˆ˜</li>
      <li>time, early sipping, optimizer, criterion, learning rate scheduler ì •ì˜</li>
      <li><code class="language-plaintext highlighter-rouge">lr_scheduler.OneCycleLR</code>ëŠ” learning rateë¥¼ ë¹ ë¥´ê²Œ ìµœëŒ€ í•™ìŠµë¥ ê¹Œì§€ ì¦ê°€ì‹œì¼°ë‹¤ê°€ ë‹¤ì‹œ ê°ì†Œì‹œí‚¤ë©´ì„œ ìµœì í™” ê³¼ì •
        <ul>
          <li><code class="language-plaintext highlighter-rouge">optimizer</code> : ì‚¬ìš©í•˜ëŠ” optimizer</li>
          <li><code class="language-plaintext highlighter-rouge">steps_per_epoch</code> : 1 epochê°€ ëª‡ ë²ˆì˜ updateê°€ ë°œìƒí•˜ëŠ”ì§€ (mini-batch)</li>
          <li><code class="language-plaintext highlighter-rouge">pct_start</code> : learning rateê°€ ì¦ê°€í•˜ëŠ” êµ¬ê°„ì˜ ë¹„ìœ¨ / <code class="language-plaintext highlighter-rouge">epochs</code> : ì „ì²´ epoch ìˆ˜</li>
          <li><code class="language-plaintext highlighter-rouge">max_lr</code> : ìµœëŒ€ learning rate</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig8.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>each epochì—ì„œëŠ” train loderì—ì„œ batch ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë°›ê³ </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig9.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">with torch.cuda.amp.autocast():</code>ëŠ” <code class="language-plaintext highlighter-rouge">float16</code>ê³¼ <code class="language-plaintext highlighter-rouge">float32</code>ë¥¼ ìë™ìœ¼ë¡œ ìºìŠ¤íŒ…</li>
  <li>ëª¨ë¸ì´ ì˜ˆì¸¡í•œ <code class="language-plaintext highlighter-rouge">outputs</code>ì™€ ì •ë‹µ <code class="language-plaintext highlighter-rouge">batch_y</code>ë¥¼ ë¹„êµí•˜ì—¬ loss ê³„ì‚°</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig10.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>epochì— ê±¸ë¦° ì‹œê°„ê³¼ lossë¥¼ ì¶œë ¥í•˜ê³  backwardë¡œ parametersë¥¼ update</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig11.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>Validation setì— ëŒ€í•œ lossë¡œ early stopping ì—¬ë¶€ë¥¼ ê²°ì •í•˜ê³  í•™ìŠµì´ ì¢…ë£Œë˜ë©´ ëª¨ë¸ ì €ì¥</li>
  <li>vali í•¨ìˆ˜ëŠ” íŠ¹ì´ ì‚¬í•­ ì—†ìœ¼ë¯€ë¡œ pass</li>
</ul>

<h3 id="42-test">4.2. test</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig12.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>test datasetê³¼, í•™ìŠµë˜ì–´ ì €ì¥ëœ modelì„ loadí•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig13.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>trainê³¼ ë¹„ìŠ·í•˜ê²Œ batch ë‹¨ìœ„ë¡œ ëª¨ë¸ì— ë„£ì–´ì„œ ì˜ˆì¸¡ê°’ì„ ì–»ëŠ”ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig14.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>batch 20ê°œë§ˆë‹¤ ë¬¶ì–´ì„œ visualizaitonì„ í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig15.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ìµœì¢…ì ì¸ ì˜ˆì¸¡ê³¼ lossë¥¼ <code class="language-plaintext highlighter-rouge">results.txt</code>ì— ì €ì¥í•œë‹¤.</li>
</ul>

<h3 id="43-predict">4.3. predict</h3>

<p>pass</p>

<h2 id="5-modelspathformerpy">5. models/Pathformer.py</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">from models import PathFormer</code> ì´ë¯€ë¡œ í•´ë‹¹ ê²½ë¡œë¡œ ê°€ì„œ pathformerì˜ archtectureë¥¼ ë³´ì</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">forward</code>ëŠ” normalization \(\to\) <code class="language-plaintext highlighter-rouge">start_fc</code> \(\to\) for <code class="language-plaintext highlighter-rouge">layer</code> in <code class="language-plaintext highlighter-rouge">self.AMS_lists</code> \(\to\) de-normalizationë¡œ êµ¬ì„±ëœë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>ì— ë“¤ì–´ì˜¨ xì˜ shapeì€ <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes])</code>ì´ë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">seq_len</code>ì€ ê´€ì¸¡í•˜ëŠ” ê³¼ê±° ì‹œì  ìˆ˜, <code class="language-plaintext highlighter-rouge">num_nodes</code>ëŠ” multivariateì—ì„œ variates ê°œìˆ˜</li>
    </ul>
  </li>
  <li>xê°€ unsqueezeë˜ì–´ normalization, <code class="language-plaintext highlighter-rouge">start_fc</code>ë¥¼ í†µê³¼í•˜ë©´ <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes, d_model])</code>ì´ ëœë‹¤. (ì•„ë˜ <code class="language-plaintext highlighter-rouge">__init__</code> ì°¸ê³ )</li>
  <li>ì´ì œ <code class="language-plaintext highlighter-rouge">AMS_list</code>ì˜ <code class="language-plaintext highlighter-rouge">layers</code>ë¥¼ í†µê³¼í•˜ê³  denormalizationì„ í†µê³¼í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig17.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>ì„ ë³´ë©´ <code class="language-plaintext highlighter-rouge">self.AMS_lists</code>ëŠ” <code class="language-plaintext highlighter-rouge">layers.AMS</code>ì—ì„œ importí•œë‹¤.</li>
  <li>AMS layerê°€ pathformerëŠ” ì „ë¶€ì´ë‹ˆ ì‚´í´ë³´ì</li>
</ul>

<h2 id="6-layersamspy">6. Layers/AMS.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">self.seasonality_and_trend_decompose</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.noisy_top_k_gating</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.cv_squared</code></li>
  <li><code class="language-plaintext highlighter-rouge">SparseDispatcher</code>ì™€ <code class="language-plaintext highlighter-rouge">SparseDispatcher.dispatch</code>, <code class="language-plaintext highlighter-rouge">SparseDispatcher.combine</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.experts</code></li>
  <li>ê°ê°ì— ëŒ€í•´ì„œ í•˜ë‚˜ì”© ì‚´í´ë³´ë„ë¡ í•œë‹¤.</li>
</ul>

<h3 id="61-selfseasonality_and_trend_decompose">6.1. self.seasonality_and_trend_decompose</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig19.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig20.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>AMS class ì•ˆì—ì„œ ì •ì˜ëœ í•¨ìˆ˜</li>
  <li><strong>seasonalityì™€ trendë¥¼ xì—ì„œ ê°ê° ê³„ì‚°</strong>í•˜ê¸° ë•Œë¬¸ì— \(seasonal + trend = x\)ê°€ ì•„ë‹˜
    <ul>
      <li>í•´ë‹¹ í•¨ìˆ˜ì˜ ê²°ê³¼ëŠ” xì— seasonalityì™€ trendë¥¼ ë”í•œ ê²°ê³¼ì´ë‹¤.</li>
    </ul>
  </li>
  <li>ì²˜ìŒì— <code class="language-plaintext highlighter-rouge">x = x[:, :, :, 0]</code>ì€ <code class="language-plaintext highlighter-rouge">d_model</code> ì°¨ì›ìœ¼ë¡œ í‘œí˜„ëœ xì—ì„œ ì²« ë²ˆì§¸ dimensionë§Œ ì‚¬ìš©í•´ì„œ decomposeí•œë‹¤ëŠ” ì˜ë¯¸</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig21.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>seasonality_modelì€ <code class="language-plaintext highlighter-rouge">FourierLayer</code>
    <ul>
      <li>í‘¸ë¦¬ì— ë³€í™˜(fft) í›„ amplitudeê°€ ë†’ì€ frequency \(k\)â€‹â€‹ê°œë¥¼ inverse í‘¸ë¦¬ì— ë³€í™˜(extrapolate)</li>
    </ul>
  </li>
  <li>trend_modelì€ <code class="language-plaintext highlighter-rouge">series_decomp_multi</code>
    <ul>
      <li>ë‹¤ì–‘í•œ í¬ê¸°ì˜ kernel sizeë¡œ moving averageë¥¼ softmax</li>
    </ul>
  </li>
</ul>

<h3 id="62-selfnoisy_top_k_gating">6.2. self.noisy_top_k_gating</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig22.png" alt="ì‚¬ì§„1" />
<img src="/assets/img/pytorch/pathformer_code/fig23.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">start_linear.squeeze</code>ì™€ <code class="language-plaintext highlighter-rouge">w_gate</code>ë¡œ <code class="language-plaintext highlighter-rouge">torch.Size([batch, seq_len, num_node])</code>ê°€ <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>ê°€ ëœë‹¤.</li>
  <li>ê°™ì€ í¬ê¸° <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>ì˜ <code class="language-plaintext highlighter-rouge">logit</code>ì„ ë§Œë“¤ê³  \(top-k\) logitì„ <code class="language-plaintext highlighter-rouge">gates</code>ì— ë„£ëŠ”ë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">scatter</code>ëŠ” íŠ¹ì • ì¸ë±ìŠ¤ ìœ„ì¹˜ì— ê°’ì„ í• ë‹¹í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.</li>
      <li><code class="language-plaintext highlighter-rouge">gate</code>ì˜ shapeì€ <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_experts])</code>ê°€ ë˜ëŠ”ë°, ê° í–‰(batch)ì—ì„œ kê°œë¥¼ ì œì™¸í•˜ê³ ëŠ” ë‹¤ 0ì´ë‹¤.</li>
      <li>ê·¸ë¦¬ê³  ê° í–‰(batch)ë§ˆë‹¤ ê·¸ kê°œê°€ ì–´ë–¤ expertsì¸ì§€ëŠ” ë‹¤ë¥´ë‹¤.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">load</code>ëŠ” ê° expertê°€ ë°°ì¹˜ ì „ì²´ì—ì„œ ì–¼ë§ˆë‚˜ ì„ íƒë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤.
    <ul>
      <li>shapeì€ <code class="language-plaintext highlighter-rouge">torch.Size([num_experts])</code>ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="63-selfcv_squared">6.3. self.cv_squared</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë‹¤ì‹œ AMS.forwardë¡œ ëŒì•„ì˜¤ì</li>
  <li>ê° expertë§ˆë‹¤ ëª¨ë“  ë°°ì¹˜ì— ëŒ€í•´ sumì„ í•´ì„œ <code class="language-plaintext highlighter-rouge">importance</code>ë¥¼ ê³„ì‚°í•˜ë©´ <code class="language-plaintext highlighter-rouge">num_experts</code>ê°œì˜ ìˆ«ìê°€ ëœë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">cv_squared</code>ë¥¼ í†µí•´ <code class="language-plaintext highlighter-rouge">num_experts</code>ê°œì˜ ìˆ«ìì˜ ë³€ë™ê³„ìˆ˜ë¥¼ êµ¬í•´ì„œ <code class="language-plaintext highlighter-rouge">balance_loss</code>ë¥¼ êµ¬í•œë‹¤.
    <ul>
      <li>ë³€ë™ê³„ìˆ˜ëŠ” \(\frac{\sigma^2}{\mu^2}\)ì´ë‹¤.</li>
      <li>ì´ ê°’ì´ í¬ë©´ íŠ¹ì • expertsì— importanceê°€ ëª°ë ¤ìˆìŒì„ ì˜ë¯¸í•œë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig24.png" alt="ì‚¬ì§„1" /></p>

<h3 id="64-sparsedispatcher-ì–´ë ¤ì›€-ì£¼ì˜">6.4. SparseDispatcher (*ì–´ë ¤ì›€ ì£¼ì˜)</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig25.png" alt="ì‚¬ì§„1" />
<img src="/assets/img/pytorch/pathformer_code/fig26.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>ì—ì„œ ì¤€ë¹„í•´ë†“ëŠ” ê²ƒë“¤ì´ ë§ìœ¼ë‹ˆ í•˜ë‚˜í•˜ë‚˜ ë³´ë„ë¡ í•œë‹¤. \(k=2\), <code class="language-plaintext highlighter-rouge">num_experts</code>=4ì¸ ê²½ìš°ì´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig27.png" alt="ì‚¬ì§„1" /></p>
<ul>
  <li>ê° í–‰ì€ batchë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— í–‰ì˜ ê°œìˆ˜ëŠ” batch size (ì—¬ê¸°ì„  512)</li>
  <li>ê° í–‰ì—ëŠ” <code class="language-plaintext highlighter-rouge">num_experts</code>ê°œì˜ ìˆ«ìê°€ ìˆê³  ê·¸ ì¤‘ \(k\)ê°œë§Œ non-negative, ë‚˜ë¨¸ì§€ëŠ” 0</li>
  <li>ì²« í–‰ì—ì„œ 2, 3ë²ˆì§¸ ìˆ«ìê°€ ì–‘ìˆ˜ë¼ëŠ” ê²ƒì€, ì²«ë²ˆì§¸ ë°°ì¹˜ì—ì„œ 2, 3ë²ˆì§¸ expertsê°€ ì„ íƒë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸</li>
  <li>ë°”ë¡œ ì•„ë˜ì— ìˆëŠ” <code class="language-plaintext highlighter-rouge">torch.nonzero(gates)</code>ì—ì„œë„ ê·¸ ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.
    <ul>
      <li>ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œëŠ” index 1, 2ì¸ expertsê°€, ë§ˆì§€ë§‰ ë°°ì¹˜ì—ì„œëŠ” index 1, 2ì¸ expertsê°€ ì„ íƒë¨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig28.png" alt="ì‚¬ì§„1" /></p>
<ul>
  <li>ì´ì œ sortë¥¼ í•˜ëŠ”ë° ì²«ë²ˆì§¸ ì—´ì€ ì–´ì°¨í”¼ indexë¼ì„œ ì •ë ¬ë˜ì–´ìˆê³ 
    <ul>
      <li>(ë‘ ë²ˆì§¸ ì—´ì´ ì •ë ¬ë˜ë©´ì„œ ì„ì´ê¸° ë•Œë¬¸ì— ë‘ ë²ˆì§¸ ì—´ì˜ ìˆ«ìê°€ ì²« ë²ˆì§¸ ì—´ì˜ ë°°ì¹˜ indexì™€ ìƒê´€ ì—†ê²Œ ëœë‹¤)</li>
    </ul>
  </li>
  <li>ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">index_sorted_experts</code>ëŠ” ì •ë ¬ëœ ìˆ«ìê°€ ëª‡ ë²ˆì§¸ indexì— ìˆë˜ ìˆ«ìì¸ì§€ë¥¼ í‘œì‹œí•´ì¤€ë‹¤.
    <ul>
      <li>(ì—¬ê¸°ì„œë¶€í„° í—·ê°ˆë¦¬ê¸° ì‹œì‘í•¨)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig29.png" alt="ì‚¬ì§„1" /></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">self._expert_index</code>ëŠ” ê° ë°°ì¹˜ì—ì„œ ì„ íƒëœ expertsì˜ indexë¥¼ ì •ë ¬í•œ ê²ƒì´ë‹¤.
    <ul>
      <li>ê° ë°°ì¹˜ë§ˆë‹¤ \(k\)ê°œì”© ìˆìœ¼ë‹ˆ ì´ batch_size \(\times k\)ê°œì˜ ìˆ«ìê² ë‹¤.</li>
    </ul>
  </li>
  <li>ê·¸ë¦¬ê³  ê·¸ê±¸ ë‹¤ì‹œ batch indexë¡œ ë˜ëŒë¦´ ìˆ˜ê°€ ìˆì„ ê²ƒì´ë‹¤.
    <ul>
      <li>ì¦‰ <code class="language-plaintext highlighter-rouge">self._batch_index</code>ê°€ 1, 3, 6,â€¦ì´ë¼ëŠ” ê²ƒì€ expert 0ì´ ì„ íƒë˜ì—ˆë˜ batchê°€ 1, 3, â€¦ì´ê³ </li>
      <li>ê·¸ ë‹¤ìŒ expert 1ì´ ì„ íƒëœ batchë“¤ì´ ëª‡ ë²ˆì§¸ batchì¸ì§€ ì­‰ ë‚˜ì—´ì´ ëœë‹¤. (ì´ê±¸ ë§ˆì§€ë§‰ expertê¹Œì§€ ë°˜ë³µ)</li>
    </ul>
  </li>
  <li>ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">self._part_sizes</code>ëŠ” ëª¨ë“  batches í†µí‹€ì–´ì„œ ê° expertê°€ ëª‡ ë²ˆ ì„ íƒë˜ì—ˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig30.png" alt="ì‚¬ì§„1" /></p>
<ul>
  <li>ì´ì œ <code class="language-plaintext highlighter-rouge">gates_exp</code>ëŠ” expert 0ì´ ì„ íƒë˜ì—ˆë˜ batchesë¥¼ ì­‰ ë‚˜ì—´í•˜ê³ , ê·¸ ë‹¤ìŒì— expert 1ì´ ì„ íƒë˜ì—ˆë˜ batchesë¥¼ ì­‰ ë‚˜ì—´í•˜ê³ â€¦ ë§ˆì§€ë§‰ expertê°€ ì„ íƒë˜ì—ˆë˜ batchesê¹Œì§€ ë‚˜ì—´í•œ ê²ƒì´ë‹¤.</li>
  <li>ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">self._nonzero_gates</code>ëŠ” expert \(i\) (\(i = 1, ...,\) <code class="language-plaintext highlighter-rouge">num_experts</code>)ê°€ ì„ íƒëœ ë°°ì¹˜ì—ì„œ expert \(i\)ì˜ gatesë¥¼ ë‚˜ì—´í•œ ê²ƒì´ë‹¤.</li>
</ul>

<h3 id="641-sparsedispatcherdispatch">6.4.1. SparseDispatcher.dispatch</h3>

<ul>
  <li>ì´ì œ dispatchì—ì„œëŠ” ê° expertì— ì²˜ë¦¬í•´ì•¼ í•  batchesë¥¼ í• ë‹¹í•œë‹¤.</li>
  <li>ë§Œì•½ ì§€ê¸ˆì²˜ëŸ¼ inpì˜ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">torch.Size([512, 96, 7, 16])</code>, <code class="language-plaintext highlighter-rouge">self._batch_index</code>ì˜ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">torch.Size([1024])</code>, ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">self._part_sizes</code>ê°€ <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>ë¼ê³  ê°€ì •í•˜ë©´:</li>
  <li><code class="language-plaintext highlighter-rouge">inp[self._batch_index]</code>ì—ì„œëŠ” inp í…ì„œì—ì„œ 1024ê°œì˜ ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬, í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">torch.Size([1024, 96, 7, 16])</code>ì¸ ìƒˆë¡œìš´ í…ì„œë¥¼ ìƒì„±í•œë‹¤.</li>
  <li>ê·¸ë¦¬ê³  ì²« ë²ˆì§¸ ì°¨ì›(batch ì°¨ì›, 1024ê°œ)ì„ <code class="language-plaintext highlighter-rouge">self._part_sizes</code> = <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>ë¡œ ë‚˜ëˆˆë‹¤.</li>
  <li>ê²°ê³¼ëŠ” ê° expertì—ê²Œ í• ë‹¹ëœ batchesì˜ ë¦¬ìŠ¤íŠ¸ì´ë©°, ê° í…ì„œì˜ í¬ê¸°ëŠ”:
    <ul>
      <li>ì²« ë²ˆì§¸ expert: [262, 96, 7, 16]</li>
      <li>ë‘ ë²ˆì§¸ expert: [348, 96, 7, 16]</li>
      <li>ì„¸ ë²ˆì§¸ expert: [249, 96, 7, 16]</li>
      <li>ë„¤ ë²ˆì§¸ expert: [165, 96, 7, 16]</li>
    </ul>
  </li>
  <li>ì´ê±¸ ë¦¬ìŠ¤íŠ¸ë¡œ returní•œë‹¤.</li>
</ul>

<h3 id="642-sparsedispatchercombine">6.4.2. SparseDispatcher.combine</h3>

<ul>
  <li>ì´ì œ ê°ê°ì„ í•´ë‹¹ expertì— í†µê³¼ì‹œí‚¨ë‹¤.</li>
  <li>expertëŠ” <code class="language-plaintext highlighter-rouge">TransformerLayer</code>ì´ë‹¤. (Pathformer.pyì˜ __init__ì°¸ê³ )</li>
  <li>ê·¸ë¦¬ê³  ê·¸ ê²°ê³¼ë¥¼ ë‹¤ì‹œ combineí•œë‹¤.
    <ul>
      <li>ê·¸ëŸ°ë° ìœ„ì—ì„œ combine í•¨ìˆ˜ë¥¼ ì˜ ë³´ë©´ ì²˜ìŒì— <code class="language-plaintext highlighter-rouge">.exp()</code>ë¥¼ í•˜ê³  ë‹¤ì‹œ <code class="language-plaintext highlighter-rouge">.log()</code>ë¥¼ í•´ì£¼ëŠ”ë°,</li>
      <li><code class="language-plaintext highlighter-rouge">.exp()</code>ì—ì„œ NaNì´ ë‚˜ì˜¬ ìˆ˜ê°€ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì.</li>
      <li>(ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œëŠ” í•´ë‹¹ì‚¬í•­ ì—†ì§€ë§Œ ë‚´ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì—ì„œëŠ” ë°œìƒí–ˆë‹¤.)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ residual_connectionë§Œ ì ìš©í•´ì£¼ë©´ ëë‚œë‹¤.</li>
  <li>ì—¬ê¸°ê¹Œì§€ê°€ í•˜ë‚˜ì˜ <code class="language-plaintext highlighter-rouge">AMS</code> layerì´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì—¬ê¸°ì„œ for ì•ˆì— ìˆëŠ” layerê°€ AMS layerì´ë‹¤.</li>
  <li>
    <p>ë§ˆì§€ë§‰ìœ¼ë¡œ de-normalizationì„ í•˜ë©´ ëì´ë‹¤.</p>
  </li>
  <li>ë‚˜ë¨¸ì§€ëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì†Œê°œí•œ <code class="language-plaintext highlighter-rouge">3. run.py</code>ì™€ <code class="language-plaintext highlighter-rouge">4. exp_main.py</code>ê°€ ì „ë¶€ì´ë‹¤.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Pathformer github](https://github.com/decisionintelligence/pathformer)]]></summary></entry><entry><title type="html">Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIRâ€™24 Best Paper)</title><link href="http://localhost:4000/timeseries/2024-09-03-SyNCRec/" rel="alternate" type="text/html" title="Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIRâ€™24 Best Paper)" /><published>2024-09-03T00:00:00+09:00</published><updated>2024-09-03T15:04:04+09:00</updated><id>http://localhost:4000/timeseries/SyNCRec</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-09-03-SyNCRec/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Cross-Domain Sequential Recommendation (CDSR)ì€ multiple domainì—ì„œì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ Single-Domain Sequential Recommendation (SDSR)ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ</li>
  <li>í•˜ì§€ë§Œ <strong>negative transfer</strong> : lack of relation btw domainsì€ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸</li>
  <li>ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ”
    <ol>
      <li>estimates the degree of <strong>negative transfer</strong> of each domain</li>
      <li>adaptively assigns it as a <strong>weight factor</strong> to the prediction loss
        <ul>
          <li>to control gradient flows through domains with significant negative transfer !</li>
        </ul>
      </li>
      <li>developed <strong>auxiliary loss</strong> that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis</li>
    </ol>
  </li>
  <li>ì´ëŸ¬í•œ CDSRê³¼ SDSRì˜ cooperative learningì€ collaborative dynamics between pacers and runners in a marathonì™€ ìœ ì‚¬í•¨</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Single-Domain Sequential Recommendation (SDSR)
    <ul>
      <li>focuses on <strong>recommending the next item</strong> within a <strong>specific</strong> domain using <strong>only</strong> the <strong>single</strong>-domain sequence</li>
    </ul>
  </li>
  <li>Cross-Domain Sequential Recommendation (CDSR)
    <ul>
      <li><strong>predicts</strong> the <strong>next item</strong> a user will interact with, by leveraging their historical <strong>interaction</strong> sequences across <strong>multiple</strong> domains</li>
    </ul>
  </li>
  <li>ë‘˜ì˜ ì°¨ì´ëŠ” ê²°êµ­ ë‹¤ë¥¸ domainsì˜ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ”ì§€ ì—¬ë¶€</li>
  <li>CDSRì€ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë‹¤ë¥¸ domainsì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì§€ë§Œ í•­ìƒ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê±´ ì•„ë‹˜
    <ul>
      <li>ë§Œì•½ ê·¸ê²ƒ ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë” ì•ˆì¢‹ì•„ì§„ë‹¤ë©´, ê·¸ê±´ <strong>negative transfer</strong>ê°€ ìˆì—ˆê¸° ë•Œë¬¸</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>
    <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” SyNCRec: Asymmetric Cooperative Network for Cross-Domain Sequential Recommendationì„ ì œì•ˆ</p>
  </li>
  <li>
    <ol>
      <li>assess the degree of <strong>negative transfer</strong> of each domain
        <ul>
          <li>by comparing the performance of CDSR and SDSR</li>
        </ul>
      </li>
      <li>adaptively assign this value as <strong>weight to the prediction loss</strong> corresponding to a specific domain
        <ul>
          <li>to reduces its flow in domains with significant negative transfer !</li>
        </ul>
      </li>
      <li>developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis
        <ul>
          <li>to exploit the effective correlation signals inherent in the representation pairs of SDSR and CDSR tasks within a specific domain</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>SDSRì€ negative transferë¥¼ ì¤„ì´ê¸° ìœ„í•œ pacerì˜ ì—­í• ì„ í•¨
    <ul>
      <li>(ë§ˆë¼í†¤ì—ì„œ runnerê°€ ë„ˆë¬´ ë¹ ë¥´ê±°ë‚˜ ëŠë¦¬ê²Œ í•˜ì§€ ì•Šê²Œ í•´ì£¼ëŠ” pacer)</li>
    </ul>
  </li>
  <li>íŠ¹íˆ CDSRì´ SDSRë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì•˜ë˜ (=negative transferê°€ ë°œìƒí•œ) ë„ë©”ì¸ì—ì„œ ì„±ëŠ¥ í–¥ìƒë¨</li>
  <li>ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ domain-specific modelsë¥¼ ë§Œë“¤ í•„ìš”ê°€ ì—†ì„ ê²ƒì„ ê¸°ëŒ€í•¨</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="21-single-domain-sequential-recommendation">2.1. Single-Domain Sequential Recommendation</h3>

<ul>
  <li>SDSR : temporal dynamics in user-item interactionsë¥¼ ë””ìì¸
    <ul>
      <li>GRU-based models : GRU4Rec, STAMP, NARM</li>
      <li>Attention-mechanism : SASRec, BERT4Rec, SINE, LightSANs</li>
      <li>Others : NextItNet(CNN), TransRec(Markov chain), â€¦</li>
    </ul>
  </li>
</ul>

<h3 id="22-cross-domain-sequential-recommendation">2.2 Cross-Domain Sequential Recommendation</h3>

<ul>
  <li>CDSR : information from various other domainsë¥¼ leverage
    <ul>
      <li>Matrix factorization : CMF, CLFM, â€¦</li>
      <li>Multi-task learning : DTCDR, DeepAPF, BiTGCF, CAT-ART</li>
      <li>\(\pi-Net\) :  introduced gating mechanisms designed to transfer information from a single domain to another paired domain</li>
      <li>\(C^2DSR\) : employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations</li>
      <li>\(MIFN\) :  introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains</li>
      <li>\(MAN\) : designed group-prototype attention mechanisms to capture domainspecific and cross-domain relationships</li>
    </ul>
  </li>
  <li>Howeverâ€¦ ê²°êµ­ì—ëŠ” ëª¨ë‘ domain pair ë¼ë¦¬ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§
    <ul>
      <li>3ê°œ ì´ìƒì˜ domainsì˜ ê´€ê³„ë¥¼ íŒŒì•…í•  ë•Œ, domainsì´ ì—„ì²­ ë§ì„ ë•Œì—ëŠ” ì–´ë ¤ì›€</li>
      <li>ê·¸ë˜ì„œ CGRecì—ì„œ CDSRì„ ì œì•ˆí•˜ë©´ì„œ negative transfer ê°œë…ì„ ì œì•ˆ
        <ul>
          <li>high negative transferë¥¼ ê°€ì§€ëŠ” domainì— panaltyë¥¼ ì£¼ëŠ” ë°©ì‹</li>
          <li>í•˜ì§€ë§Œ ì—¬ì „íˆ SDSRë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì€ domainì´ ê½¤ ìˆìŒ</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ ë³¸ ë…¼ë¬¸ì—ì„œì˜ ëª©í‘œëŠ” 3ê°œ ì´ìƒì˜ <strong>ëª¨ë“ </strong> ë„ë©”ì¸ì—ì„œ negative transferë¥¼ <strong>íš¨ìœ¨ì </strong>ìœ¼ë¡œ ì¤„ì´ëŠ” ê²ƒ</li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<ul>
  <li>Domains : \(\mathcal{D}=\{A, B, C, \ldots\}\) where \(\mid \mathcal{D}\mid  \geq 3\)
    <ul>
      <li>\(d \in \mathcal D\) ëŠ” í•˜ë‚˜ì˜ íŠ¹ì • ë„ë©”ì¸ì„ ì˜ë¯¸,</li>
      <li>\(V^d\)ëŠ” set of items specific to the domain \(d\), \(V\)ëŠ” total item set across all domains</li>
    </ul>
  </li>
</ul>

<h3 id="definition-1-single--and-cross-domain-sequential-recommendation">Definition 1. Single- and Cross-Domain Sequential Recommendation</h3>

<ul>
  <li>The single-domain sequences of domain \(d\) : \(X^d=\left[(\mathrm{SOS}), x_1^d, x_2^d, \ldots, x_{\mid X^d\mid -1}^d\right]\)â€‹</li>
  <li>\(x_t^d\) :  interaction occurring at time \(t\)</li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ cross-domain sequenceëŠ” \(X=\left(X^A, X^B, X^C, \ldots\right)\)ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ</li>
  <li>ì˜ˆë¥¼ ë“¤ì–´, \(X=\left[(\mathrm{SOS}), x_1^A, x_2^B, x_3^A, x_4^B, x_5^A, x_6^C, x_7^C\right]\)ì€ \(X^A=\left[(\mathrm{SOS}), x_1^A, x_3^A, x_5^A\right], X^B=\left[(\mathrm{SOS}), x_2^B, x_4^B\right], \text { and } X^C=[(\mathrm{SOS})\left., x_6^C, x_7^C\right]\)ìœ¼ë¡œ split ê°€ëŠ¥</li>
  <li>SDSRì€ í•˜ë‚˜ì˜ domain ì•ˆì—ì„œ recommending, CDSRì€ ì „ì²´ ë„ë©”ì¸ì—ì„œ recommending</li>
</ul>

<h3 id="definition-2-negative-transfer-gap-ntg">Definition 2. Negative Transfer Gap (NTG)</h3>

<ul>
  <li>\(\mathcal{L}_\pi^d\)ëŠ” domain \(d\)ì—ì„œì˜ model \(\pi\)ì˜ lossë¥¼ ì˜ë¯¸ (SDSR ë˜ëŠ” CDSR)</li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ Negative transferëŠ” \(\phi_\pi(d) = \mathcal{L}_\pi^d\left(X^d\right)-\mathcal{L}_\pi^d(X)\)</li>
</ul>

<h3 id="problem-statement">Problem Statement</h3>

<ul>
  <li>historical cross-domain sequences \(X_{1:t}\)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ëª©í‘œëŠ” ë‹¤ìŒ item \(x_{t+1}^d = \underset{x_{t+1}^d \in V^d}{\operatorname{argmax}} P\left(x_{t+1}^d \mid X_{1: t}\right)\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ</li>
  <li>ë§Œì•½ \(\mid \mathcal{D}\mid\)ê°œì˜ single-domain sequences (for SDSR)ê³¼ 1ê°œì˜ sequence (for CDSR)ê°€ ìˆë‹¤ë©´
    <ul>
      <li>multi-tasking learning mannerì˜ ëª¨ë¸ í•˜ë‚˜ëŠ” \(\mid \mathcal{D}\mid +1\)ê°œì˜ next item prediction tasksë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<h2 id="4-model">4. Model</h2>

<p><img src="/assets/img/timeseries/SyNCRec/fig2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="41-shared-embedding-layer">4.1. Shared Embedding Layer</h3>

<ul>
  <li>ì—¬ê¸°ì„œëŠ” <strong>initialized representations</strong> of itemsë¥¼ ì–»ëŠ”ë‹¤.
    <ul>
      <li>for \(\mid \mathcal{D}\mid\) single-domain sequences \(X^d\), and one cross-domain sequence \(X\)</li>
    </ul>
  </li>
  <li>Item embedding matrix \(M^d \in \mathbb R^{\mid V^d\mid \times r}\)â€‹ì´ê³ 
    <ul>
      <li>\(\mid V^d\mid\)ëŠ” domain dì˜ items ê°œìˆ˜, rì€ embedding dimension</li>
    </ul>
  </li>
  <li>ëª¨ë“  domainsì— ëŒ€í•´ concatí•˜ë©´ \(M \in \mathbb R^{\mid V\mid \times r}\)
    <ul>
      <li>\(\mid V\mid\)ëŠ” ëª¨ë“  ë„ë©”ì¸ì—ì„œ items ê°œìˆ˜</li>
    </ul>
  </li>
  <li>ì—¬ê¸°ì„œ ìµœê·¼ Tê°œë§Œì„ ì‚¬ìš© (Tê°œë³´ë‹¤ ì ë‹¤ë©´ ì•ìª½ì— paddingìœ¼ë¡œ ë§ì¶°ì¤Œ)
    <ul>
      <li>ê·¸ëŸ¬ë©´ \(\mathbf{E}^d \in \mathbb{R}^{T \times r} \text { and } \mathbf{E} \in \mathbb{R}^{T \times r}\)ë¥¼ ì–»ìŒ (ê°ê° Fig2(c-1), (c-2))</li>
      <li>\(\mid \mathcal D\mid\)ê°œì˜ \(\mathbf{E}^d\)ë¥¼ aggregationí•œ ê²ƒì´ \(\mathbf{E}^{\text {single }}\) (Fig2(c-1))</li>
      <li>ì°¸ê³ ë¡œ \(\mathbf E, \mathbf E^d\)ì—ëŠ” learnable positional embedding ë”í•´ì ¸ìˆìŒ</li>
      <li>\(t\)-th stepì—ì„œì˜ \(\mathbf E, \mathbf E^d\)ëŠ” ê°ê° \(\mathbf e, \mathbf e^d\)ë¡œ ì •ì˜</li>
    </ul>
  </li>
</ul>

<h3 id="42-asymmetric-cooperative-network-with-mixture-of-sequential-experts-acmoe">4.2. Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMoE)</h3>

<ul>
  <li>Negative Transfer (NTG)ëŠ” <strong>loss of the SDSR</strong>ê³¼ <strong>the loss of CDSR</strong>ì˜ ì°¨ì´ë¡œ ì •ì˜
    <ul>
      <li>NTGê°€ ì‘ìœ¼ë©´ ë‹¤ë¥¸ domainsì˜ ì •ë³´ê°€ ë„ì›€ì´ ì•ˆë˜ëŠ” ê±°ê³  í¬ë©´ ë„ì›€ì´ ë˜ëŠ” ê²ƒ</li>
    </ul>
  </li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ weight for the prediction loss in the domainë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤
    <ul>
      <li>gradient flowë¥¼ ì‘ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œë‹¤</li>
    </ul>
  </li>
  <li>Multi-gate Mixture of Sequential Experts (MoE) architectureë¥¼ ì‚¬ìš©í•˜ì—¬ SDSRê³¼ CDSRë¥¼ ìˆ˜í–‰í•˜ê³ 
    <ul>
      <li><strong>models</strong> relationships between different tasks and <strong>learns</strong> task-specific functionalities</li>
      <li>enabling it to effectively leverage shared representations</li>
    </ul>
  </li>
  <li>SDSRê³¼ CDSRì€ ì„œë¡œ ê°„ì„­í•˜ì§€ ì•Šê³ , expertsë¡œëŠ” Transformerë¥¼ ì‚¬ìš©</li>
</ul>

<h3 id="421-architecture">4.2.1. Architecture</h3>

<ul>
  <li>
    <p><strong>ë¨¼ì € SDSRì„ ë³´ì</strong></p>
  </li>
  <li>
    <p>shared embedding layerë¡œë¶€í„° initialized representations of single- and cross-domain sequences,</p>

    <ul>
      <li>ì¦‰ \(\mathbf E, \mathbf E^d\)ê°€ ì£¼ì–´ì ¸ìˆì„ ë•Œ, ê° expertëŠ” many-to-many sequence learningì„ ìˆ˜í–‰</li>
    </ul>
  </li>
  <li>
    <p>domain \(d\)ì˜ output : \(\begin{aligned}
&amp; \left(\mathbf{Y}^d\right)^{\text {single }}=h^d\left(f^d\left(\mathbf{E}^d\right)\right) \\
&amp; f^d\left(\mathbf{E}^d\right)=\sum_{k=1}^j g^d\left(\mathbf{E}^d\right)_k \mathrm{SG}\left(f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)\right)+\sum_{k=j+1}^K g^d\left(\mathbf{E}^d\right)_k f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)
\end{aligned}\)</p>

    <ul>
      <li>\(h^d\) : the tower network for domain \(d\)â€‹ (Fig. 2(c-7))
        <ul>
          <li>feed-forward network with layer normalization</li>
        </ul>
      </li>
      <li>\(f^d\) : the multi-gated mixture of the sequential experts layer</li>
      <li>\(SG\)â€‹ :  the stopgradient operation (Fig. 2(c-4))
        <ul>
          <li>forward passì—ì„œëŠ” identity function</li>
          <li>backward passì—ì„œëŠ” SG ì•ˆì— ìˆëŠ” ê²ƒë“¤ì˜ gradientëŠ” drop</li>
          <li>ìœ„ ì‹ì—ì„œëŠ” \(j+1 \sim K\)ë²ˆì§¸ expertsë§Œ unique sequential pattern of single-domain sequencesë¥¼ í•™ìŠµ</li>
        </ul>
      </li>
      <li>\(f_{\text {TRM }}^k\)â€‹ :  the ğ‘˜-th transformerbased sequential expert (Fig. 2(c-3))</li>
      <li>\(g^d\) :  gating network for domain \(d\) (Fig. 2(c-6))
        <ul>
          <li>\(g^d\left(\mathbf{E}^d\right)=\operatorname{softmax}\left(W_g^d \mathbf{E}^d\right)\) where \(W_g^d \in \mathbb{R}^{K \times d T}\) is trainable FC</li>
        </ul>
      </li>
      <li>The \(t\)-th element of \(\mathrm{Y}^{\text {single }}\)ëŠ” \(\left(y_t^d\right)^{\text {single }}\)</li>
    </ul>
  </li>
  <li>
    <p><strong>ë‹¤ìŒìœ¼ë¡œ CDSRì„ ë³´ì</strong></p>
  </li>
  <li>
    <p>ACMoE module : \(\begin{aligned}
&amp; \mathbf{Y}^{\text {cross }}=h^{\text {cross }}\left(f^{\text {cross }}(\mathbf{E})\right) \\
&amp; f^{\text {cross }}(\mathbf{E})=\sum_{k=1}^j g^{\text {cross }}(\mathbf{E})_k f_{\mathrm{TRM}}^k(\mathbf{E})+\sum_{k=j+1}^K g^{\text {cross }}(\mathbf{E})_k \operatorname{SG}\left(f_{\mathrm{TRM}}^k(\mathbf{E})\right)
\end{aligned}\)</p>

    <ul>
      <li>\(h^{cross}\) :  the tower network (Fig. 2(c-9))</li>
      <li>\(f^{\text {cross }}\) : the multi-gated mixture of sequential experts layer for a cross-domain sequence</li>
      <li>\(SG\)ëŠ” \(j+1\sim K\)-th \(f^k_{TRM}\)ì—ë§Œ ì‚¬ìš©
        <ul>
          <li>ê·¸ëŸ¬ë©´ \(1\sim j\)â€‹ë²ˆì§¸ expertsê°€cross-domain sequencesì—ì„œ the distinct sequential patterns presentë¥¼ í•™ìŠµ</li>
        </ul>
      </li>
      <li>\(g^{\text {cross }}(\mathbf{E})=\operatorname{softmax}\left(W_a^{c r o s s} \mathbf{E}\right)\)â€‹ : gating network for the crossdomain sequence (Fig. 2(c-8))</li>
    </ul>
  </li>
  <li>
    <p>\(\left(y_t^d\right)^{\text {single }} \text { and }\left(y_t\right)^{\text {cross }}\)ëŠ” two representations of different views for the same item</p>
  </li>
</ul>

<h3 id="422-transformer-experts">4.2.2. Transformer Experts</h3>

<ul>
  <li>ê°ê°ì˜ Multi-head Self-Attentionì— \(Z \in \mathbb{R}^{T \times r}\) ê°€ linear transformation
    <ul>
      <li>\(\to\) \(\text { queries } Q_i \in \mathbb{R}^{T \times r / p} \text {, keys } K_i \in \mathbb{R}^{T \times r / p} \text {, } \text { values } V_i \in \mathbb{R}^{T \times r / p}\)ê°€ ë¨</li>
    </ul>
  </li>
  <li>
    <p>\(\begin{aligned}
&amp; \operatorname{Attn}\left(Q_i, K_i, V_i\right)=\operatorname{softmax}\left(\frac{Q_i K_i^{\top}}{\sqrt{r / p}}\right) V_i, Q_i=Z \mathrm{~W}_i^Q, K_i=Z \mathrm{~W}_i^K, V_i=Z \mathrm{~W}_i^V
\end{aligned}\) ê±°ì³ final outputì€ \(\mathbf{H} \in \mathbb{R}^{T \times r}\)</p>
  </li>
  <li>ë§ˆì§€ë§‰ìœ¼ë¡œ \(\operatorname{FFN}(\mathbf{H})=\left[\mathrm{FC}\left(\mathbf{H}_1\right)\left\\mid \mathrm{FC}\left(\mathbf{H}_2\right)\right\\mid , \ldots, \\mid  \mathrm{FC}\left(\mathbf{H}_T\right)\right]\)
    <ul>
      <li>where \(\mathrm{FC}\left(\mathbf{H}_t\right)=\operatorname{GELU}\left(\mathbf{H}_t \mathrm{~W}_1+b_1\right) \mathrm{W}_2+b_2\)</li>
      <li>\(\mathbf{H}_t\) : ğ‘¡-th representation of \(\mathbf{H}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-loss-correction-with-negative-transfer-gap-lc-ntg">4.3. Loss Correction with Negative Transfer Gap (LC-NTG)</h3>

<h3 id="431--single-domain-item-prediction">4.3.1.  Single-Domain Item Prediction</h3>

<ul>
  <li>Fig 2(e-1)</li>
  <li>single domoin sequence \(X_{1: t}^d\)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ ì•„ì´í…œ \(x_{t+1}^d\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ pairwise ranking lossë¥¼ ì‚¬ìš©
    <ul>
      <li>ì¦‰ \(l_t^d=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}^d\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}^d\right)\right), \mathcal{L}_{\text {single }}^d=\sum_{t=1}^T l_t^d\)
        <ul>
          <li>where \(x^{d+}\) : ground-truth item paired with a negative item \(x^{d-}\) sampled froem Unif</li>
          <li>\(P\left(x_{t+1}^d=x^d \mid X_{1: t}^d\right)\) = \(\sigma\left(\left(y_t^d\right)^{\text {single }} \cdot M\left(x^d\right)\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="432-cross-domain-item-prediction">4.3.2. Cross-Domain Item Prediction</h3>

<ul>
  <li>CDSR \(l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t\)
    <ul>
      <li>where \(P\left(x_{t+1}^d=x^d \mid X_{1: t}\right) \text { is obtained by } \sigma\left(\left(y_t\right)^{\text {cross }} \cdot M\left(x^d\right)\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="433--calculating-the-negative-transfer-gap">4.3.3.  Calculating the Negative Transfer Gap</h3>

<ul>
  <li>ì´ì œ NTGë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. \(\phi_\pi(d)=\sum_{t=1}^T\left(l_t^d-l_t\right)\)
    <ul>
      <li>where \(l_t^d\) and \(l_t\) are losses of the SDSR and CDSR tasks in time step \(t\) for the domain \(d\), respectively, calculated with our model \(\pi\)</li>
    </ul>
  </li>
  <li>\(\lambda=\left(\lambda_1, \lambda_2, \ldots, \lambda_{\mid \mathcal{D}\mid }\right)\)ë¥¼ ê° domainì—ì„œì˜ NTGë¼ê³  í•˜ë©´ \(\lambda_d \leftarrow \operatorname{softmax}\left(\alpha * \lambda_d+\beta * \phi_\pi(d) ; \delta\right)\)ë¡œ ê³„ì‚°
    <ul>
      <li>where \(\alpha \text { and } \beta\) are learnable parameters</li>
    </ul>
  </li>
</ul>

<h3 id="434-loss-correction">4.3.4. Loss Correction</h3>

<ul>
  <li>NTGëŠ” weight for the cross-domain item prediction lossë¡œ í™œìš©ë¨
    <ul>
      <li>lossëŠ” \(l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t\)</li>
    </ul>
  </li>
  <li>re-aggregate : multiplying the relative NTG for each domain separately
    <ul>
      <li>
\[\mathcal{L}_{\text {cross }}^{l c} = =\sum_{t=1}^T \sum_{d=1}^{\mid \mathcal{D}\mid } \lambda_d \log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right)\]
      </li>
    </ul>
  </li>
  <li>ì´ë ‡ê²Œ í•˜ë©´ NTGê°€ ë°œìƒí•˜ëŠ” domainì—ì„œì˜ gradient flowë¥¼ ì¤„ì´ëŠ” ê²ƒ</li>
</ul>

<h2 id="44-single-cross-mutual-information-maximization-sc-mim">4.4. Single-Cross Mutual Information Maximization (SC-MIM)</h2>

<ul>
  <li>SC-MIM: SDSR and CDSR tasks ì‚¬ì´ì˜ ì •ë³´ë¥¼ ì˜ transferí•˜ê¸° ìœ„í•œ ë°©ë²•
    <ul>
      <li>mutual informationìœ¼ë¡œ ë‘ tasksì˜ correlation signalsë¥¼ íŒŒì•…</li>
      <li>mutual information: \(I(X, Y)=D_{K L}(p(X, Y) \\mid  p(X) p(Y))=\mathbb{E}_{p(X, Y)}\left[\log \frac{p(X, Y)}{p(X) p(Y)}\right]\)â€‹</li>
    </ul>
  </li>
  <li>í•˜ì§€ë§Œ ì´ mutual informationì„ high-dimdì—ì„œ êµ¬í•˜ëŠ” ê±´ ì–´ë µê¸° ë•Œë¬¸ì— lower boundë¡œ InfoNCEë¥¼ ì‚¬ìš©
    <ul>
      <li>lower bound : \(I(X, Y) \geq \mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\mathbb{E}_{q(\hat{Y})}\left(\log \sum_{\hat{y} \in \hat{Y}} \exp \rho_\theta(x, \hat{y})\right)\right]+\log \mid \hat{Y}\mid\)
        <ul>
          <li>where \(x, y\)ëŠ” ê°™ì€ inputì˜ ì„œë¡œ ë‹¤ë¥¸ view points</li>
          <li>\(\rho_\theta\) ëŠ” similarity function,</li>
        </ul>
      </li>
      <li>InfoNCEë¥¼ maximizingí•˜ëŠ” ê²ƒì€ standard cross-entropy lossë¥¼ maximizingí•˜ëŠ” ê²ƒê³¼ ê°™ìŒ
        <ul>
          <li>: \($\mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\log \sum_{\hat{y} \in Y} \exp \rho_\theta(x, \hat{y})\right]\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ì•„ë¬´íŠ¼ ëŒì•„ì™€ì„œ ìš°ë¦¬ëŠ” \($\mathbf{Y}^{\text {single }}$ and $\mathbf{Y}^{\text {cross }}\)ì˜ mutual informationì„ maximizingí•˜ê³  ì‹¶ìŒ
    <ul>
      <li>ê·¸ëŸ¬ë¯€ë¡œ cross-domain representation \(\mathbf{Y}^{\text {ross }}\)ë¥¼ domainë³„ë¡œ splití•´ì„œ \((\mathbf{Y^d})^{\text {ross }}\) êµ¬í•˜ê³ </li>
      <li>ì•„ë˜ ì‹ì²˜ëŸ¼ ê³„ì‚°
        <ul>
          <li>: \(\begin{aligned} &amp; \mathcal{L}_{S C-M I M}^d=\rho\left(\left(\mathbf{Y}^d\right)^{\text {single }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)-\log \sum_{u-} \exp \left(\rho\left(\left(\mathbf{Y}^d\right)^{\text {single- }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)\right)\end{aligned}\)</li>
          <li>where \(u-\)ëŠ” other users in a training batch,</li>
          <li>\(\left(\mathbf{Y}^d\right)^{\text {single- }}\)ëŠ” subsequence of domain \(ğ‘‘\) of user \(ğ‘¢âˆ’\)â€‹</li>
          <li>\(\rho(\cdot, \cdot)\)ëŠ” \(\rho(U, V)=\sigma\left(U^{\top} \cdot W^H \cdot V\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="45-model-training-and-evaluation">4.5. Model Training and Evaluation</h3>

<ul>
  <li>Total training loss : \(\mathcal{L}=\eta\left(\sum_{d=1}^{\mid \mathcal{D}\mid }\left(\mathcal{L}_{\text {single }}^d\right)+\mathcal{L}_{\text {cross }}^{l c}\right)+(1-\eta) \sum_{d=1}^{\mid \mathcal{D}\mid } \mathcal{L}_{S C-M I M}^d\)
    <ul>
      <li>where \(\eta\) is the harmonic factor</li>
      <li>evaluationí•  ë•Œì—ëŠ” cross-domain representationë§Œ ì‚¬ìš©</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-dataset">5.1. Dataset</h3>

<h3 id="52-experimental-setting">5.2. Experimental Setting</h3>

<ul>
  <li>ë¨¼ì € Amazon datasetê³¼ Telco datasetì— ëŒ€í•œ ì„±ëŠ¥</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table23.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>
    <p>Research Questions:</p>

    <ul>
      <li>
        <p>(RQ1): Does the performance of our model surpass the current stateof-the-art baselines in practical applications that involve more than three domains?</p>
      </li>
      <li>
        <p>(RQ2): Can our model effectively address the challenge of negative transfer across all domains in the CDSR task?</p>
      </li>
      <li>
        <p>(RQ3): What is the impact of various components of our model on its performance in CDSR tasks?</p>
      </li>
      <li>
        <p>(RQ4): How do variations in hyper-parameter settings influence the performance of our model?</p>
      </li>
      <li>
        <p>(RQ5): How does the model perform when deployed online ?</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="53-performance-evaluation-rq1">5.3. Performance Evaluation (RQ1)</h3>

<ul>
  <li>First, The effectiveness of our model can be observed.
    <ul>
      <li>ë‹¤ë¥¸ baseline modelsë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨</li>
    </ul>
  </li>
  <li>Second, Integrating information from all domains simultaneously in a model can improve performance in each domain compared to modeling a pairwise domain-domain relationship.
    <ul>
      <li>ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” CDSR taskì—ì„œ domainë¼ë¦¬ì˜ ì •ë³´ë¥¼ ê²°í•©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="54-discussion-of-the-negative-transfer-rq2">5.4. Discussion of the negative transfer (RQ2)</h3>

<ul>
  <li>ê¸°ì¡´ baseline modelsëŠ” SDSRë³´ë‹¤ CDSRì˜ ì„±ëŠ¥ì´ ë” ì•ˆì¢‹ì•˜ì§€ë§Œ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ëª¨ë¸ì€ ê·¸ë ‡ì§€ ì•Šë‹¤</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table4.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="55-discussion-of-model-variants-rq3">5.5 Discussion of Model Variants (RQ3)</h3>

<ul>
  <li>LC-NTG, SC-MIM, ACMoE ì„¸ ê°€ì§€ components ëª¨ë‘ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ í•„ìš”í•˜ë‹¤</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table5.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="56-hyperparameter-analysis-rq4">5.6. Hyperparameter Analysis (RQ4)</h3>

<p><img src="/assets/img/timeseries/SyNCRec/fig3.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="6-online-ab-test-rq5">6. Online A/B Test (RQ5)</h2>

<p>pass</p>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>Negative transferë¥¼ ë‹¤ë£¨ëŠ” CDSR frameworkë¥¼ ì œì•ˆ
    <ul>
      <li>Negative transferë¥¼ ì¸¡ì •í•˜ê³  prediction lossì˜ weightë¡œ í™œìš©</li>
    </ul>
  </li>
  <li>SDSR and CDSR tasksì˜ ì •ë³´ë¥¼ êµí™˜ì‹œí‚¤ëŠ” Auxiliary loss ì œì•ˆ</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[SIGIR'24 Best Paper](https://arxiv.org/pdf/2407.11245)]]></summary></entry><entry><title type="html">MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-05-MG-TSD/" rel="alternate" type="text/html" title="MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)" /><published>2024-08-05T00:00:00+09:00</published><updated>2024-09-03T15:04:04+09:00</updated><id>http://localhost:4000/timeseries/MG-TSD</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-05-MG-TSD/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>ì–´ë–»ê²Œ Diffusion modelì˜ ì„±ëŠ¥ì„ time series forecastingì— í™œìš©í•  ìˆ˜ ìˆëŠ”ê°€</li>
  <li><strong>M</strong>ulti-<strong>G</strong>ranularity <strong>T</strong>ime <strong>S</strong>eries <strong>D</strong>iffusion <strong>(MG- TSD)</strong>
    <ul>
      <li>leveraging the inherent granularity levels</li>
      <li>intuition: diffusion stepì—ì„œ ì ì°¨ gaussian noiseë¡œ ë§Œë“œëŠ” ê²ƒì„ fine \(\to\) coarseë¡œ ì´í•´</li>
      <li>novel multi-granularity guidance diffusion loss function</li>
      <li>method to effectively utilize coarse-grained data across various granularity levels</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>ìµœê·¼ì—ëŠ” Time series predictive ëª©ì ìœ¼ë¡œ conditional generative modelì„ í™œìš©
    <ul>
      <li>ì²˜ìŒì—ëŠ” Auto-regressive ë°©ì‹ìœ¼ë¡œ í•˜ë‹¤ê°€ CSDIë„ í–ˆì—ˆìŒ</li>
    </ul>
  </li>
  <li>í•˜ì§€ë§Œ ë¬¸ì œëŠ” Diffusionì´ instabilityí•˜ë‹¤ëŠ” ì 
    <ul>
      <li>Imageì—ì„œ diffusionì€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì–´ì„œ ì¥ì ì´ì—ˆëŠ”ë°</li>
      <li>ì‹œê³„ì—´ ì˜ˆì¸¡ ê´€ì ì—ì„œëŠ” ê·¸ê²ƒì´ ì„±ëŠ¥ í•˜ë½ì˜ ì›ì¸ì´ ë  ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/MG-TSD/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>Diffusion stepì—ì„œ ì ì°¨ gaussian noiseë¡œ ë§Œë“œëŠ” ê²ƒì„ fine \(\to\) coarseë¡œ ì´í•´í•œë‹¤ë©´
    <ul>
      <li>Diffusion modelì´ <strong>labelsì„ the source of guidance</strong>ë¡œ í•„ìš”ë¡œ í•˜ëŠ” ë¬¸ì œì—ì„œ</li>
      <li>Time seriesì˜ fine featureê°€ ê·¸ labels as the source of guidance ì—­í• ì„ í•  ìˆ˜ ìˆì„ ê²ƒ</li>
    </ul>
  </li>
  <li>MG-TSDì—ì„œëŠ” coarse-grained dataë¥¼ denoising process í•™ìŠµì˜ guideë¡œ ì¤€ë‹¤.
    <ul>
      <li>\(\to\) intermediate latent statesì—ì„œì˜ constraintsë¡œ ì‘ìš©</li>
      <li>\(\to\) coarser featureëŠ” ë” ë¹ ë¥´ê²Œ ìƒì„±í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ê·¸ë§Œí¼ finer feature recoveryë„ ìš©ì´</li>
      <li>\(\to\) coarse-grained dataì˜ trendì™€ patternì„ ë³´ì¡´í•˜ëŠ” samplingì„ ë§Œë“¬</li>
      <li>\(\to\)â€‹ reduces variability and results in high-quality predictions</li>
    </ul>
  </li>
</ul>

<h2 id="2-background">2. Background</h2>

<ul>
  <li>TimeGrad Model
    <ul>
      <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad Paper</a> <a href="https://lpppj.github.io/timeseries/2024-07-09-Timegrad">TimeGrad Review</a></li>
    </ul>
  </li>
  <li>\(\boldsymbol{X}^{(1)}=\left[\boldsymbol{x}_1^1, \ldots, \boldsymbol{x}_t^1, \ldots, \boldsymbol{x}_T^1\right]\) is the original observed data, where \(t \in[1, T]\) and \(\boldsymbol{x}_t \in \mathbb{R}^D\)
    <ul>
      <li>Mathematical expressions: \(q_{\mathcal{X}}\left(\boldsymbol{x}_{t_0: T}^1 \mid\left\{\boldsymbol{x}_{1: t_0-1}^1\right\}\right)=\prod_{t=t_0}^T q_{\mathcal{X}}\left(\boldsymbol{x}_t^1 \mid\left\{\boldsymbol{x}_{1: t-1}^1\right\}\right)\)</li>
    </ul>
  </li>
</ul>

<h2 id="3-method">3. Method</h2>

<h3 id="31-mg-tsd-model-architecture">3.1. MG-TSD Model Architecture</h3>

<p><img src="/assets/img/timeseries/MG-TSD/fig2.png" alt="ê·¸ë¦¼2" /></p>

<h3 id="multi-granularity-data-generator">Multi-granularity Data Generator</h3>

<p>: for generating multi-granularity data from observations</p>

<ul>
  <li>historical sliding windows with different sizesë¥¼ í†µí•´ fine \(\to\) coaseë¡œ smoothing out</li>
  <li>ì¦‰ \(\boldsymbol{X}^{(g)}=f\left(\boldsymbol{X}^{(1)}, s^g\right)\) with pre-defined sliding window size \(s^g\)</li>
  <li>ì´ ë•Œ non-overlappingí•˜ê²Œ windowë¥¼ slicingí•˜ê³ , \(\boldsymbol{X}^{(g)}\)ëŠ” \(s^g\)ë²ˆ ë³µì œí•´ì„œ \([1, T]\)ë¡œ ë§ì¶¤</li>
</ul>

<h3 id="temporal-process-module">Temporal Process Module</h3>

<p>: designed to capture the temporal dynamics of the multi-granularity time series data</p>

<ul>
  <li>ê°ê°ì˜ granularity level \(g\)ì—ì„œ GRUì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ timestep \(t\)ë¥¼ \(\mathbf{h}_t^g\)ë¡œ encoding</li>
</ul>

<h3 id="guided-diffusion-process-module">Guided Diffusion Process Module</h3>

<p>: designed to generate stable time series predictions at each timestep \(t\)</p>

<ul>
  <li>multi-granularity dataë¥¼ í™œìš©í•˜ì—¬ diffusion learning processì˜ guideë¡œ ì œê³µ</li>
</ul>

<h3 id="32-multi-granularity-guided-diffusion">3.2. Multi-Granularity Guided Diffusion</h3>

<p>: Guided Diffusion Process Moduleì— ëŒ€í•œ details</p>

<h3 id="321-coarse-grained-guidance">3.2.1. Coarse-grained Guidance</h3>

<p>: the derivation of a heuristic guidance loss for the two- granularity case</p>

<ul>
  <li>consider two granularities at a fixed timestep \(t\)
    <ul>
      <li>: \(\text { finest-grained data } \boldsymbol{x}_t^{g_1}\left(g_1=1\right) \text { from } \boldsymbol{X}^{\left(g_1\right)}\) &amp; \(\text { coarse-grained data } \boldsymbol{x}_t^g \text { from } \boldsymbol{X}^{(g)}\)â€‹</li>
    </ul>
  </li>
  <li>ë¨¼ì € coarse-grained targets \(x^g\)ë¥¼ intermediate diffusion step \(N_*^g \in[1, N-1]\)ì— introduce
    <ul>
      <li>ì¦‰ objective functionì´ \(\log p_\theta\left(\boldsymbol{x}^g\right)\)</li>
    </ul>
  </li>
  <li>ê·¸ëŸ¬ë©´ denoising processì—ì„œ recoverëœ coarser featuresëŠ” ì‹¤ì œ coarse-grained sampleì˜ ì •ë³´ë¥¼ ë§ì´ ê°€ì§€ê³  ìˆì„í…Œë‹ˆ
    <ul>
      <li>fine-grained featureë¥¼ recoverí•˜ê¸°ë„ ì‰¬ì›Œì§ˆ ê²ƒ</li>
    </ul>
  </li>
  <li>\(\theta\)-parameterized: \(p_\theta\left(\boldsymbol{x}_{N_*^g}\right)=\int p_\theta\left(\boldsymbol{x}_{N_*^g: N}\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}=\int p\left(\boldsymbol{x}_N\right) \prod_{N_*^g+1}^N p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}\)â€‹
    <ul>
      <li>where \(\boldsymbol{x}_N \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I}), p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right)=\mathcal{N}\left(\boldsymbol{x}_{n-1} ; \boldsymbol{\mu}_\theta\left(\boldsymbol{x}_n, n\right), \boldsymbol{\Sigma}_\theta\left(\boldsymbol{x}_n, n\right)\right)\)â€‹</li>
    </ul>
  </li>
  <li>ì´ê±´ \(N_*^g\)ë²ˆì§¸ diffusion stepì—ì„œ \(N\)ë²ˆì§¸ê¹Œì§€ ì´ \(N-N_*^g\) stepsì˜ forward processì´ë¯€ë¡œ
    <ul>
      <li>the guidance objective: \(\log p_\theta\left(\boldsymbol{x}^g\right)=\log \int p_\theta\left(\boldsymbol{x}_{N_*^g}^g, \boldsymbol{x}_{N_*^g+1}^g, \ldots, \boldsymbol{x}_N^g\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}^g\)â€‹</li>
    </ul>
  </li>
  <li>sampleì— ëŒ€í•œ loss ëŒ€ì‹  noiseì— ëŒ€í•œ loss ì‚¬ìš©
    <ul>
      <li>loss: \(\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}^g, n}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_n^g, n\right)\right\|^2\right]\)â€‹</li>
      <li>where \(\boldsymbol{x}_n^g=\left(\prod_{i=N_{\boldsymbol{z}}^g}^n \alpha_i^1\right) \boldsymbol{x}^g+\sqrt{ } \mathbf{1}-\prod_{i=N^g}^n \alpha_i^1 \boldsymbol{\epsilon} \text { and } \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})\)</li>
    </ul>
  </li>
</ul>

<h3 id="322-multi-granularity-guidance">3.2.2. Multi-granularity Guidance</h3>

<ul>
  <li>
    <p>Multi-granularity Data Generatorê°€ Gê°œì˜ granularity levelsë§ˆë‹¤ data ìƒì„±: \(\boldsymbol{X}^{(1)}, \boldsymbol{X}^{(2)}, \ldots, \boldsymbol{X}^{(G)}\)</p>
  </li>
  <li>Share ratio: \(r_g:=1-\left(N_*^g-1\right) / N\)
    <ul>
      <li>: the shared percentage of variance schedule between the gth granularity data and the finest-grained data</li>
      <li>ex. finest-grained dataì—ì„œëŠ” \(N_*^1=1 \text { and } r^1=1\)â€‹
        <ul>
          <li>variance schedule for granularity \(g\) is, \(\alpha_n^g\left(N_*^g\right)= \begin{cases}1 &amp; \text { if } n=1, \ldots, N_*^g \\ \alpha_n^1 &amp; \text { if } n=N_*^g+1, \ldots, N\end{cases}\)â€‹</li>
          <li>and \(\left\{\beta_n^g\right\}_{n=1}^N=\left\{1-\alpha_n^g\right\}_{n=1}^N\)â€‹</li>
          <li>accordingly, \(a_n^g\left(N_*^g\right)=\prod_{k=1}^n \alpha_k^g \text {, and } b_n^g\left(N_*^g\right)=1-a_n^g\left(N_*^g\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ì´ ë•Œ \(N^g_*\)ëŠ” : represents the diffusion index for starting sharing the variance schedule across granularity level \(g \in\{1, \ldots, G\}\)</li>
  <li>ì´ë ‡ê²Œ ë˜ë©´ larger coarser granularity levelì¼ìˆ˜ë¡ \(N^g_*\)ê°€ ì»¤ì§„ë‹¤ëŠ” ëœ»
    <ul>
      <li>ì¦‰ coarserí• ìˆ˜ë¡ fineí•œ ì •ë³´ëŠ” ì¤„ì–´ë“¤í…Œë‹ˆ ì´ì „ diffusion stepê³¼ ì°¨ì´ê°€ í¬ì§€ ì•Šì„ ê²ƒ</li>
      <li>ê·¸ëŸ¬ë‹ˆê¹Œ \(N^g_*\)ë¥¼ í¬ê²Œ í•´ì„œ fine-grained featureë¥¼ ìƒì„±í•  stepsë¥¼ ë§ì´ ì¤Œ</li>
    </ul>
  </li>
  <li>Then the guidance loss function \(L^{(g)}(\theta)\) for \(g\)-th granularity \(x^g_{n,t}\) at timestep \(t\) and diffusion step \(n\),
    <ul>
      <li>can be expressed as: \(L^{(g)}(\theta)=\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, n} \|\left(\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon}, n, \mathbf{h}_{t-1}^g\right) \|_2^2\right.\)</li>
      <li>where \(\mathbf{h}_t^g=\mathrm{RNN}_\theta\left(\boldsymbol{x}_t^g, \mathbf{h}_{t-1}^g\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="training">Training</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm1.png" alt="ê·¸ë¦¼41" /></p>

<ul>
  <li>ìµœì¢…ì ì¸ training objectivesëŠ” ëª¨ë“  granularitiesì—ì„œì˜ Lossì˜ weighted sum
    <ul>
      <li>: \(L^{\text {final }}=\omega^1 L^{(1)}(\theta)+L^{\text {guidance }}(\theta)=\sum_{q=1}^G \omega^g \mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, t}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_{n, t}^g, n, \mathbf{h}_{t-1}^g\right)\right\|^2\right]\)</li>
      <li>where \(\boldsymbol{x}_{n, t}^g=\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon} \text { and } \sum_{g=1}^G \omega^g=1\)</li>
      <li>ì´ ë•Œ denoising networkì˜ parametersëŠ” shared across all granularities</li>
    </ul>
  </li>
</ul>

<h3 id="inference">Inference</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm2.png" alt="ê·¸ë¦¼42" /></p>

<ul>
  <li>ìš°ë¦¬ì˜ ëª©í‘œëŠ” íŠ¹ì •í•œ prediction stepsì— ëŒ€í•œ  finest-grained dataì— ëŒ€í•œ ì˜ˆì¸¡
    <ul>
      <li>\(t_0-1\) ì‹œì ê¹Œì§€ ì£¼ì–´ì¡Œë‹¤ë©´ ì•„ë˜ algorithm 2ë¥¼ ë”°ë¼ \(t_0\)ì‹œì ì— ëŒ€í•œ ë°ì´í„° ìƒì„±,</li>
      <li>ìš°ë¦¬ê°€ ì›í•˜ëŠ” forecast horizonì´ ë  ë•Œê¹Œì§€ ë°˜ë³µ</li>
      <li>hidden statesì— conditional inputsìœ¼ë¡œ ë¬´ì—‡ì„ ë„£ëŠ”ì§€ì— ë”°ë¼ì„œ ê·¸ì— í•´ë‹¹í•˜ëŠ” granularity levelsë¡œ ìƒ˜í”Œë§</li>
    </ul>
  </li>
</ul>

<h3 id="selection-of-share-ratio">Selection of share ratio</h3>

<ul>
  <li>ìœ„ì—ì„œëŠ” share ratio \(r_g:=1-\left(N_*^g-1\right) / N\)ë¥¼ heuristicí•˜ê²Œ \(N^g_*\)ì— ë”°ë¼ ê²°ì •ë˜ë„ë¡ í–ˆìŒ
    <ul>
      <li>Diffusion step \(N^g_*\)ëŠ” \(q\left(\boldsymbol{x}^g\right) \text { and } p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\)ì˜ ê±°ë¦¬ê°€ ê°€ì¥ ì‘ì„ ë•Œë¡œ ì„¤ì • !</li>
      <li>: \(\to\) \(N_*^g:=\arg \min_n \mathcal{D}\left(q\left(\boldsymbol{x}^g\right), p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\right)\)â€‹
        <ul>
          <li>\(\mathcal{D}\)ëŠ” ë‘ ë¶„í¬ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” metricì´ ë¨ (KL-divergence, â€¦)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/MG-TSD/table12.png" alt="ê·¸ë¦¼112" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig3.png" alt="ê·¸ë¦¼3" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/table3.png" alt="ê·¸ë¦¼13" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig4.png" alt="ê·¸ë¦¼4" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Multi-Granularity Time Series Diffusion (MG-TSD)
    <ul>
      <li>leverages the inherent granularity levels within the data, as given targets at intermediate diffusion steps to guide the learning process of diffusion models</li>
      <li>to effectively utilize coarse-grained data across various granularity levels.</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=CZiY6OLktd)]]></summary></entry><entry><title type="html">Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/" rel="alternate" type="text/html" title="Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)" /><published>2024-08-04T00:00:00+09:00</published><updated>2024-08-04T18:52:22+09:00</updated><id>http://localhost:4000/timeseries/Diffusion-TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Diffusion-TS: uses an encoder-decoder transformer with disentangled temporal representations</li>
  <li>train the model to directly reconstruct the <strong>sample</strong> instead of the <strong>noise</strong> in each diffusion step</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Synthesizing realistic time series dataëŠ” ë°ì´í„° ê³µìœ ê°€ ê°œì¸ì •ë³´ ì¹¨í•´ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì‚¬ë¡€ì—ì„œì˜ ì†”ë£¨ì…˜</li>
  <li>ì§€ê¸ˆê¹Œì§€ Diffusionì„ í™œìš©í•œ time series generationì€ ëŒ€ë¶€ë¶„ task-agnostic generation
    <ul>
      <li>ì²«ë²ˆì§¸ ë¬¸ì œëŠ” RNN-based Autoregressive ë°©ì‹: limited long-range performance due to error accumulation and slow inference speed</li>
      <li>ë‘ë²ˆì§¸ ë¬¸ì œëŠ” diffusion processì—ì„œ noiseë¥¼ ì¶”ê°€í•  ë•Œ ì‹œê³„ì—´ì˜ combinations of independent components(trend, seasonal, â€¦)ì´ ë§ê°€ì§€ëŠ” ë¬¸ì œ (íŠ¹íˆ ì£¼ê¸°ì„±ì´ ëšœë ·í•œ ê²½ìš° interpretabilityê°€ ë¶€ì¡± <a href="https://openreview.net/pdf?id=rdjeCNUS6TG">Liu et al., (2022)</a>)</li>
    </ul>
  </li>
  <li><strong>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Transformerë¥¼ í™œìš©í•˜ì—¬ trendì™€ seasonalì„ non-autoregressiveí•˜ê²Œ ìƒì„±</strong>
    <ul>
      <li>by imposing different forms of constraints on different representations.</li>
    </ul>
  </li>
  <li>For Reconstruct the <strong>samples</strong> rather than the <strong>noises</strong> in each diffusion step, Fourier-based loss ì‚¬ìš©</li>
</ul>

<h2 id="2-problem-statement">2. Problem Statement</h2>

<ul>
  <li>Nê°œë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì…‹ \(D A=\left\{X_{1: \tau}^i\right\}_{i=1}^N\)â€‹
    <ul>
      <li>where \(X_{1: \tau}=\left(x_1, \ldots, x_\tau\right) \in \mathbb{R}^{\tau \times d}\)</li>
    </ul>
  </li>
  <li>ëª©í‘œëŠ” Gaussian vectors \(Z_i=\left(z_1^i, \ldots, z_t^i\right) \in \mathbb{R}^{\tau \times d \times T}\)ë¥¼ DAì™€ ë¹„ìŠ·í•œ \(\hat{X}_{1: \tau}^i=G\left(Z_i\right)\)ë¡œ ë°”ê¾¸ëŠ” Generator \(G\)ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒ</li>
  <li>Time series modelì€ trendì™€ ì—¬ëŸ¬ ê°œì˜ seasonalityë¡œ êµ¬ì„± : \(x_j=\zeta_j+\sum_{i=1}^m s_{i, j}+e_j\)
    <ul>
      <li>where \(j=0,1, \ldots, \tau-1\)</li>
      <li>\(x_j\) : observed time series</li>
      <li>\(\zeta_j\): trend component</li>
      <li>\(s_{i,j}\): \(i\)-th seasonal component</li>
      <li>\(e_j\): remainder part (contatins the noise and some outliers at time t)</li>
    </ul>
  </li>
</ul>

<h2 id="3-diffusion-ts-interpretable-diffusion-for-time-series">3. Diffusion-TS: Interpretable Diffusion for Time Series</h2>

<ul>
  <li>ì´ëŸ¬í•œ interpretable decomposition architectureì˜ ê·¼ê±°ëŠ” 3ê°€ì§€
    <ul>
      <li>ì²«ì§¸, disentangled patterns in the diffusion modelì€ ì•„ì§ ì—°êµ¬ë˜ì§€ ì•ŠìŒ</li>
      <li>ë‘˜ì§¸, specific designs of architecture and objective ë•ë¶„ì— interpretable</li>
      <li>ì…‹ì§¸, explainable disentangled representations ë•ë¶„ì— complex dynamics íŒŒì•…</li>
    </ul>
  </li>
</ul>

<h3 id="31-diffusion-framework">3.1. Diffusion Framework</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>Forward process
    <ul>
      <li>\(x_0 \sim q(x)\)ì—ì„œ ì ì  noisy into Gaussian noise \(x_T \sim \mathcal{N}(0, \mathbf{I})\)</li>
      <li>Parameterization: \(q\left(x_t \mid x_{t-1}\right)=\mathcal{N}\left(x_t ; \sqrt{ } 1-\beta_t x_{t-1}, \beta_t \mathbf{I}\right) \text { with } \beta_t \in(0,1)\)</li>
    </ul>
  </li>
  <li>
    <p>Reverse process</p>

    <ul>
      <li>
        <p>ë°˜ëŒ€ë¡œ \(p_\theta\left(x_{t-1} \mid x_t\right)=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)\)</p>
      </li>
      <li>
        <p>MSE: \(\mathcal{L}\left(x_0\right)=\sum_{t=1}^T \underset{q\left(x_t \mid x_0\right)}{\mathbb{E}}\left\|\mu\left(x_t, x_0\right)-\mu_\theta\left(x_t, t\right)\right\|^2\)</p>
        <ul>
          <li>where \(\mu\left(x_t, x_0\right) \text { is the mean of the posterior } q\left(x_{t-1} \mid x_0, x_t\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="32-decomposition-model-architecture">3.2. Decomposition Model Architecture</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig2.png" alt="ê·¸ë¦¼2" /></p>

<ul>
  <li>Noisy sequenceê°€ encoder í†µê³¼í•´ì„œ decoderë¡œ ë“¤ì–´ì˜´ (ì´ˆë¡ìƒ‰)</li>
  <li>DecoderëŠ” multilayer structure, ê° layerì—ëŠ” <strong>Transformer Block</strong>, <strong>FFN</strong>, <strong>Trend and Fourier synthetic layer</strong>ê°€ í¬í•¨ë¨</li>
  <li>ê° layerëŠ” ì‹œê³„ì—´ì˜ ê° componentë¥¼ ìƒì„±í•˜ëŠ” ì—­í• 
    <ul>
      <li>componentì— í•´ë‹¹í•˜ëŠ” inductive biasë¥¼ ê° layerì— ë°˜ì˜í•´ì¤Œìœ¼ë¡œì¨ í•™ìŠµì´ ì‰¬ì›Œì§</li>
      <li>Trend representation captures the intrinsic trend which changes gradually and smoothly</li>
      <li>Seasonality representation illustrates the periodic patterns of the signal</li>
      <li>Error representation characterizes the remaining parts after removing trend and periodicity</li>
    </ul>
  </li>
  <li>\(w_{(\cdot)}^{i, t}\) where \(i \in 1, \ldots, D\)ëŠ” \(i\)ë²ˆì§¸ decoder blockì—ì„œì˜ diffusion step \(t\)ë¥¼ ì˜ë¯¸</li>
</ul>

<h3 id="trend-synthesis">Trend Synthesis</h3>

<ul>
  <li>smooth underlying mean of the data, which aims to model slow-varying behavior</li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ Trend \(V_{t r}^t\)ë¥¼ ìœ„í•´ Polynomial regressor ì‚¬ìš©
    <ul>
      <li>\(V_{t r}^t=\sum_{i=1}^D\left(C \cdot \operatorname{Linear}\left(w_{t r}^{i, t}\right)+\mathcal{X}_{t r}^{i, t}\right)\) where \(C=\left[1, c, \ldots, c^p\right]\)</li>
      <li>\(\mathcal{X}_{t r}^{i, t}\)ëŠ” the mean value of the output of the \(i\)â€‹-th decoder block</li>
      <li>\(C\)ëŠ” slow-varying poly spaceì¸ë°, matrix of powers of vector \(c=[0,1,2, \ldots, \tau-2, \tau-1]^T / \tau\)</li>
      <li>\(p\)ëŠ” small degree (e.g. \(p\)â€‹=3) to model low frequency behavior</li>
    </ul>
  </li>
</ul>

<h3 id="seasonality--error-synthesis">Seasonality &amp; Error Synthesis</h3>

<ul>
  <li>ì´ì œ Trend, Seasonality, Error ëª¨ë‘ ìƒê°í•´ë³´ì.</li>
  <li><strong>ê²°êµ­ ë¬¸ì œëŠ” noisy input \(x_t\)ì—ì„œ seasonal patternsë¥¼ êµ¬ë¶„í•´ë‚´ëŠ” ê²ƒ !</strong></li>
  <li>í‘¸ë¦¬ì— ì‹œë¦¬ì¦ˆì˜ trigonometric representation of seasonal componentsë¥¼ ê¸°ë°˜ìœ¼ë¡œ Fourier basesë¥¼ í™œìš©í•œ Fourier synthetic layersì—ì„œ seasonal component íŒŒì•…</li>
</ul>

<p><img src="/assets/img/timeseries/Diffusion-TS/fomula456.png" alt="ê·¸ë¦¼456" /></p>

<ul>
  <li>\(A_{i, t}^{(k)}, \Phi_{i, t}^{(k)}\) are the phase, amplitude of the \(k\)-th frequency after the DFT \(\mathcal F\) repectively</li>
  <li>\(f_k\)ëŠ” Fourier frequency of the corresponding index \(k\)</li>
  <li>ê²°êµ­ the Fourier synthetic layerëŠ” ì§„í­(amplitude)ì´ í° frequencyë¥¼ ì°¾ê³ , ê·¸ frequencyë“¤ë§Œ IDFT.
    <ul>
      <li>ê·¸ê±¸ seasonalityë¡œ ë³¸ë‹¤. (Pathformerë‘ ê°™ì€ ë°©ì‹)</li>
    </ul>
  </li>
  <li>ìµœì¢…ì ìœ¼ë¡œ original signal: \(\hat{x}_0\left(x_t, t, \theta\right)=V_{t r}^t+\sum_{i=1}^D S_{i, t}+R\)â€‹
    <ul>
      <li>\(R\): output of the last decoder block, which can be regarded as the sum of residual periodicity and other noise.</li>
    </ul>
  </li>
</ul>

<h3 id="33-fourier-based-traning-objective">3.3 Fourier-based Traning Objective</h3>

<ul>
  <li>\(\hat{x}_0\left(x_t, t, \theta\right)\)ë¥¼ directly estimate
    <ul>
      <li>Reverse process: \(x_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \hat{x}_0\left(x_t, t, \theta\right)+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} x_t+\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t z_t\)</li>
      <li>where \(z_t \sim \mathcal{N}(0, \mathbf{I}), \alpha_t=1-\beta_t \text { and } \bar{\alpha}_t=\prod_{s=1}^t \alpha_s\)â€‹</li>
    </ul>
  </li>
  <li>Reweighting strategy: \(\mathcal{L}_{\text {simple }}=\mathbb{E}_{t, x_0}\left[w_t\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2\right], \quad w_t=\frac{\lambda \alpha_t\left(1-\bar{\alpha}_t\right)}{\beta_t^2}\)â€‹
    <ul>
      <li>where \(\lambda\) is constant (i.e. 0.01)</li>
      <li>ì¦‰ small tì—ì„œ down-weighted, ëª¨ë¸ì´ larger diffusion stepì— ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“¬</li>
    </ul>
  </li>
  <li>Fourier-based loss termì´ time serie reconstructionì—ì„œëŠ” ë” ì¢‹ë‹¤ <a href="https://arxiv.org/pdf/2208.05836">Fons et al. (2022)</a>
    <ul>
      <li>: \(\mathcal{L}_\theta=\mathbb{E}_{t, x_0}\left[w_t\left[\lambda_1\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2+\lambda_2\left\|\mathcal{F} \mathcal{F} \mathcal{T}\left(x_0\right)-\mathcal{F F} \mathcal{T}\left(\hat{x}_0\left(x_t, t, \theta\right)\right)\right\|^2\right]\right]\)</li>
    </ul>
  </li>
</ul>

<h3 id="34-conditional-generation-for-time-series-applications">3.4. Conditional Generation for Time Series Applications</h3>

<ul>
  <li><strong>Conditional extensions of the Diffusion-TS</strong>, in which the modeled \(x_0\) is conditioned on targets \(y\)â€‹</li>
  <li>ëª©í‘œëŠ” pre-trained diffusion modelê³¼ the gradients of a classifierë¥¼ í™œìš©í•˜ì—¬
    <ul>
      <li>Posterior \(p\left(x_{0: T} \mid y\right)=\prod_{t=1}^T p\left(x_{t-1} \mid x_t, y\right)\)ì—ì„œ samplingí•˜ëŠ” ê²ƒ</li>
    </ul>
  </li>
  <li>\(p\left(x_{t-1} \mid x_t, y\right) \propto p\left(x_{t-1} \mid x_t\right) p\left(y \mid x_{t-1}, x_t\right)\)ì´ë¯€ë¡œ bayse theoremì„ í†µí•´ gradient update
    <ul>
      <li>Score function \(\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t, y\right)=\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t\right)+\nabla_{x_{t-1}} \log p\left(y \mid x_{t-1}\right)\)</li>
      <li>\(\log p\left(x_{t-1} \mid x_t\right)\)ì€ diffusion modelì—ì„œ ì •ì˜ë¨.</li>
      <li>\(\log p\left(y \mid x_{t-1}\right)\)ëŠ” classifierì—ì„œ parametrizeë˜ë©°, \(\nabla_{x_{t-1}} \log p\left(y \mid x_{0 \mid t-1}\right)\)ë¡œ ê·¼ì‚¬ë¨</li>
    </ul>
  </li>
  <li>ì¦‰ classifierê°€ ë†’ì€ likelihoodë¥¼ ê°€ì§„ ì˜ì—­ì—ì„œ sampleì´ ìƒì„±ë˜ë„ë¡ í•˜ëŠ” ê²ƒ
    <ul>
      <li>: \(\tilde{x}_0\left(x_t, t, \theta\right)=\hat{x}_0\left(x_t, t, \theta\right)+\eta \nabla_{x_t}\left(\left\|x_a-\hat{x}_a\left(x_t, t, \theta\right)\right\|_2^2+\gamma \log p\left(x_{t-1} \mid x_t\right)\right)\)</li>
      <li>where Conditional part \(x_a\), generative part \(x_b\)</li>
      <li>gradient termì€ reconstruction-based guidance, \(\eta\)ë¡œ ê°•ë„ ì¡°ì ˆ</li>
    </ul>
  </li>
  <li>ê° diffusion stepì—ì„œ ì´ gradient updateë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ quality ë†’ì¸ë‹¤</li>
  <li>Replacing: \(\tilde{x}_a\left(x_t, t, \theta\right):=\sqrt{\bar{\alpha}_t} x_a+\sqrt{ } 1-\bar{\alpha}_t \epsilon\)ì„ í†µí•´, \(\tilde{x}_0\)ë¥¼ ì‚¬ìš©í•œ sample \(x_{t-1}\)ê°€ ìƒì„±ë¨</li>
</ul>

<h2 id="4-empirical-evaluaiton">4. Empirical Evaluaiton</h2>

<h3 id="42-metrics">4.2. Metrics</h3>

<ul>
  <li>Discriminative score (Yoon et al., 2019): measures the similarity using a classification model to distinguish between the original and synthetic data as a supervised task;</li>
  <li>Predictive score (Yoon et al., 2019):  measures the usefulness of the synthesized data by training a post-hoc sequence model to predict next-step temporal vectors using the train-synthesis-and-test-real (TSTR) method;</li>
  <li>Context-Frechet Inception Distance (Context-FID) score Â´ (Paul et al., 2022):  quantifies the quality of the synthetic time series samples by computing the difference between representations of time series that fit into the local context;</li>
  <li>Correlational score (Ni et al., 2020): uses the absolute error between cross correlation matrices by real data and synthetic data to assess the temporal dependency</li>
</ul>

<h3 id="43-interpretability-results">4.3. Interpretability Results</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig3.png" alt="ê·¸ë¦¼3" /></p>

<ul>
  <li>the corrupted samples (shown in (a)) with 50 steps of noise added as input</li>
  <li>outputs the signals (shown in (c)) that try to restore the ground truth (shown in (b))</li>
  <li>with the aid of the decomposition of temporal trend (shown in (d)) and season &amp; error (shown in (e)).</li>
  <li>Result: As would be expected, the trend curve follows the overall shape of the signal, while the season &amp; error oscillates around zero !</li>
</ul>

<h3 id="44-unconditional-time-series-generation">4.4. Unconditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table1.png" alt="ê·¸ë¦¼21" /></p>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig4.png" alt="ê·¸ë¦¼4" /></p>

<h3 id="45-conditional-time-series-generation">4.5. Conditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig6.png" alt="ê·¸ë¦¼6" /></p>

<h3 id="46-ablaction-study">4.6. Ablaction Study</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table2.png" alt="ê·¸ë¦¼22" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Diffusion-TS, a DDPM-based method for general time series generation
    <ul>
      <li>TS-specific loss design and transformer-based deep decomposition architecture</li>
    </ul>
  </li>
  <li>Unconditionalë¡œ í›ˆë ¨ëœ modelì´ ì‰½ê²Œ conditionalë¡œ í™•ì¥ë  ìˆ˜ ìˆìŒ
    <ul>
      <li>by combining gradients into the sampling !</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/pdf/2403.01742)]]></summary></entry></feed>