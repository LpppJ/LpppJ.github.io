---
layout: post
related_posts:
  _
title: 
description: >
  [ICLR 2024](https://openreview.net/pdf?id=CZiY6OLktd)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)

## Abstract

- 어떻게 Diffusion model의 성능을 time series forecasting에 활용할 수 있는가
- **M**ulti-**G**ranularity **T**ime **S**eries **D**iffusion **(MG- TSD)**
  - leveraging the inherent granularity levels
  - intuition: diffusion step에서 점차 gaussian noise로 만드는 것을 fine $$\to$$ coarse로 이해
  - novel multi-granularity guidance diffusion loss function
  - method to effectively utilize coarse-grained data across various granularity levels

## 1. Introduciton

- 최근에는 Time series predictive 목적으로 conditional generative model을 활용
  - 처음에는 Auto-regressive 방식으로 하다가 CSDI도 했었음
- 하지만 문제는 Diffusion이 instability하다는 점
  - Image에서 diffusion은 다양한 이미지를 만들 수 있어서 장점이었는데
  - 시계열 예측 관점에서는 그것이 성능 하락의 원인이 될 수 있음

![그림1](/assets/img/timeseries/MG-TSD/fig1.png)

- Diffusion step에서 점차 gaussian noise로 만드는 것을 fine $$\to$$ coarse로 이해한다면
  - Diffusion model이 **labels을 the source of guidance**로 필요로 하는 문제에서
  - Time series의 fine feature가 그 labels as the source of guidance 역할을 할 수 있을 것

- MG-TSD에서는 coarse-grained data를 denoising process 학습의 guide로 준다.
  - $$\to$$ intermediate latent states에서의 constraints로 작용
  - $$\to$$ coarser feature는 더 빠르게 생성할 수 있기 때문에, 그만큼 finer feature recovery도 용이
  - $$\to$$ coarse-grained data의 trend와 pattern을 보존하는 sampling을 만듬
  - $$\to$$​ reduces variability and results in high-quality predictions

## 2. Background

- TimeGrad Model
  - [TimeGrad Paper](https://arxiv.org/pdf/2101.12072) [TimeGrad Review](https://lpppj.github.io/timeseries/2024-07-09-Timegrad)

- $$\boldsymbol{X}^{(1)}=\left[\boldsymbol{x}_1^1, \ldots, \boldsymbol{x}_t^1, \ldots, \boldsymbol{x}_T^1\right]$$ is the original observed data, where $$t \in[1, T]$$ and $$\boldsymbol{x}_t \in \mathbb{R}^D$$
  - Mathematical expressions: $$q_{\mathcal{X}}\left(\boldsymbol{x}_{t_0: T}^1 \mid\left\{\boldsymbol{x}_{1: t_0-1}^1\right\}\right)=\prod_{t=t_0}^T q_{\mathcal{X}}\left(\boldsymbol{x}_t^1 \mid\left\{\boldsymbol{x}_{1: t-1}^1\right\}\right)$$

## 3. Method

### 3.1. MG-TSD Model Architecture

![그림2](/assets/img/timeseries/MG-TSD/fig2.png)

### Multi-granularity Data Generator

: for generating multi-granularity data from observations

- historical sliding windows with different sizes를 통해 fine $$\to$$ coase로 smoothing out
- 즉 $$\boldsymbol{X}^{(g)}=f\left(\boldsymbol{X}^{(1)}, s^g\right)$$ with pre-defined sliding window size $$s^g$$ 
- 이 때 non-overlapping하게 window를 slicing하고, $$\boldsymbol{X}^{(g)}$$는 $$s^g$$번 복제해서 $$[1, T]$$로 맞춤

### Temporal Process Module

: designed to capture the temporal dynamics of the multi-granularity time series data

- 각각의 granularity level $$g$$에서 GRU와 같은 방식으로 timestep $$t$$를 $$\mathbf{h}_t^g$$로 encoding

### Guided Diffusion Process Module

: designed to generate stable time series predictions at each timestep $$t$$

- multi-granularity data를 활용하여 diffusion learning process의 guide로 제공

### 3.2. Multi-Granularity Guided Diffusion

: Guided Diffusion Process Module에 대한 details

### 3.2.1. Coarse-grained Guidance

: the derivation of a heuristic guidance loss for the two- granularity case

- consider two granularities at a fixed timestep $$t$$
  - : $$\text { finest-grained data } \boldsymbol{x}_t^{g_1}\left(g_1=1\right) \text { from } \boldsymbol{X}^{\left(g_1\right)}$$ & $$\text { coarse-grained data } \boldsymbol{x}_t^g \text { from } \boldsymbol{X}^{(g)}$$​
- 먼저 coarse-grained targets $$x^g$$를 intermediate diffusion step $$N_*^g \in[1, N-1]$$에 introduce
  - 즉 objective function이 $$\log p_\theta\left(\boldsymbol{x}^g\right)$$ 
- 그러면 denoising process에서 recover된 coarser features는 실제 coarse-grained sample의 정보를 많이 가지고 있을테니
  - fine-grained feature를 recover하기도 쉬워질 것
- $$\theta$$-parameterized: $$p_\theta\left(\boldsymbol{x}_{N_*^g}\right)=\int p_\theta\left(\boldsymbol{x}_{N_*^g: N}\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}=\int p\left(\boldsymbol{x}_N\right) \prod_{N_*^g+1}^N p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}$$​
  - where $$\boldsymbol{x}_N \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I}), p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right)=\mathcal{N}\left(\boldsymbol{x}_{n-1} ; \boldsymbol{\mu}_\theta\left(\boldsymbol{x}_n, n\right), \boldsymbol{\Sigma}_\theta\left(\boldsymbol{x}_n, n\right)\right)$$​
- 이건 $$N_*^g$$번째 diffusion step에서 $$N$$번째까지 총 $$N-N_*^g$$ steps의 forward process이므로
  - the guidance objective: $$\log p_\theta\left(\boldsymbol{x}^g\right)=\log \int p_\theta\left(\boldsymbol{x}_{N_*^g}^g, \boldsymbol{x}_{N_*^g+1}^g, \ldots, \boldsymbol{x}_N^g\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}^g$$​
- sample에 대한 loss 대신 noise에 대한 loss 사용
  - loss: $$\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}^g, n}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_n^g, n\right)\right\|^2\right]$$​
  - where $$\boldsymbol{x}_n^g=\left(\prod_{i=N_{\boldsymbol{z}}^g}^n \alpha_i^1\right) \boldsymbol{x}^g+\sqrt{ } \mathbf{1}-\prod_{i=N^g}^n \alpha_i^1 \boldsymbol{\epsilon} \text { and } \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})$$

### 3.2.2. Multi-granularity Guidance

- Multi-granularity Data Generator가 G개의 granularity levels마다 data 생성: $$\boldsymbol{X}^{(1)}, \boldsymbol{X}^{(2)}, \ldots, \boldsymbol{X}^{(G)}$$

- Share ratio: $$r_g:=1-\left(N_*^g-1\right) / N$$
  - : the shared percentage of variance schedule between the gth granularity data and the finest-grained data
  - ex. finest-grained data에서는 $$N_*^1=1 \text { and } r^1=1$$​
    - variance schedule for granularity $$g$$ is, $$\alpha_n^g\left(N_*^g\right)= \begin{cases}1 & \text { if } n=1, \ldots, N_*^g \\ \alpha_n^1 & \text { if } n=N_*^g+1, \ldots, N\end{cases}$$​
    - and $$\left\{\beta_n^g\right\}_{n=1}^N=\left\{1-\alpha_n^g\right\}_{n=1}^N$$​
    - accordingly, $$a_n^g\left(N_*^g\right)=\prod_{k=1}^n \alpha_k^g \text {, and } b_n^g\left(N_*^g\right)=1-a_n^g\left(N_*^g\right)$$
- 이 때 $$N^g_*$$는 : represents the diffusion index for starting sharing the variance schedule across granularity level $$g \in\{1, \ldots, G\}$$
- 이렇게 되면 larger coarser granularity level일수록 $$N^g_*$$가 커진다는 뜻
  - 즉 coarser할수록 fine한 정보는 줄어들테니 이전 diffusion step과 차이가 크지 않을 것
  - 그러니까 $$N^g_*$$를 크게 해서 fine-grained feature를 생성할 steps를 많이 줌

- Then the guidance loss function $$L^{(g)}(\theta)$$ for $$g$$-th granularity $$x^g_{n,t}$$ at timestep $$t$$ and diffusion step $$n$$,
  - can be expressed as: $$L^{(g)}(\theta)=\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, n} \|\left(\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon}, n, \mathbf{h}_{t-1}^g\right) \|_2^2\right.$$
  - where $$\mathbf{h}_t^g=\mathrm{RNN}_\theta\left(\boldsymbol{x}_t^g, \mathbf{h}_{t-1}^g\right)$$

### Training

![그림41](/assets/img/timeseries/MG-TSD/algorithm1.png)

- 최종적인 training objectives는 모든 granularities에서의 Loss의 weighted sum
  - : $$L^{\text {final }}=\omega^1 L^{(1)}(\theta)+L^{\text {guidance }}(\theta)=\sum_{q=1}^G \omega^g \mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, t}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_{n, t}^g, n, \mathbf{h}_{t-1}^g\right)\right\|^2\right]$$
  - where $$\boldsymbol{x}_{n, t}^g=\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon} \text { and } \sum_{g=1}^G \omega^g=1$$
  - 이 때 denoising network의 parameters는 shared across all granularities

### Inference

![그림42](/assets/img/timeseries/MG-TSD/algorithm2.png)

- 우리의 목표는 특정한 prediction steps에 대한  finest-grained data에 대한 예측
  - $$t_0-1$$ 시점까지 주어졌다면 아래 algorithm 2를 따라 $$t_0$$시점에 대한 데이터 생성,
  - 우리가 원하는 forecast horizon이 될 때까지 반복
  - hidden states에 conditional inputs으로 무엇을 넣는지에 따라서 그에 해당하는 granularity levels로 샘플링

### Selection of share ratio

- 위에서는 share ratio $$r_g:=1-\left(N_*^g-1\right) / N$$를 heuristic하게 $$N^g_*$$에 따라 결정되도록 했음
  - Diffusion step $$N^g_*$$는 $$q\left(\boldsymbol{x}^g\right) \text { and } p_\theta\left(\boldsymbol{x}_n^{g_1}\right)$$의 거리가 가장 작을 때로 설정 !
  - : $$\to$$ $$N_*^g:=\arg \min_n \mathcal{D}\left(q\left(\boldsymbol{x}^g\right), p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\right)$$​
    - $$\mathcal{D}$$는 두 분포의 거리를 측정하는 metric이 됨 (KL-divergence, ...)

## 4. Experiments

![그림112](/assets/img/timeseries/MG-TSD/table12.png)

![그림3](/assets/img/timeseries/MG-TSD/fig3.png)

![그림13](/assets/img/timeseries/MG-TSD/table3.png)

![그림4](/assets/img/timeseries/MG-TSD/fig4.png)

## 5. Conclusion

- Multi-Granularity Time Series Diffusion (MG-TSD)
  - leverages the inherent granularity levels within the data, as given targets at intermediate diffusion steps to guide the learning process of diffusion models
  - to effectively utilize coarse-grained data across various granularity levels.