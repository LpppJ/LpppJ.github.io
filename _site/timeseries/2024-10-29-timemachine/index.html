<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v9.1.6 <https://hydejack.com/>
-->







<head>
  






  
    
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024) | LpppJ</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)" />
<meta name="author" content="GW Jeong" />
<meta property="og:locale" content="en" />
<meta name="description" content="ECAI 2024" />
<meta property="og:description" content="ECAI 2024" />
<link rel="canonical" href="http://localhost:4000/timeseries/2024-10-29-timemachine/" />
<meta property="og:url" content="http://localhost:4000/timeseries/2024-10-29-timemachine/" />
<meta property="og:site_name" content="LpppJ" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-29T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"GW Jeong"},"dateModified":"2024-10-29T21:25:07+09:00","datePublished":"2024-10-29T00:00:00+09:00","description":"ECAI 2024","headline":"TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/timeseries/2024-10-29-timemachine/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/me/logo.jpg"},"name":"GW Jeong"},"url":"http://localhost:4000/timeseries/2024-10-29-timemachine/"}</script>
<!-- End Jekyll SEO tag -->


  

  



  <meta name="theme-color" content="rgb(230, 217, 195)">


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="LpppJ">
<meta name="apple-mobile-web-app-status-bar-style" content="default">

<meta name="application-name" content="LpppJ">

<meta name="generator" content="Hydejack v9.1.6" />


<link rel="alternate" href="http://localhost:4000/timeseries/2024-10-29-timemachine/" hreflang="en">

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="LpppJ" />


<link rel="shortcut icon"    href="/assets/icons/favicon.ico">
<link rel="apple-touch-icon" href="/assets/icons/icon-192x192.png">

<link rel="manifest" href="/assets/site.webmanifest">

<link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com">



<link rel="preload" href="/assets/img/swipe.svg" as="image" id="_hrefSwipeSVG">






<script>!function(r,c){"use strict";function a(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}r.loadJS=function(e,t){var n=c.createElement("script"),e=(n.src=e,t&&a(n,"load",t,{once:!0}),c.scripts[0]);return e.parentNode.insertBefore(n,e),n},r._loaded=!1,r.loadJSDeferred=function(e,t){var n=c.createElement("script");function o(){r._loaded=!0,t&&a(n,"load",t,{once:!0});var e=c.scripts[0];e.parentNode.insertBefore(n,e)}return n.src=e,r._loaded?o():a(r,"load",o,{once:!0}),n},r.setRel=r.setRelStylesheet=function(e){a(c.getElementById(e),"load",function(){this.rel="stylesheet"},{once:!0})}}(window,document);
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
!function(w) {
  w._baseURL = '/';
  w._publicPath = '/assets/js/';
  w._noPushState = false;
  w._noDrawer = false;
  w._noNavbar = false;
  w._noToc = false;
  w._noSearch = false;
  w._search = {
    DATA_URL: '/assets/sitedata.json?no-cache',
    STORAGE_KEY: 'mini-search/',
    INDEX_KEY: 'index--2024-11-07T18:09:04+09:00',
  };
  w._clapButton = true;
}(window);</script>


<script async src="/assets/bower_components/MathJax/es5/tex-mml-chtml.js" id="_MathJax"></script>


<!--[if gt IE 8]><!---->

  




<link rel="stylesheet" href="/assets/css/hydejack-9.1.6.css" id="_stylePreload">
<link rel="stylesheet" href="/assets/icomoon/style.css" id="_iconsPreload">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap" id="_fontsPreload">



  <style id="_pageStyle">

html{--accent-color: rgb(94, 97, 94);--accent-color-faded: rgba(94, 97, 94, 0.5);--accent-color-highlight: rgba(94, 97, 94, 0.1);--accent-color-darkened: #4b4e4b;--theme-color: rgb(230, 217, 195)}
</style>


<!--<![endif]-->




</head>

<body class="no-break-layout">
  


<hy-push-state
  id="_pushState"
  replace-selector="#_main"
  link-selector="a[href]:not([href^='/assets/']):not(.external):not(.no-push-state)"
  script-selector="script"
  duration="500"
  hashchange
>
  
  
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <div class="nav-btn-bar">
      <a id="_menu" class="nav-btn no-hover" href="#_drawer--opened">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <div class="nav-span"></div>
    </div>
  </div>
</div>
<hr class="sr-only" hidden />

  <main
  id="_main"
  class="content layout-post"
  role="main"
>
  <nav id="breadcrumbs" class="screen-only"><ul>
  
  
    <li><a href="/">home</a></li>
    
      <li>
        
          <span>/</span>
          
          
          <a href="/timeseries/">timeseries</a>
        
      </li>
    
      <li>
        
          <span>/</span>
          <span>2024-10-29-timemachine</span>
        
      </li>
    
  
</ul></nav>
  










<article id="post-timeseries-timemachine" class="page post mb6" role="article">
  <header>
    <h1 class="post-title flip-project-title">
      
        TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)
      
    </h1>

    <div class="post-date">
      
      <span class="ellipsis mr1">
        <time datetime="2024-10-29T00:00:00+09:00">29 Oct 2024</time> in <span>Timeseries</span> 
      </span>
      
    </div>

    
    

    



  
    <p class="note-sm" >
      <a href="https://arxiv.org/abs/2403.09898">ECAI 2024</a>

    </p>
  


  </header>

  
    <h2 id="abstract">Abstract</h2>

<ul>
  <li>Long-term time-series forecasting(LTSF)에서 우리는 long-term dependencies를 capture하는데
    <ul>
      <li>linear scalability와 computational efficiency를 유지해야 함</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 TimeMachine은 Mamba를 활용하여
    <ul>
      <li>the unique properties of time series를 발견하여
        <ul>
          <li>multi-scales에서 salient contextual cues를 만들고</li>
        </ul>
      </li>
      <li>quadruple-Mamba architecture를 합쳐서
        <ul>
          <li>channel-mixing and channel-independence를 한 번에 통합</li>
        </ul>
      </li>
      <li>서로 다른 scales에서의 global / local contexts를 effective하게 selection할 수 있게 함</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>LTSF에서 Capturing long-term dependencies가 핵심</li>
  <li><strong>Linear model</strong> : DLinear, TiDE
    <ul>
      <li>may not well capture long-range correlations</li>
    </ul>
  </li>
  <li><strong>Transformer-based</strong> : iTransformer, PatchTST, Crossformer
    <ul>
      <li>suffer from the quadratic complexity</li>
    </ul>
  </li>
  <li><strong>state-space models (SSMs)</strong>
    <ul>
      <li>inferring over very long sequences</li>
      <li>context-aware selectivity</li>
      <li>LTSF에서도 활용될 수 있는가
        <ul>
          <li>highly content- and context-selective SSM이 최근에 많이 나오고 있고</li>
          <li>effectively representing the context in time series에 쓸 수 있을 것</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Transforemr-based approach에서는 each observation이나 sub-series, 아니면 time series를 token(patch)로 만드는데
    <ul>
      <li>SSM에서 이러한 접근을 그대로 쓰면 성능이 안나옴</li>
      <li><strong>그래서 salient contextual cues tailored to SSM을 extract하는 것이 먼저 !</strong></li>
    </ul>
  </li>
  <li>기존에는 channel-mixing way도 있고 (ex. Informer, FEDformer, and Autoformer, …)
    <ul>
      <li>channel-independence way도 있는데, (ex. PatchTST, TiDE, …)</li>
      <li>본 논문에서는 unified architecture : applicable to both scenarios!</li>
    </ul>
  </li>
  <li>그리고 Time series에는 downsampling해도 temporal relations가 유지된다는 특징이 있으니
    <ul>
      <li>모든 time points를 token으로 만드는 건 redundant하고, PatchTST처럼 patch를 사용하는 건 good</li>
      <li>하지만 pre-defined small patch는 fixed resolution에서의 context만 제공</li>
      <li>그러니 iTransformer처럼 whole look-back window를 token으로 만드는 것이 낫고</li>
      <li>하지만 iTransformer처럼 channel-independence에서는 select sub-token contents가 잘 안됨</li>
      <li>그러므로, SSM 쓰면 더 잘 될 것</li>
    </ul>
  </li>
  <li>그러니 본 논문에서는 TimeMachine을 제안
    <ul>
      <li>MTS를 2개의 scale에서 context-aware prediction하기 위해 SSM을 사용
        <ul>
          <li>high, low resolution이라는 2개의 scale마다 2개의 mamba를 사용.
            <ul>
              <li>하나는 global perspectives for the channel-mixing</li>
              <li>다른 하나는 both global and local perspectives for the channel-independence</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>이렇게 4개의 SSM modules를 사용해서 channel-independent, -dependent를 통합하고
        <ul>
          <li>즉 btw-channel correlation이 “있으면” 잡아내고 “없으면” independent 처럼</li>
          <li>다양한 scales에서 global and local contextual information을 효율적으로 selection</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li>Non-Transformer-based Supervised Approaches
    <ul>
      <li>Classical methods : ARIMA, VARMAX, GARCH, RNN, …</li>
      <li>MLP-based models : DLinear, TiDE, RLinear, …</li>
      <li>CNN-based : TimesNet, Scinet, …</li>
    </ul>
  </li>
  <li>Transformer-based Supervised Learning methods
    <ul>
      <li>iTransformer, PatchTST, Crossformer, FEDformer, stationary, Flowformer, and Autoformer</li>
      <li>time series를 token series로 만들고 self-attention</li>
      <li>하지만 quadratic time and memory complexity</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-method">3. Proposed Method</h2>

<ul>
  <li>input sequence \(\mathbf{x}=\left[x_1, \ldots, x_L\right]\)
    <ul>
      <li>\(x_t \in \mathcal{R}^M\) representing a vector of \(M\) channels at time point \(t\)</li>
    </ul>
  </li>
  <li><strong>Normalization</strong>
    <ul>
      <li>the original MTS \(\mathbf{x}\) into \(\mathbf{x}^0=\left[\mathbf{x}_1^{(0)}, \cdots, \mathbf{x}_L^{(0)}\right] \in \mathcal{R}^{M \times L}\), via \(\mathbf{x}^{(0)}=\operatorname{Normalize}(\mathbf{x})\).</li>
      <li>Here, Normalize \((\cdot)\) represents a normalization operation RevIN</li>
    </ul>
  </li>
  <li><strong>Channel Mixing vs. Channel Independence</strong>
    <ul>
      <li>PatchTST에서는 Channel Independence가 좋다고 하지만
        <ul>
          <li>그건 length에 비해 channels가 많지 않을 때 이야기고,</li>
          <li>channels가 많을 때에는 Channel Mixing이 더 낫다</li>
        </ul>
      </li>
      <li>TimeMachine은 “potentially” inter-channel correlation을 잡고
        <ul>
          <li>Channel Independence일 때에는 independence를 찾음</li>
        </ul>
      </li>
      <li>input의 shape은 BML, output은 BMT</li>
    </ul>
  </li>
  <li><strong>Embedded Representations</strong>
    <ul>
      <li>2-stage embedded representation</li>
      <li>\(\mathbf{x}^{(1)}=E_1\left(\mathbf{x}^{(0)}\right), \quad \mathbf{x}^{(2)}=E_2\left(D O\left(\mathbf{x}^{(1)}\right)\right)\), where
        <ul>
          <li>\(E_1: \mathbb{R}^{M \times L} \rightarrow \mathbb{R}^{M \times n_1}\) and \(E_2: \mathbb{R}^{M \times n_1} \rightarrow \mathbb{R}^{M \times n_2}\)은 MLP</li>
          <li>DO는 dropout, (MLP 쓰니까 overfitting 방지)</li>
        </ul>
      </li>
      <li>이렇게 input length에 상관없이 fixed-length tokens로 embedding</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timemachine/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p><strong>Integrated Quadruple Mambas</strong> (fig1 보면서 이해하면 좋음)</p>

    <ul>
      <li>
        <p>\(E_1, E_2\) 각각의 embedding level에서 2개의 mamba를 사용</p>

        <ul>
          <li>\(E_1\) level에서 사용되는 2개의 mamba의 input은 \(D O\left(\mathbf{x}^{(1)}\right)\)</li>
          <li>\(E_2\) level에서 사용되는 2개의 mamba의 input은 \(D O\left(\mathbf{x}^{(2)}\right)\)</li>
        </ul>
      </li>
      <li>
        <p>첫 번째 mamba block 안에서는 2개의 FC-layers가 linear projection하고</p>

        <ul>
          <li>하나만 1d causal conv와 SiLU activation 통과, 그리고 structured SSM으로 간다</li>
          <li>그 다음 남은 하나의 linear projection을 더하고 FC-layer를 한 번 더 태움</li>
          <li>이때 <strong>continuous-time SSM</strong>은 input sequence \(u(t)\)를 latente state \(h(t)\)를 통해 output \(v(t)\)로 보낸다.
            <ul>
              <li>즉 \(d h(t) / d t=A h(t)+B u(t), \quad v(t)=C h(t)\)
                <ul>
                  <li>\(h(t)\) is \(N\)-dimensional (\(N\)은 state expansion factor)</li>
                  <li>\(u(t)\) is \(D\)-dimensional (\(D\)는 dimension factor)</li>
                  <li>\(v(t)\)의 dimension도 \(D\)</li>
                  <li>\(A\), \(B\), \(C\)는 coefficient matrices of proper size</li>
                </ul>
              </li>
              <li><strong>여기서 \(A\), \(B\), \(C\), 그리고 hidden state를 time interval \(\Delta\)에 대한 함수로 놓음</strong></li>
              <li>이것이 모델을 input에 adaptive하게 context selectivity를 강화하는 방법
                <ul>
                  <li>즉 \(h_k=\bar{A} h_{k-1}+\bar{B} u_k, \quad v_k=C h_k\)</li>
                  <li>where \(h_k, u_k\), and \(v_k\) are respectively samples of \(h(t), u(t)\), and \(v(t)\) at time \(k \Delta\),</li>
                  <li>\(\bar{A}=\exp (\Delta A), \quad \bar{B}=(\Delta A)^{-1}(\exp (\Delta A)-I) \Delta B\).</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>(continuous 말고) <strong>SSM</strong>은 \(B\), \(C\), \(\Delta\)가 input에 따라 달라짐
            <ul>
              <li>\(B, C \leftarrow \operatorname{Linear}_N(u)\), \(\Delta \leftarrow \text{softplus}(parameter +Linear _D\left(\right. Linear \left.\left._1(u)\right)\right)\)</li>
              <li>coefficient matrices는 current token을 보고 정보를 selectively propagate하게 함</li>
              <li><strong>channel-mixing case</strong>에서는 각 univariate가 token(dim=\(n_2\))이 되고
                <ul>
                  <li><strong>Inner mambas</strong>에서는 \(BMn_2\)이 나오는데</li>
                  <li>Left / right inner mamba의 k번째 변수의 output은 \(v_{L, k}, v_{R, k} \in \mathcal{R}^{n_2}\)
                    <ul>
                      <li>둘을 더하고 embedding된 \(\mathbf{x}^{(2)}\)을 skip connection하면 \(\mathbf{x}^{(3)}=\mathbf{v}_L \bigoplus \mathbf{v}_R \bigoplus \mathbf{x}^{(2)}\) (Element-wise addition)</li>
                      <li>그 다음 linear mapping \(P_1: \mathbf{x}^{(3)} \rightarrow\mathbf{x}^{(4)} \in \mathcal{R}^{M \times n_1}\)</li>
                    </ul>
                  </li>
                  <li><strong>Outer mambas</strong>에서도 비슷하게
                    <ul>
                      <li>\(v_{L, k}^*, v_{R, k}^* \in \mathcal{R}^{n_1}\)구하고 \(\mathbf{x}^{(5)} \in \mathcal{R}^{M \times n_1}\)랑 해서 셋이 더함</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li><strong>channel-independence</strong>에서는 처음에 \(B M L \mapsto(B \times M) 1 L\) 이렇게 reshape을 해서 마치 match가 \(BM\)개이고 univariates인 것처럼 처리
                <ul>
                  <li>Outer든 inner이든
                    <ul>
                      <li>mamba 하나는 input dim =1, token length =\(n_1\) or \(n_2\),</li>
                      <li>다른 하나는 input dim =\(n_1\) or \(n_2\), token length =1</li>
                    </ul>
                  </li>
                  <li>이렇게 하면 global context and local context 동시에 학습 가능하고</li>
                  <li>fine and coarse scales with high- and low-resolution 각각의 context 추출</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Channel mixing은 변수 개수가 많을 때 하고 independence랑 switch하려면</p>

        <ul>
          <li>input sequence를 그냥 transposed하면 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Output Projection</p>

    <ul>
      <li>MLP 쓰고 \(P_1\) performs a mapping \(\mathcal{R}^{M \times n_2} \rightarrow \mathcal{R}^{M \times n_1}\), \(P_2\)는 \(\mathbb{R}^{M \times 2 n_1} \rightarrow \mathbb{R}^{M \times T}\)</li>
      <li>Residual connection도 fig1처럼 해주고</li>
      <li>Outer Mambas에서 나온 \(\mathbf{x}^{(5)}\)랑 Inner Mambas에서 나온 \(\mathbf{x}^{(4)}\)를 concat해서 사용하게 됨
        <ul>
          <li>즉 \(\mathbf{x}^{(6)}=\mathbf{x}^{(5)} \|\left(\mathbf{x}^{(4)} \bigoplus \mathbf{x}^{(1)}\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-result-analysis">4. Result Analysis</h2>

<h3 id="41-datasets">4.1. Datasets</h3>

<ul>
  <li>seven benchmark datasets extensively used for LTSF:
    <ul>
      <li>Weather, Traffic, Electricity, and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2)</li>
    </ul>
  </li>
</ul>

<h3 id="42-experimental-environment">4.2. Experimental Environment</h3>

<ul>
  <li>본 논문에서 제시하는 TimeMachine을 11 SOTA models와 비교 :
    <ul>
      <li>including iTransformer, PatchTST, DLinear, RLinear, Autoformer, Crossformer, TiDE, Scinet, TimesNet, FEDformer, and Stationary</li>
    </ul>
  </li>
</ul>

<h3 id="43-quantitative-results">4.3. Quantitative Results</h3>

<p><img src="/assets/img/timeseries/timemachine/fig2.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/timemachine/fig3.png" alt="그림1" /></p>

<h3 id="44-qualitative-result">4.4. Qualitative Result</h3>

<p><img src="/assets/img/timeseries/timemachine/table2.png" alt="그림1" /></p>

<h2 id="5--hyperparameter-sensitivity-analysis-and-ablation-study">5.  Hyperparameter Sensitivity Analysis and Ablation Study</h2>

<h3 id="51-effect-of-mlps-parameters-n1-n2">5.1. Effect of MLPs’ Parameters (n1, n2)</h3>

<ul>
  <li>MLP의 size인 \(n_1, n_2\)를 다양하게 해봤는데 별 차이 없음 (fig5)
    <ul>
      <li>MLP에 heavily dependent하지 않다는 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timemachine/fig5.png" alt="그림1" /></p>

<h3 id="52-sensitivity-of-dropouts">5.2. Sensitivity of Dropouts</h3>

<ul>
  <li>Dropout ratio 적당하게 0.7 사용</li>
</ul>

<h3 id="53-ablation-of-residual-connections">5.3. Ablation of Residual Connections</h3>

<ul>
  <li>Residual connections 쓰는 것이 좋더라</li>
</ul>

<h3 id="54-effects-of-mambas-local-convolutional-width">5.4. Effects of Mambas’ Local Convolutional Width</h3>

<ul>
  <li>각 Mamba 안에도 parameters가 있을거니까 local convolutional kernel widths를 2 and 4로실험 해봤더니 2가 낫더라</li>
</ul>

<h3 id="55-ablation-on-state-expansion-factor-of-mambas">5.5. Ablation on State Expansion Factor of Mambas</h3>

<p><img src="/assets/img/timeseries/timemachine/fig6.png" alt="그림1" /></p>

<ul>
  <li>State Expansion Factor를 8부터 256까지 해봤는데 256이 제일 좋아서 defualt로 설정</li>
</ul>

<h3 id="56-ablation-on-mamba-dimension-expansion-factor">5.6. Ablation on Mamba Dimension Expansion Factor</h3>

<ul>
  <li>dimension expansion factor (\(E\))도 있었는데, 크게하면 메모리는 많이 먹는데 성능 향상으로 이어지지는 않아서 그냥 1로 둔다</li>
</ul>

<h2 id="6-strengths-and-limitations">6. Strengths and Limitations</h2>

<ul>
  <li>memory efficiency and stable performance across varying look-back and prediction lengths !</li>
  <li>Weather에서 1등 못한게 limitation. (….?ㅋㅋ)</li>
</ul>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>LTSF with linear scalability and small memory footprints !</li>
  <li>integrated quadruple-Mamba architecture
    <ul>
      <li>to predict with rich global and local contextual cues at multiple scales</li>
      <li>\(\to\) unifies channel-mixing and channel-independence situations</li>
    </ul>
  </li>
</ul>

  
</article>



  <hr class="dingbat related mb6" />






  
     


  <aside class="about related mt4 mb4" role="complementary">
    
    

<div class="author mt4">
  

  
    


<img
  
    src="https://via.placeholder.com/128x128"
    srcset="/assets/img/me/me.JPG 1x,/assets/img/me/me.JPG 2x"
    
  
  alt="GW Jeong"
  class="avatar"
  
  width="120"
  height="120"
  loading="lazy"
/>

  

  
  
  <h2  class="page-title hr-bottom">
    About
  </h2>

  <p>Bachelor’s degree in Applied Statistics. Yonsei Univ. (2018~2024) <br />
DataScienceLab 8th 학회장 (2022~2023) <br />
Master’s degree in Statitstics. Yonsei Univ. (2024~)</p>


  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>

  </aside>


  

  
  

  
    


  

  
  

  
    

  


  
<footer class="content" role="contentinfo">
  <hr/>
  
    <p><small class="copyright">© Geonwoo Jeong.
</small></p>
  
  
    <p><small>Powered by <a class="external" href="https://hydejack.com/">Hydejack</a> v<span id="_version">9.1.6</span></small></p>
  <hr class="sr-only"/>
</footer>


</main>

  <hy-drawer
  id="_drawer"
  class=""
  side="left"
  threshold="10"
  noscroll
  
>
  <header id="_sidebar" class="sidebar" role="banner">
    




<div class="sidebar-bg sidebar-overlay" style="background-color:rgb(230, 217, 195);background-image:url(/assets/img/me/sidebar.jpg)"></div>

    <div class="sidebar-sticky">
  <div class="sidebar-about">
    
      <a class="no-hover" href="/" tabindex="-1">
        <img src="/assets/img/me/logo.jpg" class="avatar" alt="LpppJ" width="120" height="120" loading="lazy" />
      </a>
    
    <a class="sidebar-title" href="/"><h2 class="h1">LpppJ</h2></a>
    
    
      <p class="">
        DataScience and AI

      </p>
    
  </div>

  <nav class="sidebar-nav heading" role="navigation">
    <span class="sr-only">Navigation:</span>
<ul>
  
    
      
      <li>
        <a
          id="_drawer--opened"
          href="/timeseries/"
          class="sidebar-nav-item "
          
        >
          TimeSeries
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/pytorch/"
          class="sidebar-nav-item "
          
        >
          Pytorch
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/stat/"
          class="sidebar-nav-item "
          
        >
          Statistics
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/project/"
          class="sidebar-nav-item "
          
        >
          Project
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/presentation/"
          class="sidebar-nav-item "
          
        >
          Presentation
        </a>
      </li>
    
  
</ul>

  </nav>

  
  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>
  </header>
</hy-drawer>
<hr class="sr-only" hidden />

</hy-push-state>


  <!--[if gt IE 10]><!---->
  <script nomodule>!function(){var t,n=document.createElement("script");!("noModule"in n)&&"onbeforeload"in n&&(t=!1,document.addEventListener("beforeload",function(e){if(e.target===n)t=!0;else if(!e.target.hasAttribute("nomodule")||!t)return;e.preventDefault()},!0),n.type="module",n.src=".",document.head.appendChild(n),n.remove())}();
</script>
  <script src="/assets/js/hydejack-9.1.6.js" type="module"></script>
  <script src="/assets/js/LEGACY-hydejack-9.1.6.js" nomodule defer></script>
  

  

<!--<![endif]-->
  <!-- <script>
  document.querySelector('hy-push-state').setAttribute('prefetch', '');

  document.querySelectorAll('.sidebar a[href^="/"]').forEach(function (el) { 
    el.addEventListener('click', function (e) {
      if (el.pathname === window.location.pathname) {
        e.preventDefault();
        e.stopPropagation();
        document.querySelector('hy-drawer').close();
      }
    });
  });
</script> -->

<!--
Code for integrating CloudFlare's email protection with Hydejack's single page app loading.
-->
<script>
  document.getElementById('_pushState').addEventListener('hy-push-state-after', function (e) {
    function e(e){
      (console.error?console.error:console.log).call(console,e)
    }

    function t(e){
      return l.innerHTML='<a href="'+e.replace(/"/g,"&quot;")+'"></a>',l.childNodes[0].getAttribute("href")
    }

    function r(e,t){
      var r=e.substr(t,2);return parseInt(r,16)
    }

    function n(e,n){
      for(var o="",c=r(e,n),a=n+2;a<e.length;a+=2){
        var l=r(e,a)^c;
        o+=String.fromCharCode(l)
      }
      return t(o)
    }

    var o="/cdn-cgi/l/email-protection#",
        c=".__cf_email__",
        a="data-cfemail",
        l=document.createElement("div");

    !function(){
      for(var t=document.getElementsByTagName("a"),r=0;r<t.length;r++)
        try{
          var c=t[r],a=c.href.indexOf(o);
          a>-1&&(c.href="mailto:"+n(c.href,a+o.length))
        }catch(t){
          e(t)
        }
    }(),
    function(){
      for(var t=document.querySelectorAll(c),r=0;r<t.length;r++)
        try{
          var o=t[r],l=n(o.getAttribute(a),0),i=document.createTextNode(l);
          o.parentNode.replaceChild(i,o)
        }catch(t){
          e(t)
        }
    }()
  });
</script>





<div hidden>
  
  <h2 class="sr-only">Templates (for web app):</h2>

  <template id="_animation-template">
  <div class="animation-main fixed-top">
    <nav id="breadcrumbs" class="screen-only"><ul>
  
  
</ul></nav>
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

  <template id="_loading-template">
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

  <template id="_error-template">
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

  <template id="_permalink-template">
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="content-hash"></span>
  </a>
</template>

</div>


</body>
</html>
