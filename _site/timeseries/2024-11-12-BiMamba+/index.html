<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v9.1.6 <https://hydejack.com/>
-->







<head>
  






  
    
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024) | LpppJ</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)" />
<meta name="author" content="GW Jeong" />
<meta property="og:locale" content="en" />
<meta name="description" content="Arxiv 2024" />
<meta property="og:description" content="Arxiv 2024" />
<link rel="canonical" href="http://localhost:4000/timeseries/2024-11-12-BiMamba+/" />
<meta property="og:url" content="http://localhost:4000/timeseries/2024-11-12-BiMamba+/" />
<meta property="og:site_name" content="LpppJ" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-11-12T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"GW Jeong"},"dateModified":"2024-11-14T00:13:05+09:00","datePublished":"2024-11-12T00:00:00+09:00","description":"Arxiv 2024","headline":"Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/timeseries/2024-11-12-BiMamba+/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/me/logo.jpg"},"name":"GW Jeong"},"url":"http://localhost:4000/timeseries/2024-11-12-BiMamba+/"}</script>
<!-- End Jekyll SEO tag -->


  

  



  <meta name="theme-color" content="rgb(230, 217, 195)">


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="LpppJ">
<meta name="apple-mobile-web-app-status-bar-style" content="default">

<meta name="application-name" content="LpppJ">

<meta name="generator" content="Hydejack v9.1.6" />


<link rel="alternate" href="http://localhost:4000/timeseries/2024-11-12-BiMamba+/" hreflang="en">

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="LpppJ" />


<link rel="shortcut icon"    href="/assets/icons/favicon.ico">
<link rel="apple-touch-icon" href="/assets/icons/icon-192x192.png">

<link rel="manifest" href="/assets/site.webmanifest">

<link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com">



<link rel="preload" href="/assets/img/swipe.svg" as="image" id="_hrefSwipeSVG">






<script>!function(r,c){"use strict";function a(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}r.loadJS=function(e,t){var n=c.createElement("script"),e=(n.src=e,t&&a(n,"load",t,{once:!0}),c.scripts[0]);return e.parentNode.insertBefore(n,e),n},r._loaded=!1,r.loadJSDeferred=function(e,t){var n=c.createElement("script");function o(){r._loaded=!0,t&&a(n,"load",t,{once:!0});var e=c.scripts[0];e.parentNode.insertBefore(n,e)}return n.src=e,r._loaded?o():a(r,"load",o,{once:!0}),n},r.setRel=r.setRelStylesheet=function(e){a(c.getElementById(e),"load",function(){this.rel="stylesheet"},{once:!0})}}(window,document);
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
!function(w) {
  w._baseURL = '/';
  w._publicPath = '/assets/js/';
  w._noPushState = false;
  w._noDrawer = false;
  w._noNavbar = false;
  w._noToc = false;
  w._noSearch = false;
  w._search = {
    DATA_URL: '/assets/sitedata.json?no-cache',
    STORAGE_KEY: 'mini-search/',
    INDEX_KEY: 'index--2024-11-27T16:56:58+09:00',
  };
  w._clapButton = true;
}(window);</script>


<script async src="/assets/bower_components/MathJax/es5/tex-mml-chtml.js" id="_MathJax"></script>


<!--[if gt IE 8]><!---->

  




<link rel="stylesheet" href="/assets/css/hydejack-9.1.6.css" id="_stylePreload">
<link rel="stylesheet" href="/assets/icomoon/style.css" id="_iconsPreload">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap" id="_fontsPreload">



  <style id="_pageStyle">

html{--accent-color: rgb(94, 97, 94);--accent-color-faded: rgba(94, 97, 94, 0.5);--accent-color-highlight: rgba(94, 97, 94, 0.1);--accent-color-darkened: #4b4e4b;--theme-color: rgb(230, 217, 195)}
</style>


<!--<![endif]-->




</head>

<body class="no-break-layout">
  


<hy-push-state
  id="_pushState"
  replace-selector="#_main"
  link-selector="a[href]:not([href^='/assets/']):not(.external):not(.no-push-state)"
  script-selector="script"
  duration="500"
  hashchange
>
  
  
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <div class="nav-btn-bar">
      <a id="_menu" class="nav-btn no-hover" href="#_drawer--opened">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <div class="nav-span"></div>
    </div>
  </div>
</div>
<hr class="sr-only" hidden />

  <main
  id="_main"
  class="content layout-post"
  role="main"
>
  <nav id="breadcrumbs" class="screen-only"><ul>
  
  
    <li><a href="/">home</a></li>
    
      <li>
        
          <span>/</span>
          
          
          <a href="/timeseries/">timeseries</a>
        
      </li>
    
      <li>
        
          <span>/</span>
          <span>2024-11-12-BiMamba </span>
        
      </li>
    
  
</ul></nav>
  










<article id="post-timeseries-BiMamba+" class="page post mb6" role="article">
  <header>
    <h1 class="post-title flip-project-title">
      
        Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)
      
    </h1>

    <div class="post-date">
      
      <span class="ellipsis mr1">
        <time datetime="2024-11-12T00:00:00+09:00">12 Nov 2024</time> in <span>Timeseries</span> 
      </span>
      
    </div>

    
    

    



  
    <p class="note-sm" >
      <a href="https://arxiv.org/pdf/2404.15772">Arxiv 2024</a>

    </p>
  


  </header>

  
    <h2 id="abstract">Abstract</h2>

<ul>
  <li>LTSF :  long-term dependencies capturing and <strong>sparse semantic characteristics</strong></li>
  <li>Mamba
    <ul>
      <li>the selective capability on input data</li>
      <li>the hardware-aware parallel computing algorithm</li>
    </ul>
  </li>
  <li><strong>Mamba+ block</strong>
    <ul>
      <li>by adding a forget gate inside Mamba,</li>
      <li>to selectively combine the new features with the historical features in a complementary manner</li>
    </ul>
  </li>
  <li><strong>Bi-Mamba+</strong>
    <ul>
      <li>apply Mamba+ both forward and backward</li>
    </ul>
  </li>
  <li>MTS는 시나리오마다 dependency가 다름
    <ul>
      <li>(varying emphasis on intra- or inter-series dependencies)</li>
      <li>\(\to\) series-relation-aware decider
        <ul>
          <li>: controls the utilization of channel-independent or channel-mixing tokenization strategy</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer-based models
    <ul>
      <li>implicitly models the <strong>inter-series dependencies</strong> through channel-mixing embeddings</li>
      <li>However the <strong>quadratic complexity</strong> of the self-attention mechanism</li>
      <li>Informer, Autoformer : sparse attention
        <ul>
          <li>But balancing computational efficiency and predicting performance는 본질적 해결 X</li>
          <li>게다가 not explicitly capture the inter-series dependencies</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>state-space models (SSM) : design of selective scanning</li>
  <li><strong>Long-term time series modeling</strong> : patching manner (patch-wise tokens)으로 하겠다</li>
  <li><strong>Emphasis on intra- or inter-series dependencies</strong> : 데이터셋마다 intra- or inter-sequence dependencies 둘 중 뭐가 중요한지가 다름</li>
  <li>그래서 <strong>Mamba+</strong>를 디자인함
    <ul>
      <li>adding a forget gate in Mamba,</li>
      <li>to selectively combine the new features with the historical features in a complementary manner</li>
      <li>therefore preserving historical information in a longer range</li>
    </ul>
  </li>
  <li><strong>Mamba+</strong>에 기반한 <strong>Bidirectional Mamba+ (BiMamba+)</strong>를 제안
    <ul>
      <li>to model the MTS data from both forward and backward,
        <ul>
          <li>enhancing the model’s robustness and ability to capture interactions between time series elements</li>
        </ul>
      </li>
      <li>Series-Relation-Aware (SRA) decider
        <ul>
          <li>measures the proportion of highly correlated series pairs in the MTS data</li>
          <li>to automatically choose channel-independent or channelmixing tokenization strategies</li>
        </ul>
      </li>
      <li>patch-wise tokens
        <ul>
          <li>contain richer semantic information</li>
          <li>and encourage the model to learn the long-term dependencies</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="time-series-forecasting">Time Series Forecasting</h3>

<ul>
  <li>Transformer-based models : quadratic complexity to the length of the sequence
    <ul>
      <li>Informer(Zhou et al. 2021) : ProbSparse mechanism</li>
      <li>Autoformer(Wu et al. 2021) : time series decomposition</li>
      <li>Pyraformer(Liu et al. 2021) : pyramidal attention module</li>
      <li>FEDformer(Zhou et al. 2022) : frequency enhanced Transformer through frequency domain mapping</li>
      <li>PatchTST(Nie et al. 2023) : divides each univariate sequence into patches
        <ul>
          <li>and uses patch-wise self-attention to model temporal dependencies</li>
        </ul>
      </li>
      <li>Crossformer(Zhang and Yan 2023) : Cross-Dimension attention</li>
      <li>iTransformer(Liu et al. 2023) : inverts the attention layers
        <ul>
          <li>to straightly model inter-series dependencies</li>
          <li>But, the tokenization approach is simply passing the whole sequence through a Multilayer Perceptron (MLP),
            <ul>
              <li>which overlooks the complex evolutionary patterns inside the time series</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="ssm-based-models">SSM-based models</h3>

<ul>
  <li>RNN-based models :
    <ul>
      <li>maintain a hidden state which is updated with each input element</li>
      <li>limits the training speed and leads to forgetting long-term information</li>
    </ul>
  </li>
  <li>CNN-based models :
    <ul>
      <li>parallel computing and have faster training speed</li>
      <li>limits the inference speed and overlook the long-term global information</li>
    </ul>
  </li>
  <li>State Space Models (SSM) :
    <ul>
      <li>trained in parallel like CNN and inferences fastly like RNN</li>
    </ul>
  </li>
  <li>Mamba
    <ul>
      <li>parameterized matrices and a hardware-aware parallel computing algorithm to SSM</li>
      <li><strong>S-Mamba</strong>(Wang et al. 2024)</li>
      <li>embeds each univariate time series like iTransformer</li>
      <li>and feeds the embeddings into Mamba blocks
        <ul>
          <li>to model the relationships of different time series</li>
        </ul>
      </li>
      <li>However, the tokenization approach may overlook the complex evolutionary patterns</li>
      <li><strong>MambaMixer</strong>(Behrouz et al. 2024)
        <ul>
          <li>adjusts the Mamba block to bidirectional</li>
          <li>and uses two improved blocks to capture inter/intra-series dependencies simultaneously</li>
          <li>However, the gating branch is used to filter new features
            <ul>
              <li>(of both forward and backward directions)</li>
              <li>which may cause challenges for extracting new features</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>TimeMachine</strong>(Ahamed and Cheng 2024)
        <ul>
          <li>a multi-scale quadruple-Mamba architecture
            <ul>
              <li>to unify the handling of channel-mixing and channelindependence situations</li>
            </ul>
          </li>
          <li>However, simply based on the length of historical observations and variable number of different datasets
            <ul>
              <li>the characteristics of the MTS data are not fully considered.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<h3 id="31-preliminaries">3.1. Preliminaries</h3>

<ul>
  <li>Long-term multivariate time series forecasting
    <ul>
      <li>\(\mathbf{X}_{i n}=\left[x_1, x_2, \ldots, x_L\right] \in \mathbb{R}^{L \times M}\), we predict the future values \(\mathbf{X}_{\text {out }}=\left[x_{L+1}, x_{L+2}, \ldots, x_{L+H}\right] \in \mathbb{R}^{H \times M}\)</li>
    </ul>
  </li>
  <li>State Space Models
    <ul>
      <li>using first-order differential equations, \(h^{\prime}(t)=\mathbf{A} h(t)+\mathbf{B} x(t), \quad y(t)=\mathbf{C} h(t)\)
        <ul>
          <li>where \(\mathbf{A} \in \mathbb{R}^{N \times N}, \mathbf{B} \in \mathbb{R}^{D \times N} \text { and } \mathbf{C} \in \mathbb{R}^{N \times D}\)</li>
        </ul>
      </li>
      <li>can be discretized :
        <ul>
          <li>\(\begin{aligned}
&amp; \overline{\mathbf{A}}=\exp (\Delta \mathbf{A}), \\
&amp; \overline{\mathbf{B}}=(\Delta \mathbf{A})^{-1}(\exp (\Delta \mathbf{A})-\mathbf{I}) \cdot \Delta \mathbf{B}
\end{aligned}\),</li>
          <li>\(h_k=\overline{\mathbf{A}} h_{k-1}+\overline{\mathbf{B}} x_k, \quad y_k=\mathbf{C} h_k\),</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>S4</strong>(Gu et al. 2021b) :
    <ul>
      <li>HIPPO Matrix(Gu et al. 2020) to the initialization of matrix A</li>
    </ul>
  </li>
  <li><strong>Mamba</strong>(Gu and Dao 2023) :
    <ul>
      <li>parameterizes the matrices \(\mathbf{B}, \mathbf{C}\) and \(\Delta\) in a data-driven manner,</li>
      <li>introducing a selection mechanism into S4 model</li>
    </ul>
  </li>
</ul>

<h3 id="32-overview">3.2. Overview</h3>

<p><img src="/assets/img/timeseries/BiMamba+/fig1.png" alt="그림1" /></p>

<ul>
  <li>step 1) calculate the tokenization strategy indicator through the SRA decider</li>
  <li>step 2) divide the input series into patches and generate patch-wise tokens
    <ul>
      <li>based on the tokenization strategy indicator</li>
    </ul>
  </li>
  <li>step 3) obtained tokens are fed into multiple Bi-Mamba+ encoders</li>
  <li>step 4) a flatten head and a linear projector to get the final output</li>
</ul>

<h3 id="33-instance-normalization">3.3. Instance Normalization</h3>

<ul>
  <li>the input sequence의 non-stationary statistics를 제거하기 위해 RevIN (Kim et al. 2022) 사용</li>
</ul>

<h3 id="34-token-generalization">3.4. Token Generalization</h3>

<ul>
  <li>SRA Decider
    <ul>
      <li>Channel-independence / dependence는 데이터셋마다 다름</li>
      <li>\(\to\) automatic tokenization process
        <ul>
          <li>데이터셋마다 \(T=\left\{t^1, t^2, \ldots, t^M\right\}\)에 대해
            <ul>
              <li>Spearman correlation coefficients of different series \(t^i \text { and } t^j\) 계산 \(\rho_{i, j}\)</li>
              <li>\(\rho_{i, j}=1-\frac{6 \sum_{k=0}^n\left(\operatorname{Rank}\left(t_k^i\right)-\operatorname{Rank}\left(t_k^j\right)\right)^2}{n\left(n^2-1\right)}\),
                <ul>
                  <li>where \(n\) : the number of observations</li>
                  <li>\(\operatorname{Rank}\left(t_k^i\right)\) : the rank level of the \(k\)-th element in the specific time series \(t^i\)</li>
                </ul>
              </li>
              <li>threshold \(\lambda\)를 정하고 relevant series \(\rho_{\max }^\lambda\) and \(\rho_{\max }^0\)를 센 다음
                <ul>
                  <li>relation ratio \(r=\rho_{\max }^\lambda / \rho_{\max }^0 \geq 1-\lambda\)이면 channel-mixing</li>
                  <li>그렇지 않으면 channelindependent strategy</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/alg1.png" alt="그림1" /></p>

<h3 id="35-tokenization-process">3.5. Tokenization Process</h3>

<ul>
  <li>PatchTST처럼 \(x_{1: L}^i\)를 \(J = \left\lceil\frac{L-P}{S}+1\right\rceil\)개의 patch로 나누고
    <ul>
      <li>(S는 stride, P는 patch에 들어가는 시점의 개수)</li>
      <li>channel-independent strategy에서는 각 patch를 D차원으로 embedding
        <ul>
          <li>\(\to\) \(\mathbb{E}_{\text {ind }} \in \mathbb{R}^{M \times J \times D}\)</li>
        </ul>
      </li>
      <li>channel-mixing strategy에서는 같은 시점의 다른 변수들도 group으로 만들고 각 group을 tokenization
        <ul>
          <li>\(\to \mathbb{E}_{\operatorname{mix}} \in\mathbb{R}^{J \times M \times D}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="36-mamba-block">3.6. Mamba+ Block</h3>

<p><img src="/assets/img/timeseries/BiMamba+/mamba.png" alt="그림1" /></p>

<ul>
  <li><strong>기존 Mamba</strong>는 2개의 branches를 사용
    <ul>
      <li>\(b_1\)에서는 1d-conv와 SSM을 통과, 다른 하나 \(b_2\)에서는 그냥 SiLU activation 통과</li>
      <li>\(b_1\)의 SSM 안에 HIPPO가 있긴 해도 \(b_2\) 때문에 최근 정보가 더 우선시되는 문제</li>
    </ul>
  </li>
  <li>그래서 <strong>Mamba+ block</strong>에서는
    <ul>
      <li>forget gate \(\text{gate}_f=1-\text{gate}_{b_2}\)를 추가</li>
      <li>\(\text{gate}_f\)와 \(\text{gate}_{b_2}\)는 new features와 forgotten historical features를 선택적 결합
        <ul>
          <li>\(\to\) preserving historical information !</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/fig2.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/BiMamba+/alg2.png" alt="그림1" /></p>

<h3 id="37-bidirectional-mamba-encoder">3.7. Bidirectional Mamba+ Encoder</h3>

<ul>
  <li>Channel-mixing이면 \(\mathbb{E}_x^{(l)} \in \mathbb{R}^{J \times M \times D}\) and \(\mathbb{E}_x^{(0)}=\mathbb{E}_{m i x}\)
    <ul>
      <li>otherwise \(\mathbb{E}_x^{(l)} \in \mathbb{R}^{M \times J \times D}\) and \(\mathbb{E}_x^{(0)}=\mathbb{E}_{\text {ind }}\)</li>
    </ul>
  </li>
  <li>2개의 Bi-Mamba+ block, 각각 forward and backward
    <ul>
      <li>각각의 input을 \(\mathbb{E}_{x, d i r}^{(l)}\) where \(\operatorname{dir} \in\{\text{forward,backward}\}\)이라 하면</li>
      <li>\(\mathbb{E}_x^{(l+1)}=\sum_{\text {dir }}^{\{\text {forward,backward }\}} \mathcal{F}\left(\mathbb{E}_{y, \text { dir }}^{(l)}, \mathbb{E}_{x, d i r}^{(l)}\right)\)가 다음 layer의 input이 됨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/alg3.png" alt="그림1" /></p>

<h3 id="38-loss-function">3.8. Loss Function</h3>

<ul>
  <li>MSE : \(\mathcal{L}(Y, \hat{Y})=\frac{1}{\mid Y \mid} \sum_{i=1}^{\mid Y \mid}\left(y_{(i)}-\hat{y}_{(i)}\right)^2\)</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<h3 id="41-datasets">4.1. Datasets</h3>

<p><img src="/assets/img/timeseries/BiMamba+/table1.png" alt="그림1" /></p>

<h3 id="42-baseline-models">4.2. Baseline Models</h3>

<ul>
  <li>Autoformer (Wu et al. 2021)
    <ul>
      <li>series decomposition technique with Auto-Correlation mechanism</li>
    </ul>
  </li>
  <li>PatchTST (Nie et al. 2023)
    <ul>
      <li>patching and channel independent techniques</li>
    </ul>
  </li>
  <li>Crossformer (Zhang and Yan 2023)
    <ul>
      <li>PatchTST + Attention layer (for capture inter-series dependencies)</li>
    </ul>
  </li>
  <li>iTransformer (Liu et al. 2023)
    <ul>
      <li>inverts the modeling method of Transformer</li>
    </ul>
  </li>
  <li>DLinear (Zeng et al. 2023)
    <ul>
      <li>decomposes time series into two different components</li>
    </ul>
  </li>
  <li>TimesNet (Wu et al. 2022)
    <ul>
      <li>transforming the 1-D time series into a set of 2-D tensors</li>
    </ul>
  </li>
  <li>WITRAN (Jia et al. 2024)
    <ul>
      <li>RNN structure that process the univariate input sequence
        <ul>
          <li>in the 2-D space with a fixed scale</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>CrossGNN (Huang et al. 2024)
    <ul>
      <li>time series in a multi-scale way</li>
      <li>GNN to capture both cross-scale and cross-series dependencies</li>
    </ul>
  </li>
  <li>S-Mamba (Wang et al. 2024)
    <ul>
      <li>generates embeddings for each time series through a simple MLP layer</li>
      <li>and uses Mamba to extract inter-series dependencies</li>
    </ul>
  </li>
</ul>

<h3 id="43-experimental-settings">4.3. Experimental Settings</h3>

<ul>
  <li>\(L=96\) for all models on all datasets, \(H \in\{96,192,336,720\}\)</li>
  <li>\(S=\frac{1}{2} P\) and use patch length \(P=\frac{1}{4} L\)</li>
  <li>SRA decider, we set \(\lambda=0.6\)</li>
  <li>for <strong>Bi-Mamba+, PatchTST and Crossformer</strong> that use patching technique, we set \(D=128\) for Weather, Traffic, Electricity, Solar and \(D=64\) for ETT datasets,
    <ul>
      <li>while for <strong>S-Mamba and iTransformer</strong> that map the whole sequence to tokens, we set \(D=512\) for Weather, Traffic, Electricity, Solar and \(D=256\) for ETT datasets.</li>
    </ul>
  </li>
  <li>As for parameters within Mamba+ block은 Ahamed and Cheng 2024; Wang et al. 2024처럼</li>
  <li><strong>convolutional kernel size</strong> d_conv =2 and <strong>hidden state expansion</strong> expand =1 on all datasets.</li>
  <li><strong>hidden dimension</strong> d_state =16 for Weather, Electricity and Traffic and d_state =8 for ETT datasets.</li>
  <li><strong>encoder layer</strong> \(l \in\{1,2,3\}\),</li>
  <li><strong>learning rate</strong>는 \([5 \mathrm{e}-5,1 \mathrm{e}-4,2 \mathrm{e}-4,5 \mathrm{e}-4,1 \mathrm{e}-3, 2 \mathrm{e}-3,5 \mathrm{e}-3]\)</li>
</ul>

<h3 id="44-main-results">4.4. Main Results</h3>

<p><img src="/assets/img/timeseries/BiMamba+/table3.png" alt="그림1" /></p>

<h3 id="45-ablation-study">4.5. Ablation Study</h3>

<p><img src="/assets/img/timeseries/BiMamba+/table4.png" alt="그림1" /></p>

<ul>
  <li>(a) w/o SRA-I which use channel-independent strategy only</li>
  <li>(b) w/o SRA-M which use channelmixing strategy only</li>
  <li>(c) w/o Bi which use forward direction Mamba block only</li>
  <li>(d) w/o Residual that removes the residual connection</li>
  <li>(e) S-Mamba</li>
  <li>
    <p>(f) PatchTST used for the benchmark models</p>
  </li>
  <li><strong>filter threshold</strong> \(\lambda\)에 따른 tokenization strategy indicator</li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/fig3.png" alt="그림1" /></p>

<ul>
  <li>length of patches \(P\)에 따른 MSE</li>
</ul>

<p><img src="/assets/img/timeseries/BiMamba+/fig4.png" alt="그림1" /></p>

<h3 id="46-model-efficiency">4.6. Model efficiency</h3>

<p><img src="/assets/img/timeseries/BiMamba+/fig7-1.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/BiMamba+/fig7-2.png" alt="그림1" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li><strong>Bi-Mamba+</strong></li>
  <li>adding forget gate in Mamba
    <ul>
      <li>to selectively combine the added new features with the forgotten historical features in a complementary manner,</li>
      <li>therefore preserving historical information in a longer range</li>
    </ul>
  </li>
  <li>dividing the time series into patches
    <ul>
      <li>for inter-series dependencies at a finer granularity</li>
    </ul>
  </li>
</ul>

  
</article>



  <hr class="dingbat related mb6" />






  
     


  <aside class="about related mt4 mb4" role="complementary">
    
    

<div class="author mt4">
  

  
    


<img
  
    src="https://via.placeholder.com/128x128"
    srcset="/assets/img/me/me.JPG 1x,/assets/img/me/me.JPG 2x"
    
  
  alt="GW Jeong"
  class="avatar"
  
  width="120"
  height="120"
  loading="lazy"
/>

  

  
  
  <h2  class="page-title hr-bottom">
    About
  </h2>

  <p>Bachelor’s degree in Applied Statistics. Yonsei Univ. (2018~2024) <br />
DataScienceLab 8th 학회장 (2022~2023) <br />
Master’s degree in Statitstics. Yonsei Univ. (2024~)</p>


  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>

  </aside>


  

  
  

  
    


  

  
  

  
    

  


  
<footer class="content" role="contentinfo">
  <hr/>
  
    <p><small class="copyright">© Geonwoo Jeong.
</small></p>
  
  
    <p><small>Powered by <a class="external" href="https://hydejack.com/">Hydejack</a> v<span id="_version">9.1.6</span></small></p>
  <hr class="sr-only"/>
</footer>


</main>

  <hy-drawer
  id="_drawer"
  class=""
  side="left"
  threshold="10"
  noscroll
  
>
  <header id="_sidebar" class="sidebar" role="banner">
    




<div class="sidebar-bg sidebar-overlay" style="background-color:rgb(230, 217, 195);background-image:url(/assets/img/me/sidebar.jpg)"></div>

    <div class="sidebar-sticky">
  <div class="sidebar-about">
    
      <a class="no-hover" href="/" tabindex="-1">
        <img src="/assets/img/me/logo.jpg" class="avatar" alt="LpppJ" width="120" height="120" loading="lazy" />
      </a>
    
    <a class="sidebar-title" href="/"><h2 class="h1">LpppJ</h2></a>
    
    
      <p class="">
        DataScience and AI

      </p>
    
  </div>

  <nav class="sidebar-nav heading" role="navigation">
    <span class="sr-only">Navigation:</span>
<ul>
  
    
      
      <li>
        <a
          id="_drawer--opened"
          href="/timeseries/"
          class="sidebar-nav-item "
          
        >
          TimeSeries
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/pytorch/"
          class="sidebar-nav-item "
          
        >
          Pytorch
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/stat/"
          class="sidebar-nav-item "
          
        >
          Statistics
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/project/"
          class="sidebar-nav-item "
          
        >
          Project
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/presentation/"
          class="sidebar-nav-item "
          
        >
          Presentation
        </a>
      </li>
    
  
</ul>

  </nav>

  
  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>
  </header>
</hy-drawer>
<hr class="sr-only" hidden />

</hy-push-state>


  <!--[if gt IE 10]><!---->
  <script nomodule>!function(){var t,n=document.createElement("script");!("noModule"in n)&&"onbeforeload"in n&&(t=!1,document.addEventListener("beforeload",function(e){if(e.target===n)t=!0;else if(!e.target.hasAttribute("nomodule")||!t)return;e.preventDefault()},!0),n.type="module",n.src=".",document.head.appendChild(n),n.remove())}();
</script>
  <script src="/assets/js/hydejack-9.1.6.js" type="module"></script>
  <script src="/assets/js/LEGACY-hydejack-9.1.6.js" nomodule defer></script>
  

  

<!--<![endif]-->
  <!-- <script>
  document.querySelector('hy-push-state').setAttribute('prefetch', '');

  document.querySelectorAll('.sidebar a[href^="/"]').forEach(function (el) { 
    el.addEventListener('click', function (e) {
      if (el.pathname === window.location.pathname) {
        e.preventDefault();
        e.stopPropagation();
        document.querySelector('hy-drawer').close();
      }
    });
  });
</script> -->

<!--
Code for integrating CloudFlare's email protection with Hydejack's single page app loading.
-->
<script>
  document.getElementById('_pushState').addEventListener('hy-push-state-after', function (e) {
    function e(e){
      (console.error?console.error:console.log).call(console,e)
    }

    function t(e){
      return l.innerHTML='<a href="'+e.replace(/"/g,"&quot;")+'"></a>',l.childNodes[0].getAttribute("href")
    }

    function r(e,t){
      var r=e.substr(t,2);return parseInt(r,16)
    }

    function n(e,n){
      for(var o="",c=r(e,n),a=n+2;a<e.length;a+=2){
        var l=r(e,a)^c;
        o+=String.fromCharCode(l)
      }
      return t(o)
    }

    var o="/cdn-cgi/l/email-protection#",
        c=".__cf_email__",
        a="data-cfemail",
        l=document.createElement("div");

    !function(){
      for(var t=document.getElementsByTagName("a"),r=0;r<t.length;r++)
        try{
          var c=t[r],a=c.href.indexOf(o);
          a>-1&&(c.href="mailto:"+n(c.href,a+o.length))
        }catch(t){
          e(t)
        }
    }(),
    function(){
      for(var t=document.querySelectorAll(c),r=0;r<t.length;r++)
        try{
          var o=t[r],l=n(o.getAttribute(a),0),i=document.createTextNode(l);
          o.parentNode.replaceChild(i,o)
        }catch(t){
          e(t)
        }
    }()
  });
</script>





<div hidden>
  
  <h2 class="sr-only">Templates (for web app):</h2>

  <template id="_animation-template">
  <div class="animation-main fixed-top">
    <nav id="breadcrumbs" class="screen-only"><ul>
  
  
</ul></nav>
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

  <template id="_loading-template">
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

  <template id="_error-template">
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

  <template id="_permalink-template">
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="content-hash"></span>
  </a>
</template>

</div>


</body>
</html>
