---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2412.09871)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Byte Latent Transformer: Patches Scale Better Than Tokens (Arxiv 2024)

## Abstract

- Byte Latent Transformer (BLT)
  - **new byte-level LLM architecture**
  - tokenizer-free
- BLT encodes bytes into **dynamically sized patches** (primary units of computation)
- Patches are segmented based on the **entropy of the next byte**
  - allocating more compute and model capacity where increased data complexity demands it
- Feasibility of scaling models trained on raw bytes without a fixed vocabulary
  - By dynamically selecting long patches when data is predictable

## 1. Introduciton

- 기존 LLM : end-to-end train... but **except for tokenization !**
  - groups bytes into a **static** set of tokens (heuristic)
  - $$\to$$ bias how a string is compressed
  - $$\to$$ shortcomings such as
    - domain/modality sensitivity
    - sensitivity to input noise
    - a lack of orthographic knowledge
    - and multilingual inequity
- LLMs에서 long sequence 다루려면 Tokenization은 필수
  - training on bytes : costly at scale
  - 그래서 self-attention or attention-free architectures
    - But small models일때나 말이 됨
    - 왜냐면 Transformer로 효율적으로 하려고 해봐야 어차피  Scale 관점에서 **large FFN on every byte**가 문제
- 본 논문에서 dynamic, learnable method for **grouping bytes into patches** 제안
  - mixes byte and patch information
  - tokenizaiton과 다르게 no fixed vocabulary for patches
  - Arbitrary groups of bytes are mapped to latent patch representations !
- 기존 Tokenization-based LLMs allocate the **same** amount of compute to **every** token.
  - $$\to$$ trades efficiency for performance 불가피
  - 왜냐면 예측의 complexity랑 token이랑 not always correlated
- 그러니 compute를 필요한 곳에 할당하자
  - ending of most words 예측은 쉬워서 large transformer 필요 X (low-entropy)
- 이 생각을 반영한 BLT:
  - 3 transformer blocks: **two small byte-level local** models and **a large global latent transformer**
  - how to dynamically allocate compute = how to group bytes into patches 
  - BLT segments data based on the **entropy of the next-byte prediction**

## 2. Patching: From Individual Bytes to Groups of Bytes

![그림1](/assets/img/LLM/BLT/fig3.png)

- Patching : segmenting the sequence of byte $$\boldsymbol{x}=\left\{x_i, \mid i=1, \ldots n\right\}$$ into $$p=\left\{p_j \mid j=1, \ldots, m\right\}, m<n$$
  - by mapping each $$x_i$$ to the set $$\{0,1\}$$ where 1 indicates the start of a new patch
  - the computational cost = the number of main Transformer execution
    - BLT에서는 = the number of patches !
    - 따라서 average size of a patch, or simply patch size 매우 중요함
- 그러므로 여기서는 patching functions을 다룸
  - patching with a fixed number of bytes per patch
  - whitespace patching
  - dynamically patching with entropies from a small byte LM

### 2.1. Strided Patching Every K Bytes

- 일반적으로는 bytes into patches of fixed size $$k$$ as done in MegaByte
  - changing the average patch size 쉽고 control the FLOP cost 쉬움
  - But ! compute is not dynamically allocated
    - 공백(whitespace) 예측하려고 Transformer 쓴다거나
    - byte가 많은 math에 compute를 안쓴다거나...
  - 그러다보니 inconsistent and non-contextual patching of similar byte sequences (같은 걸 다르게 split)

### 2.2. Space Patching

- creates new patches after any space-like bytes (= 공백 이후에 patch 시작)
- natural boundaries for linguistic units in many languages (=언어의 관점에서 자연스러운 접근)
- words are patched in the same way across sequences
  - FLOPs are allocated for hard predictions which often follow spaces (공백 바로 뒤)
  - ex. “Who composed the Magic Flute?”에 답하기 위해서 바로 맞추라고 하면 어렵지만 M을 주면 Mozart 맞추기 쉬워짐 !
- But ! cannot gracefully handle all languages and domains
  - 무엇보다도 cannot vary thepatch size

### 2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM

- 2.2 같은 rule-based heuristic such as whitespace 말고 data-driven approach 원함
  - identify high uncertainty next-byte predictions, 즉 entropy patching
- 먼저 a small byte-level auto-regressive language model 하나 학습시키고
  - 그 다음 compute next byte entropies under the LM distribution $$p_e$$  over the byte vocabulary $$\mathcal{V} :$$
  - $$H\left(x_i\right)=\sum_{v \in \mathcal{V}} p_e\left(x_i=v \mid x_{<i}\right) \log p_e\left(x_i=v \mid \boldsymbol{x}_{<i}\right)$$.
  - Entropy가 주어졌을 때 patch boundaries는 어떻게 찾냐
    - 첫째, entropy가 그냥 threshold보다 높은 지점
    - 둘째, 갑자기 entropy가 커진 지점
    - i.e., $$\begin{aligned}\text{Global Constraint} \quad H\left(x_t\right)>\theta_g$ \\
      \text{Approx. Monotonic Constraint} \quad H\left(x_t\right)-H\left(x_{t-1}\right)>\theta_r\end{aligned}$$

### 2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching

- 대부분의 LLMs (Llama 3 포함) use a subword tokenizer like BPE
- 본 논문에서는
  - "**tokens**": byte-groups drawn from a finite vocabulary determined prior totraining
  - "**patches**": dynamically grouped sequences without a fixed vocabulary
  - 둘의 차이는 with tokens, the model has no direct access to theunderlying byte features

![그림1](/assets/img/LLM/BLT/fig4.png)

- BLT는 the trade off between the vocabulary **size** and **compute**를 re-define함
  - 원래는 vocabulary size 커짐 = larger tokens = larger final projection layer
  - Llama 3도 Llama 2에 비해 average token size를 3.7에서 4.4 bytes로 늘리면서 embedding table size는 4배 됨
- BLT는 결국 byte sequence 내에서 지금이 patch boundary인가 아닌지 판단하면 되고
  - 즉 = Latent Transformer로 more compute 할지말지 판단
  - 이건 아직 생성 안된 rest of sequence랑 독립
  - 즉 incremental patching $$f_p\left(x_{<i}\right)=f_p(\boldsymbol{x})_{<i}$$이 아님
  - 즉 같은 prefix라도 다르게 tokenize 가능

## 3. BLT Architecture

![그림1](/assets/img/LLM/BLT/fig2.png)

- BLT의 구성은:
  - **A large** global autoregressive language model
    - that operates on patch representations,
  - **Two** smaller local models
    - encode sequences of bytes into patches
    - decode patch representations back into bytes

### 3.1 Latent Global Transformer Model

- The Latent Global Transformer is an autoregressive transformer model $$\mathcal{G}$$ with $$l_{\mathcal{G}}$$ layers
  - sequence of latent input patch representations $$p_j$$ $$\to$$ sequence of output patch representations $$o_j$$로 변환
- block-causal attention mask 사용
  - 현재 문서 내 현재 배치까지 범위에서만 attention
  - pre-training, inference에서 FLOP 대부분이라 언제 실행할지가 complexity 결정

### 3.2. Local Encoder

![그림1](/assets/img/LLM/BLT/fig5.png)

- The Local Encoder Model, denoted by $$\mathcal{E}$$
  - lightweight transformer-based model with layers $$l_{\mathcal E} << l_{\mathcal G}$$
  - input bytes $$b_i$$ $$\to$$ patch representation $$p_j$$
  - Transformer랑 다른 건 cross-attention layer after each transformer layer (pool byte representations into patch representations 하는 부분) - 3.2.2에서 이해됨
  - input sequence of bytes, $$b_i$$, are embedded using a $$\mathbb{R}^{256 \times h_{\mathcal E}}$$ matrix, denoted as $$x_i$$
  - hash-embeddings는 optional
- 즉 alternating transformer and cross-attention layers가 input bytes $$b_i$$의 representation을 $$\to$$ patch representation $$p_j$$
- local block causal attention mask; 사용해서 each byte attends to a fixed window of $$\mathcal w_{\mathcal{E}}$$
  - 다른 patch 볼 수 있지만 현재 문서 내에서만 !

### 3.2.1. Encoder Hash n-gram Embeddings

- 결국 to incorporate information about the preceding bytes가 중요
- byte $$b_i$$를 as individual로, 그리고 as part of a byte n-gram으로 둘다 봄
  - Byte-gram: $$g_{i, n}=\left\{b_{i-n+1}, \ldots, b_i\right\}$$ (with a fixed size for each size $$n \in\{3,4,5,6,7,8\}$$)
  - 그 다음 individual embedding에 다 더함. i.e.,
    - $$e_i  =x_i+\sum_{n=3, \ldots, 8} E_n^{\text {hash }}\left(\operatorname{Hash}\left(g_{i, n}\right)\right)$$.
    - where $$\operatorname{Hash}\left(g_{i, n}\right)=\operatorname{RollPolyHash}\left(g_{i, n}\right) \% \mid E_n^{\text {hash }}\mid$$
  - 그 다음에 normalizing !

### 3.2.2 Encoder Multi-Headed Cross-Attention

- 이제 진짜 bytes-sequence를 patch-sequence로 바꿔보자
- Perceiver의 입력 cross-attention과 비슷하지만 patch 크기가 동적 !
- Patch represenation은 관련된 byte만으로 학습됨
  - initialization은 byte representation에 pooling
  - Cross-Attention을 통해 Patch represenation이 풍부한 맥락을 반영 
  - $$\begin{aligned} P_{0, j} & =\mathcal{E}_C\left(f_{\text {bytes }}\left(\left(p_j\right)\right), f \text { is a pooling function }\right. \\ P_l & =P_{l-1}+W_o\left(\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\right) \\ \text { where } Q_j & =W_q\left(P_{l-1, j}\right), K_i=W_k\left(h_{l-1, i}\right), V_i=W_v\left(h_{l-1, i}\right) \\ h_l & =\text { Encoder-Transformer-Layer }\left(h_{l-1}\right)\end{aligned}$$.

### 3.3. Local Decoder

- Local encoder와 비슷하게 lightweight transformer-based model with $$l_{\mathcal D} << l_{\mathcal G}$$
- patch representation $$o_j$$ $$\to$$ raw bytes $$y_i$$
- 이번에는 Cross-Attention으로 byte representation으로 변환
  - $$\begin{aligned}
    & D_0=h_{l_{\varepsilon}} \\
    & B_l=D_{l-1}+W_o\left(\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\right) \text {, } \\
    & \text { where } Q_i=W_q\left(d_{l-1, i}\right), K_i=W_k\left(\mathcal{D}_C\left(o_j\right)\right), V_i=W_v\left(\mathcal{D}_C\left(o_j\right)\right) \\
    & D_l=\text { Decoder-Transformer-layer }{ }_l\left(B_l\right)
    \end{aligned}$$.
  - Encoder는 byte가 K/V, patch가 Q / Decoder는 patch가 K/V, byte가 Q

## 4. Experimental Setup

### 4.1. Pre-training Datasets

- LLaMA 2 (Touvron et al., 2023)으로 BLT model scaling law experiments
-  BLT-1Tdㅡ로 Llama3와 비교

### 4.2. Entropy Model

- BLT의 Entropy-Based Patching을 위한 language model
  - Byte-level auto-regressive transformer 사용
  - trained on the same training distribution as the full BLT model

### 4.3. Entropy Threshold and Equalizing Context Length

- 공정한 비교를 위해 patch size가 동적으로 변하더라도 same average context length i.e.,
  - the number of bytes ineach **batch** remains constant **in expectation**
  - 모든 배치가 동일한 패치 개수를 가지도록 하여 large patch size로 인한 memory spikes 방지

### 4.4. Entropy Model Context

![그림1](/assets/img/LLM/BLT/fig9.png)

- entropy patching yields progressively **larger patches** in structured content like multiple choice tasks
- 반복되다보니 entropy가 낮아지는 것 (lower entropy on the repeated content)
- 따라서 *new lines*에서 entropy context 리셋하고 앞서 설명한 approximate monontonicity constraint 적용

### 4.5. FLOPs Estimation

- 주로 FLOPs가 발생하는 곳 3:
  - FFN, Self-attention, Output projection !
  - $$\begin{aligned} \mathrm{FL}_{\mathrm{BLT}} & =\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{G}}, l_{\mathcal{G}}, m=n_{c t x} / n_p, V=0\right) / n_p \\ & +\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{E}}, l_{\mathcal{E}}, m=w_{\mathcal{E}}, V=0\right) \\ & +\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{D}}, l_{\mathcal{D}}, m=w_{\mathcal{D}}, V=256\right) \\ & +\operatorname{Cross} \operatorname{Attn} . \mathrm{FL}\left(h_{\mathcal{E}}, l_{\mathcal{E}}, m=n_p, r=n_p / k\right) \times k / n_p \\ & + \text { Cross Attn. FL }\left(h_{\mathcal{D}}, l_{\mathcal{D}}, m=k, r=k / n_p\right)\end{aligned}$$.

### 4.6. Bits-Per-Byte Estimation

- Perplexity per **token**는 비교가 어렵기 때문에, **Byte**-Level에서 평가하는 Bits-Per-Byte(**BPB**) 사용
  - $$\operatorname{BPB}(x)=\frac{\mathcal{L}_{C E}(\boldsymbol{x})}{\ln (2) \cdot n_{\text {bytes }}}$$.

### 4.7. Transformer Architecture Hyperparameters

- Llama 3와 동일한 hyperparameters setting
  - Activation: SwiGLU
  - Positional embedding: RoPE
  - Normalization: RMSNorm
  - Attention optimization: Flash Attention

### 4.8. BLT-Specific Hyperparameters

- N-gram Hash Embeddings : 3~8

pass

## 5. Scaling Trends

### 5.1. Parameter Matched Compute Optimal Scaling Trends

- compute-optimal scaling of BLT
  - compared in terms of training **FLOPs** and language modeling **performance**
- Larger patch sizes in BLT (6 or 8 bytes) **reduce inference FLOPs by up to 50%** while maintaining comparable performance to BPE models.

### 5.2. Beyond Compute Optimal Task Evaluations

![그림1](/assets/img/LLM/BLT/table1.png)

- Evaluations include :**common sense reasoning, world knowledge, and code generation** tasks
- Llama 3 보다 뛰어난 이유는
  - Better use of compute through dynamic patching !
  - Direct modeling of byte-level information rather than tokens
  - Larger patch size로 inference FLOPs reduction !

### 5.3. Patches Scale Better Than Tokens

![그림1](/assets/img/LLM/BLT/table2.png)

- 모델 크기와 패치 크기를 동시에 증가시키면서도 동일한 학습 및 추론 FLOP 예산을 유지

## 6. Byte Modeling Improves Robustness

### 6.1. Character-Level Tasks

![그림1](/assets/img/LLM/BLT/fig7.png)

- Byte-level models are inherently more robust to **input noise** and better at handling **character-level variations.**

![그림1](/assets/img/LLM/BLT/table3.png)

### 6.2. Training BLT from Llama 3

- Pre-traind tokenizer-based model을 leverage해서 더 빠르게 수렴할 수 있는가
- BLT from Llama 3.1이 from Llama 3보다 좋음 ! 

![그림1](/assets/img/LLM/BLT/table5.png)

## 7. Ablations and Discussion

![그림1](/assets/img/LLM/BLT/fig8.png)

![그림1](/assets/img/LLM/BLT/table6.png)

![그림1](/assets/img/LLM/BLT/table78.png)

![그림1](/assets/img/LLM/BLT/table9.png)

## 8. Related Work

Pass

## 9. Limitations and FutureWork

- Scaling laws were calculated for BPE-level transformers and may lead to suboptimal (data, parameter sizes) ratios in the case of BLT
- 실험들이 <1B parameters에서 진행. >8B parameters에서는 확장 가능성만 제시했을 뿐
- Entropy model을 별도로 훈련해서 patching하지만 이것까지도 end-to-end 하고자 함

## 10. Conclusion

- Fixed-vocabulary tokenization에 의존하지 않는 Byte Latent Transformer (BLT)
  - Byte를 learnable, dynamic하게 patching
  - 소폭의 성능 손실을 감수하는 대신, 추론 시 FLOP 연산량을 최대 50%까지 줄일 수 있음
- 기존 LLM은 fixed context에서 model size만 증가
  - BLT는 동일한 inference FLOP 내에서 model size와 patch size를 동시에 확장할 수 있음

