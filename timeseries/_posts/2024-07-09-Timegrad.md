---
layout: post
related_posts:
  _
title: 
description: >
  [ICML 2021](https://arxiv.org/pdf/2210.02186)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Timegrad :Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (ICML 2021)

## Abstract

- **TimeGrad** : auto-regressive model for multivariate time series forecasting, using diffusion
  - learns gradients by optimizing a variational bound on the data likelihood
  - inference는 white noise에서 학습한 분포의 sample로 convert (though a Markov chain)

## 1. Introduction

pass

## 2. Diffusion Probabilistic Model

pass

## 3. TimeGrad Method

- Multivariate time series $$x_{i, t}^0 \in \mathbb{R}$$ for $$i \in\{1, \ldots, D\}$$ where $$t$$ is the time index
  - at time $$t$$, $$\mathbf{x}_t^0 \in \mathbb{R}^D$$
  - context window $$\left[1, t_0\right)$$, prediction interval $$\left[t_0, T\right]$$
- TimeGrad 이전까지는 full joint distribution at each time step을 모델링 했어야 함
  - 하지만 full covariance matrix을 모델링 하는 것은 computation cost 측면에서 impractical
  - 그래서 Gaussians with low-rank covariance matrices으로 approximate 하기도 함 (Vec-LSTM)

- 본 논문에서는 과거 데이터를 보고 미래 시점의 conditional distribution을 학습
  - formula : $$q_{\mathcal{X}}\left(\mathbf{x}_{t_0: T}^0 \mid \mathbf{x}_{1: t_0-1}^0, \mathbf{c}_{1: T}\right)=\Pi_{t=t_0}^T q_{\mathcal{X}}\left(\mathbf{x}_t^0 \mid \mathbf{x}_{1: t-1}^0, \mathbf{c}_{1: T}\right)$$​​

![그림1](/assets/img/timeseries/Timegrad/fig1.png)

### 3.1. Training

- 이전 시점 데이터 $$\mathbf{x}_{t-1}^0$$과 covariates $$\mathbf{c}_{t-1}$$이 들어오면 hidden state $$\mathbf{h}_{t-2}$$를 $$\mathbf{h}_{t-1}$$​로 업데이트
  - 즉 $$\mathbf{h}_t=\mathrm{RNN}_\theta\left(\operatorname{concat}\left(\mathbf{x}_t^0, \mathbf{c}_t\right), \mathbf{h}_{t-1}\right)$$
- 그러면 위에 있는 fomula는 $$\Pi_{t=t_0}^T p_\theta\left(\mathbf{x}_t^0 \mid \mathbf{h}_{t-1}\right)$$가 되고
  - Negative log-likelihood $$\sum_{t=t_0}^T-\log p_\theta\left(\mathbf{x}_t^0 \mid \mathbf{h}_{t-1}\right)$$를 minimize하도록 학습

### 3.2. Inference

- Inference할 때에는 한 시점씩 auto-regressive하게 작동
  - 만약 다음 시점의 sample $$\mathbf{x}_{T+1}^0$$을 얻었다면 위에서 설명한 것처럼 hidden state $$\mathbf{h}_{T+1}$$를 얻고
  - 같은 과정을 반복. 얻은 sample로 또 다음 sample을 얻고...

### 3.3. Scaling

- 각 context window를 scale normalizing
- Residual connection은 사용하지 않음

### 3.4. Covariates

- $$\mathbf{c}_t$$는 time-dependent and time- independent embeddings으로 구성되는 embeddings for categorical features
- All covariates are thus known for the periods we wish to forecast !

## 4. Experiments

- 사용한 데이터셋

![그림11](/assets/img/timeseries/Timegrad/table1.png)

- Model architecture

![그림2](/assets/img/timeseries/Timegrad/fig2.png)

- Results

![그림12](/assets/img/timeseries/Timegrad/table2.png)

