<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-07-09T00:34:15+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (ICML 2021)</title><link href="http://localhost:4000/timeseries/2024-07-09-Timegrad/" rel="alternate" type="text/html" title="Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (ICML 2021)" /><published>2024-07-09T00:00:00+09:00</published><updated>2024-07-09T00:34:09+09:00</updated><id>http://localhost:4000/timeseries/Timegrad</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-09-Timegrad/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li><strong>TimeGrad</strong> : auto-regressive model for multivariate time series forecasting, using diffusion
    <ul>
      <li>learns gradients by optimizing a variational bound on the data likelihood</li>
      <li>inference는 white noise에서 학습한 분포의 sample로 convert (though a Markov chain)</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<p>pass</p>

<h2 id="2-diffusion-probabilistic-model">2. Diffusion Probabilistic Model</h2>

<p>pass</p>

<h2 id="3-timegrad-method">3. TimeGrad Method</h2>

<ul>
  <li>Multivariate time series \(x_{i, t}^0 \in \mathbb{R}\) for \(i \in\{1, \ldots, D\}\) where \(t\) is the time index
    <ul>
      <li>at time \(t\), \(\mathbf{x}_t^0 \in \mathbb{R}^D\)</li>
      <li>context window \(\left[1, t_0\right)\), prediction interval \(\left[t_0, T\right]\)</li>
    </ul>
  </li>
  <li>TimeGrad 이전까지는 full joint distribution at each time step을 모델링 했어야 함
    <ul>
      <li>하지만 full covariance matrix을 모델링 하는 것은 computation cost 측면에서 impractical</li>
      <li>그래서 Gaussians with low-rank covariance matrices으로 approximate 하기도 함 (Vec-LSTM)</li>
    </ul>
  </li>
  <li>본 논문에서는 과거 데이터를 보고 미래 시점의 conditional distribution을 학습
    <ul>
      <li>formula : \(q_{\mathcal{X}}\left(\mathbf{x}_{t_0: T}^0 \mid \mathbf{x}_{1: t_0-1}^0, \mathbf{c}_{1: T}\right)=\Pi_{t=t_0}^T q_{\mathcal{X}}\left(\mathbf{x}_t^0 \mid \mathbf{x}_{1: t-1}^0, \mathbf{c}_{1: T}\right)\)​​</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/fig1.png" alt="그림1" /></p>

<h3 id="31-training">3.1. Training</h3>

<ul>
  <li>이전 시점 데이터 \(\mathbf{x}_{t-1}^0\)과 covariates \(\mathbf{c}_{t-1}\)이 들어오면 hidden state \(\mathbf{h}_{t-2}\)를 \(\mathbf{h}_{t-1}\)​로 업데이트
    <ul>
      <li>즉 \(\mathbf{h}_t=\mathrm{RNN}_\theta\left(\operatorname{concat}\left(\mathbf{x}_t^0, \mathbf{c}_t\right), \mathbf{h}_{t-1}\right)\)</li>
    </ul>
  </li>
  <li>그러면 위에 있는 fomula는 \(\Pi_{t=t_0}^T p_\theta\left(\mathbf{x}_t^0 \mid \mathbf{h}_{t-1}\right)\)가 되고
    <ul>
      <li>Negative log-likelihood \(\sum_{t=t_0}^T-\log p_\theta\left(\mathbf{x}_t^0 \mid \mathbf{h}_{t-1}\right)\)를 minimize하도록 학습</li>
    </ul>
  </li>
</ul>

<h3 id="32-inference">3.2. Inference</h3>

<ul>
  <li>Inference할 때에는 한 시점씩 auto-regressive하게 작동
    <ul>
      <li>만약 다음 시점의 sample \(\mathbf{x}_{T+1}^0\)을 얻었다면 위에서 설명한 것처럼 hidden state \(\mathbf{h}_{T+1}\)를 얻고</li>
      <li>같은 과정을 반복. 얻은 sample로 또 다음 sample을 얻고…</li>
    </ul>
  </li>
</ul>

<h3 id="33-scaling">3.3. Scaling</h3>

<ul>
  <li>각 context window를 scale normalizing</li>
  <li>Residual connection은 사용하지 않음</li>
</ul>

<h3 id="34-covariates">3.4. Covariates</h3>

<ul>
  <li>\(\mathbf{c}_t\)는 time-dependent and time- independent embeddings으로 구성되는 embeddings for categorical features</li>
  <li>All covariates are thus known for the periods we wish to forecast !</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>사용한 데이터셋</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/table1.png" alt="그림11" /></p>

<ul>
  <li>Model architecture</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/fig2.png" alt="그림2" /></p>

<ul>
  <li>Results</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/table2.png" alt="그림12" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2021](https://arxiv.org/pdf/2210.02186)]]></summary></entry><entry><title type="html">TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis (ICLR 2023)</title><link href="http://localhost:4000/timeseries/2024-07-08-TimesNet/" rel="alternate" type="text/html" title="TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis (ICLR 2023)" /><published>2024-07-08T00:00:00+09:00</published><updated>2024-07-08T13:16:56+09:00</updated><id>http://localhost:4000/timeseries/TimesNet</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-08-TimesNet/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Temporal variation modeling을 위해서 multi-periodicity in time series를 파악
    <ul>
      <li>complex temporal variations into the multiple intraperiod- and interperiod-variations</li>
      <li>각각이 2D tensor의 행과 열이 된다.</li>
    </ul>
  </li>
  <li>TimesBlock : task-general backbone for TS analysis</li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>다른 타입의 데이터(language, video, …) Time series는 시점마다 하나의 숫자가 기록되고, 연속적으로 기록되기 때문에 충분한 semantic information을 제공하지 않는다.
    <ul>
      <li>그래서 더 informative하고 inherent properties(continuity, periodicity, trend, …)를 reflect하는 temporal variations를 파악하고자 함</li>
      <li>하지만 temporal patterns는 복잡하고 다양한 variations가 mix and overlap 되어있음</li>
    </ul>
  </li>
  <li>복잡한 temporal variations를 파악하기 위해
    <ul>
      <li>RNN-based approach는 sequential computation paradigm 때문에 어렵고</li>
      <li>TCN-based approach는 1d conv의 locality property 때문에 long-term dependencies 파악 어려움</li>
      <li>Transformer-based approach도 directly find out하기는 어려움
        <ul>
          <li>그래서 intricate temporal variations을 찾기 위해 <strong>multi-periodicity</strong>를 활용</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>multiple periods는 overlap and interact with each other. 그래서 intractable
    <ul>
      <li>또한 각 periods에서 variation of each time point는 adjacent area뿐만 아니라 adjacent periods의 variation에도 많은 영향을 받음
        <ul>
          <li>전자가 intraperiod-variation (short-term temporal patterns within a period),</li>
          <li>후자가 interperiod-variation (long-term trends of consecutive different periods)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>1d time series에는 두 가지 variations를 동시에 표현하기 어려움 그래서 2d tensor로 만들고
    <ul>
      <li>각 columns에는 time points within a period,</li>
      <li>각 row에는 time points at the same phase among different periods</li>
    </ul>
  </li>
  <li>TimesNet은 learned periods로 multi-periodicity를 발견하고 intraperiod- and interperiod-variations를 capture</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>Classical methods : ARIMA, Holt-Winter, Prophet
    <ul>
      <li>temporal variations가 pre-defined pattern을 따른다고 가정하지만 실제로는 X</li>
    </ul>
  </li>
  <li>MLP-based
    <ul>
      <li>adopt the MLP along the temporal dimension</li>
      <li>encode the temporal dependencies into the <strong>fixed</strong> parameter of MLP layers</li>
    </ul>
  </li>
  <li>TCN-based
    <ul>
      <li>convolutional kernels that slide along the temporal dimension</li>
    </ul>
  </li>
  <li>RNN-based
    <ul>
      <li>state transitions among time steps</li>
    </ul>
  </li>
  <li>Autoformer
    <ul>
      <li>the series-wise temporal dependencies based on the learned periods</li>
      <li>deep decomposition architecture to obtain the seasonal and trend parts of input series</li>
    </ul>
  </li>
  <li>FEDformer
    <ul>
      <li>mixture-of-expert design to enhance the seasonal-trend decomposition</li>
      <li>sparse attention within the frequency domain</li>
    </ul>
  </li>
  <li><strong>TimesNet : temporal 2D-variations derived by periodicity</strong></li>
</ul>

<h2 id="3-timesnet">3. TimesNet</h2>

<ul>
  <li>TimesBlock
    <ul>
      <li>transform the 1D time series into 2D space</li>
      <li>simultaneously model the two types of variations by a parameter-efficient inception block</li>
    </ul>
  </li>
</ul>

<h3 id="31-transform-1d-variations-into-2d-variations">3.1. Transform 1d-variations into 2d-variations</h3>

<p><img src="/assets/img/timeseries/TimesNet/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p>Two Temporal variations, simultaneously !</p>

    <ul>
      <li>
        <p>with its adjacent area (intraperiod-variations)</p>
      </li>
      <li>
        <p>with the same phase among different periods (interperiod-variations)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>원래 Time series는 \(\mathbf{X}_{1 \mathrm{D}} \in \mathbb{R}^{T \times C}\)</p>
  </li>
  <li>
    <p>먼저  Fast Fourier Transform (FFT)으로 periods를 찾음</p>

    <p>\(\mathbf{A}=\operatorname{Avg}\left(\operatorname{Amp}\left(\operatorname{FFT}\left(\mathbf{X}_{1 \mathrm{D}}\right)\right)\right),\left\{f_1, \cdots, f_k\right\}=\underset{f_* \in\left\{1, \cdots,\left[\frac{T}{2}\right]\right\}}{\arg \operatorname{Topk}}(\mathbf{A}), p_i=\left\lceil\frac{T}{f_i}\right\rceil, i \in\{1, \cdots, k\}\)</p>
    <ul>
      <li>\(FFT(\cdot)\)은 Fast Fourier Transform, \(\text{Amp}(\cdot)\)은 amplitude 값 계산</li>
      <li>\(\mathbf{A} \in \mathbb{R}^T\)는 각 frequency에서 계산된 amplitude (averaged from C dimensions by \(\text{Avg}(\cdot)\))
        <ul>
          <li>즉 \(\mathbf{A}_j\)는 intensity of the frequency-j periodic basis function (period length는 \(\left\lceil\frac{T}{j}\right\rceil\))</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>top k개의 amplitude에 해당하는 most significant frequencies \(\left\{f_1, \cdots, f_k\right\}\)만 사용</p>

    <ul>
      <li>불필요한 high frequencies는 필요 없으니까</li>
      <li>해당하는 period length는 \(\left\{p_1, \cdots, p_k\right\}\)</li>
      <li>즉 \(\mathbf{A},\left\{f_1, \cdots, f_k\right\},\left\{p_1, \cdots, p_k\right\}=\operatorname{Period}\left(\mathbf{X}_{1 \mathrm{D}}\right)\)</li>
    </ul>
  </li>
  <li>
    <p>이제 \(\mathbf{X}_{1 \mathrm{D}} \in \mathbb{R}^{T \times C}\)를 여러 개의 2d-tensors로 바꿈</p>

    <ul>
      <li>\(\mathbf{X}_{2 \mathrm{D}}^i=\operatorname{Reshape}_{p_i, f_i}\left(\text { Padding }\left(\mathbf{X}_{1 \mathrm{D}}\right)\right), i \in\{1, \cdots, k\}\)​</li>
      <li>Padding은 \(\operatorname{Reshape}_{p_i, f_i}(\cdot)\)을 위해 수행</li>
      <li>즉 \(\mathbf{X}_{2 \mathrm{D}}^i \in \mathbb{R}^{p_i \times f_i \times C}\)는 time series based on frequency-\(f_i\)
        <ul>
          <li>열은 intraperiod-variation, 행은 interperiod-variation, under the corresponding period length \(p_i\)​</li>
        </ul>
      </li>
      <li>최종적으로는 서로 다른 주기 k개에 대한 2d-tensors \(\left\{\mathbf{X}_{2 \mathrm{D}}^1, \cdots, \mathbf{X}_{2 \mathrm{D}}^k\right\}\)​​를 얻음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TimesNet/fig2.png" alt="그림2" /></p>

<h3 id="32-timesblock">3.2. TimesBlock</h3>

<p><img src="/assets/img/timeseries/TimesNet/fig3.png" alt="그림3" /></p>

<ul>
  <li>처음에는 \(\mathbf{X}_{1 \mathrm{D}}^0=\operatorname{Embed}\left(\mathbf{X}_{1 \mathrm{D}}\right)\)하고 \(l=1\)부터는 \(\mathbf{X}_{1 \mathrm{D}}^l=\text { TimesBlock }\left(\mathbf{X}_{1 \mathrm{D}}^{l-1}\right)+\mathbf{X}_{1 \mathrm{D}}^{l-1}\)</li>
  <li>각 TimesBlock은 two successive parts:
    <ul>
      <li>capturing temporal 2D-variations</li>
      <li>adaptively aggregating representations from different periods</li>
    </ul>
  </li>
</ul>

<h3 id="capturing-temporal-2d-variations">Capturing temporal 2D-variations</h3>

\[\begin{aligned}
\mathbf{A}^{l-1},\left\{f_1, \cdots, f_k\right\},\left\{p_1, \cdots, p_k\right\} &amp; =\operatorname{Period}\left(\mathbf{X}_{1 \mathrm{D}}^{l-1}\right) \\
\mathbf{X}_{2 \mathrm{D}}^{l, i} &amp; =\operatorname{Reshape}_{p_i, f_i}\left(\operatorname{Padding}\left(\mathbf{X}_{1 \mathrm{D}}^{l-1}\right)\right), i \in\{1, \cdots, k\} \\
\widehat{\mathbf{X}}_{2 \mathrm{D}}^{l, i} &amp; =\operatorname{Inception}\left(\mathbf{X}_{2 \mathrm{D}}^{l, i}\right), i \in\{1, \cdots, k\} \\
\widehat{\mathbf{X}}_{1 \mathrm{D}}^{l, i} &amp; =\operatorname{Trunc}\left(\operatorname{Reshape}_{1,\left(p_i \times f_i\right)}\left(\widehat{\mathbf{X}}_{2 \mathrm{D}}^{l, i}\right)\right), i \in\{1, \cdots, k\},
\end{aligned}\]

<ul>
  <li>\(\mathbf{X}_{2 \mathrm{D}}^{l, i} \in \mathbb{R}^{p_i \times f_i \times d_{\text {model }}}\)은 \(i\)-번째 transformed 2d-tensor</li>
  <li>그 다음  parameter-efficient inception block \(\text{Inception}(·)\)을 거쳐 \(\widehat{\mathbf{X}}_{2D}^{l, i}\)를 얻고</li>
  <li>
    <p>Aggregation을 위해 \(\text{Trunc}(\cdot)\)을 사용하여  \(\widehat{\mathbf{X}}_{\mathrm{1D}}^{l, i} \in \mathbb{R}^{T \times d_{\mathrm{model}}}\)로 되돌린다.</p>
  </li>
  <li>intraperiod-variation (columns) : cover adjacent time points</li>
  <li>interperiod-variation (rows) : cover adjacent periods</li>
  <li>서로 다른 여러 개의 2d-tensors \(\left\{\mathbf{X}_{2 \mathrm{D}}^{l, 1}, \cdots, \mathbf{X}_{2 \mathrm{D}}^{l, k}\right\}\)에 대해 shared inception block
    <ul>
      <li>for parameter efficiency, invariant to the selection of hyper-parameter k</li>
    </ul>
  </li>
</ul>

<h3 id="adaptive-aggregation">Adaptive Aggregation</h3>

<ul>
  <li>이제 k개의 서로 다른 1d-representations \(\left\{\widehat{\mathbf{X}}_{1 \mathrm{D}}^{l, 1}, \cdots, \widehat{\mathbf{X}}_{1 \mathrm{D}}^{l, k}\right\}\)를 fuse해서 다음 layer에 전달해야 함
    <ul>
      <li>amplitudes \(\mathbf{A}\)는 각 frequencies and periods의 상대적인 중요도라 할 수 있으므로</li>
      <li>\(\begin{aligned}
\widehat{\mathbf{A}}_{f_1}^{l-1}, \cdots, \widehat{\mathbf{A}}_{f_k}^{l-1} &amp; =\operatorname{Softmax}\left(\mathbf{A}_{f_1}^{l-1}, \cdots, \mathbf{A}_{f_k}^{l-1}\right) \\
\mathbf{X}_{1 \mathrm{D}}^l &amp; =\sum_{i=1}^k \widehat{\mathbf{A}}_{f_i}^{l-1} \times \widehat{\mathbf{X}}_{1 \mathrm{D}}^{l, i}
\end{aligned}\)로 aggregate</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/TimesNet/table1.png" alt="그림11" /></p>

<p><img src="/assets/img/timeseries/TimesNet/fig4.png" alt="그림4" /></p>

<h3 id="main-results">Main results</h3>

<p><img src="/assets/img/timeseries/TimesNet/table2.png" alt="그림12" /></p>

<h3 id="imputation-task">Imputation task</h3>

<p><img src="/assets/img/timeseries/TimesNet/table4.png" alt="그림14" /></p>

<h3 id="classification-task">Classification task</h3>

<p><img src="/assets/img/timeseries/TimesNet/fig5.png" alt="그림5" /></p>

<h3 id="anomaly-detection-task">Anomaly detection task</h3>

<p><img src="/assets/img/timeseries/TimesNet/table5.png" alt="그림15" /></p>

<h3 id="representation-analysis">Representation analysis</h3>

<p><img src="/assets/img/timeseries/TimesNet/fig6.png" alt="그림6" /></p>

<h2 id="5-conclusion-and-future-work">5. Conclusion and Future Work</h2>

<ul>
  <li>TimeNet은 multi-periodicity를 기반으로 복잡한 temporal variations를 파악
    <ul>
      <li>intraperiod- and interperiod-variations in 2D space by a parameter-efficient inception block.</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://arxiv.org/pdf/2210.02186)]]></summary></entry><entry><title type="html">MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting (Arxiv 2023)</title><link href="http://localhost:4000/timeseries/2024-07-06-MultiResFormer/" rel="alternate" type="text/html" title="MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting (Arxiv 2023)" /><published>2024-07-06T00:00:00+09:00</published><updated>2024-07-06T12:23:19+09:00</updated><id>http://localhost:4000/timeseries/MultiResFormer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-06-MultiResFormer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer-based models는 TS를 segments로 나눠서 encode (<strong>patches</strong>)
    <ul>
      <li>다양한 <strong>scale</strong>(i.e.. <strong>resolutions</strong>)에서의 TS를 모델링</li>
    </ul>
  </li>
  <li>하지만 pre-defined scale(patch의 길이)은…
    <ul>
      <li>variety of intricate temporal dependencies를 찾기 어려움</li>
    </ul>
  </li>
  <li>그래서 MultiResFormer는 adaptive time scale.
    <ul>
      <li>patch length를 주어진 데이터의 주기성을 보고 찾겠다</li>
      <li>그 다음 intraperiod and interperiod dependencies 학습</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>TSF를 위한 vanila Transformer의 modification 3:</li>
  <li>첫째. Efficient attention mechanisms for sub-quadratic attention computation
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2012.07436">Informer(2021)</a>, <a href="https://openreview.net/pdf?id=0EXmFzUn5I">Pyraformer(2022)</a></li>
    </ul>
  </li>
  <li>둘째. Breaking the point-wise nature of dot-product attention for segment or series level dependency modeling
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2106.13008">Autoformer(2021)</a>, <a href="https://arxiv.org/pdf/2201.12740">Fedformer(2022)</a>, <a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>, <a href="https://openreview.net/pdf?id=vSVLM2j9eie">Crossformer(2023)</a></li>
    </ul>
  </li>
  <li>셋째. Modeling sequences at multiple time scales with hierarchical representation learning
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2204.13767">Triformer(2022)</a>, <a href="https://arxiv.org/pdf/2206.04038">Scaleformer(2023)</a>, <a href="https://openreview.net/pdf?id=lJkOCMP2aW">Pathformer(2024)</a></li>
    </ul>
  </li>
  <li>하지만 세 번째 multi-scale methods의 경우 pre-defined resolution으로 인해 generalization이 안된다.</li>
  <li>그래서 본 논문에서는 데이터의 underlying periodicities을 파악해서 데이터에 맞게 multi-resolution view</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig1.png" alt="그림1" /></p>

<ul>
  <li>본 논문에서는 two core Transformer sublayers를 repurpose:
    <ul>
      <li><strong>Multi-headed attention</strong> for “interperiod” variation modeling
        <ul>
          <li>MHA는 전역적인 패치 간 의존성을 모델링하는 데 강점을 가지고 있어 interperiod 변동을 모델링하는 데 적합</li>
        </ul>
      </li>
      <li><strong>Position-wise Feed-Forward network</strong> for “intraperiod” variation modeling
        <ul>
          <li>FFN은 각 위치 내의 복잡한 의존성을 모델링하는 데 강점을 가지고 있어 intraperiod 변동을 모델링하는 데 적합</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>고려해야 할 사항들
    <ul>
      <li>어떻게 different resolution branches끼리 parameter-sharing을 할까 ?
        <ul>
          <li>일단 parameter-sharing을 해야 특정 scale에 overfitting되는 걸 방지하는 건 맞음</li>
          <li>patch length 모르니까 linear projection 못 쓰고, 그냥 padding하는 건 모델 학습을 방해함</li>
          <li>그래서 각 scale에서의 patches의 길이를 맞추기 위한 <strong>interpolation scheme</strong> 사용</li>
          <li>그리고 <strong>resolution embedding</strong>으로 scale-awareness</li>
        </ul>
      </li>
      <li>계산 복잡도를 어떻게 줄일까 ?
        <ul>
          <li>이미 interpolation scheme으로 patches의 길이를 맞춰줬으니 별도의 embedding 필요 없음
            <ul>
              <li>Dlinear model의 성공 사례에서 영감 받음</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="time-series-transformer">Time series Transformer</h3>

<ul>
  <li>Transformer의 quadratic complexity 때문에 longer series는 모델링하기 어려웠음
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a> : sparse attention blocks where each token attends to others with an exponential step size</li>
      <li><a href="https://arxiv.org/pdf/2012.07436">Informer(2021)</a> : entropy-based measurement to filter out uninformative keys for \(O(N)\)</li>
      <li><a href="https://arxiv.org/pdf/2204.13767">Triformer(2022)</a>, <a href="https://openreview.net/pdf?id=0EXmFzUn5I">Pyraformer(2022)</a> : adopt CNN-like approaches for local attention operations</li>
      <li><a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a> : patch단위로 attention 연산 하니까 \(O(N^2/S^2)\)</li>
    </ul>
  </li>
  <li>단일 시점의 데이터는 정보가 별로 없음 (<a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>)
    <ul>
      <li>단일 시점을 토큰으로 하는 transformer는 localized patterns을 간과할 수 있음</li>
    </ul>
  </li>
  <li>channel-mixing embedding은 over-fitting 발생시킬 수 있음 (<a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>)</li>
</ul>

<h3 id="multi-resolution-time-series-modeling">Multi-Resolution Time Series Modeling</h3>

<ul>
  <li><a href="https://arxiv.org/pdf/2210.02186">TimesNet(2023)</a>에서 adaptive multi-resolution modeling 하긴 함
    <ul>
      <li>하지만 input length를 맞춰줘야 해서 flatten해야 하고 longer series 예측 못함</li>
      <li>channel-mixing embedding이 불가피해서 overfitting</li>
    </ul>
  </li>
</ul>

<h2 id="3-adaptive-multi-resolution-time-series-modeling-with-transformers">3. Adaptive Multi-Resolution Time Series Modeling with Transformers</h2>

<ul>
  <li>\(\mathbf{X}_{1 \ldots I}=\left(\mathbf{x}_1, \ldots, \mathbf{x}_I\right) \in \mathbb{R}^{I \times V}\)로 \(\mathbf{X}_{I+1 \ldots I+O}=\left(\mathbf{x}_{I+1}, \ldots, \mathbf{x}_{I+O}\right) \in \mathbb{R}^{O \times V}\)​ 예측</li>
</ul>

<h3 id="31-multiresformer">3.1. MultiResFormer</h3>

<p><img src="/assets/img/timeseries/MultiResFormer/fig2.png" alt="그림2" /></p>

<ul>
  <li>The periodicity-aware patching module, detecting salient periodicities (section 3.2.)</li>
  <li>The Transformer Encoder block, shared across all resolution branches (section 3.3.)</li>
  <li>aggregate the representations derived within each resolution branch into \(\mathbf{X}^{(l)} \in \mathbf{R}^{I \times V}\)​ (section 3.4.)</li>
</ul>

<h3 id="32-salient-periodicity-detection">3.2. Salient Periodicity Detection</h3>

<ul>
  <li>
    <p>salient periodicites of the input series는 Fast Fourier Transform (FFT)으로 찾음</p>

\[\begin{aligned}
\mathbf{A} &amp; =\operatorname{Avg}(\operatorname{Amp}(\operatorname{FFT}(\mathbf{X}))) \\
\left\{f_1, \ldots, f_k\right\} &amp; =\underset{f_* \in\left\{1, \ldots,\left\lfloor\frac{I}{2}\right\rfloor\right\}}{\operatorname{argTopk}}(\mathbf{A}) \\
\text { Period }_i &amp; =\left\lceil\frac{I}{f_i}\right\rceil
\end{aligned}\]
  </li>
  <li>
    <p>Gradient-based 방식이 아니라서 미분이 필요없음</p>
  </li>
</ul>

<h3 id="33-multi-resolution-modeling-with-a-shared-transformer-block">3.3. Multi-Resolution Modeling with a Shared Transformer Block</h3>

<ul>
  <li>PatchTST처럼 fixed patch length 사용할 땐 high-dimensional embeddings을 위한 linear transformations가 필요했는데, 이제는 patch embedding layers가 필요없어짐 (efficient)</li>
  <li>다양한 patch length를 사용한다고 해서 길이를 맞춰주기 위해 padding을 한다고 하더라도, MHA와 FFN에는 masking mechanism이 없기 때문에 모델 성능 저하
    <ul>
      <li>그러므로 interpolation으로 original patches의 temporal characteristics 보존</li>
    </ul>
  </li>
  <li>one resolution branch의 Periodi가 주어지면 길이 Periodi의 겹치치 않는 patch로 분할
    <ul>
      <li>그 다음 길이 d가 되도록 linearly interpolate</li>
      <li>shape of the patch-based representation of the input series : \(V \times\left\lceil\frac{I}{\text { Period }_i}\right\rceil \times d\)</li>
    </ul>
  </li>
  <li>각 resolution branch에서 same-sized patches로 연산이 이루어지기 때문에 resolution embedding을 linearly interpolate에 더해줌 (transformer block에게 해상도 알려주기 위해)</li>
  <li>MHA to capture patch-wise dependencies (interperiod variation modeling)
    <ul>
      <li>FFN layers for capturing dependencies within each patch (intraperiod variation modeling)</li>
    </ul>
  </li>
</ul>

<h3 id="34-adaptive-aggregation">3.4. Adaptive Aggregation</h3>

<ul>
  <li>i 번째 resolution에서 representation의 shape은 \(V \times\left\lceil\frac{I}{\text { Period }_i}\right\rceil \times d\)​</li>
  <li>interpolation으로 \(I \times V\)로 representation</li>
  <li>
    <p>\(I \times V\)으로 표현된 모든 k개의 resolution에서의 representation을 adaptive aggregation:</p>

\[\begin{aligned}
  &amp; \left\{A_1, \ldots, A_k\right\}=\underset{f_* \in\left\{1, \ldots,\left\lfloor\frac{I}{2}\right\rfloor\right\}}{\operatorname{Topk}}(\mathbf{A}) \\
  &amp; \left\{w_1, \ldots, w_k\right\}=\operatorname{Softmax}\left(\left\{A_1, \ldots, A_k\right\}\right)
  \end{aligned}\]
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>Main Results</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/table12.png" alt="그림3" /></p>

<ul>
  <li>Ablation Study</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/table4.png" alt="그림4" /></p>

<ul>
  <li>Varying Look-back Window Size</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig3.png" alt="그림5" /></p>

<ul>
  <li>Representation analysis</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig4.png" alt="그림6" /></p>

<ul>
  <li>Efficiency Comparison</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig5.png" alt="그림7" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>각 transformer blocks 내에서 FFT로 데이터의 underlying periodicities를 파악하고 resolution 결정</li>
  <li>각 transformer blocks 내에서 interpolation 덕분에 resolution끼리 parameter sharing</li>
  <li>블록 내의 encoder는 interpolation으로 input size와 output representation의 size가 같아서
    <ul>
      <li>embedding layer가 필요없고 final linear prediction head layer에서의 parameters 개수가 적음</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2311.18780)]]></summary></entry><entry><title type="html">Neyman-Pearson Hypothesis Testing</title><link href="http://localhost:4000/stat/2024-07-02-NPtest/" rel="alternate" type="text/html" title="Neyman-Pearson Hypothesis Testing" /><published>2024-07-02T00:00:00+09:00</published><updated>2024-07-04T09:50:22+09:00</updated><id>http://localhost:4000/stat/NPtest</id><content type="html" xml:base="http://localhost:4000/stat/2024-07-02-NPtest/"><![CDATA[<h2 id="basics-of-hypothesis-testing">Basics of Hypothesis Testing</h2>

<ul>
  <li>일반적으로 가설 검정을 construction하는 과정은 아래와 같다.
    <ul>
      <li>Test statistic \(T_n=T_n\left(X_1, \ldots, X_n\right)\) 선택</li>
      <li>Rejection region (기각역) 설정</li>
      <li>만약 \(T_n \in R\)이면 귀무가설 기각, 그렇지 않으면 기각할 수 없다.</li>
    </ul>
  </li>
  <li>가설 검정을 하는 이유는 귀무가설이 참인지 거짓인지 판단하기 위함이 아니다.
    <ul>
      <li>정확히는 <strong>귀무가설을 기각할 충분한 evidence</strong>가 있는지를 판단하기 위함이다.</li>
    </ul>
  </li>
</ul>

<h2 id="neyman-pearson-paradigm">Neyman-Pearson paradigm</h2>

<ul>
  <li>
    <p>\(H_0: \theta \in \Theta_0 \quad\) versus \(\quad H_1: \theta \in \Theta_1\)</p>
  </li>
  <li>Pick an \(\alpha \in(0,1)\)​</li>
  <li>Then try to maximize \(\beta(\theta)\) over \(\Theta_1\) subject to \(\sup _{\theta \in \Theta_0} \beta(\theta) \leq \alpha\)
    <ul>
      <li>where Power function : \(\beta(\theta)=P_\theta\left(\left(X_1, \ldots, X_n\right) \in R\right)\)​</li>
    </ul>
  </li>
  <li>가설 검정할 때 test statistic이 \(Z_{1-\alpha}\)보다 크면 귀무가설 기각하고… 어쩌고 그게 어디서 나온 것인지를 아래 예시를 통해 알아보자.</li>
</ul>

<h3 id="example--one-sided-test">example : One sided test</h3>

<ul>
  <li>Suppose \(X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, \sigma^2\right)\) with \(\sigma^2\) known</li>
  <li>\(H_0: \theta=\theta_0 \quad\) versus \(\quad H_1: \theta&gt;\theta_0\) (one sided)</li>
  <li>natural test statistic은 \(T_n\left(X_1, \ldots, X_n\right)=\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta_0}{\sigma / \sqrt{n}}\)이다.</li>
  <li>Power function : \(\beta(\theta)=P_\theta\left(T_n&gt;t\right)=P_\theta\left(\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta}{\sigma / \sqrt{n}}&gt;t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\)에서
    <ul>
      <li>Thresholdld \(t\)를 결정해야 한다.</li>
    </ul>
  </li>
  <li>\(\theta\)는 true parameter라는 점에서 \(\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta}{\sigma / \sqrt{n}} \sim N(0,1)\)이므로 \(\beta(\theta)=P\left(Z&gt;t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)=1-\Phi\left(t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\)이다.</li>
  <li>이제 Neyman-Pearson paradigm을 implement한다.
    <ul>
      <li>\(\sup _{\theta \in \Theta_0}\left\{1-\Phi\left(t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\right\} \leq \alpha\)인데 귀무가설에서는 \(\theta = \theta_0\)이므로</li>
      <li>\(1-\Phi(t) \leq \alpha\)이고 \(t\)에 대해 정리하면 \(t=\Phi^{-1}(1-\alpha)\)이다.</li>
    </ul>
  </li>
</ul>

<h2 id="neyman-pearson-procedure">Neyman-Pearson Procedure</h2>

<ul>
  <li>먼저 test function을 정의한다.
    <ul>
      <li>\(\phi(\boldsymbol{x})= \begin{cases}1, &amp; \text { if } \boldsymbol{x} \in R \\ 0, &amp; \text { if } \boldsymbol{x} \notin R\end{cases}\) , 즉 \(\phi(\boldsymbol{x})=1\)은 귀무가설 기각을 의미한다.</li>
      <li>이 notation에 따르면 \(\text { power }=\int \phi(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x}\) 이고 \(\text { size }=\int \phi(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}\)이다.</li>
    </ul>
  </li>
  <li>Neyman–Pearson test statistic은 likelihood ratio이다.
    <ul>
      <li>\(\Lambda(\boldsymbol{x})=\frac{L\left(\theta_0 \mid \boldsymbol{x}\right)}{L\left(\theta_1 \mid \boldsymbol{x}\right)}=\frac{f_0(\boldsymbol{x})}{f_1(\boldsymbol{x})}\)로 setting하고 \(P_0\left(\Lambda(\boldsymbol{X}) \leq t^*\right)=\alpha\)가 되도록 \(t^*\)를 결정한다.</li>
      <li>이렇게 likelihood ratio로 test function을 결정하는 방식이 유의수준이 \(\alpha\)인 모든 test 중에서 가장 power가 높다.</li>
      <li>이것을 증명하는 것이 The Neyman–Pearson Lemma이다.</li>
    </ul>
  </li>
</ul>

<h2 id="the-neymanpearson-lemma">The Neyman–Pearson Lemma</h2>

<ul>
  <li>
    <p>한 마디로 말하자면 귀무가설 하에서 type 1 error를 유의수준 \(\alpha\)와 같게 했을 때, type 2 error의 확률이 최소화된다는 것 (most powerful)</p>
  </li>
  <li>Consider a test with hypotheses \(H_0: \theta=\theta_0\) and \(H_1: \theta=\theta_1\)
    <ul>
      <li>where the pdf (or pmf) is \(f_i(\boldsymbol{x})\) for \(i=0,1\).</li>
    </ul>
  </li>
  <li>Consider the Neyman-Pearson test \(\phi_{\mathrm{NP}}(\boldsymbol{x})=\mathbb{1}\left(\frac{f_0(\boldsymbol{x})}{f_1(\boldsymbol{x})} \leq t^*\right)\),
    <ul>
      <li>where \(t^*\) is chosen such that \(\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}=\alpha\).</li>
    </ul>
  </li>
  <li>Consider any arbitrary test \(\phi_A(\boldsymbol{x})\) such that \(\int \phi_A(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x} \leq \alpha\),
    <ul>
      <li>i.e. level \(\alpha\) test.</li>
    </ul>
  </li>
  <li>Then the power of \(\phi_A(\boldsymbol{x})\) is at most the power of the Neyman-Pearson test,
    <ul>
      <li>that is \(\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x} \geq \int \phi_A(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x}\)</li>
    </ul>
  </li>
</ul>

<h3 id="proof">Proof</h3>

<ul>
  <li>
    <p>먼저 \(\int \underbrace{\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right)}_{T_1} \underbrace{\left(f_1(\boldsymbol{x})-\frac{f_0(\boldsymbol{x})}{t^*}\right)}_{T_2} d \boldsymbol{x} \geq 0\)​를 보인다.</p>

    <ul>
      <li>NP와 arbitrary test가 같은 결정을 내렸다면 \(T_1=0\)이므로 성립한다.</li>
      <li>NP와 arbitrary test가 다른 결정을 내렸다면 \(T_1\)과 \(T_2\)의 부호가 같으므로 성립한다.</li>
    </ul>
  </li>
  <li>
    <p>이제 다음과 같이 전개가 가능하다.</p>

    <p>\(\begin{aligned}
\int\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right) f_1(\boldsymbol{x}) d \boldsymbol{x} &amp; \geq \frac{1}{t^*} \int\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right) f_0(\boldsymbol{x}) d \boldsymbol{x} \\
&amp; =\frac{1}{t^*}(\underbrace{\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}}_{=\alpha}-\underbrace{\int \phi_A(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}}_{\leq \alpha} \geq 0
\end{aligned}\)​</p>
  </li>
  <li>
    <p>위 결과를 통해 the power of the NP test가 the power of any other test보다 크다는 것을 알 수 있다.</p>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[Basics of Hypothesis Testing]]></summary></entry><entry><title type="html">Minimax Estimator and Stein’s Paradox</title><link href="http://localhost:4000/stat/2024-06-27-minimax/" rel="alternate" type="text/html" title="Minimax Estimator and Stein’s Paradox" /><published>2024-06-27T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/stat/minimax</id><content type="html" xml:base="http://localhost:4000/stat/2024-06-27-minimax/"><![CDATA[<h2 id="1-minimax-estimator">1. Minimax Estimator</h2>

<ul>
  <li>Minimax estimator는 최악이 가장 좋은 estimator이다.</li>
  <li>최악이라는 말은 true parameter \(\theta\)에 대한 estimator의 risk이다.
    <ul>
      <li>Risk : \(R(\theta, \widehat{\theta}(X))=\mathbb{E}_{X \sim f_\theta}[(\widehat{\theta}(X) - \theta)^2]\)</li>
    </ul>
  </li>
  <li>즉 Minimax estimator \(\hat \theta\)는 아래를 만족하는 estimator이다.
    <ul>
      <li>: \(\sup _{\theta \in \Theta} R(\theta, \widehat{\theta})=\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta})\)</li>
    </ul>
  </li>
</ul>

<h3 id="boundig-the-minimax-risk">Boundig the Minimax Risk</h3>

<ul>
  <li>
    <p>Minimax Risk의 Upper bound와 Lower bound에 대해서 알아보자</p>
  </li>
  <li>
    <p>Upper bound는 그냥 다른 estimator의 maximum risk를 사용하면 된다.</p>
  </li>
  <li>
    <p>Lower bound를 찾는 방법은 아래와 같다.</p>

    <ul>
      <li>Bayes estimator의 Bayes risk는 (어떤 prior \(\pi\)를 사용하더라도) minimax risk의 lower bound가 된다.</li>
      <li>
        <p>수식으로 표현하면 \(B_\pi\left(\widehat{\theta}_{\text {low }}\right) \leq B_\pi\left(\hat{\theta}_{\text {minimax}}\right) \leq \sup _\theta R\left(\theta, \hat{\theta}_{\text {minimax}}\right)=\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta})\)</p>
      </li>
      <li>직관적인 의미는 minimax risk는 risk의 maximum을 고려하지만, bayes risk는 risk를 prior를 가중치로 한 weighted average이다.</li>
      <li>평균은 최댓값보다 작다.</li>
    </ul>
  </li>
</ul>

<h3 id="example--d-dim-gaussian">Example : d-dim Gaussian</h3>

<ul>
  <li>
    <p>\(X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, I_d\right)\), then the average \(\widehat{\theta}=\frac{1}{n} \sum_{i=1}^n X_i\)가 minimax estimator of \(\theta\) w.r.t the squared loss임을 보이자.</p>
  </li>
  <li>
    <p>Upper bound</p>

    <ul>
      <li>\(\widehat{\theta} \sim N\left(\theta, I_d / n\right)\)의 Risk는 \(R(\theta, \widehat{\theta})=\mathbb{E}\left[\sum_{i=1}^d\left(\hat{\theta}_i-\theta_i\right)^2\right]=\mathbb{E}\left[\sum_{i=1}^d Z_i^2\right]=\frac{d}{n}\)이므로</li>
      <li>\(\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta}) \leq R(\theta, \widehat{\theta})=\frac{d}{n}\)이다.</li>
    </ul>
  </li>
  <li>
    <p>Lower bound</p>

    <ul>
      <li>먼저 Bayes estimator를 구하고 bayes risk를 구할 것이다.</li>
      <li>\(\theta \in \mathbb R\)이므로 prior는 \(N(0, c^2I)\)를 사용한다. \(c^2\)가 매우 크면 non-informative prior이다.</li>
      <li>Bayes estimator는 \(\hat \theta_{\text{bayes}}=\frac{c^2}{c^2+\frac{1}{n}}\hat \theta\)이다.</li>
      <li>
\[R\left(\theta, \widehat{\theta}_{\text {Bayes }}\right)=\mathbb{E}_{X_1, \ldots, X_X \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, I_d\right)}\left\|\frac{c^2}{c^2+1 / n} \widehat{\theta}-\theta\right\|^2 = \frac{\|\theta\|_2^2}{n^2 \beta^2}+\frac{c^4}{\beta^2} \frac{d}{n}\]
        <ul>
          <li>where \(\widehat{\theta}=\theta+W\) and \(W \sim N\left(0, I_d / n\right)\), and \(\beta:=c^2+1 / n\)</li>
        </ul>
      </li>
      <li>이제 Bayes risk는 다음과 같다.
        <ul>
          <li>
\[\begin{aligned}
B_\pi\left(\frac{c^2}{c^2+1 / n} \widehat{\theta}\right) &amp; =\mathbb{E}_{\theta \sim \pi}\left[R\left(\theta, \widehat{\theta}_{\text {Bayes }}\right)\right] \\
&amp; =\frac{c^2 d}{n^2 \beta^2}+\frac{c^4}{\beta^2} \frac{d}{n}=\frac{c^2 d}{n \beta}=\frac{d}{n\left(1+1 /\left(n c^2\right)\right)}
\end{aligned}\]
          </li>
        </ul>
      </li>
      <li>그러므로 \(\frac{d}{n\left(1+1 /\left(n c^2\right)\right)} \leq R_n \leq \frac{d}{n}\)인데 c는 arbitrary이니까 무한대로 보낼 수 있다.</li>
      <li>upper bound와 lower bound가 모두 \(\frac{d}{n}\)이므로 \(\hat \theta\)는 minimax estimator이다.</li>
    </ul>
  </li>
</ul>

<h2 id="2-steins-paradox">2. Stein’s Paradox</h2>

<ul>
  <li>
    <p>먼저 admissible의 의미를 이해해야 한다.</p>

    <ul>
      <li>Estimator가 admissible하다는 것은</li>
      <li>모든 true parameters에 대해 다른 estimator보다 risk가 작거나 같고,</li>
      <li>적어도 하나의 true parameters에 대해 다른 estimator보다 risk가 작다는 뜻이다.</li>
    </ul>
  </li>
  <li>
    <p>모든 Minimax estimator가 admissible한 것은 아니다. 특히 parameter의 dimension이 3 이상일 때 그렇다.</p>
  </li>
  <li>
    <p>예를 들어 \(Y_i \sim N\left(\theta_i, 1\right) \quad \text { for } i \in\{1, \ldots, d\}\) 세팅에서는 minimax estimator는 \(\widehat{\theta}=Y=\left(Y_1, \ldots, Y_d\right)^{\top}\)이다.</p>
  </li>
  <li>
    <p>하지만 James-Stein estimator \(\widehat{\theta}_{\mathrm{JS}}=\left(1-\frac{d-2}{\|Y\|^2}\right) Y\)는 \(\mathbb{E}\left[\left\|\widehat{\theta}_{\mathrm{JS}}-\theta\right\|^2\right]=d-(d-2)^2 \mathbb{E}\left[\frac{1}{\|Y\|^2}\right]&lt;d\)​이다.</p>

    <ul>
      <li>즉 James-Stein estimator의 risk가 더 작기 때문에 minimax estimator가 admissible하지 않다.</li>
    </ul>
  </li>
  <li>
    <p><strong>Proof</strong></p>

    <ul>
      <li>
        <p>Squared term을 전개하면 \(\mathbb{E}\left[\left\|\widehat{\theta}_{\mathrm{JS}}-\theta\right\|^2\right]=\mathbb{E}\left[\|Y-\theta\|^2\right]+\mathbb{E}\left[\frac{(d-2)^2}{\|Y\|^2}\right]-2(d-2) \sum_{i=1}^d \mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]\)</p>
      </li>
      <li>
        <p>가장 우측에 있는 \(\mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]\)를 보자</p>

        <ul>
          <li>
            <p>\(\|Y-\theta\|^2 \sim \chi^2(d)\)이므로 expectation이 d이다.</p>
          </li>
          <li>
            <p>Gaussian setting이므로 \(\mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]=\int \cdots \int \underbrace{\frac{y_i}{\|y\|^2}}_{=g_i(y)} \underbrace{\frac{y_i-\theta}{(2 \pi)^{d / 2}} e^{-\|y-\theta\|^2 / 2}}_{=-\frac{\partial f(y)}{\partial y_i}} d y_1 \cdots d y_d\)이다.</p>
          </li>
          <li>
            <p>아래와 같은 계산 과정을 통해 \(\sum_{i=1}^d \mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]=(d-2) \mathbb{E}\left[\frac{1}{\|Y\|^2}\right]\)이다.</p>

\[\begin{aligned}
\mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right] &amp; =\int \cdots \int \frac{\partial g_i(y)}{\partial y_i} f(y) d y_1 \cdots d y_d \\
&amp; =\int \cdots \int \frac{\|y\|^2-2 y_i^2}{\|y\|^4} f(y) d y_1 \cdots d y_d \\
&amp; =\mathbb{E}\left[\frac{1}{\|Y\|^2}\right]-\mathbb{E}\left[\frac{2 Y_i^2}{\|Y\|^4}\right]
\end{aligned}\]
          </li>
          <li>
            <p>이걸 첫 줄의 식에 대입하면 증명 끝이다.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[1. Minimax Estimator]]></summary></entry><entry><title type="html">Analysis of non-parametric kernel regression</title><link href="http://localhost:4000/stat/2024-05-26-kernel_reg/" rel="alternate" type="text/html" title="Analysis of non-parametric kernel regression" /><published>2024-05-26T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/stat/kernel_reg</id><content type="html" xml:base="http://localhost:4000/stat/2024-05-26-kernel_reg/"><![CDATA[<ul>
  <li>일반적인 regression은 response variable Y와 covariate X의 관계를 이해하는 방법 중 하나로 일종의 conditional expectation이다. <br />
\(r(x)=\mathbb{E}[Y \mid X=x]=\int y f(y \mid x) dy\)</li>
  <li>non-parametric regession을 하는 기본적인 방법 중 하나는 kernel regression이다. <br />
estimator는 \(\widehat{r}(x)=\sum_{i=1}^n w_i(x) Y_i\) where \(w_i(x)=\frac{K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)}\)
    <ul>
      <li>weights는 x 주변에 높은 가중치를 두고, h는 smoothing을 결정하는 bandwidth이다.</li>
      <li>간단한 kernel의 예시로 Gaussian kernel을 사용한다. \(K(x)=\frac{1}{\sqrt{2 \pi}} \exp \left(-x^2 / 2\right)\)</li>
    </ul>
  </li>
</ul>

<h2 id="assumptions">Assumptions</h2>
<ul>
  <li>증명을 복잡하지 않게 하기 위한 가정 \(y_i=r\left(x_i\right)+\epsilon_i\), where
    <ul>
      <li><strong>Design Assumption</strong> : \(x_i\) is one-dimensional, and <strong>equally spaced</strong> on \([0,1]\)
        <ul>
          <li>사실 꼭 필요한 가정은 아니지만 특정 x 근처에 다른 x가 존재함을 가정하기 위한 가정이다.</li>
        </ul>
      </li>
      <li><strong>Regression function Assumption</strong> : \(r(x)=\mathbb{E}[Y \mid X=x] \text { is } L \text {-Lipschitz}\) i.e. \(\left\vert\frac{d}{d x} r(x)\right\vert \leq L\)</li>
      <li><strong>Noise Assumption</strong> : \(\mathbb{E}\left[\epsilon_i\right]=0, \operatorname{Var}\left[\epsilon_i\right]=\sigma^2\) and iid</li>
      <li><strong>Kernel Assumption</strong> : spherical kernel \(K(x)=\mathbb{1}(-1 \leq x \leq 1)\)</li>
    </ul>
  </li>
</ul>

<h2 id="kernel-regression">Kernel Regression</h2>
<ul>
  <li>Under the <strong>Assumptions</strong> with \(h \ge 1/(n-1)\), <br />
\(\begin{aligned} R(\widehat{r}, r) &amp; = MSE(\widehat{r}, r) \\ &amp; =\int_0^1\{\widehat{r}(x)-r(x)\}^2 d x \\ &amp; =\int_0^1 \operatorname{bias}^2(x) d x+\int_0^1 \operatorname{Var}(\widehat{r}(x)) d x \leq L^2 h^2+\frac{\sigma^2}{(n-1) h} \end{aligned}\)</li>
  <li>Proof : 적분 구간이 [0,1]이므로 \(\max _{x \in[0,1]}\left\vert\operatorname{bias}(x)\right\vert \leq L h\), 그리고 \(\max _{x \in[0,1]} \operatorname{Var}(\widehat{r}(x)) \leq \frac{\sigma^2}{(n-1) h}\)를 증명
    <ul>
      <li><strong>Bounding the bias</strong> <br />
\(\begin{aligned} \left\vert \operatorname{bias}(x)\right\vert=\left\vert \mathbb{E} \widehat{r}(x)-r(x)\right\vert &amp; \stackrel{\text { (i) }}{=}\left\vert \mathbb{E}\left[\sum_{i=1}^n\left(w_i(x)\left(Y_i-r(x)\right)\right)\right]\right\vert \\ &amp;  \stackrel{\text { (ii) }}{=} \left\vert\sum_{i=1}^n\left(w_i(x)\left(r\left(X_i\right)-r(x)\right)\right)\right\vert \\ &amp; \stackrel{\text { (iii) }}{\leq} \sum_{i=1}^n w_i(x)\left\vert r\left(X_i\right)-r(x)\right\vert \\ &amp; \stackrel{\text { (iv) }}{\leq} L h \sum_{i=1}^n w_i(x)=L h,\end{aligned}\) <br />
(i) is holds since \(\sum_{i=1}^n w_i(x)=\frac{\sum_{i=1}^n K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)}=1\) <br />
(ii) follows by \(\mathbb{E}\left[Y_i \mid X_i\right]=r\left(X_i\right)\) <br />
(iii) uses triangle inequality <br />
(iv) if \(\left\vert X_i-x\right\vert \leq h,\) then \(\left\vert r\left(X_i\right)-r(x)\right\vert \leq Lh\) by the Lipschitz  and if \(\left\vert X_i-x\right\vert&gt;h,\) then \(w_i(x)=0\)</li>
      <li><strong>Bounding the variance</strong>
        <ul>
          <li>먼저 weights에 대한 bound \(w_i(x)=\frac{K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)}=\frac{\mathbb{1}\left(\left\vert x-X_i\right\vert \leq h\right)}{\sum_{j=1}^n \mathbb{1}\left(\left\vert x-X_j\right\Vert \leq h\right)} \leq \frac{1}{(n-1) h}\)를 보인다.</li>
          <li>즉 \(\sum_{j=1}^n \mathbb{1}\left(\left\vert x-X_j\right\vert \leq h\right) \geq (n-1)h\)를 보인다.
            <ul>
              <li>모든 \(h \geq 1 /(n-1)\)에 대해 \(\min _{x \in[0,1]} \sum_{j=1}^n \mathbb{1}\left(\left\vert x-X_j\right\vert \leq h\right)=\sum_{j=1}^n \mathbb{1}\left(\left\vert X_1-X_j\right\vert \leq h\right)\)이므로 (sum is minimized at the boundary)</li>
              <li>\(h \in\left[\frac{k}{n-1}, \frac{k+1}{n-1}\right)\)의 경우에 대해 <br />
\(\\ \begin{aligned} \min _{x \in[0,1]} \sum_{j=1}^n \mathbb{1}\left(\left\vert x-X_j\right\vert \leq h\right) &amp; =\sum_{j=1}^n \mathbb{1}\left(\left\vert X_1-X_j\right\vert \leq h\right) \\ &amp; \geq \sum_{j=1}^n \mathbb{1}\left(\left\vert X_1-X_j\right\vert \leq \frac{k}{n-1}\right)=k+1 \\ &amp; \geq(n-1) h \end{aligned}\)</li>
              <li>그러므로 <br />
\(\\ \begin{aligned} \operatorname{Var}(\widehat{r}(x)) &amp; =\mathbb{E}\left[(\widehat{r}(x)-\mathbb{E}(\widehat{r}(x)))^2\right]=\mathbb{E}[(\sum_{i=1}^n(w_i(x) \underbrace{\left(Y_i-r\left(X_i\right)\right.}_{=\epsilon_i})))^2] \\ &amp; =\mathbb{E}\left[\left(\sum_{i=1}^n \epsilon_i w_i(x)\right)^2\right] \\ &amp; =\sum_{i=1}^n w_i(x)^2 \mathbb{E}\left(\epsilon_i^2\right)=\sigma^2 \sum_{i=1}^n w_i(x)^2 \\ &amp; \leq \sigma^2 \max _{1 \leq i \leq n} w_i(x) \sum_{i=1}^n w_i(x) \leq \frac{\sigma^2}{(n-1) h} \end{aligned}\)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Proof : 적분 구간이 [0,1]이므로 \(\max _{x \in[0,1]}\left\vert\operatorname{bias}(x)\right\vert \leq L h\), 그리고 \(\max _{x \in[0,1]} \operatorname{Var}(\widehat{r}(x)) \leq \frac{\sigma^2}{(n-1) h}\)이 증명되었다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[일반적인 regression은 response variable Y와 covariate X의 관계를 이해하는 방법 중 하나로 일종의 conditional expectation이다. \(r(x)=\mathbb{E}[Y \mid X=x]=\int y f(y \mid x) dy\) non-parametric regession을 하는 기본적인 방법 중 하나는 kernel regression이다. estimator는 \(\widehat{r}(x)=\sum_{i=1}^n w_i(x) Y_i\) where \(w_i(x)=\frac{K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)}\) weights는 x 주변에 높은 가중치를 두고, h는 smoothing을 결정하는 bandwidth이다. 간단한 kernel의 예시로 Gaussian kernel을 사용한다. \(K(x)=\frac{1}{\sqrt{2 \pi}} \exp \left(-x^2 / 2\right)\)]]></summary></entry><entry><title type="html">TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-05-23-Timemixer/" rel="alternate" type="text/html" title="TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting (ICLR 2024)" /><published>2024-05-23T00:00:00+09:00</published><updated>2024-07-05T18:29:41+09:00</updated><id>http://localhost:4000/timeseries/Timemixer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-05-23-Timemixer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Real world에서 time series는 복잡한 temporal variations 때문에 forecasting이 어려움</li>
  <li>지금까지는 그냥 decomposition하고 multiperiodicity 분석 했는데,</li>
  <li>본 논문에서는 time series를 다양한 scale로 sampling하면 별개의 patterns가 관찰될 것이라는 intuition에 따라 multiscale-mixing을 제안
    <ul>
      <li>microscopic information은 fine scale, macroscopic information은 coarse scales</li>
    </ul>
  </li>
  <li><strong>TimeMixer</strong> : fully MLP-based architecture
    <ul>
      <li>with <strong>Past-Decomposable-Mixing</strong> (PDM) for past extraction
        <ul>
          <li>applies the decomposition to multiscale series</li>
          <li>and mixes the decomposed seasonal and trend components
            <ul>
              <li>즉 the microscopic seasonal과 macroscopic trend를 통합</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>with <strong>Future-Multipredictor-Mixing</strong> (FMM) for future prediction
        <ul>
          <li>multiscale observation을 통한 완성도 있는 예측을 위해 multiple predictors를 ensemble</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>Time series의 representative models는 temporal variations를 파악
    <ul>
      <li>foundation backbone 별로 보면</li>
      <li>CNN 계열로는 <a href="https://openreview.net/forum?id=zt53IDUR1U">MICN(2023)</a>, <a href="https://arxiv.org/abs/2210.02186">TimesNet(2023)</a>, <a href="https://link.springer.com/article/10.1007/s00500-020-04954-0">TCN(2020)</a>, RNN 계열로는 <a href="https://arxiv.org/abs/1703.07015">LSTNet(2018)</a>, <a href="https://arxiv.org/abs/1704.02971">DA-RNN(2017)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0169207019301888">DeepAR(2020)</a>이 있고</li>
      <li>Transformer 계열로는 <a href="https://arxiv.org/abs/2012.07436">Informer(2021)</a>, <a href="https://arxiv.org/abs/2106.13008">Autoformer(2021)</a>, <a href="https://arxiv.org/abs/2201.12740">Fedformer(2022)</a>, <a href="https://arxiv.org/abs/2211.14730">PatchTST(2023)</a>가 있다.</li>
      <li>그리고 MLP 계열로는 <a href="https://arxiv.org/abs/2205.13504">DLinear(2023)</a>, <a href="https://arxiv.org/abs/2207.01186">LightTS(2022)</a>, <a href="https://arxiv.org/abs/2201.12886">N-hits(2023)</a>가 있다.</li>
    </ul>
  </li>
  <li>지금까지는 그냥 decomposition하고 multiperiodicity 분석
    <ul>
      <li>decomposition : predictable component(seasonal, trend)와 complex temporal patterns를 분리</li>
      <li>multiperiodicity analysis : mixed temporal variations을 주기가 다른 여러 components로 disentangle</li>
    </ul>
  </li>
  <li>본 논문의 intuition : sampling scales가 달라지면 temporal variations도 달라진다.
    <ul>
      <li>microscopic information을 담은 fine scale과 macroscopic information을 담고 있는 coarse scales에서의 변동이 joint하게 미래 변동을 결정한다.</li>
      <li>multiscale mixing을 통해 scale에 따른 변동을 구분하고 그걸 다 고려해서 complementary한 예측을 할 수 있을 것</li>
    </ul>
  </li>
  <li><strong>Past-Decomposable-Mixing</strong> (PDM) : average downsampling을 통해 multiscale observations 생성</li>
  <li><strong>Future-Multipredictor-Mixing</strong> (FMM) : multiscale에서의 seasonal, trend를 합쳐서 multiple predictors를 ensemble</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="21-temporal-modeling-in-deep-tsf">2.1. Temporal Modeling in Deep TSF</h3>

<ul>
  <li>Foundation backbones에 따라 4가지 계열로 구분 : CNN, RNN, Transformer, MLP
    <ul>
      <li>CNN, RNN-based deep models는 limited receptive field 때문에 long-term forecasting이 어려움</li>
      <li>Transformer-based models는 long-term temporal depdendencies 파악</li>
      <li>MLP-based models는 단순한 구조로도 복잡한 모델의 성능만큼 도달할 수 있음을 제시</li>
    </ul>
  </li>
  <li>Temporal multiscale designs models이 있긴 하지만 <a href="https://openreview.net/forum?id=0EXmFzUn5I">Pyraformer(2021)</a>, <a href="https://arxiv.org/abs/2106.09305">SCINet(2022)</a>
    <ul>
      <li>예측 과정에서 multiscale information을 동시에 활용하지는 않음</li>
    </ul>
  </li>
</ul>

<h3 id="22-mixing-networks">2.2. Mixing Networks</h3>

<ul>
  <li>Computer Vision과 NLP 분야에서 사용되는 mixing <a href="https://arxiv.org/abs/2105.01601">MLP-Mixer(2021)</a>, <a href="https://arxiv.org/abs/2105.03824">FNet(2022)</a></li>
</ul>

<h2 id="3-timemixer">3. TimeMixer</h2>

<ul>
  <li>P길이의 과거를 보고 F길이의 미래를 예측</li>
  <li>disentangled variations(past information extraction) &amp; complementary forecasting(future prediction)</li>
</ul>

<h3 id="31-multiscale-mixing-architecture">3.1. Multiscale Mixing Architecture</h3>

<p><img src="/assets/img/timeseries/Timemixer/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p>먼저 complex variations를 disentangle하기 위해 downsampling한다.</p>

    <ul>
      <li>\(\mathbf{x} \in \mathbb{R}^{P \times C}\)를 M개의 scale로 downsampling하면</li>
      <li>\(\mathcal{X}=\left\{\mathbf{x}_0, \cdots, \mathbf{x}_M\right\} \text {, where } \mathbf{x}_m \in \mathbb{R}^{\left\lfloor \frac{P}{2^m}\right\rfloor \times C}, m \in\{0, \cdots, M\}\)를 얻는다.</li>
      <li>\(C\)는 변수 개수</li>
      <li>즉 \(\mathbf{x}_0\)는 input 원본이고, \(\mathbf{x}_m\)은 m개씩 average pooling한 것이다.</li>
      <li>그리고 embedding layer 통과하면 \(\mathcal{X}^0=\operatorname{Embed}(\mathcal{X})\)를 얻는다.</li>
    </ul>
  </li>
  <li>
    <p>이제 <strong>Past-Decomposable-Mixing</strong> (PDM, for mixing past information)</p>

\[x^l=\operatorname{PDM}\left(x^{l-1}\right),\quad l \in\{0, \cdots, L\}, \quad \mathbf{x}_m^l \in \mathbb{R}^{\left\lfloor\frac{P}{2^m}\right\rfloor \times d_{\text {model }}}\]

    <ul>
      <li>(자세한 건 다음 section 3.2.)</li>
    </ul>
  </li>
  <li>
    <p>다음으로 <strong>Future-Multipredictor-Mixing</strong> (FMM, for future prediction)</p>

\[\widehat{\mathbf{x}}=\operatorname{FMM}\left(\mathcal{X}^L\right), \quad \widehat{\mathbf{x}} \in \mathbb{R}^{F \times C}\]

    <ul>
      <li>(자세한 건 다음 section 3.3.)</li>
    </ul>
  </li>
</ul>

<h3 id="32-past-decomposable-mixing">3.2. Past Decomposable Mixing</h3>

<ul>
  <li>
    <p>Seasonal과 trend는 scale에 따라 다르게 나타나므로</p>

    <ul>
      <li>Seasonal끼리 scale별로 구해서 mixing, trend끼리 scale별로 구해서 mixing</li>
    </ul>
  </li>
  <li>
    <p>\(l\)-번째 PDM block에서는</p>
    <ul>
      <li>ts \(\mathcal{X}_l\)를 seasonal part \(\mathcal{S}^l=\left\{\mathbf{s}_0^l, \cdots, \mathbf{s}_M^l\right\}\)과 trend parts \(\mathcal{T}^l=\left\{\mathbf{t}_0^l, \cdots, \mathbf{t}_M^l\right\}\)로 decompose</li>
    </ul>

\[\begin{gathered}
\mathbf{s}_m^l, \mathbf{t}_m^l=\text { SeriesDecomp }\left(\mathbf{x}_m^l\right), m \in\{0, \cdots, M\}, \\
\mathcal{X}^l=\mathcal{X}^{l-1}+\text { FeedForward }\left(\operatorname{S-Mix}\left(\left\{\mathbf{s}_m^l\right\}_{m=0}^M\right)+\text { T-Mix }\left(\left\{\mathbf{t}_m^l\right\}_{m=0}^M\right)\right)
\end{gathered}\]

    <ul>
      <li>\(\text { FeedForward(} \cdot)\) contains two linear layers w/ GELU</li>
      <li>\(\operatorname{S}-\operatorname{Mix}(\cdot), T-\operatorname{Mix}(\cdot)\)​는 지금부터 설명할 mixing</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/fig2.png" alt="그림2" /></p>

<ul>
  <li><strong>Seasonal Mixing</strong>
    <ul>
      <li>(Box &amp; Jenkins, 1970)의 seasonality analysis에 따르면
        <ul>
          <li>larger periods는 smaller periods의 aggregation</li>
          <li>그러므로 residual하게 bottom-up approach</li>
          <li>(coarser scales의 seasonality를 위해 lower-level fine-scale 사용)</li>
          <li>
\[\text { for } m: 1 \rightarrow M \text { do: } \quad \mathbf{s}_m^l=\mathbf{s}_m^l+\text { Bottom-Up-Mixing }\left(\mathbf{s}_{m-1}^l\right)\]
            <ul>
              <li>\(\text { Bottom-Up-Mixing(} \cdot \text {) }\): input dim \(\left\lfloor\frac{P}{2^{m-1}}\right\rfloor\), output dim \(\left\lfloor\frac{P}{2^{m}}\right\rfloor\)인 two linear layers with GELU</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Trend Mixing</strong>
    <ul>
      <li>seasonal parts와 반대로
        <ul>
          <li>detailed variations는 noise로 보일 뿐이고 coarser scales가 finer scales를 guide</li>
          <li>그러므로 residual하게 top-down mixing</li>
          <li>
\[\text { for } m:(M-1) \rightarrow 0 \text { do: } \quad \mathbf{t}_m^l=\mathbf{t}_m^l+\text { Top-Down-Mixing }\left(\mathbf{t}_{m+1}^l\right)\]
            <ul>
              <li>\(\text { Top-Down-Mixing(} \cdot \text {) }\) : : input dim \(\left\lfloor\frac{P}{2^{m+1}}\right\rfloor\), output dim \(\left\lfloor\frac{P}{2^{m}}\right\rfloor\)인 two linear layers with GELU</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>결론적으로 seasonality는 from fine to coarse, 반대로 trend는 coarse to fine하게 multiscale mixing in past information extraction</li>
</ul>

<h3 id="33-future-multipredictor-mixing">3.3. Future MultiPredictor Mixing</h3>

<ul>
  <li>\(L\)개의 PDM block으로 \(\mathcal{X}^L=\left\{\mathbf{x}_0^L, \cdots, \mathbf{x}_M^L\right\}, \mathbf{x}_m^L \in \mathbb{R}^{\left\lfloor\frac{P}{2^m}\right\rfloor \times d_{\text {model }}}\)를 얻었다.</li>
  <li>서로 다른 scale인 \(\mathbf{x}_m^L\)들은 서로 다른 variations를 present하고 있기 때문에 모든 scale에서 예측을 하고 각각의 예측을 aggregate한다. (ensemble)</li>
  <li>
\[\widehat{\mathbf{x}}_m=\operatorname{Predictor}_m\left(\mathbf{x}_m^L\right), m \in\{0, \cdots, M\}, \widehat{\mathbf{x}}=\sum_{m=0}^M \widehat{\mathbf{x}}_m\]
    <ul>
      <li>\(\widehat{\mathbf{x}}_m, \widehat{\mathbf{x}} \in \mathbb{R}^{F \times C}\)​</li>
    </ul>
  </li>
  <li>\(\text { Predictor }_m(\cdot)\)은 one single linear layer인데 \(\left\lfloor\frac{P}{2^m}\right\rfloor\)길이의 과거 정보로부터 F 길이의 future를 regress</li>
  <li>FMM은 mixed multiscale series로 complementary forecasting하는  ensemble of multiple predictors</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>Summary</li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/table1.png" alt="그림11" /></p>

<ul>
  <li>Main results</li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/table2.png" alt="그림12" /></p>

<ul>
  <li>Ablations</li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/table5.png" alt="그림15" /></p>

<ul>
  <li>Decomposition과 multiscale의 각 components에 대한 visualization</li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/fig3.png" alt="그림3" /></p>

<p><img src="/assets/img/timeseries/Timemixer/fig4.png" alt="그림4" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Empowered by Past-Decomposable-Mixing and Future- Multipredictor-Mixing blocks, TimeMixer took advantage of both disentangled variations and complementary forecasting capabilities.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=7oLshfEIC2)]]></summary></entry><entry><title type="html">Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-05-23-Pathformer/" rel="alternate" type="text/html" title="Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting (ICLR 2024)" /><published>2024-05-23T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/timeseries/Pathformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-05-23-Pathformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>기존 Transformer for TS 모델들은 limited or fixed scales</li>
  <li>Pathformer는 temporal resolution과 temporal distance를 합치는 방식
    <ul>
      <li>다양한 크기의 patches를 사용해서 서로 다른 temporal resolutions를 만들고</li>
      <li>dual attention으로 temporal dependencies(global correlation과 local details)</li>
    </ul>
  </li>
  <li>Input의 varying temporal dynamics에 따라 adaptive multi-scale</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>최근 simpler linear model의 성능이 transformer 보다 잘 나옴 - transformer 디자인 수정 필요</li>
  <li>Time Series에서는 multiple scales를 고려하는 것이 중요 (일, 월 등…)
    <ul>
      <li><strong>Temporal resolution</strong>: patch의 길이 (fine-grained or coarse grained)</li>
      <li><strong>Temporal distance</strong> : time steps 사이의 거리
<img src="/assets/img/timeseries/Pathformer/fig1.png" alt="그림1" /></li>
      <li>그림 왼쪽 위에서 blue patch와 orange patch는 Temporal resolution가 다른 것이고</li>
      <li>그림 오른쪽에서 black arrows와 colored arrows는 Temporal distance가 다른 것</li>
    </ul>
  </li>
  <li>그래서 Transformer로 multi-scale modeling 하려는데 challenges 2개
    <ul>
      <li><strong>incompleteness of multi-scale modeling</strong>
        <ul>
          <li>단순히 Temporal resolution (patch 길이) 늘리는 건 다양한 dependency 파악에 도움 안됨</li>
          <li>차라리 Temporal distance (time steps 간격) 다양하게 하면 좋은데 Temporal distance는 patch size (data division)에 따라 달라짐</li>
        </ul>
      </li>
      <li><strong>fixed multi-scale modeling process</strong>
        <ul>
          <li>different series prefer different scales !</li>
          <li>그림 왼쪽 위 series는 rapid fluctuation, fine-grained, short-term characteriestics</li>
          <li>그림 왼쪽 아래 series는 coarse-grained and long-term characteriestics</li>
          <li>모든 데이터에 fixed multi-scale modeling하면 안되는데 매번 optimal scale 찾으려니 오래걸림</li>
        </ul>
      </li>
      <li>그래서 <strong>adaptive multi-scale modeling</strong>이 필요하다.
        <ul>
          <li>adaptively models the current data from certain multiple scales</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>기존에는 그냥 다양한 길이의 patch로 자른 뒤에, patch 안에서 temporal dependencies를, patch 끼리 global correlation을 파악하는 dual attention</li>
  <li>본 논문에서 제시하는 Pathformer는
    <ul>
      <li>Multi-scale router가 seasonality와 trend를 보고 적절한 patch size들을 결정</li>
      <li>그 다음 multi-scale characteristics를 weighted aggregation</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>Time Series forecasting
    <ul>
      <li>Statistical modeling, GNN(spatial dependency), RNN(temporal dependency), CNN(sub-series features), TimesNet(1-dim \(\to\) 2-dim), LLM-based…</li>
    </ul>
  </li>
  <li>Transformer
    <ul>
      <li>Informer : prob-sparse self-attention to select important keys</li>
      <li>Triformer :  manages to reduce the complexity</li>
      <li>Autoformer : auto-correlation mechanisms to replace self- attention</li>
      <li>FEDformer : the perspective of frequency to model temporal dynamics</li>
      <li>하지만 simple linear model의 성능이 더 좋은 경우가 많았음</li>
      <li>PatchTST : patching and channel independence로 transformer의 가능성 제시</li>
    </ul>
  </li>
  <li>Multi-scale Modeling for Time Series
    <ul>
      <li>N-HiTs : multi-rate data sampling and hierarchical interpolation for diverse temporal resolutions</li>
      <li>Pyraformer : pyramid attention to extract features at different temporal resolutions</li>
      <li>하지만 fixed scale이었고, 본 논문에서는 adaptive multi scale 제안</li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<p><img src="/assets/img/timeseries/Pathformer/fig2.png" alt="그림2" /></p>

<ul>
  <li>Instance Norm - Stacking of <strong>Adaptive Multi-Scale Blocks</strong> - Predictor(FC)로 구성</li>
  <li>Adaptive Multi-Scale Blocks은 multi-scale Transformer block과 adaptive pathways으로 구성
    <ul>
      <li>multi-scale Transformer block
        <ul>
          <li>다양한 size의 patch division and dual attention으로 multi-scale temporal resolution and distances 통합</li>
        </ul>
      </li>
      <li>adaptive pathways
        <ul>
          <li>multi-scale router가 다양한 patch size르 고르고</li>
          <li>aggregator에서 mutli-scale characteristics를 weighted aggregation</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="31-multi-scale-transformer-block">3.1. Multi-scale Transformer Block</h3>

<ul>
  <li><strong>Multi-scale Division</strong>
    <ul>
      <li>\(\mathrm{X} \in \mathbb{R}^{H \times d}\)를 P개의 patch \(\left(\mathrm{X}^1, \mathrm{X}^2, \ldots, \mathrm{X}^P\right)\)로 나눔, patch size는 M개 \(\mathcal{S}=\left\{S_1, \ldots, S_M\right\}\) 즉 \(P = H/S\)</li>
      <li>다양한 크기의 patch size로 다양한 temporal resolutions를 dual attention</li>
    </ul>
  </li>
  <li><strong>Dual attention</strong>
<img src="/assets/img/timeseries/Pathformer/fig3.png" alt="그림3" />
    <ul>
      <li>Intra-patch attention : relationships btw time steps within each patch
        <ul>
          <li>먼저 \(X_{\text {intra }}^i \in \mathbb{R}^{S \times d_m}\) patch를 embedding하고 \(\operatorname{Attn}_{\text {intra }}^i=\operatorname{Softmax}\left(Q_{\text {intra }}^i\left(K_{\text {intra }}^i\right)^T / \sqrt{d_m}\right) V_{\text {intra }}^i \in \mathbb{R}^{1 \times d_m}\)</li>
          <li>모든 patches에 대해 다 합치면 \(\operatorname{Attn}_{\text {intra }}=\operatorname{Concat}\left(\operatorname{Attn}_{\text {intra }}^1, \ldots, \operatorname{Attn}_{\text {intra }}^P\right) \in \mathbb{R}^{P \times d_m}\)​</li>
          <li>Linear transformation으로 \(\operatorname{Attn}_{\text {intra }} \in \mathbb{R}^{P \times S \times d_m}\)</li>
        </ul>
      </li>
      <li>Inter-patch attention : relationships btw patches to capture global correlations
        <ul>
          <li>먼저 feature embedding and rearrange : \(\mathrm{X}_{\text {inter }} \in \mathbb{R}^{P \times d_m^{\prime}} \text {, where } d_m^{\prime}=S \cdot d_m\)</li>
          <li>아까처럼  linear mapping으로 \(\operatorname{Attn}_{\text {inter }}=\operatorname{Softmax}\left(Q_{\text {inter }}\left(K_{\text {inter }}\right)^T / \sqrt{d_m^{\prime}}\right) V_{\text {inter }} \in \mathbb{R}^{P \times S \times d_m}\)</li>
        </ul>
      </li>
      <li>둘을 더하면 \(\mathrm{Attn}_{\text {intra }} + \mathrm{Attn}_{\text {intra }}=\text { Attn } \in \mathbb{R}^{P \times S \times d_m}\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-adaptive-pathways">3.2. Adaptive Pathways</h3>

<ul>
  <li>different series may prefer diverse scales \(\to\) model needs to figure out critical scales based on the input</li>
  <li><strong>Multi-scale router</strong> selects specific sizes of patch division based on the input data</li>
  <li>
    <p><strong>Multi-scale aggregator</strong> combines multi-scale characteristics(weighted aggregation) \(\to\) output of the Transformer block</p>
  </li>
  <li><strong>Multi-scale router</strong> : selects the optimal sizes for patch division
    <ul>
      <li>by its complex inherent characteristics and dynamic patterns</li>
      <li>seasonality and trend decomposition \(\to\) extract periodicity and trend patterns</li>
      <li><strong>Seasonality decomposition</strong>
        <ul>
          <li>Discern Fourier Transform (DFT) : X를 푸리에 basis로 decomopose하고 the largest amplitudes 선택 (to keep the sparsity of frequency domain)</li>
          <li>IDFT로 periodic patterns \(\mathrm{X}_{\text {sea }}=\operatorname{IDFT}\left(\left\{f_1, \ldots, f_{K_f}\right\}, A, \Phi\right)\) 얻음
            <ul>
              <li>\(\Phi\), \(A\) :the phase and amplitude of each frequency from \(\operatorname{DFT}(\mathrm{X})\)</li>
              <li>\(\left\{f_1, \ldots, f_{K_f}\right\}\) : the frequencies with the top \(K_f\) amplitudes</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Trend decomposition</strong>
        <ul>
          <li>seasonality를 제외한 부분 \(\mathrm{X}_{\mathrm{rem}}=\mathrm{X}-\mathrm{X}_{\text {sea }}\)에 대해 different kernels of average pooling for moving averages to extract trend patterns</li>
          <li>정리하면 \(\mathrm{X}_{\text {trend }}=\operatorname{Softmax}\left(L\left(\mathrm{X}_{\mathrm{rem}}\right)\right) \cdot\left(\operatorname{Avgpool}\left(\mathrm{X}_{\mathrm{rem}}\right)_{\text {kernel }_1}, \ldots, \operatorname{Avgpool}\left(\mathrm{X}_{\mathrm{rem}}\right)_{\text {kernel }_N}\right)\)</li>
        </ul>
      </li>
      <li>input \(X\)에 seasonality pattern and trend pattern를 더하고 linear mapping으로 \(\mathrm{X}_{\text {trans }} \in \mathbb{R}^d\)</li>
      <li>마지막으로 routing function \(R\left(\mathrm{X}_{\text {trans }}\right)=\operatorname{Softmax}\left(\mathrm{X}_{\text {trans }} W_r+\epsilon \cdot \operatorname{Softplus}\left(\mathrm{X}_{\text {trans }} W_{\text {noise }}\right)\right), \epsilon \sim \mathcal{N}(0,1)\) 을 통해 pathway weights \(\in \mathbb{R}^{M}\)을 구한다.
        <ul>
          <li>\(W_r \text { and } W_{\text {noise }} \in \mathbb{R}^{d \times M}\)은 leanable parameters, \(d\)는 feature dim, \(M\)은 patch size의 개수</li>
          <li>noise를 추가한 이유는 patch size가 같은 것만 나오는 것을 방지하기 위함</li>
          <li>이렇게 만든 M개의 pathway weights 중에서 top K개를 제외하고 0으로 만든 걸 \(\bar{R}\left(\mathrm{X}_{\text {trans }}\right)\)로 표기</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Multi-Scale Aggregator</strong>
    <ul>
      <li>\(\bar{R}\left(\mathrm{X}_{\text {trans }}\right)_i&gt;0\)이 의미하는 것은 patch size \(S_i\)로 나누고 dual attention을 수행함을 의미</li>
      <li>그러므로 AMS block의 final output은 \(\mathrm{X}_{\text {out }}=\sum_{i=1}^M \mathcal{I}\left(\bar{R}\left(\mathrm{X}_{\text {trans }}\right)_i&gt;0\right) R\left(\mathrm{X}_{\text {trans }}\right)_i T_i\left(\mathrm{X}_{\text {out }}^i\right)\)
        <ul>
          <li>\(\mathrm{X}_{\text {out }}^i\)는 output of the multi-scale Transformer with the patch size \(S_i\)</li>
          <li>\(T_i\)는 align the temporal dimension from different scales하는 transformation function</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/Pathformer/table1.png" alt="그림4" /></p>

<p><img src="/assets/img/timeseries/Pathformer/table3.png" alt="그림5" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>Pathformer : Multi-Scale Transformer with Adaptive Pathways for TSF
    <ul>
      <li>integrates multi-scale temporal resolutions and temporal distances</li>
      <li>by patch division with multiple patch sizes and dual attention (modeling multi-scale characteristics)</li>
      <li>adaptive pathways dynamically select and aggregate scale-specific characteristics</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=lJkOCMP2aW)]]></summary></entry><entry><title type="html">Chernoff Bound (upper bound on the tail probability)</title><link href="http://localhost:4000/stat/2024-05-23-ChernoffBound/" rel="alternate" type="text/html" title="Chernoff Bound (upper bound on the tail probability)" /><published>2024-05-23T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/stat/ChernoffBound</id><content type="html" xml:base="http://localhost:4000/stat/2024-05-23-ChernoffBound/"><![CDATA[<h2 id="prerequirement">Prerequirement</h2>
<ul>
  <li>Markov inequality \(\mathrm{P}(X \geq a) \leq \frac{\mathrm{E}(X)}{a}\)</li>
  <li>Chebyshev inequality \(\operatorname{Pr}(\mid X-\mu\mid \geq k \sigma) \leq \frac{1}{k^2}\)</li>
</ul>

<h2 id="chernoff-bound">Chernoff Bound</h2>
<ul>
  <li>확률변수 X에 대한 tail probability <br />
\(\begin{aligned}P(X \geqslant a) &amp; =P\left(e^{t X} \geqslant e^{t a}\right) \\ &amp; \leqslant \frac{\mathbb{E}\left[e^{t X}\right]}{e^{t a}} \text { by Markov inequality } . \\ &amp; =M(t) e^{-t a}\end{aligned}\)</li>
</ul>

<h2 id="example--bernoulli-confidence-sets-vs-hoeffdings-inequality">Example : Bernoulli Confidence sets vs. Hoeffding’s inequality</h2>
<ul>
  <li>Under asymptotic normality <br />
\(\begin{aligned}P_\theta\left(\theta \in C_n\right)&amp;=P_\theta\left(\widehat{\theta}-z_{\alpha / 2} \sqrt{\operatorname{Var}_\theta[\widehat{\theta}]} \leq \theta \leq \widehat{\theta}+z_{\alpha / 2} \sqrt{\operatorname{Var}_\theta[\widehat{\theta}]}\right)\\C_n&amp;=\left(\widehat{p}-z_{\alpha / 2} \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}, \widehat{p}+z_{\alpha / 2} \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\right)\end{aligned}\)</li>
  <li>Hoeffding’s inequality <br />
\(P(\mid \widehat{p}-p\mid \geq t) \leq 2 e^{-2 n t^2}\)</li>
  <li><strong>Under asymptotic normality로 얻은 Confidence sets은 항상 Hoeffding’s inequality 보다 tight(short)하다.</strong>
    <ul>
      <li><strong>증명</strong> :  \(z_{\alpha / 2} \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}} \leq \sqrt{\frac{\log (2 / \alpha)}{2 n}}\)에서 \(\widehat{p}(1-\widehat{p}) \le \frac{1}{4}\) 이므로</li>
      <li>(1) Exponential Markov Inequality<br />
\(\begin{aligned} P(z \geqslant z) &amp; =P\left(e^{t Z} e^{t z}\right) \\ &amp; \leqslant \frac{\mathbb{E}\left[e^{t Z}\right]}{e^{t z}}=e^{\frac{t^2}{2}-t z} \\ &amp; \leqslant e^{-z^2 / 2} \quad \text{ when }t=z\end{aligned}\)</li>
      <li>(2) Chernoff Bounds<br />
\(\begin{aligned}  p(Z \geqslant z) &amp;\leqslant e^{-z^2 / 2} \\ p\left(z \geqslant z_{\frac{\alpha}{2}}\right) &amp;\leqslant e^{-z_{\frac{\alpha}{2}}^2 / 2} \\  \alpha / 2 &amp;\leqslant e^{-z_{\frac{\alpha}{2}}^2 / 2} \\ \log{\frac{\alpha}{2}}&amp;\leqslant-\frac{z_{\frac{\alpha}{2}}^2}{2} \\ z_{\frac{\alpha}{2}}^2 &amp;\leq 2 \log{\frac{\alpha}{2}} \\ \end{aligned}\)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[Prerequirement Markov inequality \(\mathrm{P}(X \geq a) \leq \frac{\mathrm{E}(X)}{a}\) Chebyshev inequality \(\operatorname{Pr}(\mid X-\mu\mid \geq k \sigma) \leq \frac{1}{k^2}\)]]></summary></entry><entry><title type="html">(Corrformer, NMI 2023) Code Review 3 - Encoder</title><link href="http://localhost:4000/pytorch/2024-05-05-corrformer3/" rel="alternate" type="text/html" title="(Corrformer, NMI 2023) Code Review 3 - Encoder" /><published>2024-05-05T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/pytorch/corrformer3</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-05-05-corrformer3/"><![CDATA[<p><img src="/assets/img/pytorch/corrformer0/corrformer010.jpeg" alt="사진10" /></p>
<ul>
  <li>우리는 <code class="language-plaintext highlighter-rouge">exp_main</code>의 <code class="language-plaintext highlighter-rouge">train</code> 메소드를 실행하고 있다.</li>
  <li><code class="language-plaintext highlighter-rouge">get_data</code>는 이미 살펴보았고 <code class="language-plaintext highlighter-rouge">self.model</code>에 들어가는 Corrformer를 이해하기 위해 <code class="language-plaintext highlighter-rouge">Corrformer.py</code>를 살펴보고 있다.
    <ul>
      <li>step 0. Initialization and Normalizaiton</li>
      <li>step 1. Data Embedding Instance</li>
      <li>step 2. Encoder Instance</li>
      <li>step 3. Decoder Instance</li>
      <li>step 4. forward</li>
    </ul>
  </li>
  <li>forward 전체 코드
<img src="/assets/img/pytorch/corrformer1/fig6.png" alt="사진6" /></li>
</ul>

<h3 id="step-2-encoder-instance">step 2. Encoder Instance</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">enc_out = self.encoder(enc_out, attn_mask=enc_self_mask)</code>
<img src="/assets/img/pytorch/corrformer2/corrformer23.png" alt="사진3" /></li>
  <li>지난 포스팅에서 봤던 <code class="language-plaintext highlighter-rouge">enc_out = self.enc_embedding(x_enc, x_mark_enc) #(350, 48, 768)</code>가 <code class="language-plaintext highlighter-rouge">encoder</code>의 input</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">encoder</code>는 `layers’에 있는 Encoder, MultiCorrelation, AutoCorrelation, CrossCorrelation, CausalConv를 import
<img src="/assets/img/pytorch/corrformer3/corrformer31.png" alt="사진31" /></p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">self.encoder</code> instance를 만드는데, parameters를 잠시 지우고 구조만 보면 아래와 같다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">Encoder</span><span class="p">([</span>
        <span class="nc">EncoderLayer</span><span class="p">(</span>
            <span class="nc">MultiCorrelation</span><span class="p">(</span>
                <span class="nc">AutoCorrelationLayer</span><span class="p">(</span><span class="nc">AutoCorrelation</span><span class="p">()),</span>
                <span class="nc">CrossCorrelationLayer</span><span class="p">(</span><span class="nc">CrossCorrelation</span><span class="p">(</span><span class="nc">CausalConv</span><span class="p">()))</span>
            <span class="p">))])</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Encoder</code> 안에 <code class="language-plaintext highlighter-rouge">EncoderLayer</code>가 있다. <code class="language-plaintext highlighter-rouge">EncoderLayer</code> 안에는 <code class="language-plaintext highlighter-rouge">MultiCorrelation</code>이 있는데, <code class="language-plaintext highlighter-rouge">MultiCorrelation</code> 안에는 <code class="language-plaintext highlighter-rouge">AutoCorrelationLayer(AutoCorrelation())</code>과 <code class="language-plaintext highlighter-rouge">CrossCorrelationLayer(CrossCorrelation(CausalConv()))</code>이 있다.</li>
</ul>

<h3 id="causalconv">CausalConv()</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Corrformer.py
</span><span class="nc">CausalConv</span><span class="p">(</span>
    <span class="n">num_inputs</span><span class="o">=</span><span class="n">configs</span><span class="p">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="n">configs</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">label_len</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pred_len</span><span class="p">),</span>
    <span class="n">num_channels</span><span class="o">=</span><span class="p">[</span><span class="n">configs</span><span class="p">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="n">configs</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">label_len</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pred_len</span><span class="p">)]</span> <span class="o">*</span> <span class="n">configs</span><span class="p">.</span><span class="n">dec_tcn_layers</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Corrformer.sh
# --d_model 768 \ --n_heads 16 \ --label_len 24 \ --pred_len 24 \ --dec_tcn_layers 1 \
</span>
<span class="c1"># Causal_Conv.py
</span><span class="k">class</span> <span class="nc">CausalConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CausalConv</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">num_levels</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">num_channels</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_levels</span><span class="p">):</span>
            <span class="n">dilation_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">**</span> <span class="n">i</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="n">num_inputs</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">num_channels</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">out_channels</span> <span class="o">=</span> <span class="n">num_channels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="nc">CausalBlock</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation_size</span><span class="p">,</span>
                                   <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dilation_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)]</span>

        <span class="n">self</span><span class="p">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Corrformer github](https://github.com/thuml/Corrformer)]]></summary></entry></feed>