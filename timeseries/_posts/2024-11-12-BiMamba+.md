---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2404.15772)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)

## Abstract

- LTSF :  long-term dependencies capturing and **sparse semantic characteristics**
- Mamba
  - the selective capability on input data
  - the hardware-aware parallel computing algorithm
- **Mamba+ block**
  - by adding a forget gate inside Mamba,
  - to selectively combine the new features with the historical features in a complementary manner

- **Bi-Mamba+**
  - apply Mamba+ both forward and backward
- MTS는 시나리오마다 dependency가 다름
  - (varying emphasis on intra- or inter-series dependencies)
  - $$\to$$ series-relation-aware decider
    - : controls the utilization of channel-independent or channel-mixing tokenization strategy

## 1. Introduction

- Transformer-based models
  - implicitly models the **inter-series dependencies** through channel-mixing embeddings
  - However the **quadratic complexity** of the self-attention mechanism
  - Informer, Autoformer : sparse attention
    - But balancing computational efficiency and predicting performance는 본질적 해결 X
    - 게다가 not explicitly capture the inter-series dependencies
-  state-space models (SSM) : design of selective scanning
  - **Long-term time series modeling** : patching manner (patch-wise tokens)으로 하겠다
  - **Emphasis on intra- or inter-series dependencies** : 데이터셋마다 intra- or inter-sequence dependencies 둘 중 뭐가 중요한지가 다름
- 그래서 **Mamba+**를 디자인함
  - adding a forget gate in Mamba,
  - to selectively combine the new features with the historical features in a complementary manner
  - therefore preserving historical information in a longer range
- **Mamba+**에 기반한 **Bidirectional Mamba+ (BiMamba+)**를 제안
  - to model the MTS data from both forward and backward,
    - enhancing the model’s robustness and ability to capture interactions between time series elements
  - Series-Relation-Aware (SRA) decider
    - measures the proportion of highly correlated series pairs in the MTS data
    - to automatically choose channel-independent or channelmixing tokenization strategies
  - patch-wise tokens
    - contain richer semantic information
    - and encourage the model to learn the long-term dependencies

## 2. Related Work

### Time Series Forecasting

- Transformer-based models : quadratic complexity to the length of the sequence
  - Informer(Zhou et al. 2021) : ProbSparse mechanism
  -  Autoformer(Wu et al. 2021) : time series decomposition
  -  Pyraformer(Liu et al. 2021) : pyramidal attention module
  - FEDformer(Zhou et al. 2022) : frequency enhanced Transformer through frequency domain mapping
  - PatchTST(Nie et al. 2023) : divides each univariate sequence into patches 
    - and uses patch-wise self-attention to model temporal dependencies
  - Crossformer(Zhang and Yan 2023) : Cross-Dimension attention 
  - iTransformer(Liu et al. 2023) : inverts the attention layers
    - to straightly model inter-series dependencies 
    - But, the tokenization approach is simply passing the whole sequence through a Multilayer Perceptron (MLP),
      - which overlooks the complex evolutionary patterns inside the time series

### SSM-based models

- RNN-based models :
  - maintain a hidden state which is updated with each input element
  - limits the training speed and leads to forgetting long-term information
- CNN-based models : 
  - parallel computing and have faster training speed
  - limits the inference speed and overlook the long-term global information
- State Space Models (SSM) :
  - trained in parallel like CNN and inferences fastly like RNN
- Mamba
  - parameterized matrices and a hardware-aware parallel computing algorithm to SSM
  -  **S-Mamba**(Wang et al. 2024)
    - embeds each univariate time series like iTransformer
    - and feeds the embeddings into Mamba blocks
      - to model the relationships of different time series
    - However, the tokenization approach may overlook the complex evolutionary patterns
  - **MambaMixer**(Behrouz et al. 2024)
    - adjusts the Mamba block to bidirectional
    - and uses two improved blocks to capture inter/intra-series dependencies simultaneously
    - However, the gating branch is used to filter new features
      - (of both forward and backward directions)
      - which may cause challenges for extracting new features
  - **TimeMachine**(Ahamed and Cheng 2024)
    - a multi-scale quadruple-Mamba architecture
      - to unify the handling of channel-mixing and channelindependence situations
    - However, simply based on the length of historical observations and variable number of different datasets
      - the characteristics of the MTS data are not fully considered.

## 3. Methodology

### 3.1. Preliminaries

- Long-term multivariate time series forecasting
  - $$\mathbf{X}_{i n}=\left[x_1, x_2, \ldots, x_L\right] \in \mathbb{R}^{L \times M}$$, we predict the future values $$\mathbf{X}_{\text {out }}=\left[x_{L+1}, x_{L+2}, \ldots, x_{L+H}\right] \in \mathbb{R}^{H \times M}$$
- State Space Models
  - using first-order differential equations, $$h^{\prime}(t)=\mathbf{A} h(t)+\mathbf{B} x(t), \quad y(t)=\mathbf{C} h(t)$$
    - where $$\mathbf{A} \in \mathbb{R}^{N \times N}, \mathbf{B} \in \mathbb{R}^{D \times N} \text { and } \mathbf{C} \in \mathbb{R}^{N \times D}$$
  - can be discretized :
    - $$\begin{aligned}
      & \overline{\mathbf{A}}=\exp (\Delta \mathbf{A}), \\
      & \overline{\mathbf{B}}=(\Delta \mathbf{A})^{-1}(\exp (\Delta \mathbf{A})-\mathbf{I}) \cdot \Delta \mathbf{B}
      \end{aligned}$$,
    - $$h_k=\overline{\mathbf{A}} h_{k-1}+\overline{\mathbf{B}} x_k, \quad y_k=\mathbf{C} h_k$$,
- **S4**(Gu et al. 2021b) :
  - HIPPO Matrix(Gu et al. 2020) to the initialization of matrix A
- **Mamba**(Gu and Dao 2023) :
  - parameterizes the matrices $$\mathbf{B}, \mathbf{C}$$ and $$\Delta$$ in a data-driven manner,
  - introducing a selection mechanism into S4 model

### 3.2. Overview

![그림1](/assets/img/timeseries/BiMamba+/fig1.png)

- step 1) calculate the tokenization strategy indicator through the SRA decider
- step 2) divide the input series into patches and generate patch-wise tokens
  - based on the tokenization strategy indicator
- step 3) obtained tokens are fed into multiple Bi-Mamba+ encoders
- step 4) a flatten head and a linear projector to get the final output

### 3.3. Instance Normalization

- the input sequence의 non-stationary statistics를 제거하기 위해 RevIN (Kim et al. 2022) 사용

### 3.4. Token Generalization

- SRA Decider
  - Channel-independence / dependence는 데이터셋마다 다름
  - $$\to$$ automatic tokenization process
    - 데이터셋마다 $$T=\left\{t^1, t^2, \ldots, t^M\right\}$$에 대해
      - Spearman correlation coefficients of different series $$t^i \text { and } t^j$$ 계산 $$\rho_{i, j}$$
      - $$\rho_{i, j}=1-\frac{6 \sum_{k=0}^n\left(\operatorname{Rank}\left(t_k^i\right)-\operatorname{Rank}\left(t_k^j\right)\right)^2}{n\left(n^2-1\right)}$$,
        - where $$n$$ : the number of observations
        - $$\operatorname{Rank}\left(t_k^i\right)$$ : the rank level of the $$k$$-th element in the specific time series $$t^i$$
      - threshold $$\lambda$$를 정하고 relevant series $$\rho_{\max }^\lambda$$ and $$\rho_{\max }^0$$를 센 다음
        - relation ratio $$r=\rho_{\max }^\lambda / \rho_{\max }^0 \geq 1-\lambda$$이면 channel-mixing
        - 그렇지 않으면 channelindependent strategy

![그림1](/assets/img/timeseries/BiMamba+/fig2.png)

![그림1](/assets/img/timeseries/BiMamba+/alg1.png)

### 3.5. Tokenization Process

- PatchTST처럼 $$x_{1: L}^i$$를 $$J = \left\lceil\frac{L-P}{S}+1\right\rceil$$개의 patch로 나누고
  - (S는 stride, P는 patch에 들어가는 시점의 개수)
  - channel-independent strategy에서는 각 patch를 D차원으로 embedding
    - $$\to$$ $$\mathbb{E}_{\text {ind }} \in \mathbb{R}^{M \times J \times D}$$
  - channel-mixing strategy에서는 같은 시점의 다른 변수들도 group으로 만들고 각 group을 tokenization
    - $$\to \mathbb{E}_{\operatorname{mix}} \in\mathbb{R}^{J \times M \times D}$$.

### 3.6. Mamba+ Block

![그림1](/assets/img/timeseries/BiMamba+/mamba.png)

- **기존 Mamba**는 2개의 branches를 사용
  - $$b_1$$에서는 1d-conv와 SSM을 통과, 다른 하나 $$b_2$$에서는 그냥 SiLU activation 통과
  - $$b_1$$의 SSM 안에 HIPPO가 있긴 해도 $$b_2$$ 때문에 최근 정보가 더 우선시되는 문제
- 그래서 **Mamba+ block**에서는
  - forget gate $$\text{gate}_f=1-\text{gate}_{b_2}$$를 추가
  - $$\text{gate}_f$$와 $$\text{gate}_{b_2}$$는 new features와 forgotten historical features를 선택적 결합
    - $$\to$$ preserving historical information !

![그림1](/assets/img/timeseries/BiMamba+/alg2.png)

### 3.7. Bidirectional Mamba+ Encoder

- Channel-mixing이면 $$\mathbb{E}_x^{(l)} \in \mathbb{R}^{J \times M \times D}$$ and $$\mathbb{E}_x^{(0)}=\mathbb{E}_{m i x}$$
  - otherwise $$\mathbb{E}_x^{(l)} \in \mathbb{R}^{M \times J \times D}$$ and $$\mathbb{E}_x^{(0)}=\mathbb{E}_{\text {ind }}$$
- 2개의 Bi-Mamba+ block, 각각 forward and backward
  - 각각의 input을 $$\mathbb{E}_{x, d i r}^{(l)}$$ where $$\operatorname{dir} \in\{\text{forward,backward}\}$$이라 하면
  - $$\mathbb{E}_x^{(l+1)}=\sum_{\text {dir }}^{\{\text {forward,backward }\}} \mathcal{F}\left(\mathbb{E}_{y, \text { dir }}^{(l)}, \mathbb{E}_{x, d i r}^{(l)}\right)$$가 다음 layer의 input이 됨

![그림1](/assets/img/timeseries/BiMamba+/alg3.png)

### 3.8. Loss Function

- MSE : $$\mathcal{L}(Y, \hat{Y})=\frac{1}{\mid Y \mid} \sum_{i=1}^{\mid Y \mid}\left(y_{(i)}-\hat{y}_{(i)}\right)^2$$

## 4. Experiments

### 4.1. Datasets

![그림1](/assets/img/timeseries/BiMamba+/table1.png)

### 4.2. Baseline Models

- Autoformer (Wu et al. 2021)
  - series decomposition technique with Auto-Correlation mechanism
- PatchTST (Nie et al. 2023)
  - patching and channel independent techniques
- Crossformer (Zhang and Yan 2023)
  - PatchTST + Attention layer (for capture inter-series dependencies)
- iTransformer (Liu et al. 2023)
  - inverts the modeling method of Transformer
- DLinear (Zeng et al. 2023)
  - decomposes time series into two different components
- TimesNet (Wu et al. 2022)
  - transforming the 1-D time series into a set of 2-D tensors
- WITRAN (Jia et al. 2024)
  - RNN structure that process the univariate input sequence
    - in the 2-D space with a fixed scale
- CrossGNN (Huang et al. 2024)
  - time series in a multi-scale way
  - GNN to capture both cross-scale and cross-series dependencies
- S-Mamba (Wang et al. 2024)
  - generates embeddings for each time series through a simple MLP layer
  - and uses Mamba to extract inter-series dependencies

### 4.3. Experimental Settings

- $$L=96$$ for all models on all datasets, $$H \in\{96,192,336,720\}$$
- $$S=\frac{1}{2} P$$ and use patch length $$P=\frac{1}{4} L$$
- SRA decider, we set $$\lambda=0.6$$
- for **Bi-Mamba+, PatchTST and Crossformer** that use patching technique, we set $$D=128$$ for Weather, Traffic, Electricity, Solar and $$D=64$$ for ETT datasets,
  - while for **S-Mamba and iTransformer** that map the whole sequence to tokens, we set $$D=512$$ for Weather, Traffic, Electricity, Solar and $$D=256$$ for ETT datasets.
- As for parameters within Mamba+ block은 Ahamed and Cheng 2024; Wang et al. 2024처럼
- **convolutional kernel size** d_conv =2 and **hidden state expansion** expand =1 on all datasets.
- **hidden dimension** d_state =16 for Weather, Electricity and Traffic and d_state =8 for ETT datasets.
- **encoder layer** $$l \in\{1,2,3\}$$,
- **learning rate**는 $$[5 \mathrm{e}-5,1 \mathrm{e}-4,2 \mathrm{e}-4,5 \mathrm{e}-4,1 \mathrm{e}-3, 2 \mathrm{e}-3,5 \mathrm{e}-3]$$

### 4.4. Main Results

![그림1](/assets/img/timeseries/BiMamba+/table3.png)

### 4.5. Ablation Study

![그림1](/assets/img/timeseries/BiMamba+/table4.png)

- (a) w/o SRA-I which use channel-independent strategy only
- (b) w/o SRA-M which use channelmixing strategy only
- (c) w/o Bi which use forward direction Mamba block only
- (d) w/o Residual that removes the residual connection
- (e) S-Mamba
- (f) PatchTST used for the benchmark models

- **filter threshold** $$\lambda$$에 따른 tokenization strategy indicator

![그림1](/assets/img/timeseries/BiMamba+/fig3.png)

- length of patches $$P$$에 따른 MSE

![그림1](/assets/img/timeseries/BiMamba+/fig4.png)

### 4.6. Model efficiency

![그림1](/assets/img/timeseries/BiMamba+/fig7-1.png)

![그림1](/assets/img/timeseries/BiMamba+/fig7-2.png)

## 5. Conclusion

-  **Bi-Mamba+**
  - adding forget gate in Mamba
    - to selectively combine the added new features with the forgotten historical features in a complementary manner,
    - therefore preserving historical information in a longer range
  - dividing the time series into patches
    - for inter-series dependencies at a finer granularity