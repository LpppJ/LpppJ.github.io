<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-09-24T18:29:29+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">(Code Review, ICLR 2022) Raindrop</title><link href="http://localhost:4000/pytorch/2024-09-24-raindrop/" rel="alternate" type="text/html" title="(Code Review, ICLR 2022) Raindrop" /><published>2024-09-24T00:00:00+09:00</published><updated>2024-09-24T18:29:29+09:00</updated><id>http://localhost:4000/pytorch/raindrop</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-24-raindrop/"><![CDATA[<p><a href="https://arxiv.org/abs/2110.05357">(Paper) Graph-Guided Network for Irregularly Sampled Multivariate Time Series</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">(Paper Review, ICLR 2022) Raindrop</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig1.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig2.png" alt="사진1" /></p>

<ul>
  <li>먼저 터미널에 git clone과 requirements를 입력하여 install 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">python Raindrop.py</code>로 P19, P12, PAM 데이터셋에 대한 성능을 볼 수 있다.</li>
</ul>

<h2 id="2-raindroppy">2. Raindrop.py</h2>

<h3 id="21-data-preparing">2.1. Data Preparing</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig3.png" alt="사진1" />
<img src="/assets/img/pytorch/raindrop_code/fig4.png" alt="사진1" /></p>

<ul>
  <li>parser를 통해 aurgments를 만들고</li>
  <li>본 논문에서 제시하는 model은 irregular time series를 다룬다.
    <ul>
      <li>그러므로 <code class="language-plaintext highlighter-rouge">missing ratio</code>, 즉 feature를 masking하는 비율을 미리 결정해준다. (option)</li>
      <li>일단은 0(no missing)으로 두고 코드를 이해해보자</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig5.png" alt="사진1" /></p>

<ul>
  <li>사전에 정한 <code class="language-plaintext highlighter-rouge">missing ratio</code>를 사용한다.</li>
  <li>epoch 수와 learning rate도 미리 정한다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig6.png" alt="사진1" /></p>

<ul>
  <li>P19 데이터셋을 사용한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">d_static</code>과 <code class="language-plaintext highlighter-rouge">d_inp</code>는 시간에 따라 변하지 않는(정적) / 변하는(동적) 변수의 개수</li>
      <li><code class="language-plaintext highlighter-rouge">static_info</code>는 <code class="language-plaintext highlighter-rouge">d_static</code> 변수가 있는지 없는지 (bool)</li>
      <li><code class="language-plaintext highlighter-rouge">max_len</code>은, batch 내 샘플마다 시계열의 길이가 다른데 최대 길이
        <ul>
          <li>만약 <code class="language-plaintext highlighter-rouge">max_len</code>보다 짧다면 그 부분은 다 0으로 기록되어있다.</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">n_classes</code>는 샘플에 속하는 class의 개수</li>
    </ul>
  </li>
  <li>
    <p>다른 데이터셋을 사용한다면 위의 변수들은 달라질 수 있다.</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">d_ob</code>는 각 변수를 몇 차원으로 표현할지를 의미한다.</li>
  <li>그래서 <code class="language-plaintext highlighter-rouge">d_model</code>은 동적 변수의 개수인 <code class="language-plaintext highlighter-rouge">d_inp</code>에 <code class="language-plaintext highlighter-rouge">d_ob</code>를 곱한 값이 된다.</li>
  <li><code class="language-plaintext highlighter-rouge">nhid</code>는 FFN의 dimension인데 <code class="language-plaintext highlighter-rouge">d_model</code>의 2배를 사용</li>
  <li><code class="language-plaintext highlighter-rouge">nlayers</code>는 layer의 개수, <code class="language-plaintext highlighter-rouge">nhead</code>는 MHA(multi-head attention)에서 heads 개수이고 모두 2개를 사용</li>
  <li><code class="language-plaintext highlighter-rouge">dropout</code>은 TransformerEncoderLayer에서 사용하는 dropout ratio</li>
  <li><code class="language-plaintext highlighter-rouge">aggreg</code>는 나중에 각 배치마다, 각 시점을 vector로 표현할텐데 그걸 모든 시점에 대해 합칠 때 <strong>평균</strong>을 사용</li>
  <li><code class="language-plaintext highlighter-rouge">MAX</code>는 positional encoder에 들어가는 MAX parameter인데
    <ul>
      <li>막상 positional encoder 코드를 보면 <code class="language-plaintext highlighter-rouge">MAX</code>라는 변수를 사용하지 않으니 신경 안써도 된다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig7.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n_run</code>은 데이터셋에 대해 몇 번을 실험해서 기록할지를 의미한다.</li>
  <li><code class="language-plaintext highlighter-rouge">n_splits</code>는 데이터가 5등분 되어있어서 5를 사용한다.</li>
  <li>그리고 본 model을 평가하기 위한 성능 지표를 기록할 arrays를 만들어놓는다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig8.png" alt="사진1" /></p>

<ul>
  <li>그리고 불러온 데이터셋을 train(0.8) / valid(0.1) / test(0.1)로 나누고 label(y)도 따로 준비한다.</li>
  <li>P19 데이터셋의 경우 train에는 31042개의 샘플이 있다. (샘플은 한 명의 환자 정도로 생각할 수 있다.)
    <ul>
      <li>그리고 각 샘플은 <code class="language-plaintext highlighter-rouge">torch.size([# of timesetps,  # of features])</code>인 tensor이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig9.png" alt="사진1" /></p>

<ul>
  <li>T는 time steps의 수, F는 (동적) 변수의 개수이고, D는 (정적) 변수의 개수가 된다.
    <ul>
      <li>동적 변수는 <code class="language-plaintext highlighter-rouge">Ptrain</code>의 <code class="language-plaintext highlighter-rouge">arr</code>에, 정적 변수는 <code class="language-plaintext highlighter-rouge">Ptrain</code>의 <code class="language-plaintext highlighter-rouge">extended_static</code>에 따로 준비하고 있다.</li>
    </ul>
  </li>
  <li>그리고 normalization을 위해 모든 변수들의 평균과 표준편차를 얻는다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">getStats</code> 함수에는 사용하는데 특이사항 없으므로 skip</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig10.png" alt="사진1" /></p>

<ul>
  <li>각각의 shape은 아래와 같다.</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>의 동적 변수들의 개수가 34개였는데 68이 된 이유는 :
    <ul>
      <li>같은 크기의 Mask를 옆에 이어붙였기 때문이다.</li>
      <li>Mask는 <code class="language-plaintext highlighter-rouge">M = 1*(input_tensor &gt; 0) + 0*(input_tensor &lt;= 0)</code>이다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>은 31042개의 텐서인데, 각 tensor는 해당 샘플의 길이를 알려준다.
    <ul>
      <li>즉 만약 0번째 샘플의 길이가 40이라면, 0번째 tensor는<code class="language-plaintext highlighter-rouge">[1, 2, ..., 40, 0, 0, ...]</code>이다.</li>
      <li>일단 숫자는 <code class="language-plaintext highlighter-rouge">max_len</code>개인데 해당 샘플의 길이까지만 index를 기록하고 뒷부분은 zero padding</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">y_train</code>은 각 샘플의 정답 label이다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig11.png" alt="사진1" /></p>

<ul>
  <li>이제 <code class="language-plaintext highlighter-rouge">global_structure</code>를 정의하는데, 각각의 동적 변수가 상호작용하는지를 0, 1로 표현
    <ul>
      <li>adjacency matrix의 역할을 한다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">missing_ratio</code>가 존재했다면 몇몇 feature를 masking한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>이 <code class="language-plaintext highlighter-rouge">sample</code>이면 각 샘플(환자)마다 독립적으로 특성을 무작위 제거</li>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>이 <code class="language-plaintext highlighter-rouge">set</code>이면 미리 계산된 density scores를 사용하여 제거할 특성을 결정하고 모든 샘플에서 제거</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig12.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>의 shape을 <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps,  batch_size,  # of features(w/masking)])</code>으로,</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>의 shape을 <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps, batch_size])</code>로 setting</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig13.png" alt="사진1" /></p>

<ul>
  <li>앞서 소개한 parameters를 한 번 출력해보았다.</li>
  <li>지금은 masking ratio가 0이다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig14.png" alt="사진1" /></p>

<ul>
  <li>parameters의 descriptions는 위와 같다.</li>
</ul>

<h3 id="22-model-setting">2.2. Model setting</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig15.png" alt="사진1" /></p>

<ul>
  <li>이제 model, criterion, optimiazer, scheduler를 정의한다.</li>
  <li>model은 2d tensor로 표현된 샘플마다 classification하도록 설계되었으므로 CrossEntropyLoss를 사용</li>
  <li>아직 input을 model에 넣은 건 아님.
    <ul>
      <li>input이 model에 들어가면 어떤 과정을 거치는지는 아래 3. <code class="language-plaintext highlighter-rouge">models_rd.py</code>에서 보도록 한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig16.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">idx_0</code>은 <code class="language-plaintext highlighter-rouge">y</code>가 0인 samples의 index, <code class="language-plaintext highlighter-rouge">idx_1</code>은 반대</li>
  <li>label이 1인 샘플의 개수가 적은 unbalancing 문제를 해결하기 위해 3배로 늘림 (왜 <strong>3</strong>배인지는 모름)</li>
  <li>batch_size가 128인데 label이 0과 1인 samples를 절반씩 채울테니
    <ul>
      <li>n_batches는 개수가 더 적은 label 기준으로 모든 samples를 한 번씩 다 볼 수 있도록 설정했다.</li>
      <li>사실 label이 1인 samples를 3배 했으니 label이 1인 샘플을 3번씩 보는 꼴이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig17.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>이제 epoch를 시작하는데, label이 0인 샘플과 1인 샘플에서 무작위로 <code class="language-plaintext highlighter-rouge">batch_size/2</code>개씩 가져온다.</li>
  <li>사실 label이 1인 samples를 3배 했으니 여기서는 중복된 샘플이 나올 가능성이 있다.</li>
  <li>model에 들어갈 input tensors의 shape을 미리 확인해두자</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig19.png" alt="사진1" /></p>

<ul>
  <li>이제 model에 들어가고 통상적인 backpropagation을 거친다.</li>
  <li>model에 들어가면 어떤 일이 일어나는지 알아보자.</li>
</ul>

<h2 id="3-models_rdpy">3. models_rd.py</h2>

<h3 id="31-init">3.1. <strong>init</strong></h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>이 상당히 많지만 지금 다 알 필요는 없다.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>에서 사용할 때 다시 올라와서 보면 될 듯</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig20.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig21.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig22.png" alt="사진1" /></p>

<h2 id="32-forward">3.2. forward</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig23.png" alt="사진1" /></p>

<ul>
  <li>P19 데이터셋의 경우 input shape은 주황색 주석과 같다.</li>
  <li>src로 들어오는 P의 경우 34개의 변수였는데 같은 크기의 Mask를 옆에 이어붙인 것이니 다시 분리
    <ul>
      <li>각각을 missing_mask, src라고 부름</li>
    </ul>
  </li>
  <li>그 다음 34개의 변수를 <code class="language-plaintext highlighter-rouge">d_ob</code>(여기선 4)번 반복해서 src의 representation capacity를 키워주고
    <ul>
      <li>ReLu를 통과시켜서 non-linearity를 표현할 수 있게 한다.</li>
      <li>그 다음 dropout을 거친다.</li>
    </ul>
  </li>
  <li>결국 <code class="language-plaintext highlighter-rouge">h</code>는 src를 확장시키고 learnable weights와 ReLu를 곱해 모델이 학습할 수 있는 형태로 만든 것</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig24.png" alt="사진1" /></p>

<ul>
  <li>이제 batch에 있는 각 sample마다 mask를 만든다.</li>
  <li>sample에 값이 있으면 mask에는 False가 되고 값이 없으면 mask가 True가 된다.</li>
  <li>mask의 길이는 60으로 고정이지만 sample마다 길이가 다르기 때문에 어디까지 False이고 언제부터 True인지는 sample마다 다르다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig25.png" alt="사진1" /></p>

<ul>
  <li>다음으로 <code class="language-plaintext highlighter-rouge">global_structure</code>를 adjacency matrix로 사용한다.
    <ul>
      <li>shape은 동적 변수의 개수 <code class="language-plaintext highlighter-rouge">d_inp</code> x <code class="language-plaintext highlighter-rouge">d_inp</code>가 되므로 각 동적 변수를 연결 여부를 (0,1)로 표현한다.</li>
      <li>epoch가 진행되면서 바뀔 수도 있으니 대각성분은 항상 1로 update해준다.</li>
    </ul>
  </li>
  <li>그 다음 edge_index와 edge_weights를 미리 구해놓는다.
    <ul>
      <li>연결된 nodes의 index와 그 weights를 의미함</li>
    </ul>
  </li>
  <li>그 다음 batch에 있는 각 sample마다 (동적) 변수들의 global structure(edge)를 고려한 representation을 저장할 공간 <code class="language-plaintext highlighter-rouge">output</code>을 미리 만들어놓는다.
    <ul>
      <li>각 sample마다 <code class="language-plaintext highlighter-rouge">torch([# of time steps,  d_inp x d_ob])</code> shape의 tensor가 들어갈 예정이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig26.png" alt="사진1" /></p>

<ul>
  <li>이제 아까 만든 <code class="language-plaintext highlighter-rouge">h</code>를 <code class="language-plaintext highlighter-rouge">x</code>로 받아서 (<code class="language-plaintext highlighter-rouge">x=h</code>) 하나의 sample에 대한 <code class="language-plaintext highlighter-rouge">h</code>를 <code class="language-plaintext highlighter-rouge">stepdata</code> 가져온다</li>
  <li><code class="language-plaintext highlighter-rouge">p_t</code>는 각 timestep을 <code class="language-plaintext highlighter-rouge">d_pe = 16</code>차원 vector로 embedding한 것이다. (init 참고)</li>
  <li>이제 <code class="language-plaintext highlighter-rouge">stepdata</code>를 <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code>로 reshape한다.
    <ul>
      <li>왜냐하면 feature끼리 attention을 수행하기 때문에 각 feature를 하나의 vector로 만들 필요가 있기 때문</li>
    </ul>
  </li>
  <li>이제 각 feature를 vector로 만든 걸 <code class="language-plaintext highlighter-rouge">ob_propagation</code>으로 정의된 attention layer에 넣는다.
    <ul>
      <li>그러면 같은 shape <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code> tensor가 return되지만</li>
      <li>해당 sample의 각각의 features를 Observation Propagation을 거쳐 representation한 결과이다.</li>
      <li><code class="language-plaintext highlighter-rouge">Ob_propagation.py</code>에 있고, 코드를 따로 첨부하지는 않겠으나 아래와 같은 과정을 거친다.
        <ul>
          <li>1) Message Passing: node 간에 정보를 전달하는 mechanism 구현</li>
          <li>2) Attention Mechanism: 각 node가 이웃 node로부터 받는 메시지의 중요도를 학습</li>
          <li>3) Egde weights: graph의 edge에 weight를 적용하여 정보 전달의 강도를 조절</li>
          <li>4) Edge prune: 중요도가 낮은 edge를 제거하여 computation efficiency 높임</li>
          <li>5) Feature Transform: linear Transform과 activation ftn으로 node의 feature를 변환</li>
          <li>6) Aggregation: 이웃 node로부터 받은 메시지를 합침</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig27.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ob_propagation-layer</code>를 한 번 더 통과시키고 shape을 맞춰서 <code class="language-plaintext highlighter-rouge">output</code>의 sample index 자리에 넣는다.
    <ul>
      <li>그리고 alpha_all에는 그 attention weights를 넣는다.
        <ul>
          <li>34개의 features끼리의 attention이니 34\(\times\)34\(=\)1156개의 숫자가 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>모든 samples에 대해서 완료하여 <code class="language-plaintext highlighter-rouge">output</code>이 완성되면 distance를 구한다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig28.png" alt="사진1" /></p>

<ul>
  <li>다음으로 time embedding을 concat한다.</li>
  <li>이러면 shape이 <code class="language-plaintext highlighter-rouge">torch.size([60, 128, 152])</code>가 되는데, 각 sample마다(128), 하나의 시점을 152차원 vector로 표현한 것이다.
    <ul>
      <li>이 152는 (동적) 변수 34개를 34\(\times\)4 = 136차원으로 표현하고, time embedding 16차원을 붙인 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig29.png" alt="사진1" /></p>

<ul>
  <li>이걸 transformer encoder에 통과시키고</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig30.png" alt="사진1" /></p>

<ul>
  <li>aggregate 하는데, 이 때 모든 시점에 대해 평균을 내준다. (<code class="language-plaintext highlighter-rouge">aggreg == mean</code>)</li>
  <li>그러면 각 sample은 모든 시점과 모든 변수를 통합하여 152차원 벡터로 표현된 결과가 나온다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig31.png" alt="사진1" /></p>

<ul>
  <li>마지막으로 (정적) 변수를 embedding한 emb를 붙여서 2-layer MLP에 넣으면</li>
  <li>각 sample에 대한 classification이 완료된다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig32.png" alt="사진1" /></p>

<ul>
  <li>Training에 따른 validation set acccuracy가 출력된다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig33.png" alt="사진1" /></p>

<ul>
  <li>그리고 classification report가 출력된다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Raindrop github](https://github.com/mims-harvard/Raindrop)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2024) Pathformer</title><link href="http://localhost:4000/pytorch/2024-09-09-pathformer/" rel="alternate" type="text/html" title="(Code Review, ICLR 2024) Pathformer" /><published>2024-09-09T00:00:00+09:00</published><updated>2024-09-10T19:09:25+09:00</updated><id>http://localhost:4000/pytorch/pathformer</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-09-pathformer/"><![CDATA[<p><a href="https://openreview.net/pdf?id=lJkOCMP2aW">(Paper) Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-05-23-Pathformer">(Paper Review, ICLR 2024) Pathformer</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig1.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig2.png" alt="사진1" /></p>

<ul>
  <li>먼저 터미널에 git clone과 requirements를 입력하여 install 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">bash scripts/multivariate/ETTm2.sh</code>로 ETTm2 데이터셋을 예측할 수 있다.</li>
</ul>

<h2 id="2-sh">2. .sh</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig3.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ETTm2.sh</code> 파일에는 <code class="language-plaintext highlighter-rouge">run.py</code>를 실행하도록 되어있다.</li>
</ul>

<h2 id="3-runpy">3. run.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig4.png" alt="사진1" /></p>

<ul>
  <li>parser를 통해 arguments를 만든다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig5.png" alt="사진1" /></p>

<ul>
  <li>그리고 <code class="language-plaintext highlighter-rouge">Exp_Main</code>에 있는 train에 arguments를 넣어준다.</li>
</ul>

<h2 id="4-exp_mainpy">4. exp_main.py</h2>

<h3 id="41-train">4.1. train</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig6.png" alt="사진1" /></p>

<ul>
  <li>model, data, optimizer, criterion을 설정하는 간단한 함수들과, <code class="language-plaintext highlighter-rouge">vali</code>, <code class="language-plaintext highlighter-rouge">train</code>, <code class="language-plaintext highlighter-rouge">test</code>, <code class="language-plaintext highlighter-rouge">predict</code> 함수가 있다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig7.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Exp_main</code> : <code class="language-plaintext highlighter-rouge">train</code>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">_get_data</code>로 train, valid, test 데이터셋을 load</li>
      <li><code class="language-plaintext highlighter-rouge">sum(p.numel() for p in self.model.parameters())</code>는 parameters 개수</li>
      <li>time, early sipping, optimizer, criterion, learning rate scheduler 정의</li>
      <li><code class="language-plaintext highlighter-rouge">lr_scheduler.OneCycleLR</code>는 learning rate를 빠르게 최대 학습률까지 증가시켰다가 다시 감소시키면서 최적화 과정
        <ul>
          <li><code class="language-plaintext highlighter-rouge">optimizer</code> : 사용하는 optimizer</li>
          <li><code class="language-plaintext highlighter-rouge">steps_per_epoch</code> : 1 epoch가 몇 번의 update가 발생하는지 (mini-batch)</li>
          <li><code class="language-plaintext highlighter-rouge">pct_start</code> : learning rate가 증가하는 구간의 비율 / <code class="language-plaintext highlighter-rouge">epochs</code> : 전체 epoch 수</li>
          <li><code class="language-plaintext highlighter-rouge">max_lr</code> : 최대 learning rate</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig8.png" alt="사진1" /></p>

<ul>
  <li>each epoch에서는 train loder에서 batch 단위로 데이터를 받고</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig9.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">with torch.cuda.amp.autocast():</code>는 <code class="language-plaintext highlighter-rouge">float16</code>과 <code class="language-plaintext highlighter-rouge">float32</code>를 자동으로 캐스팅</li>
  <li>모델이 예측한 <code class="language-plaintext highlighter-rouge">outputs</code>와 정답 <code class="language-plaintext highlighter-rouge">batch_y</code>를 비교하여 loss 계산</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig10.png" alt="사진1" /></p>

<ul>
  <li>epoch에 걸린 시간과 loss를 출력하고 backward로 parameters를 update</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig11.png" alt="사진1" /></p>

<ul>
  <li>Validation set에 대한 loss로 early stopping 여부를 결정하고 학습이 종료되면 모델 저장</li>
  <li>vali 함수는 특이 사항 없으므로 pass</li>
</ul>

<h3 id="42-test">4.2. test</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig12.png" alt="사진1" /></p>

<ul>
  <li>test dataset과, 학습되어 저장된 model을 load한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig13.png" alt="사진1" /></p>

<ul>
  <li>train과 비슷하게 batch 단위로 모델에 넣어서 예측값을 얻는다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig14.png" alt="사진1" /></p>

<ul>
  <li>batch 20개마다 묶어서 visualizaiton을 한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig15.png" alt="사진1" /></p>

<ul>
  <li>최종적인 예측과 loss를 <code class="language-plaintext highlighter-rouge">results.txt</code>에 저장한다.</li>
</ul>

<h3 id="43-predict">4.3. predict</h3>

<p>pass</p>

<h2 id="5-modelspathformerpy">5. models/Pathformer.py</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">from models import PathFormer</code> 이므로 해당 경로로 가서 pathformer의 archtecture를 보자</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">forward</code>는 normalization \(\to\) <code class="language-plaintext highlighter-rouge">start_fc</code> \(\to\) for <code class="language-plaintext highlighter-rouge">layer</code> in <code class="language-plaintext highlighter-rouge">self.AMS_lists</code> \(\to\) de-normalization로 구성된다.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>에 들어온 x의 shape은 <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes])</code>이다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">seq_len</code>은 관측하는 과거 시점 수, <code class="language-plaintext highlighter-rouge">num_nodes</code>는 multivariate에서 variates 개수</li>
    </ul>
  </li>
  <li>x가 unsqueeze되어 normalization, <code class="language-plaintext highlighter-rouge">start_fc</code>를 통과하면 <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes, d_model])</code>이 된다. (아래 <code class="language-plaintext highlighter-rouge">__init__</code> 참고)</li>
  <li>이제 <code class="language-plaintext highlighter-rouge">AMS_list</code>의 <code class="language-plaintext highlighter-rouge">layers</code>를 통과하고 denormalization을 통과한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig17.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>을 보면 <code class="language-plaintext highlighter-rouge">self.AMS_lists</code>는 <code class="language-plaintext highlighter-rouge">layers.AMS</code>에서 import한다.</li>
  <li>AMS layer가 pathformer는 전부이니 살펴보자</li>
</ul>

<h2 id="6-layersamspy">6. Layers/AMS.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">self.seasonality_and_trend_decompose</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.noisy_top_k_gating</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.cv_squared</code></li>
  <li><code class="language-plaintext highlighter-rouge">SparseDispatcher</code>와 <code class="language-plaintext highlighter-rouge">SparseDispatcher.dispatch</code>, <code class="language-plaintext highlighter-rouge">SparseDispatcher.combine</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.experts</code></li>
  <li>각각에 대해서 하나씩 살펴보도록 한다.</li>
</ul>

<h3 id="61-selfseasonality_and_trend_decompose">6.1. self.seasonality_and_trend_decompose</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig19.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig20.png" alt="사진1" /></p>

<ul>
  <li>AMS class 안에서 정의된 함수</li>
  <li><strong>seasonality와 trend를 x에서 각각 계산</strong>하기 때문에 \(seasonal + trend = x\)가 아님
    <ul>
      <li>해당 함수의 결과는 x에 seasonality와 trend를 더한 결과이다.</li>
    </ul>
  </li>
  <li>처음에 <code class="language-plaintext highlighter-rouge">x = x[:, :, :, 0]</code>은 <code class="language-plaintext highlighter-rouge">d_model</code> 차원으로 표현된 x에서 첫 번째 dimension만 사용해서 decompose한다는 의미</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig21.png" alt="사진1" /></p>

<ul>
  <li>seasonality_model은 <code class="language-plaintext highlighter-rouge">FourierLayer</code>
    <ul>
      <li>푸리에 변환(fft) 후 amplitude가 높은 frequency \(k\)​​개를 inverse 푸리에 변환(extrapolate)</li>
    </ul>
  </li>
  <li>trend_model은 <code class="language-plaintext highlighter-rouge">series_decomp_multi</code>
    <ul>
      <li>다양한 크기의 kernel size로 moving average를 softmax</li>
    </ul>
  </li>
</ul>

<h3 id="62-selfnoisy_top_k_gating">6.2. self.noisy_top_k_gating</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig22.png" alt="사진1" />
<img src="/assets/img/pytorch/pathformer_code/fig23.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">start_linear.squeeze</code>와 <code class="language-plaintext highlighter-rouge">w_gate</code>로 <code class="language-plaintext highlighter-rouge">torch.Size([batch, seq_len, num_node])</code>가 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>가 된다.</li>
  <li>같은 크기 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>의 <code class="language-plaintext highlighter-rouge">logit</code>을 만들고 \(top-k\) logit을 <code class="language-plaintext highlighter-rouge">gates</code>에 넣는다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">scatter</code>는 특정 인덱스 위치에 값을 할당하는 함수이다.</li>
      <li><code class="language-plaintext highlighter-rouge">gate</code>의 shape은 <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_experts])</code>가 되는데, 각 행(batch)에서 k개를 제외하고는 다 0이다.</li>
      <li>그리고 각 행(batch)마다 그 k개가 어떤 experts인지는 다르다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">load</code>는 각 expert가 배치 전체에서 얼마나 선택되었는지에 대한 비율을 의미한다.
    <ul>
      <li>shape은 <code class="language-plaintext highlighter-rouge">torch.Size([num_experts])</code>이다.</li>
    </ul>
  </li>
</ul>

<h3 id="63-selfcv_squared">6.3. self.cv_squared</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>다시 AMS.forward로 돌아오자</li>
  <li>각 expert마다 모든 배치에 대해 sum을 해서 <code class="language-plaintext highlighter-rouge">importance</code>를 계산하면 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자가 된다.</li>
  <li><code class="language-plaintext highlighter-rouge">cv_squared</code>를 통해 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자의 변동계수를 구해서 <code class="language-plaintext highlighter-rouge">balance_loss</code>를 구한다.
    <ul>
      <li>변동계수는 \(\frac{\sigma^2}{\mu^2}\)이다.</li>
      <li>이 값이 크면 특정 experts에 importance가 몰려있음을 의미한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig24.png" alt="사진1" /></p>

<h3 id="64-sparsedispatcher-어려움-주의">6.4. SparseDispatcher (*어려움 주의)</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig25.png" alt="사진1" />
<img src="/assets/img/pytorch/pathformer_code/fig26.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>에서 준비해놓는 것들이 많으니 하나하나 보도록 한다. \(k=2\), <code class="language-plaintext highlighter-rouge">num_experts</code>=4인 경우이다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig27.png" alt="사진1" /></p>
<ul>
  <li>각 행은 batch를 의미하기 때문에 행의 개수는 batch size (여기선 512)</li>
  <li>각 행에는 <code class="language-plaintext highlighter-rouge">num_experts</code>개의 숫자가 있고 그 중 \(k\)개만 non-negative, 나머지는 0</li>
  <li>첫 행에서 2, 3번째 숫자가 양수라는 것은, 첫번째 배치에서 2, 3번째 experts가 선택되었다는 것을 의미</li>
  <li>바로 아래에 있는 <code class="language-plaintext highlighter-rouge">torch.nonzero(gates)</code>에서도 그 사실을 알 수 있다.
    <ul>
      <li>첫 번째 배치에서는 index 1, 2인 experts가, 마지막 배치에서는 index 1, 2인 experts가 선택됨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig28.png" alt="사진1" /></p>
<ul>
  <li>이제 sort를 하는데 첫번째 열은 어차피 index라서 정렬되어있고
    <ul>
      <li>(두 번째 열이 정렬되면서 섞이기 때문에 두 번째 열의 숫자가 첫 번째 열의 배치 index와 상관 없게 된다)</li>
    </ul>
  </li>
  <li>그리고 <code class="language-plaintext highlighter-rouge">index_sorted_experts</code>는 정렬된 숫자가 몇 번째 index에 있던 숫자인지를 표시해준다.
    <ul>
      <li>(여기서부터 헷갈리기 시작함)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig29.png" alt="사진1" /></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">self._expert_index</code>는 각 배치에서 선택된 experts의 index를 정렬한 것이다.
    <ul>
      <li>각 배치마다 \(k\)개씩 있으니 총 batch_size \(\times k\)개의 숫자겠다.</li>
    </ul>
  </li>
  <li>그리고 그걸 다시 batch index로 되돌릴 수가 있을 것이다.
    <ul>
      <li>즉 <code class="language-plaintext highlighter-rouge">self._batch_index</code>가 1, 3, 6,…이라는 것은 expert 0이 선택되었던 batch가 1, 3, …이고</li>
      <li>그 다음 expert 1이 선택된 batch들이 몇 번째 batch인지 쭉 나열이 된다. (이걸 마지막 expert까지 반복)</li>
    </ul>
  </li>
  <li>마지막으로 <code class="language-plaintext highlighter-rouge">self._part_sizes</code>는 모든 batches 통틀어서 각 expert가 몇 번 선택되었는지를 의미한다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig30.png" alt="사진1" /></p>
<ul>
  <li>이제 <code class="language-plaintext highlighter-rouge">gates_exp</code>는 expert 0이 선택되었던 batches를 쭉 나열하고, 그 다음에 expert 1이 선택되었던 batches를 쭉 나열하고… 마지막 expert가 선택되었던 batches까지 나열한 것이다.</li>
  <li>그리고 <code class="language-plaintext highlighter-rouge">self._nonzero_gates</code>는 expert \(i\) (\(i = 1, ...,\) <code class="language-plaintext highlighter-rouge">num_experts</code>)가 선택된 배치에서 expert \(i\)의 gates를 나열한 것이다.</li>
</ul>

<h3 id="641-sparsedispatcherdispatch">6.4.1. SparseDispatcher.dispatch</h3>

<ul>
  <li>이제 dispatch에서는 각 expert에 처리해야 할 batches를 할당한다.</li>
  <li>만약 지금처럼 inp의 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([512, 96, 7, 16])</code>, <code class="language-plaintext highlighter-rouge">self._batch_index</code>의 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([1024])</code>, 그리고 <code class="language-plaintext highlighter-rouge">self._part_sizes</code>가 <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>라고 가정하면:</li>
  <li><code class="language-plaintext highlighter-rouge">inp[self._batch_index]</code>에서는 inp 텐서에서 1024개의 샘플을 선택하여, 크기가 <code class="language-plaintext highlighter-rouge">torch.Size([1024, 96, 7, 16])</code>인 새로운 텐서를 생성한다.</li>
  <li>그리고 첫 번째 차원(batch 차원, 1024개)을 <code class="language-plaintext highlighter-rouge">self._part_sizes</code> = <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>로 나눈다.</li>
  <li>결과는 각 expert에게 할당된 batches의 리스트이며, 각 텐서의 크기는:
    <ul>
      <li>첫 번째 expert: [262, 96, 7, 16]</li>
      <li>두 번째 expert: [348, 96, 7, 16]</li>
      <li>세 번째 expert: [249, 96, 7, 16]</li>
      <li>네 번째 expert: [165, 96, 7, 16]</li>
    </ul>
  </li>
  <li>이걸 리스트로 return한다.</li>
</ul>

<h3 id="642-sparsedispatchercombine">6.4.2. SparseDispatcher.combine</h3>

<ul>
  <li>이제 각각을 해당 expert에 통과시킨다.</li>
  <li>expert는 <code class="language-plaintext highlighter-rouge">TransformerLayer</code>이다. (Pathformer.py의 __init__참고)</li>
  <li>그리고 그 결과를 다시 combine한다.
    <ul>
      <li>그런데 위에서 combine 함수를 잘 보면 처음에 <code class="language-plaintext highlighter-rouge">.exp()</code>를 하고 다시 <code class="language-plaintext highlighter-rouge">.log()</code>를 해주는데,</li>
      <li><code class="language-plaintext highlighter-rouge">.exp()</code>에서 NaN이 나올 수가 있으니 주의하자.</li>
      <li>(벤치마크 데이터셋에서는 해당사항 없지만 내 프로젝트에서 사용하는 데이터에서는 발생했다.)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>이제 residual_connection만 적용해주면 끝난다.</li>
  <li>여기까지가 하나의 <code class="language-plaintext highlighter-rouge">AMS</code> layer이다.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="사진1" /></p>

<ul>
  <li>여기서 for 안에 있는 layer가 AMS layer이다.</li>
  <li>
    <p>마지막으로 de-normalization을 하면 끝이다.</p>
  </li>
  <li>나머지는 위에서 이미 소개한 <code class="language-plaintext highlighter-rouge">3. run.py</code>와 <code class="language-plaintext highlighter-rouge">4. exp_main.py</code>가 전부이다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Pathformer github](https://github.com/decisionintelligence/pathformer)]]></summary></entry><entry><title type="html">Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIR’24 Best Paper)</title><link href="http://localhost:4000/timeseries/2024-09-03-SyNCRec/" rel="alternate" type="text/html" title="Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIR’24 Best Paper)" /><published>2024-09-03T00:00:00+09:00</published><updated>2024-09-03T15:04:04+09:00</updated><id>http://localhost:4000/timeseries/SyNCRec</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-09-03-SyNCRec/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Cross-Domain Sequential Recommendation (CDSR)은 multiple domain에서의 정보를 활용하여 Single-Domain Sequential Recommendation (SDSR)보다 좋은 성능을 보여주었음</li>
  <li>하지만 <strong>negative transfer</strong> : lack of relation btw domains은 성능 저하의 원인</li>
  <li>그래서 본 논문에서는
    <ol>
      <li>estimates the degree of <strong>negative transfer</strong> of each domain</li>
      <li>adaptively assigns it as a <strong>weight factor</strong> to the prediction loss
        <ul>
          <li>to control gradient flows through domains with significant negative transfer !</li>
        </ul>
      </li>
      <li>developed <strong>auxiliary loss</strong> that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis</li>
    </ol>
  </li>
  <li>이러한 CDSR과 SDSR의 cooperative learning은 collaborative dynamics between pacers and runners in a marathon와 유사함</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Single-Domain Sequential Recommendation (SDSR)
    <ul>
      <li>focuses on <strong>recommending the next item</strong> within a <strong>specific</strong> domain using <strong>only</strong> the <strong>single</strong>-domain sequence</li>
    </ul>
  </li>
  <li>Cross-Domain Sequential Recommendation (CDSR)
    <ul>
      <li><strong>predicts</strong> the <strong>next item</strong> a user will interact with, by leveraging their historical <strong>interaction</strong> sequences across <strong>multiple</strong> domains</li>
    </ul>
  </li>
  <li>둘의 차이는 결국 다른 domains의 정보를 활용하는지 여부</li>
  <li>CDSR은 성능 향상을 위해 다른 domains의 정보를 활용하지만 항상 성능이 향상되는 건 아님
    <ul>
      <li>만약 그것 때문에 성능이 더 안좋아진다면, 그건 <strong>negative transfer</strong>가 있었기 때문</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p>본 논문에서는 SyNCRec: Asymmetric Cooperative Network for Cross-Domain Sequential Recommendation을 제안</p>
  </li>
  <li>
    <ol>
      <li>assess the degree of <strong>negative transfer</strong> of each domain
        <ul>
          <li>by comparing the performance of CDSR and SDSR</li>
        </ul>
      </li>
      <li>adaptively assign this value as <strong>weight to the prediction loss</strong> corresponding to a specific domain
        <ul>
          <li>to reduces its flow in domains with significant negative transfer !</li>
        </ul>
      </li>
      <li>developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis
        <ul>
          <li>to exploit the effective correlation signals inherent in the representation pairs of SDSR and CDSR tasks within a specific domain</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>SDSR은 negative transfer를 줄이기 위한 pacer의 역할을 함
    <ul>
      <li>(마라톤에서 runner가 너무 빠르거나 느리게 하지 않게 해주는 pacer)</li>
    </ul>
  </li>
  <li>특히 CDSR이 SDSR보다 성능이 안좋았던 (=negative transfer가 발생한) 도메인에서 성능 향상됨</li>
  <li>이러한 방법으로 여러 개의 domain-specific models를 만들 필요가 없을 것을 기대함</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="21-single-domain-sequential-recommendation">2.1. Single-Domain Sequential Recommendation</h3>

<ul>
  <li>SDSR : temporal dynamics in user-item interactions를 디자인
    <ul>
      <li>GRU-based models : GRU4Rec, STAMP, NARM</li>
      <li>Attention-mechanism : SASRec, BERT4Rec, SINE, LightSANs</li>
      <li>Others : NextItNet(CNN), TransRec(Markov chain), …</li>
    </ul>
  </li>
</ul>

<h3 id="22-cross-domain-sequential-recommendation">2.2 Cross-Domain Sequential Recommendation</h3>

<ul>
  <li>CDSR : information from various other domains를 leverage
    <ul>
      <li>Matrix factorization : CMF, CLFM, …</li>
      <li>Multi-task learning : DTCDR, DeepAPF, BiTGCF, CAT-ART</li>
      <li>\(\pi-Net\) :  introduced gating mechanisms designed to transfer information from a single domain to another paired domain</li>
      <li>\(C^2DSR\) : employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations</li>
      <li>\(MIFN\) :  introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains</li>
      <li>\(MAN\) : designed group-prototype attention mechanisms to capture domainspecific and cross-domain relationships</li>
    </ul>
  </li>
  <li>However… 결국에는 모두 domain pair 끼리의 관계를 모델링
    <ul>
      <li>3개 이상의 domains의 관계를 파악할 때, domains이 엄청 많을 때에는 어려움</li>
      <li>그래서 CGRec에서 CDSR을 제안하면서 negative transfer 개념을 제안
        <ul>
          <li>high negative transfer를 가지는 domain에 panalty를 주는 방식</li>
          <li>하지만 여전히 SDSR보다 성능이 안좋은 domain이 꽤 있음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>그러므로 본 논문에서의 목표는 3개 이상의 <strong>모든</strong> 도메인에서 negative transfer를 <strong>효율적</strong>으로 줄이는 것</li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<ul>
  <li>Domains : \(\mathcal{D}=\{A, B, C, \ldots\}\) where \(\mid \mathcal{D}\mid  \geq 3\)
    <ul>
      <li>\(d \in \mathcal D\) 는 하나의 특정 도메인을 의미,</li>
      <li>\(V^d\)는 set of items specific to the domain \(d\), \(V\)는 total item set across all domains</li>
    </ul>
  </li>
</ul>

<h3 id="definition-1-single--and-cross-domain-sequential-recommendation">Definition 1. Single- and Cross-Domain Sequential Recommendation</h3>

<ul>
  <li>The single-domain sequences of domain \(d\) : \(X^d=\left[(\mathrm{SOS}), x_1^d, x_2^d, \ldots, x_{\mid X^d\mid -1}^d\right]\)​</li>
  <li>\(x_t^d\) :  interaction occurring at time \(t\)</li>
  <li>그러므로 cross-domain sequence는 \(X=\left(X^A, X^B, X^C, \ldots\right)\)로 표현할 수 있음</li>
  <li>예를 들어, \(X=\left[(\mathrm{SOS}), x_1^A, x_2^B, x_3^A, x_4^B, x_5^A, x_6^C, x_7^C\right]\)은 \(X^A=\left[(\mathrm{SOS}), x_1^A, x_3^A, x_5^A\right], X^B=\left[(\mathrm{SOS}), x_2^B, x_4^B\right], \text { and } X^C=[(\mathrm{SOS})\left., x_6^C, x_7^C\right]\)으로 split 가능</li>
  <li>SDSR은 하나의 domain 안에서 recommending, CDSR은 전체 도메인에서 recommending</li>
</ul>

<h3 id="definition-2-negative-transfer-gap-ntg">Definition 2. Negative Transfer Gap (NTG)</h3>

<ul>
  <li>\(\mathcal{L}_\pi^d\)는 domain \(d\)에서의 model \(\pi\)의 loss를 의미 (SDSR 또는 CDSR)</li>
  <li>그러므로 Negative transfer는 \(\phi_\pi(d) = \mathcal{L}_\pi^d\left(X^d\right)-\mathcal{L}_\pi^d(X)\)</li>
</ul>

<h3 id="problem-statement">Problem Statement</h3>

<ul>
  <li>historical cross-domain sequences \(X_{1:t}\)가 주어졌을 때, 목표는 다음 item \(x_{t+1}^d = \underset{x_{t+1}^d \in V^d}{\operatorname{argmax}} P\left(x_{t+1}^d \mid X_{1: t}\right)\)를 예측하는 것</li>
  <li>만약 \(\mid \mathcal{D}\mid\)개의 single-domain sequences (for SDSR)과 1개의 sequence (for CDSR)가 있다면
    <ul>
      <li>multi-tasking learning manner의 모델 하나는 \(\mid \mathcal{D}\mid +1\)개의 next item prediction tasks를 수행하는 것이다.</li>
    </ul>
  </li>
</ul>

<h2 id="4-model">4. Model</h2>

<p><img src="/assets/img/timeseries/SyNCRec/fig2.png" alt="그림1" /></p>

<h3 id="41-shared-embedding-layer">4.1. Shared Embedding Layer</h3>

<ul>
  <li>여기서는 <strong>initialized representations</strong> of items를 얻는다.
    <ul>
      <li>for \(\mid \mathcal{D}\mid\) single-domain sequences \(X^d\), and one cross-domain sequence \(X\)</li>
    </ul>
  </li>
  <li>Item embedding matrix \(M^d \in \mathbb R^{\mid V^d\mid \times r}\)​이고
    <ul>
      <li>\(\mid V^d\mid\)는 domain d의 items 개수, r은 embedding dimension</li>
    </ul>
  </li>
  <li>모든 domains에 대해 concat하면 \(M \in \mathbb R^{\mid V\mid \times r}\)
    <ul>
      <li>\(\mid V\mid\)는 모든 도메인에서 items 개수</li>
    </ul>
  </li>
  <li>여기서 최근 T개만을 사용 (T개보다 적다면 앞쪽에 padding으로 맞춰줌)
    <ul>
      <li>그러면 \(\mathbf{E}^d \in \mathbb{R}^{T \times r} \text { and } \mathbf{E} \in \mathbb{R}^{T \times r}\)를 얻음 (각각 Fig2(c-1), (c-2))</li>
      <li>\(\mid \mathcal D\mid\)개의 \(\mathbf{E}^d\)를 aggregation한 것이 \(\mathbf{E}^{\text {single }}\) (Fig2(c-1))</li>
      <li>참고로 \(\mathbf E, \mathbf E^d\)에는 learnable positional embedding 더해져있음</li>
      <li>\(t\)-th step에서의 \(\mathbf E, \mathbf E^d\)는 각각 \(\mathbf e, \mathbf e^d\)로 정의</li>
    </ul>
  </li>
</ul>

<h3 id="42-asymmetric-cooperative-network-with-mixture-of-sequential-experts-acmoe">4.2. Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMoE)</h3>

<ul>
  <li>Negative Transfer (NTG)는 <strong>loss of the SDSR</strong>과 <strong>the loss of CDSR</strong>의 차이로 정의
    <ul>
      <li>NTG가 작으면 다른 domains의 정보가 도움이 안되는 거고 크면 도움이 되는 것</li>
    </ul>
  </li>
  <li>그러므로 weight for the prediction loss in the domain로 사용할 수 있다
    <ul>
      <li>gradient flow를 작게 만들기 위해서다</li>
    </ul>
  </li>
  <li>Multi-gate Mixture of Sequential Experts (MoE) architecture를 사용하여 SDSR과 CDSR를 수행하고
    <ul>
      <li><strong>models</strong> relationships between different tasks and <strong>learns</strong> task-specific functionalities</li>
      <li>enabling it to effectively leverage shared representations</li>
    </ul>
  </li>
  <li>SDSR과 CDSR은 서로 간섭하지 않고, experts로는 Transformer를 사용</li>
</ul>

<h3 id="421-architecture">4.2.1. Architecture</h3>

<ul>
  <li>
    <p><strong>먼저 SDSR을 보자</strong></p>
  </li>
  <li>
    <p>shared embedding layer로부터 initialized representations of single- and cross-domain sequences,</p>

    <ul>
      <li>즉 \(\mathbf E, \mathbf E^d\)가 주어져있을 때, 각 expert는 many-to-many sequence learning을 수행</li>
    </ul>
  </li>
  <li>
    <p>domain \(d\)의 output : \(\begin{aligned}
&amp; \left(\mathbf{Y}^d\right)^{\text {single }}=h^d\left(f^d\left(\mathbf{E}^d\right)\right) \\
&amp; f^d\left(\mathbf{E}^d\right)=\sum_{k=1}^j g^d\left(\mathbf{E}^d\right)_k \mathrm{SG}\left(f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)\right)+\sum_{k=j+1}^K g^d\left(\mathbf{E}^d\right)_k f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)
\end{aligned}\)</p>

    <ul>
      <li>\(h^d\) : the tower network for domain \(d\)​ (Fig. 2(c-7))
        <ul>
          <li>feed-forward network with layer normalization</li>
        </ul>
      </li>
      <li>\(f^d\) : the multi-gated mixture of the sequential experts layer</li>
      <li>\(SG\)​ :  the stopgradient operation (Fig. 2(c-4))
        <ul>
          <li>forward pass에서는 identity function</li>
          <li>backward pass에서는 SG 안에 있는 것들의 gradient는 drop</li>
          <li>위 식에서는 \(j+1 \sim K\)번째 experts만 unique sequential pattern of single-domain sequences를 학습</li>
        </ul>
      </li>
      <li>\(f_{\text {TRM }}^k\)​ :  the 𝑘-th transformerbased sequential expert (Fig. 2(c-3))</li>
      <li>\(g^d\) :  gating network for domain \(d\) (Fig. 2(c-6))
        <ul>
          <li>\(g^d\left(\mathbf{E}^d\right)=\operatorname{softmax}\left(W_g^d \mathbf{E}^d\right)\) where \(W_g^d \in \mathbb{R}^{K \times d T}\) is trainable FC</li>
        </ul>
      </li>
      <li>The \(t\)-th element of \(\mathrm{Y}^{\text {single }}\)는 \(\left(y_t^d\right)^{\text {single }}\)</li>
    </ul>
  </li>
  <li>
    <p><strong>다음으로 CDSR을 보자</strong></p>
  </li>
  <li>
    <p>ACMoE module : \(\begin{aligned}
&amp; \mathbf{Y}^{\text {cross }}=h^{\text {cross }}\left(f^{\text {cross }}(\mathbf{E})\right) \\
&amp; f^{\text {cross }}(\mathbf{E})=\sum_{k=1}^j g^{\text {cross }}(\mathbf{E})_k f_{\mathrm{TRM}}^k(\mathbf{E})+\sum_{k=j+1}^K g^{\text {cross }}(\mathbf{E})_k \operatorname{SG}\left(f_{\mathrm{TRM}}^k(\mathbf{E})\right)
\end{aligned}\)</p>

    <ul>
      <li>\(h^{cross}\) :  the tower network (Fig. 2(c-9))</li>
      <li>\(f^{\text {cross }}\) : the multi-gated mixture of sequential experts layer for a cross-domain sequence</li>
      <li>\(SG\)는 \(j+1\sim K\)-th \(f^k_{TRM}\)에만 사용
        <ul>
          <li>그러면 \(1\sim j\)​번째 experts가cross-domain sequences에서 the distinct sequential patterns present를 학습</li>
        </ul>
      </li>
      <li>\(g^{\text {cross }}(\mathbf{E})=\operatorname{softmax}\left(W_a^{c r o s s} \mathbf{E}\right)\)​ : gating network for the crossdomain sequence (Fig. 2(c-8))</li>
    </ul>
  </li>
  <li>
    <p>\(\left(y_t^d\right)^{\text {single }} \text { and }\left(y_t\right)^{\text {cross }}\)는 two representations of different views for the same item</p>
  </li>
</ul>

<h3 id="422-transformer-experts">4.2.2. Transformer Experts</h3>

<ul>
  <li>각각의 Multi-head Self-Attention에 \(Z \in \mathbb{R}^{T \times r}\) 가 linear transformation
    <ul>
      <li>\(\to\) \(\text { queries } Q_i \in \mathbb{R}^{T \times r / p} \text {, keys } K_i \in \mathbb{R}^{T \times r / p} \text {, } \text { values } V_i \in \mathbb{R}^{T \times r / p}\)가 됨</li>
    </ul>
  </li>
  <li>
    <p>\(\begin{aligned}
&amp; \operatorname{Attn}\left(Q_i, K_i, V_i\right)=\operatorname{softmax}\left(\frac{Q_i K_i^{\top}}{\sqrt{r / p}}\right) V_i, Q_i=Z \mathrm{~W}_i^Q, K_i=Z \mathrm{~W}_i^K, V_i=Z \mathrm{~W}_i^V
\end{aligned}\) 거쳐 final output은 \(\mathbf{H} \in \mathbb{R}^{T \times r}\)</p>
  </li>
  <li>마지막으로 \(\operatorname{FFN}(\mathbf{H})=\left[\mathrm{FC}\left(\mathbf{H}_1\right)\left\\mid \mathrm{FC}\left(\mathbf{H}_2\right)\right\\mid , \ldots, \\mid  \mathrm{FC}\left(\mathbf{H}_T\right)\right]\)
    <ul>
      <li>where \(\mathrm{FC}\left(\mathbf{H}_t\right)=\operatorname{GELU}\left(\mathbf{H}_t \mathrm{~W}_1+b_1\right) \mathrm{W}_2+b_2\)</li>
      <li>\(\mathbf{H}_t\) : 𝑡-th representation of \(\mathbf{H}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-loss-correction-with-negative-transfer-gap-lc-ntg">4.3. Loss Correction with Negative Transfer Gap (LC-NTG)</h3>

<h3 id="431--single-domain-item-prediction">4.3.1.  Single-Domain Item Prediction</h3>

<ul>
  <li>Fig 2(e-1)</li>
  <li>single domoin sequence \(X_{1: t}^d\)가 주어졌을 때 다음 아이템 \(x_{t+1}^d\)를 예측하는 것은 pairwise ranking loss를 사용
    <ul>
      <li>즉 \(l_t^d=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}^d\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}^d\right)\right), \mathcal{L}_{\text {single }}^d=\sum_{t=1}^T l_t^d\)
        <ul>
          <li>where \(x^{d+}\) : ground-truth item paired with a negative item \(x^{d-}\) sampled froem Unif</li>
          <li>\(P\left(x_{t+1}^d=x^d \mid X_{1: t}^d\right)\) = \(\sigma\left(\left(y_t^d\right)^{\text {single }} \cdot M\left(x^d\right)\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="432-cross-domain-item-prediction">4.3.2. Cross-Domain Item Prediction</h3>

<ul>
  <li>CDSR \(l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t\)
    <ul>
      <li>where \(P\left(x_{t+1}^d=x^d \mid X_{1: t}\right) \text { is obtained by } \sigma\left(\left(y_t\right)^{\text {cross }} \cdot M\left(x^d\right)\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="433--calculating-the-negative-transfer-gap">4.3.3.  Calculating the Negative Transfer Gap</h3>

<ul>
  <li>이제 NTG를 구할 수 있다. \(\phi_\pi(d)=\sum_{t=1}^T\left(l_t^d-l_t\right)\)
    <ul>
      <li>where \(l_t^d\) and \(l_t\) are losses of the SDSR and CDSR tasks in time step \(t\) for the domain \(d\), respectively, calculated with our model \(\pi\)</li>
    </ul>
  </li>
  <li>\(\lambda=\left(\lambda_1, \lambda_2, \ldots, \lambda_{\mid \mathcal{D}\mid }\right)\)를 각 domain에서의 NTG라고 하면 \(\lambda_d \leftarrow \operatorname{softmax}\left(\alpha * \lambda_d+\beta * \phi_\pi(d) ; \delta\right)\)로 계산
    <ul>
      <li>where \(\alpha \text { and } \beta\) are learnable parameters</li>
    </ul>
  </li>
</ul>

<h3 id="434-loss-correction">4.3.4. Loss Correction</h3>

<ul>
  <li>NTG는 weight for the cross-domain item prediction loss로 활용됨
    <ul>
      <li>loss는 \(l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t\)</li>
    </ul>
  </li>
  <li>re-aggregate : multiplying the relative NTG for each domain separately
    <ul>
      <li>
\[\mathcal{L}_{\text {cross }}^{l c} = =\sum_{t=1}^T \sum_{d=1}^{\mid \mathcal{D}\mid } \lambda_d \log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right)\]
      </li>
    </ul>
  </li>
  <li>이렇게 하면 NTG가 발생하는 domain에서의 gradient flow를 줄이는 것</li>
</ul>

<h2 id="44-single-cross-mutual-information-maximization-sc-mim">4.4. Single-Cross Mutual Information Maximization (SC-MIM)</h2>

<ul>
  <li>SC-MIM: SDSR and CDSR tasks 사이의 정보를 잘 transfer하기 위한 방법
    <ul>
      <li>mutual information으로 두 tasks의 correlation signals를 파악</li>
      <li>mutual information: \(I(X, Y)=D_{K L}(p(X, Y) \\mid  p(X) p(Y))=\mathbb{E}_{p(X, Y)}\left[\log \frac{p(X, Y)}{p(X) p(Y)}\right]\)​</li>
    </ul>
  </li>
  <li>하지만 이 mutual information을 high-dimd에서 구하는 건 어렵기 때문에 lower bound로 InfoNCE를 사용
    <ul>
      <li>lower bound : \(I(X, Y) \geq \mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\mathbb{E}_{q(\hat{Y})}\left(\log \sum_{\hat{y} \in \hat{Y}} \exp \rho_\theta(x, \hat{y})\right)\right]+\log \mid \hat{Y}\mid\)
        <ul>
          <li>where \(x, y\)는 같은 input의 서로 다른 view points</li>
          <li>\(\rho_\theta\) 는 similarity function,</li>
        </ul>
      </li>
      <li>InfoNCE를 maximizing하는 것은 standard cross-entropy loss를 maximizing하는 것과 같음
        <ul>
          <li>: \($\mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\log \sum_{\hat{y} \in Y} \exp \rho_\theta(x, \hat{y})\right]\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>아무튼 돌아와서 우리는 \($\mathbf{Y}^{\text {single }}$ and $\mathbf{Y}^{\text {cross }}\)의 mutual information을 maximizing하고 싶음
    <ul>
      <li>그러므로 cross-domain representation \(\mathbf{Y}^{\text {ross }}\)를 domain별로 split해서 \((\mathbf{Y^d})^{\text {ross }}\) 구하고</li>
      <li>아래 식처럼 계산
        <ul>
          <li>: \(\begin{aligned} &amp; \mathcal{L}_{S C-M I M}^d=\rho\left(\left(\mathbf{Y}^d\right)^{\text {single }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)-\log \sum_{u-} \exp \left(\rho\left(\left(\mathbf{Y}^d\right)^{\text {single- }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)\right)\end{aligned}\)</li>
          <li>where \(u-\)는 other users in a training batch,</li>
          <li>\(\left(\mathbf{Y}^d\right)^{\text {single- }}\)는 subsequence of domain \(𝑑\) of user \(𝑢−\)​</li>
          <li>\(\rho(\cdot, \cdot)\)는 \(\rho(U, V)=\sigma\left(U^{\top} \cdot W^H \cdot V\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="45-model-training-and-evaluation">4.5. Model Training and Evaluation</h3>

<ul>
  <li>Total training loss : \(\mathcal{L}=\eta\left(\sum_{d=1}^{\mid \mathcal{D}\mid }\left(\mathcal{L}_{\text {single }}^d\right)+\mathcal{L}_{\text {cross }}^{l c}\right)+(1-\eta) \sum_{d=1}^{\mid \mathcal{D}\mid } \mathcal{L}_{S C-M I M}^d\)
    <ul>
      <li>where \(\eta\) is the harmonic factor</li>
      <li>evaluation할 때에는 cross-domain representation만 사용</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-dataset">5.1. Dataset</h3>

<h3 id="52-experimental-setting">5.2. Experimental Setting</h3>

<ul>
  <li>먼저 Amazon dataset과 Telco dataset에 대한 성능</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table23.png" alt="그림1" /></p>

<ul>
  <li>
    <p>Research Questions:</p>

    <ul>
      <li>
        <p>(RQ1): Does the performance of our model surpass the current stateof-the-art baselines in practical applications that involve more than three domains?</p>
      </li>
      <li>
        <p>(RQ2): Can our model effectively address the challenge of negative transfer across all domains in the CDSR task?</p>
      </li>
      <li>
        <p>(RQ3): What is the impact of various components of our model on its performance in CDSR tasks?</p>
      </li>
      <li>
        <p>(RQ4): How do variations in hyper-parameter settings influence the performance of our model?</p>
      </li>
      <li>
        <p>(RQ5): How does the model perform when deployed online ?</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="53-performance-evaluation-rq1">5.3. Performance Evaluation (RQ1)</h3>

<ul>
  <li>First, The effectiveness of our model can be observed.
    <ul>
      <li>다른 baseline models보다 성능이 뛰어남</li>
    </ul>
  </li>
  <li>Second, Integrating information from all domains simultaneously in a model can improve performance in each domain compared to modeling a pairwise domain-domain relationship.
    <ul>
      <li>본 논문에서 제시하는 방법을 사용할 경우에는 CDSR task에서 domain끼리의 정보를 결합해서 사용하는 것이 더 효율적이다.</li>
    </ul>
  </li>
</ul>

<h3 id="54-discussion-of-the-negative-transfer-rq2">5.4. Discussion of the negative transfer (RQ2)</h3>

<ul>
  <li>기존 baseline models는 SDSR보다 CDSR의 성능이 더 안좋았지만 본 논문에서 제시하는 모델은 그렇지 않다</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table4.png" alt="그림1" /></p>

<h3 id="55-discussion-of-model-variants-rq3">5.5 Discussion of Model Variants (RQ3)</h3>

<ul>
  <li>LC-NTG, SC-MIM, ACMoE 세 가지 components 모두 성능 향상을 위해 필요하다</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table5.png" alt="그림1" /></p>

<h3 id="56-hyperparameter-analysis-rq4">5.6. Hyperparameter Analysis (RQ4)</h3>

<p><img src="/assets/img/timeseries/SyNCRec/fig3.png" alt="그림1" /></p>

<h2 id="6-online-ab-test-rq5">6. Online A/B Test (RQ5)</h2>

<p>pass</p>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>Negative transfer를 다루는 CDSR framework를 제안
    <ul>
      <li>Negative transfer를 측정하고 prediction loss의 weight로 활용</li>
    </ul>
  </li>
  <li>SDSR and CDSR tasks의 정보를 교환시키는 Auxiliary loss 제안</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[SIGIR'24 Best Paper](https://arxiv.org/pdf/2407.11245)]]></summary></entry><entry><title type="html">MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-05-MG-TSD/" rel="alternate" type="text/html" title="MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)" /><published>2024-08-05T00:00:00+09:00</published><updated>2024-09-03T15:04:04+09:00</updated><id>http://localhost:4000/timeseries/MG-TSD</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-05-MG-TSD/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>어떻게 Diffusion model의 성능을 time series forecasting에 활용할 수 있는가</li>
  <li><strong>M</strong>ulti-<strong>G</strong>ranularity <strong>T</strong>ime <strong>S</strong>eries <strong>D</strong>iffusion <strong>(MG- TSD)</strong>
    <ul>
      <li>leveraging the inherent granularity levels</li>
      <li>intuition: diffusion step에서 점차 gaussian noise로 만드는 것을 fine \(\to\) coarse로 이해</li>
      <li>novel multi-granularity guidance diffusion loss function</li>
      <li>method to effectively utilize coarse-grained data across various granularity levels</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>최근에는 Time series predictive 목적으로 conditional generative model을 활용
    <ul>
      <li>처음에는 Auto-regressive 방식으로 하다가 CSDI도 했었음</li>
    </ul>
  </li>
  <li>하지만 문제는 Diffusion이 instability하다는 점
    <ul>
      <li>Image에서 diffusion은 다양한 이미지를 만들 수 있어서 장점이었는데</li>
      <li>시계열 예측 관점에서는 그것이 성능 하락의 원인이 될 수 있음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/MG-TSD/fig1.png" alt="그림1" /></p>

<ul>
  <li>Diffusion step에서 점차 gaussian noise로 만드는 것을 fine \(\to\) coarse로 이해한다면
    <ul>
      <li>Diffusion model이 <strong>labels을 the source of guidance</strong>로 필요로 하는 문제에서</li>
      <li>Time series의 fine feature가 그 labels as the source of guidance 역할을 할 수 있을 것</li>
    </ul>
  </li>
  <li>MG-TSD에서는 coarse-grained data를 denoising process 학습의 guide로 준다.
    <ul>
      <li>\(\to\) intermediate latent states에서의 constraints로 작용</li>
      <li>\(\to\) coarser feature는 더 빠르게 생성할 수 있기 때문에, 그만큼 finer feature recovery도 용이</li>
      <li>\(\to\) coarse-grained data의 trend와 pattern을 보존하는 sampling을 만듬</li>
      <li>\(\to\)​ reduces variability and results in high-quality predictions</li>
    </ul>
  </li>
</ul>

<h2 id="2-background">2. Background</h2>

<ul>
  <li>TimeGrad Model
    <ul>
      <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad Paper</a> <a href="https://lpppj.github.io/timeseries/2024-07-09-Timegrad">TimeGrad Review</a></li>
    </ul>
  </li>
  <li>\(\boldsymbol{X}^{(1)}=\left[\boldsymbol{x}_1^1, \ldots, \boldsymbol{x}_t^1, \ldots, \boldsymbol{x}_T^1\right]\) is the original observed data, where \(t \in[1, T]\) and \(\boldsymbol{x}_t \in \mathbb{R}^D\)
    <ul>
      <li>Mathematical expressions: \(q_{\mathcal{X}}\left(\boldsymbol{x}_{t_0: T}^1 \mid\left\{\boldsymbol{x}_{1: t_0-1}^1\right\}\right)=\prod_{t=t_0}^T q_{\mathcal{X}}\left(\boldsymbol{x}_t^1 \mid\left\{\boldsymbol{x}_{1: t-1}^1\right\}\right)\)</li>
    </ul>
  </li>
</ul>

<h2 id="3-method">3. Method</h2>

<h3 id="31-mg-tsd-model-architecture">3.1. MG-TSD Model Architecture</h3>

<p><img src="/assets/img/timeseries/MG-TSD/fig2.png" alt="그림2" /></p>

<h3 id="multi-granularity-data-generator">Multi-granularity Data Generator</h3>

<p>: for generating multi-granularity data from observations</p>

<ul>
  <li>historical sliding windows with different sizes를 통해 fine \(\to\) coase로 smoothing out</li>
  <li>즉 \(\boldsymbol{X}^{(g)}=f\left(\boldsymbol{X}^{(1)}, s^g\right)\) with pre-defined sliding window size \(s^g\)</li>
  <li>이 때 non-overlapping하게 window를 slicing하고, \(\boldsymbol{X}^{(g)}\)는 \(s^g\)번 복제해서 \([1, T]\)로 맞춤</li>
</ul>

<h3 id="temporal-process-module">Temporal Process Module</h3>

<p>: designed to capture the temporal dynamics of the multi-granularity time series data</p>

<ul>
  <li>각각의 granularity level \(g\)에서 GRU와 같은 방식으로 timestep \(t\)를 \(\mathbf{h}_t^g\)로 encoding</li>
</ul>

<h3 id="guided-diffusion-process-module">Guided Diffusion Process Module</h3>

<p>: designed to generate stable time series predictions at each timestep \(t\)</p>

<ul>
  <li>multi-granularity data를 활용하여 diffusion learning process의 guide로 제공</li>
</ul>

<h3 id="32-multi-granularity-guided-diffusion">3.2. Multi-Granularity Guided Diffusion</h3>

<p>: Guided Diffusion Process Module에 대한 details</p>

<h3 id="321-coarse-grained-guidance">3.2.1. Coarse-grained Guidance</h3>

<p>: the derivation of a heuristic guidance loss for the two- granularity case</p>

<ul>
  <li>consider two granularities at a fixed timestep \(t\)
    <ul>
      <li>: \(\text { finest-grained data } \boldsymbol{x}_t^{g_1}\left(g_1=1\right) \text { from } \boldsymbol{X}^{\left(g_1\right)}\) &amp; \(\text { coarse-grained data } \boldsymbol{x}_t^g \text { from } \boldsymbol{X}^{(g)}\)​</li>
    </ul>
  </li>
  <li>먼저 coarse-grained targets \(x^g\)를 intermediate diffusion step \(N_*^g \in[1, N-1]\)에 introduce
    <ul>
      <li>즉 objective function이 \(\log p_\theta\left(\boldsymbol{x}^g\right)\)</li>
    </ul>
  </li>
  <li>그러면 denoising process에서 recover된 coarser features는 실제 coarse-grained sample의 정보를 많이 가지고 있을테니
    <ul>
      <li>fine-grained feature를 recover하기도 쉬워질 것</li>
    </ul>
  </li>
  <li>\(\theta\)-parameterized: \(p_\theta\left(\boldsymbol{x}_{N_*^g}\right)=\int p_\theta\left(\boldsymbol{x}_{N_*^g: N}\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}=\int p\left(\boldsymbol{x}_N\right) \prod_{N_*^g+1}^N p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}\)​
    <ul>
      <li>where \(\boldsymbol{x}_N \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I}), p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right)=\mathcal{N}\left(\boldsymbol{x}_{n-1} ; \boldsymbol{\mu}_\theta\left(\boldsymbol{x}_n, n\right), \boldsymbol{\Sigma}_\theta\left(\boldsymbol{x}_n, n\right)\right)\)​</li>
    </ul>
  </li>
  <li>이건 \(N_*^g\)번째 diffusion step에서 \(N\)번째까지 총 \(N-N_*^g\) steps의 forward process이므로
    <ul>
      <li>the guidance objective: \(\log p_\theta\left(\boldsymbol{x}^g\right)=\log \int p_\theta\left(\boldsymbol{x}_{N_*^g}^g, \boldsymbol{x}_{N_*^g+1}^g, \ldots, \boldsymbol{x}_N^g\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}^g\)​</li>
    </ul>
  </li>
  <li>sample에 대한 loss 대신 noise에 대한 loss 사용
    <ul>
      <li>loss: \(\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}^g, n}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_n^g, n\right)\right\|^2\right]\)​</li>
      <li>where \(\boldsymbol{x}_n^g=\left(\prod_{i=N_{\boldsymbol{z}}^g}^n \alpha_i^1\right) \boldsymbol{x}^g+\sqrt{ } \mathbf{1}-\prod_{i=N^g}^n \alpha_i^1 \boldsymbol{\epsilon} \text { and } \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})\)</li>
    </ul>
  </li>
</ul>

<h3 id="322-multi-granularity-guidance">3.2.2. Multi-granularity Guidance</h3>

<ul>
  <li>
    <p>Multi-granularity Data Generator가 G개의 granularity levels마다 data 생성: \(\boldsymbol{X}^{(1)}, \boldsymbol{X}^{(2)}, \ldots, \boldsymbol{X}^{(G)}\)</p>
  </li>
  <li>Share ratio: \(r_g:=1-\left(N_*^g-1\right) / N\)
    <ul>
      <li>: the shared percentage of variance schedule between the gth granularity data and the finest-grained data</li>
      <li>ex. finest-grained data에서는 \(N_*^1=1 \text { and } r^1=1\)​
        <ul>
          <li>variance schedule for granularity \(g\) is, \(\alpha_n^g\left(N_*^g\right)= \begin{cases}1 &amp; \text { if } n=1, \ldots, N_*^g \\ \alpha_n^1 &amp; \text { if } n=N_*^g+1, \ldots, N\end{cases}\)​</li>
          <li>and \(\left\{\beta_n^g\right\}_{n=1}^N=\left\{1-\alpha_n^g\right\}_{n=1}^N\)​</li>
          <li>accordingly, \(a_n^g\left(N_*^g\right)=\prod_{k=1}^n \alpha_k^g \text {, and } b_n^g\left(N_*^g\right)=1-a_n^g\left(N_*^g\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이 때 \(N^g_*\)는 : represents the diffusion index for starting sharing the variance schedule across granularity level \(g \in\{1, \ldots, G\}\)</li>
  <li>이렇게 되면 larger coarser granularity level일수록 \(N^g_*\)가 커진다는 뜻
    <ul>
      <li>즉 coarser할수록 fine한 정보는 줄어들테니 이전 diffusion step과 차이가 크지 않을 것</li>
      <li>그러니까 \(N^g_*\)를 크게 해서 fine-grained feature를 생성할 steps를 많이 줌</li>
    </ul>
  </li>
  <li>Then the guidance loss function \(L^{(g)}(\theta)\) for \(g\)-th granularity \(x^g_{n,t}\) at timestep \(t\) and diffusion step \(n\),
    <ul>
      <li>can be expressed as: \(L^{(g)}(\theta)=\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, n} \|\left(\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon}, n, \mathbf{h}_{t-1}^g\right) \|_2^2\right.\)</li>
      <li>where \(\mathbf{h}_t^g=\mathrm{RNN}_\theta\left(\boldsymbol{x}_t^g, \mathbf{h}_{t-1}^g\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="training">Training</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm1.png" alt="그림41" /></p>

<ul>
  <li>최종적인 training objectives는 모든 granularities에서의 Loss의 weighted sum
    <ul>
      <li>: \(L^{\text {final }}=\omega^1 L^{(1)}(\theta)+L^{\text {guidance }}(\theta)=\sum_{q=1}^G \omega^g \mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, t}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_{n, t}^g, n, \mathbf{h}_{t-1}^g\right)\right\|^2\right]\)</li>
      <li>where \(\boldsymbol{x}_{n, t}^g=\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon} \text { and } \sum_{g=1}^G \omega^g=1\)</li>
      <li>이 때 denoising network의 parameters는 shared across all granularities</li>
    </ul>
  </li>
</ul>

<h3 id="inference">Inference</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm2.png" alt="그림42" /></p>

<ul>
  <li>우리의 목표는 특정한 prediction steps에 대한  finest-grained data에 대한 예측
    <ul>
      <li>\(t_0-1\) 시점까지 주어졌다면 아래 algorithm 2를 따라 \(t_0\)시점에 대한 데이터 생성,</li>
      <li>우리가 원하는 forecast horizon이 될 때까지 반복</li>
      <li>hidden states에 conditional inputs으로 무엇을 넣는지에 따라서 그에 해당하는 granularity levels로 샘플링</li>
    </ul>
  </li>
</ul>

<h3 id="selection-of-share-ratio">Selection of share ratio</h3>

<ul>
  <li>위에서는 share ratio \(r_g:=1-\left(N_*^g-1\right) / N\)를 heuristic하게 \(N^g_*\)에 따라 결정되도록 했음
    <ul>
      <li>Diffusion step \(N^g_*\)는 \(q\left(\boldsymbol{x}^g\right) \text { and } p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\)의 거리가 가장 작을 때로 설정 !</li>
      <li>: \(\to\) \(N_*^g:=\arg \min_n \mathcal{D}\left(q\left(\boldsymbol{x}^g\right), p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\right)\)​
        <ul>
          <li>\(\mathcal{D}\)는 두 분포의 거리를 측정하는 metric이 됨 (KL-divergence, …)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/MG-TSD/table12.png" alt="그림112" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig3.png" alt="그림3" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig4.png" alt="그림4" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Multi-Granularity Time Series Diffusion (MG-TSD)
    <ul>
      <li>leverages the inherent granularity levels within the data, as given targets at intermediate diffusion steps to guide the learning process of diffusion models</li>
      <li>to effectively utilize coarse-grained data across various granularity levels.</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=CZiY6OLktd)]]></summary></entry><entry><title type="html">Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/" rel="alternate" type="text/html" title="Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)" /><published>2024-08-04T00:00:00+09:00</published><updated>2024-08-04T18:52:22+09:00</updated><id>http://localhost:4000/timeseries/Diffusion-TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Diffusion-TS: uses an encoder-decoder transformer with disentangled temporal representations</li>
  <li>train the model to directly reconstruct the <strong>sample</strong> instead of the <strong>noise</strong> in each diffusion step</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Synthesizing realistic time series data는 데이터 공유가 개인정보 침해로 이어질 수 있는 사례에서의 솔루션</li>
  <li>지금까지 Diffusion을 활용한 time series generation은 대부분 task-agnostic generation
    <ul>
      <li>첫번째 문제는 RNN-based Autoregressive 방식: limited long-range performance due to error accumulation and slow inference speed</li>
      <li>두번째 문제는 diffusion process에서 noise를 추가할 때 시계열의 combinations of independent components(trend, seasonal, …)이 망가지는 문제 (특히 주기성이 뚜렷한 경우 interpretability가 부족 <a href="https://openreview.net/pdf?id=rdjeCNUS6TG">Liu et al., (2022)</a>)</li>
    </ul>
  </li>
  <li><strong>본 논문에서는 Transformer를 활용하여 trend와 seasonal을 non-autoregressive하게 생성</strong>
    <ul>
      <li>by imposing different forms of constraints on different representations.</li>
    </ul>
  </li>
  <li>For Reconstruct the <strong>samples</strong> rather than the <strong>noises</strong> in each diffusion step, Fourier-based loss 사용</li>
</ul>

<h2 id="2-problem-statement">2. Problem Statement</h2>

<ul>
  <li>N개로 이루어진 데이터셋 \(D A=\left\{X_{1: \tau}^i\right\}_{i=1}^N\)​
    <ul>
      <li>where \(X_{1: \tau}=\left(x_1, \ldots, x_\tau\right) \in \mathbb{R}^{\tau \times d}\)</li>
    </ul>
  </li>
  <li>목표는 Gaussian vectors \(Z_i=\left(z_1^i, \ldots, z_t^i\right) \in \mathbb{R}^{\tau \times d \times T}\)를 DA와 비슷한 \(\hat{X}_{1: \tau}^i=G\left(Z_i\right)\)로 바꾸는 Generator \(G\)를 학습하는 것</li>
  <li>Time series model은 trend와 여러 개의 seasonality로 구성 : \(x_j=\zeta_j+\sum_{i=1}^m s_{i, j}+e_j\)
    <ul>
      <li>where \(j=0,1, \ldots, \tau-1\)</li>
      <li>\(x_j\) : observed time series</li>
      <li>\(\zeta_j\): trend component</li>
      <li>\(s_{i,j}\): \(i\)-th seasonal component</li>
      <li>\(e_j\): remainder part (contatins the noise and some outliers at time t)</li>
    </ul>
  </li>
</ul>

<h2 id="3-diffusion-ts-interpretable-diffusion-for-time-series">3. Diffusion-TS: Interpretable Diffusion for Time Series</h2>

<ul>
  <li>이러한 interpretable decomposition architecture의 근거는 3가지
    <ul>
      <li>첫째, disentangled patterns in the diffusion model은 아직 연구되지 않음</li>
      <li>둘째, specific designs of architecture and objective 덕분에 interpretable</li>
      <li>셋째, explainable disentangled representations 덕분에 complex dynamics 파악</li>
    </ul>
  </li>
</ul>

<h3 id="31-diffusion-framework">3.1. Diffusion Framework</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig1.png" alt="그림1" /></p>

<ul>
  <li>Forward process
    <ul>
      <li>\(x_0 \sim q(x)\)에서 점점 noisy into Gaussian noise \(x_T \sim \mathcal{N}(0, \mathbf{I})\)</li>
      <li>Parameterization: \(q\left(x_t \mid x_{t-1}\right)=\mathcal{N}\left(x_t ; \sqrt{ } 1-\beta_t x_{t-1}, \beta_t \mathbf{I}\right) \text { with } \beta_t \in(0,1)\)</li>
    </ul>
  </li>
  <li>
    <p>Reverse process</p>

    <ul>
      <li>
        <p>반대로 \(p_\theta\left(x_{t-1} \mid x_t\right)=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)\)</p>
      </li>
      <li>
        <p>MSE: \(\mathcal{L}\left(x_0\right)=\sum_{t=1}^T \underset{q\left(x_t \mid x_0\right)}{\mathbb{E}}\left\|\mu\left(x_t, x_0\right)-\mu_\theta\left(x_t, t\right)\right\|^2\)</p>
        <ul>
          <li>where \(\mu\left(x_t, x_0\right) \text { is the mean of the posterior } q\left(x_{t-1} \mid x_0, x_t\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="32-decomposition-model-architecture">3.2. Decomposition Model Architecture</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig2.png" alt="그림2" /></p>

<ul>
  <li>Noisy sequence가 encoder 통과해서 decoder로 들어옴 (초록색)</li>
  <li>Decoder는 multilayer structure, 각 layer에는 <strong>Transformer Block</strong>, <strong>FFN</strong>, <strong>Trend and Fourier synthetic layer</strong>가 포함됨</li>
  <li>각 layer는 시계열의 각 component를 생성하는 역할
    <ul>
      <li>component에 해당하는 inductive bias를 각 layer에 반영해줌으로써 학습이 쉬워짐</li>
      <li>Trend representation captures the intrinsic trend which changes gradually and smoothly</li>
      <li>Seasonality representation illustrates the periodic patterns of the signal</li>
      <li>Error representation characterizes the remaining parts after removing trend and periodicity</li>
    </ul>
  </li>
  <li>\(w_{(\cdot)}^{i, t}\) where \(i \in 1, \ldots, D\)는 \(i\)번째 decoder block에서의 diffusion step \(t\)를 의미</li>
</ul>

<h3 id="trend-synthesis">Trend Synthesis</h3>

<ul>
  <li>smooth underlying mean of the data, which aims to model slow-varying behavior</li>
  <li>그러므로 Trend \(V_{t r}^t\)를 위해 Polynomial regressor 사용
    <ul>
      <li>\(V_{t r}^t=\sum_{i=1}^D\left(C \cdot \operatorname{Linear}\left(w_{t r}^{i, t}\right)+\mathcal{X}_{t r}^{i, t}\right)\) where \(C=\left[1, c, \ldots, c^p\right]\)</li>
      <li>\(\mathcal{X}_{t r}^{i, t}\)는 the mean value of the output of the \(i\)​-th decoder block</li>
      <li>\(C\)는 slow-varying poly space인데, matrix of powers of vector \(c=[0,1,2, \ldots, \tau-2, \tau-1]^T / \tau\)</li>
      <li>\(p\)는 small degree (e.g. \(p\)​=3) to model low frequency behavior</li>
    </ul>
  </li>
</ul>

<h3 id="seasonality--error-synthesis">Seasonality &amp; Error Synthesis</h3>

<ul>
  <li>이제 Trend, Seasonality, Error 모두 생각해보자.</li>
  <li><strong>결국 문제는 noisy input \(x_t\)에서 seasonal patterns를 구분해내는 것 !</strong></li>
  <li>푸리에 시리즈의 trigonometric representation of seasonal components를 기반으로 Fourier bases를 활용한 Fourier synthetic layers에서 seasonal component 파악</li>
</ul>

<p><img src="/assets/img/timeseries/Diffusion-TS/fomula456.png" alt="그림456" /></p>

<ul>
  <li>\(A_{i, t}^{(k)}, \Phi_{i, t}^{(k)}\) are the phase, amplitude of the \(k\)-th frequency after the DFT \(\mathcal F\) repectively</li>
  <li>\(f_k\)는 Fourier frequency of the corresponding index \(k\)</li>
  <li>결국 the Fourier synthetic layer는 진폭(amplitude)이 큰 frequency를 찾고, 그 frequency들만 IDFT.
    <ul>
      <li>그걸 seasonality로 본다. (Pathformer랑 같은 방식)</li>
    </ul>
  </li>
  <li>최종적으로 original signal: \(\hat{x}_0\left(x_t, t, \theta\right)=V_{t r}^t+\sum_{i=1}^D S_{i, t}+R\)​
    <ul>
      <li>\(R\): output of the last decoder block, which can be regarded as the sum of residual periodicity and other noise.</li>
    </ul>
  </li>
</ul>

<h3 id="33-fourier-based-traning-objective">3.3 Fourier-based Traning Objective</h3>

<ul>
  <li>\(\hat{x}_0\left(x_t, t, \theta\right)\)를 directly estimate
    <ul>
      <li>Reverse process: \(x_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \hat{x}_0\left(x_t, t, \theta\right)+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} x_t+\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t z_t\)</li>
      <li>where \(z_t \sim \mathcal{N}(0, \mathbf{I}), \alpha_t=1-\beta_t \text { and } \bar{\alpha}_t=\prod_{s=1}^t \alpha_s\)​</li>
    </ul>
  </li>
  <li>Reweighting strategy: \(\mathcal{L}_{\text {simple }}=\mathbb{E}_{t, x_0}\left[w_t\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2\right], \quad w_t=\frac{\lambda \alpha_t\left(1-\bar{\alpha}_t\right)}{\beta_t^2}\)​
    <ul>
      <li>where \(\lambda\) is constant (i.e. 0.01)</li>
      <li>즉 small t에서 down-weighted, 모델이 larger diffusion step에 집중하도록 만듬</li>
    </ul>
  </li>
  <li>Fourier-based loss term이 time serie reconstruction에서는 더 좋다 <a href="https://arxiv.org/pdf/2208.05836">Fons et al. (2022)</a>
    <ul>
      <li>: \(\mathcal{L}_\theta=\mathbb{E}_{t, x_0}\left[w_t\left[\lambda_1\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2+\lambda_2\left\|\mathcal{F} \mathcal{F} \mathcal{T}\left(x_0\right)-\mathcal{F F} \mathcal{T}\left(\hat{x}_0\left(x_t, t, \theta\right)\right)\right\|^2\right]\right]\)</li>
    </ul>
  </li>
</ul>

<h3 id="34-conditional-generation-for-time-series-applications">3.4. Conditional Generation for Time Series Applications</h3>

<ul>
  <li><strong>Conditional extensions of the Diffusion-TS</strong>, in which the modeled \(x_0\) is conditioned on targets \(y\)​</li>
  <li>목표는 pre-trained diffusion model과 the gradients of a classifier를 활용하여
    <ul>
      <li>Posterior \(p\left(x_{0: T} \mid y\right)=\prod_{t=1}^T p\left(x_{t-1} \mid x_t, y\right)\)에서 sampling하는 것</li>
    </ul>
  </li>
  <li>\(p\left(x_{t-1} \mid x_t, y\right) \propto p\left(x_{t-1} \mid x_t\right) p\left(y \mid x_{t-1}, x_t\right)\)이므로 bayse theorem을 통해 gradient update
    <ul>
      <li>Score function \(\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t, y\right)=\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t\right)+\nabla_{x_{t-1}} \log p\left(y \mid x_{t-1}\right)\)</li>
      <li>\(\log p\left(x_{t-1} \mid x_t\right)\)은 diffusion model에서 정의됨.</li>
      <li>\(\log p\left(y \mid x_{t-1}\right)\)는 classifier에서 parametrize되며, \(\nabla_{x_{t-1}} \log p\left(y \mid x_{0 \mid t-1}\right)\)로 근사됨</li>
    </ul>
  </li>
  <li>즉 classifier가 높은 likelihood를 가진 영역에서 sample이 생성되도록 하는 것
    <ul>
      <li>: \(\tilde{x}_0\left(x_t, t, \theta\right)=\hat{x}_0\left(x_t, t, \theta\right)+\eta \nabla_{x_t}\left(\left\|x_a-\hat{x}_a\left(x_t, t, \theta\right)\right\|_2^2+\gamma \log p\left(x_{t-1} \mid x_t\right)\right)\)</li>
      <li>where Conditional part \(x_a\), generative part \(x_b\)</li>
      <li>gradient term은 reconstruction-based guidance, \(\eta\)로 강도 조절</li>
    </ul>
  </li>
  <li>각 diffusion step에서 이 gradient update를 여러 번 반복하여 quality 높인다</li>
  <li>Replacing: \(\tilde{x}_a\left(x_t, t, \theta\right):=\sqrt{\bar{\alpha}_t} x_a+\sqrt{ } 1-\bar{\alpha}_t \epsilon\)을 통해, \(\tilde{x}_0\)를 사용한 sample \(x_{t-1}\)가 생성됨</li>
</ul>

<h2 id="4-empirical-evaluaiton">4. Empirical Evaluaiton</h2>

<h3 id="42-metrics">4.2. Metrics</h3>

<ul>
  <li>Discriminative score (Yoon et al., 2019): measures the similarity using a classification model to distinguish between the original and synthetic data as a supervised task;</li>
  <li>Predictive score (Yoon et al., 2019):  measures the usefulness of the synthesized data by training a post-hoc sequence model to predict next-step temporal vectors using the train-synthesis-and-test-real (TSTR) method;</li>
  <li>Context-Frechet Inception Distance (Context-FID) score ´ (Paul et al., 2022):  quantifies the quality of the synthetic time series samples by computing the difference between representations of time series that fit into the local context;</li>
  <li>Correlational score (Ni et al., 2020): uses the absolute error between cross correlation matrices by real data and synthetic data to assess the temporal dependency</li>
</ul>

<h3 id="43-interpretability-results">4.3. Interpretability Results</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig3.png" alt="그림3" /></p>

<ul>
  <li>the corrupted samples (shown in (a)) with 50 steps of noise added as input</li>
  <li>outputs the signals (shown in (c)) that try to restore the ground truth (shown in (b))</li>
  <li>with the aid of the decomposition of temporal trend (shown in (d)) and season &amp; error (shown in (e)).</li>
  <li>Result: As would be expected, the trend curve follows the overall shape of the signal, while the season &amp; error oscillates around zero !</li>
</ul>

<h3 id="44-unconditional-time-series-generation">4.4. Unconditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table1.png" alt="그림21" /></p>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig4.png" alt="그림4" /></p>

<h3 id="45-conditional-time-series-generation">4.5. Conditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig6.png" alt="그림6" /></p>

<h3 id="46-ablaction-study">4.6. Ablaction Study</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table2.png" alt="그림22" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Diffusion-TS, a DDPM-based method for general time series generation
    <ul>
      <li>TS-specific loss design and transformer-based deep decomposition architecture</li>
    </ul>
  </li>
  <li>Unconditional로 훈련된 model이 쉽게 conditional로 확장될 수 있음
    <ul>
      <li>by combining gradients into the sampling !</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/pdf/2403.01742)]]></summary></entry><entry><title type="html">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)</title><link href="http://localhost:4000/timeseries/2024-07-26-Informer/" rel="alternate" type="text/html" title="Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)" /><published>2024-07-26T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/Informer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-26-Informer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer: quadratic time complexity, high memory usage, and in- herent limitation of the encoder-decoder architecture</li>
  <li><strong>Informer</strong> : efficient transformer-based model for LSTF !
    <ul>
      <li><em>ProbSparse</em> self-attention mechanism
        <ul>
          <li>\(\mathcal{O}(L \log L)\) in time complexity and memory usage</li>
        </ul>
      </li>
      <li>The self-attention distilling
        <ul>
          <li>highlights dominating attention by halving cascading layer input</li>
          <li>and efficiently handles extreme long input sequences</li>
        </ul>
      </li>
      <li>The generative style decoder
        <ul>
          <li>predicts the long time-series sequences at one forward operation, rather than a step-by-step way</li>
          <li>improves the inference speed of long-sequence predictions</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>The major challenge for LSTF is to enhance the <strong>prediction capacity to meet the increasingly long sequence</strong> demand</li>
  <li><em>can we improve Transformer models to be computation, memory, and architecture efficient, as well as maintaining higher prediction capacity?</em></li>
  <li>Vanila Transformer의 limitation 3
    <ul>
      <li>The quadratic computation of self-attention \(\mathcal{O}\left(L^2\right)\)</li>
      <li>The memory bottleneck in stacking layers for long inputs \(\mathcal{O}\left(J \cdot L^2\right)\)</li>
      <li>The speed plunge in predicting long outputs</li>
    </ul>
  </li>
</ul>

<h2 id="2-preliminary">2. Preliminary</h2>

<p>pass</p>

<h2 id="3-methodology">3. Methodology</h2>

<p><img src="/assets/img/timeseries/Informer/fig2.png" alt="그림1" /></p>

<h3 id="query-sparsity-measurement">Query Sparsity Measurement</h3>

<ul>
  <li>Based on KL divergence
    <ul>
      <li>: \(K L(q \| p)=\ln \sum_{l=1}^{L_K} e^{\mathbf{q}_i \mathbf{k}_l^{\top} / \sqrt{d}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \mathbf{q}_i \mathbf{k}_j^{\top} / \sqrt{d}-\ln L_K\)​</li>
    </ul>
  </li>
  <li>\(i\)-th query’s sparsity measurement
    <ul>
      <li>: \(M\left(\mathbf{q}_i, \mathbf{K}\right)=\ln \sum_{j=1}^{L_K} e^{\frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}\)</li>
      <li>즉 query별로 key들과의 attention이 uniform distribution과 얼마나 다른지를 측정</li>
    </ul>
  </li>
</ul>

<h3 id="probsparse-self-attention"><em>ProbSparse</em> Self-attention</h3>

<ul>
  <li>\(\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Softmax}\left(\frac{\overline{\mathbf{Q}} \mathbf{K}^{\top}}{\sqrt{d}}\right) \mathbf{V}\), where \(\overline{\mathbf{Q}}\) only contains the Top-u queries under the sparsity measurement \(M(\mathbf{q}, \mathbf{K})\)​</li>
  <li>Query 중에서 특정 key에 대해서 높은 attention을 가지는 query도 있지만 (Active) 모든 key에 대해서 비슷한 attention을 가지는 query도 있음 (Lazy)
    <ul>
      <li>굳이 모든 query를 다 볼 필요는 없다. Active = useful query이고 Lazy = trivial query</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Informer/myfig3.png" alt="그림2" /></p>

<p><img src="/assets/img/timeseries/Informer/myfig4.png" alt="그림4" /></p>

<ul>
  <li>하지만 모든 query들 중 query가 active한지 알기 위해서는 또 모든 keys와 attention을 계산해봐야 할 것 같지만,
    <ul>
      <li>그렇지 않고 keys를 sampling해서 몇 개만 가져와서 모든 query들과 attention을 계산해도 된다. (증명 : lemma1)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Informer/lemma1.png" alt="그림6" /></p>

<ul>
  <li>그렇게 찾은 useful query들만 가지고, 이제는 모든 keys와 attention을 계산한다</li>
</ul>

<h3 id="encoder-allowing-for-processing-longer-sequential-inputs-under-the-memory-usage-limitation">Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation</h3>

<p><img src="/assets/img/timeseries/Informer/fig3.png" alt="그림7" /></p>

<ul>
  <li>We use the distilling operation to privilege the superior ones with dominating features and make a focused self-attention fea- ture map in the next layer.</li>
  <li>Distilling procedure : \(\mathbf{X}_{j+1}^t=\operatorname{MaxPool}\left(\operatorname{ELU}\left(\operatorname{Conv1d}\left(\left[\mathbf{X}_j^t\right]_{\mathrm{AB}}\right)\right)\right)\)</li>
</ul>

<h3 id="decoder-generating-long-sequential-outputs-through-one-forward-procedure">Decoder: Generating Long Sequential Outputs Through One Forward Procedure</h3>

<ul>
  <li>
    <p>Transformer의 Masked-attention과 Encoder-Decoder Attention 대신 <strong>generative inference</strong></p>

    <ul>
      <li>
        <p>Decoder의 input : \(\mathbf{X}_{\mathrm{de}}^t=\operatorname{Concat}\left(\mathbf{X}_{\text {token }}^t, \mathbf{X}_{\mathbf{0}}^t\right) \in \mathbb{R}^{\left(L_{\text {token }}+L_y\right) \times d_{\text {model }}}\)</p>
      </li>
      <li>
        <p>Start token을 사용하는 대신, Target 직전 시점의 값 몇개를 start token으로 주고, 우리가 원하는 길이의 예측값을 한 번에 decoding</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/Informer/table1.png" alt="그림9" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Informer
    <ul>
      <li><em>ProbSparse</em> self- attention mechanism</li>
      <li>Distilling operation : to handle the challenges of quadratic time complexity and quadratic mem- ory usage in vanilla Transformer</li>
      <li>generative decoder alleviates : alleviates the limitation of tra- ditional encoder-decoder architecture</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2021](https://arxiv.org/pdf/2012.07436)]]></summary></entry><entry><title type="html">Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting (ICLR 2022)</title><link href="http://localhost:4000/timeseries/2024-07-26-pyraformer/" rel="alternate" type="text/html" title="Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting (ICLR 2022)" /><published>2024-07-26T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/pyraformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-26-pyraformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Pyraformer: a flexible but parsimonious model that can capture a wide range of temporal dependencies, by exploring the multi-resolution representation of the time series
    <ul>
      <li>Pyramidal attention module (PAM)</li>
      <li>the inter-scale tree structure summarizes features at different resolutions</li>
      <li>the intra-scale neighboring connections model the temporal dependencies of different ranges</li>
      <li>the maximum length of the signal traversing path in Pyraformer is a constant with regard to the sequence length L (i.e. \(\mathcal{O}(1)\))</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>시계열 예측에서 Challenge는 powerful but parsimonious model</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/fig1.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/pyraformer/table1.png" alt="그림11" /></p>

<ul>
  <li>Pyraformer: to simultaneously capture temporal dependencies of different ranges in a compact multi-resolution fashion</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-method">3. Method</h2>

<h3 id="31-pyramidal-attention-module-pam">3.1. Pyramidal Attention Module (PAM)</h3>

<ul>
  <li>The inter-scale connections form a C-ary tree, in which each parent has C children.
    <ul>
      <li>the nodes at coarser scales can be regarded as the daily, weekly, and even monthly features of the time series</li>
    </ul>
  </li>
  <li>\(\to\) The pyramidal graph offers a multi-resolution representation of the original time series !
    <ul>
      <li>long-range dependencies 파악이 쉬워짐. 그냥 이웃 노드 연결하기만 하면 되니까 (intra-scale)</li>
    </ul>
  </li>
  <li>Original Attention mechanism
    <ul>
      <li>input \(X\), output \(Y\)</li>
      <li>Query \({Q}={X} {W}_Q\), key \({K}={X} {W}_K\), value \({V}={X} {W}_V\)</li>
      <li>where \({W}_Q, {W}_K, {W}_V \in \mathbb{R}^{L \times D_K}\)</li>
      <li>Then, \({y}_i=\sum_{\ell=1}^L \frac{\exp \left({q}_i {k}_{\ell}^T / \sqrt{D_K}\right) {v}_{\ell}}{\sum_{\ell=1}^L \exp \left({q}_i {k}_{\ell}^T / \sqrt{D_K}\right)}\)</li>
      <li>time and space complexity \(\mathcal{O}(L^2)\)</li>
    </ul>
  </li>
  <li>Pyramidal Attention Module (PAM)</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/fomula2.png" alt="그림21" /></p>

<p><img src="/assets/img/timeseries/pyraformer/myfig1.png" alt="그림31" /></p>

<ul>
  <li>Then, \({y}_i=\sum_{\ell \in \mathbb{N}_{\ell}^{(s)}} \frac{\exp \left({q}_i {k}_{\ell}^T / \sqrt{d_K}\right) {v}_{\ell}}{\sum_{\ell \in \mathbb{N}_l^{(s)}} \exp \left({q}_i {k}_{\ell}^T / \sqrt{d_K}\right)}\)</li>
  <li>모든 시점끼리 attention을 하지 않고 conv filter로 nodes를 만들고 이웃 노드끼리 attention !</li>
</ul>

<h3 id="32-coarser-saleㄴ-construvtion-module-cscm">3.2. Coarser-saleㄴ Construvtion Module (CSCM)</h3>

<p><img src="/assets/img/timeseries/pyraformer/fig3.png" alt="그림3" /></p>

<ul>
  <li>PAM이 작동할 수 있도록 pyramidal 구조를 initialize하는 역할</li>
</ul>

<h3 id="33-prediction-module">3.3. Prediction Module</h3>

<ul>
  <li>input embedding 할 때에 예측하고자 하는 길이만큼 붙여서 CSCM, PAM을 통과하면</li>
  <li>예측 시점에 대한 representation을 얻을 수 있음</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/myfig3.png" alt="그림33" /></p>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/pyraformer/table2.png" alt="그림12" /></p>

<p><img src="/assets/img/timeseries/pyraformer/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/pyraformer/fig4.png" alt="그림14" /></p>

<h2 id="5-conclusion-and-outlook">5. Conclusion and Outlook</h2>

<ul>
  <li>Pyraformer: a novel model based on pyramidal attention
    <ul>
      <li>effectively describe both short and long temporal dependencies with low time and space complexity</li>
      <li>CSCM to construct a C-ary tree, and then design the PAM to pass messages in both the inter-scale and the intra-scale fashion</li>
      <li>Pyraformer can achieve the theoretical \(\mathcal{O}(L)\) complexity and \(\mathcal{O}(1)\) maximum signal traversing path length (L: input sequence length)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2022](https://arxiv.org/pdf/2012.07436)]]></summary></entry><entry><title type="html">Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)</title><link href="http://localhost:4000/timeseries/2024-07-15-timediff/" rel="alternate" type="text/html" title="Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)" /><published>2024-07-15T00:00:00+09:00</published><updated>2024-07-23T18:18:28+09:00</updated><id>http://localhost:4000/timeseries/timediff</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-15-timediff/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>TimeDiff : non-autoregressive diffusion model, w/ two novel conditioning mechanisms
    <ul>
      <li>future mixup : future prediction의 ground-truth의 일부를 conditioning하는 것을 허용</li>
      <li>autoregressive initialization : time series의 basic pattern (short term trends 등)을 모델 initialization에 사용</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Diffusion model (iterative denoising)은 이미지 생성에서 뛰어난 quality
    <ul>
      <li>하지만 time series prediction을 위해 어떻게 쓸지에 대한 연구는 아직</li>
      <li>time series는 <strong>complex dynamics, nonlinear patterns, long-temporal dependencies</strong></li>
    </ul>
  </li>
  <li>기존 diffusion model들은 decoding strategy에 따라 구분됨
    <ul>
      <li><strong>Autoregressive</strong> : future prediction이 one by one으로 generated (ex. Timegrad)
        <ul>
          <li>하지만 error accumulation 때문에 long range prediction 성능이 떨어지고</li>
          <li>하나씩 예측하다보니 inference가 느리다는 단점이 있음</li>
        </ul>
      </li>
      <li><strong>Non-autoregressive</strong> : CSDI, SSSD처럼 denoising networks에 intermediate layers를 conditioning으로 넣고 the denoising objective에 inductive bias를 introduce
        <ul>
          <li>하지만 long-range prediction performance는 Fedformer, NBeats보다 떨어짐</li>
          <li>왜냐하면 conditioning 전략이 image, textf를 위한 것이지 time series를 위한 것이 아니기 때문</li>
          <li>inductive bias를 위해 denoising objective를 사용하는 것만으로는 lookback window에서 유용한 정보를 알아내기 어렵다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>본 논문에서는 long time series prediction을 위한 conditional non-autoregressive diffusion model인 TimeDiff 제안
    <ul>
      <li>CSDI, SSSD와 다르게 conditioning module에 time series를 위한 additional inductive bias 도입
        <ul>
          <li><strong>future mixup</strong>: randomly reveals parts of the ground-truth future pre- dictions during training</li>
          <li><strong>autoregressive initialization</strong>: better initializes the model with basic components in the time series</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-preliminaries">2. Preliminaries</h2>

<h3 id="21-diffusion-models">2.1. Diffusion Models</h3>

<p>pass</p>

<h3 id="22-conditional-ddpms-for-time-series-prediction">2.2. Conditional DDPMs for Time Series Prediction</h3>

<ul>
  <li>\(\mathbf{x}_{-L+1: 0}^0 \in \mathbb{R}^{d \times L}\)를 보고 \(\mathbf{x}_{1: H}^0 \in \mathbb{R}^{d \times H}\)를 예측하는 문제</li>
  <li>\(p_\theta\left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}\right)=p_\theta\left(\mathbf{x}_{1: H}^K\right) \prod_{k=1}^K p_\theta\left(\mathbf{x}_{1: H}^{k-1} \mid \mathbf{x}_{1: H}^k, \mathbf{c}\right)\),
    <ul>
      <li>where \(\mathbf{x}_{1: H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)</li>
    </ul>
  </li>
  <li>아직 efficient denoising network \(\mu_{\theta}\)와 conditioning network \(\mathcal F\) in time series diffusion models를 어떻게 디자인할 것인지 명확하지 않음</li>
  <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad (ICML 2021)</a>
    <ul>
      <li>autoregressive manner :
 \(\begin{aligned}
p_\theta &amp; \left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\right) \\
&amp; =\prod_{t=1}^H p_\theta\left(\mathbf{x}_t^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right) \\
&amp; =\prod^H p_\theta\left(\mathbf{x}_t^K\right) \prod^K p_\theta\left(\mathbf{x}_t^{k-1} \mid \mathbf{x}_t^k, \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right)
\end{aligned}\)</li>
      <li>training objectives : \(\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_t^k, k \mid \mathbf{h}_t\right)\right\|^2\right]\)</li>
      <li>autoregressive decoding 때문에 error accumulation이 발생하고 inference가 느리고 부정확함</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/2107.03502">CSDI(NeurIPS 2021)</a>
    <ul>
      <li>time series \(\mathbf{x}_{-L+1: H}^0\) 전체를 한 번에 diffusing and denoising</li>
      <li>binary mask \(\mathbf{m} \in\{0,1\}^{d \times(L+H)}\)를 사용하여 self-supervised strategy 제안</li>
      <li>training objectives : \(\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_{\text {target }}^k, k \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{\text {observed }}^k\right)\right)\right\|^2\right]\)</li>
    </ul>
  </li>
  <li>하지만 CSDI의 한계는
    <ul>
      <li>Denoising networks가 2개의 transformers를 사용해서 complexity가 높다.</li>
      <li>conditioning에 사용되는 masking은 vision의 inpainting이랑 비슷한데
        <ul>
          <li><a href="https://arxiv.org/abs/2201.09865">(Lugmayr et al., 2022)</a>에서는 이 방식이 masking과 observed사이의 부조화 발생한다고 밝힘</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/2208.09399">SSSD (TMLR 2022)</a>
    <ul>
      <li>Transfermer를 structured state space model로 대체</li>
      <li>하지만 여전히 non-autoregressive strategy이라서 boundary disharmony가 발생할 수 있음</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-model">3. Proposed Model</h2>

<ul>
  <li>Conditional Diffusion의 conditioning은 semantic similarities across modalities 파악에 중점</li>
  <li>하지만 현실에서의 non-stationary time series는 complex temporal dependencies 파악이 중요함</li>
</ul>

<h3 id="31-forward-diffusion-process">3.1. Forward Diffusion Process</h3>

<ul>
  <li>Forward process : \(\mathbf{x}_{1: H}^k=\sqrt{\bar{\alpha}_k} \mathbf{x}_{1: H}^0+\sqrt{1-\bar{\alpha}_k} \epsilon\)
    <ul>
      <li>where \(\epsilon\) is sampled from \(\mathcal{N}(0, \mathbf{I})\) with the same size as \(\mathbf{x}_{1: H}^0\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-conditioning-the-backward-denoising-process">3.2. Conditioning the Backward Denoising Process</h3>

<ul>
  <li>Illustration</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/fig1.png" alt="그림1" /></p>

<h4 id="321-future-mixup">3.2.1. FUTURE MIXUP</h4>

<ul>
  <li>먼저 <em>future mixup</em>으로 \(\mathbf{z}_{\text {mix }}=\mathbf{m}^k \odot \mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)+\left(1-\mathbf{m}^k\right) \odot \mathbf{x}_{1: H}^0\)를 만든다.
    <ul>
      <li>past information’s mapping \(\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)과 the future ground-truth \(\mathbf{x}_{1: H}^0\)를 combine</li>
      <li>training에서 적용되는 것이고, inference에서는 \(\mathbf{z}_{\text {mix }}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)</li>
    </ul>
  </li>
</ul>

<h4 id="322-autoregressive-model">3.2.2. AUTOREGRESSIVE MODEL</h4>

<ul>
  <li>Non-autoregressive models는 masked와 observed의 경계에서 disharmony
    <ul>
      <li>그래서 linear autoregressive (AR) model \(\mathcal{M}_{a r}\) 사용. \(\mathbf{z}_{a r}=\sum_{i=-L+1}^0 \mathbf{W}_i \odot \mathbf{X}_i^0+\mathbf{B}\)
        <ul>
          <li>\(\mathbf{X}_i^0 \in \mathbb{R}^{d \times H}\) is a matrix containing \(H\) copies of \(\mathbf{x}_i^0\),</li>
          <li>\(\mathbf{W}_i\) s \(\in \mathbb{R}^{d \times H}, \mathbf{B} \in \mathbb{R}^{d \times H}\) are trainable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>complex nonlinear time series는 approximate 못하는 건 사실이지만
    <ul>
      <li>simple patterns (short-term trends) 정도는 잘 잡으니까</li>
      <li>그리고 one by one으로 하는 것이 아니라 \(\mathbf{z}_{a r}\)의 모든 columns는 동시에 계산됨</li>
    </ul>
  </li>
</ul>

<h4 id="33-denoising-network">3.3. Denoising Network</h4>

<ul>
  <li>먼저 the transformer’s sinusoidal position embedding으로 the diffusion-step embedding \(\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}\)​ 얻음
    <ul>
      <li>즉 \(\begin{aligned}
k_{\text {embedding }}= &amp; {\left[\sin \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \sin \left(10^{\frac{w \times 4}{w-1}} t\right),\right.} \\
&amp; \left.\cos \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \cos \left(10^{\frac{w \times 4}{w-1}} t\right)\right],
\end{aligned}\)​
        <ul>
          <li>where \(w=\frac{d^{\prime}}{2}\)</li>
        </ul>
      </li>
      <li>그리고 \(\mathbf{p}^k=\operatorname{SiLU}\left(\mathrm{FC}\left(\operatorname{SiLU}\left(\mathrm{FC}\left(k_{\text {embedding }}\right)\right)\right)\right) \in \mathbb{R}^{d^{\prime} \times 1}\)</li>
    </ul>
  </li>
  <li>그 다음 \(\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}\)는 diffused input \(\mathbf{x}_{1: H}^k\)의 embedding \(\mathbf{z}_1^k \in \mathbb{R}^{d^{\prime} \times H}\)에 합쳐짐 (\(2 d^{\prime} \times H\)이 됨)
    <ul>
      <li>\(\mathbf{z}_1^k\)는 여러 개의 convolution layers로 이루어진 input projection block을 통과시켜 얻음</li>
    </ul>
  </li>
  <li>그 다음 multilayer convolution-based <strong>encoder</strong> 통과하면 \(\mathbf{z}_2^k \in \mathbb{R}^{d^{\prime \prime} \times H}\)로 representation</li>
  <li>그 다음 \(\mathbf{c}\)와 \(\mathbf{z}_2^k\)를 fuse해서 \(\left(2 d+d^{\prime \prime}\right) \times H\)로 만들고
    <ul>
      <li>multiple convolution layers <strong>decoder</strong>에 넣어서 \(\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right)\in \mathbb{R^{d \times H}}\)로 만듬 (\(\mathbf{x}_{1: H}^k\)와 같은 size)</li>
    </ul>
  </li>
  <li>마지막으로 \(\mu_{\mathbf{x}}\left(\mathbf{x}_\theta\right)=\frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}^k, k \mid \mathbf{c}\right)\)을 통해 denoised output \(\begin{aligned}
\hat{\mathbf{x}}_{1: H}^{k-1}= &amp; \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
+\sigma_k \epsilon
\end{aligned}\)얻음</li>
  <li>흔히 아는 Diffusion에서는 noise \(\epsilon_\theta\left(\mathbf{x}_{1: H}^k, k\right)\)를 예측하지만, time series에서는  highly irregular noisy components라서 데이터 \(\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k\right)\)를 예측</li>
</ul>

<h3 id="34-training">3.4. Training</h3>

<ul>
  <li>Pesudo code</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/code.png" alt="그림3" /></p>

<ul>
  <li>각각의 \(\mathbf{x}_{1: H}^0\)에 대해 batch of diffusion steps \(k\)’s를 sampling하고
    <ul>
      <li>conditioned variant of loss를 minimize: \(\min _\theta \mathcal{L}(\theta)=\min _\theta \mathbb{E}_{\mathbf{x}_{1 . H}^0, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), k} \mathcal{L}_k(\theta)\)</li>
    </ul>
  </li>
</ul>

<h3 id="35-inference">3.5. Inference</h3>

<ul>
  <li>Pesudo code</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/code.png" alt="그림4" /></p>

<ul>
  <li>먼저 noise vector \(\mathbf{x}_{1 \cdot H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \in \mathbb{R}^{d \times H}\)를 생성하고</li>
  <li>\(\begin{aligned}
\hat{\mathbf{x}}_{1: H}^{k-1}= &amp; \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
+\sigma_k \epsilon
\end{aligned}\)을 반복 (\(k=1\) 까지)
    <ul>
      <li>when \(k=1\), \(\epsilon=0\)이므로 \(\hat{\mathbf{x}}_{1: H}^0\)를 final prediction으로 얻을 수 있음</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/timediff/table2.png" alt="그림12" /></p>

<p><img src="/assets/img/timeseries/timediff/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/timediff/fig2.png" alt="그림2" /></p>

<h3 id="43-ablation-study">4.3. Ablation study</h3>

<ul>
  <li><strong>The Effectiveness of Future mixup</strong></li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table4.png" alt="그림14" /></p>

<ul>
  <li>
    <p>특히 ETTh1 데이터셋에서 future mixup을 안썼을 때 성능이 많이 떨어진다.</p>
  </li>
  <li>
    <p><strong>The Mixup Strategies in Future mixup</strong></p>
    <ul>
      <li>Hard mixup : The sampled values in \(\mathbf{m}^k\) are binarized by a threshold \(\tau \in (0,1)\)</li>
      <li>Segment mixup : The mask  \(\mathbf{m}^k\), Each masked segment has a length following the geometric distribution with a mean of 3. This is then followed by an unmasked segment with mean length \(3(1 − \tau)/\tau\)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table5.png" alt="그림15" /></p>

<ul>
  <li><strong>Predicting \(\mathbf{x}_\theta\) vs Predicting \(\epsilon_\theta\)</strong></li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table6.png" alt="그림16" /></p>

<h3 id="44-integration-into-existing-diffusion-models">4.4. Integration into Existing Diffusion Models</h3>

<p><img src="/assets/img/timeseries/timediff/table7.png" alt="그림17" /></p>

<h3 id="45-inference-efficiency">4.5. Inference Efficiency</h3>

<p><img src="/assets/img/timeseries/timediff/table8.png" alt="그림18" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Timediff : diffusion model for time series prediction,
    <ul>
      <li>1)future mixup과 2)autoregressive initialization이라는 conditioning mechanisms으로</li>
      <li>conditioning network에 useful inductive bias를 추가</li>
      <li>한계점으로 변수의 개수가 많을 때 multivariate dependencies를 학습하기 어렵다
        <ul>
          <li>graph 사용 ?</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2023](https://arxiv.org/pdf/2306.05043)]]></summary></entry><entry><title type="html">CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation (NeurIPS 2021)</title><link href="http://localhost:4000/timeseries/2024-07-15-CSDI/" rel="alternate" type="text/html" title="CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation (NeurIPS 2021)" /><published>2024-07-15T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/CSDI</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-15-CSDI/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Imputation에서 autoregressive models보다 score-based diffusion models의 성능이 좋음</li>
  <li>Conditional Score-based Diffusion models for Imputation (CSDI)
    <ul>
      <li>explicitly trained for imputation</li>
      <li>can exploit correlations between observed values</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<p><img src="/assets/img/timeseries/CSDI/fig1.png" alt="그림1" /></p>

<ul>
  <li>Conditional diffusion modeld을 위해서 필요한 것들
    <ul>
      <li>observed values (i.e., conditional information)</li>
      <li>ground-truth missing values (i.e., imputation targets)</li>
      <li>하지만 실제로는 ground-truth missing values를 모르기 때문에</li>
      <li>masked language modeling처럼 self-supervised training
        <ul>
          <li>separates observed values into conditional information and imputation targets</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>RNN \(\to\) GAN and self-training(deterministic) \(\to\) GP-VAE(probabilistic)</li>
  <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad(ICML 2021)</a>에서 diffusion probabilistic models 사용하여 SOTA
    <ul>
      <li>but 과거를 보는 RNNs을 썼기 때문에 time series imputation으로 활용되기는 어려움</li>
    </ul>
  </li>
</ul>

<h2 id="3-background">3. Background</h2>

<h3 id="31-multivariate-time-series-imputation">3.1. Multivariate time series imputation</h3>

<ul>
  <li>\(\mathbf{X}=\left\{x_{1: K, 1: L}\right\} \in \mathbb{R}^{K \times L}\), \(K\)는 the number of features, \(L\)은 length of sequence</li>
  <li>observation mask : \(\mathbf{M}=\left\{m_{1: K, 1: L}\right\} \in\{0,1\}^{K \times L}\)
    <ul>
      <li>\(x_{k, l}\)가 missing이면 \(m_{k, l}=0\), observed이면 \(m_{k, l}=1\)</li>
    </ul>
  </li>
  <li>Timestamps of the time series \(\mathbf{s}=\left\{s_{1: L}\right\} \in \mathbb{R}^L\)</li>
  <li>즉 각각의 time series는 \(\{\mathbf{X}, \mathbf{M}, \mathbf{s}\}\)로 표현됨</li>
</ul>

<h3 id="32-denoising-diffusion-probabilistic-models">3.2. Denoising diffusion probabilistic models</h3>

<ul>
  <li>forward process : \(q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right):=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right) \text { where } q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right):=\mathcal{N}\left(\sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right)\)</li>
  <li>reverse process : \(\begin{aligned}
&amp; p_\theta\left(\mathbf{x}_{0: T}\right):=p\left(\mathbf{x}_T\right) \prod_{t=1}^T p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right), \quad \mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&amp; p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ; {\mu}_\theta\left(\mathbf{x}_t, t\right), \sigma_\theta\left(\mathbf{x}_t, t\right) \mathbf{I}\right)
\end{aligned}\)</li>
</ul>

<h3 id="33-imputation-with-diffusion-models">3.3 Imputation with diffusion models</h3>

<ul>
  <li>conditional observation \(\mathbf{x}_0^{\mathrm{co}} \in \mathcal{X}^{\mathrm{co}}\)를 활용해서 Imputation target \(\mathbf{x}_0^{\mathrm{ta}} \in \mathcal{X}^{\mathrm{ta}}\)을 생성</li>
  <li>Reverse process에 conditional 추가 : modeling \(p_\theta\left(\mathbf{x}_{t-1}^{\mathrm{ta}} \mid \mathbf{x}_t^{\mathrm{ta}}, \mathbf{x}_0^{\mathrm{co}}\right)\)</li>
</ul>

<h2 id="4-conditional-score-based-diffusion-model-for-imputation-csdi">4. Conditional score-based diffusion model for imputation (CSDI)</h2>

<ul>
  <li>Reverse process of the conditional diffusion model, and self-supervised training method</li>
</ul>

<h3 id="41-imputation-with-csdi">4.1. Imputation with CSDI</h3>

<ul>
  <li>all observed values of \(\mathbf{x}_0\) as conditional observations \(\mathbf{x}_0^{\mathrm{co}}\),
    <ul>
      <li>all missing values as imputation targets \(\mathbf{x}_0^{\mathrm{ta}}\)</li>
    </ul>
  </li>
  <li>Parameterization with \({\epsilon}_\theta\): \({\mu}_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}\right)={\mu}^{\mathrm{DDPM}}\left(\mathbf{x}_t^{\mathrm{ta}}, t, {\epsilon}_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}\right)\right), \quad \sigma_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}\right)=\sigma^{\mathrm{DDPM}}\left(\mathbf{x}_t^{\mathrm{ta}}, t\right)\)</li>
</ul>

<h3 id="42-training-of-csdi">4.2. Training of CSDI</h3>

<p><img src="/assets/img/timeseries/CSDI/fig2.png" alt="그림2" /></p>

<p><img src="/assets/img/timeseries/CSDI/fig6.png" alt="그림6" /></p>

<ul>
  <li>Train \({\epsilon}_\theta\) by minimizing the loss function : \(\min _\theta \mathcal{L}(\theta):=\min _\theta \mathbb{E}_{\mathbf{x}_0 \sim q\left(\mathbf{x}_0\right), {\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), t}\left\|\left({\epsilon}-{\epsilon}_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}\right)\right)\right\|_2^2\)</li>
  <li>fig2는 Masked modeling에서 아이디러를 얻은 CSDI의 self-supervised learning method
    <ul>
      <li>흰색은 missing, 파랑색은 observed이다.</li>
      <li>observed의 일부를 imputation target(빨강색)으로 분리하고 noise를 씌운다.</li>
      <li>남은 observed와 noisy target을 보고 imputation target을 맞추도록 학습한다.</li>
      <li>학습 할 때에만 이렇게 하고 실제 sampling(imputation)은 missing(흰색)에 하는 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/CSDI/table1.png" alt="그림11" /></p>

<h3 id="43-choice-of-imputation-targets-in-self-supervised-learning">4.3. Choice of imputation targets in self-supervised learning</h3>

<ul>
  <li><em>Random</em> strategy : missing patterns 모를 때 일정 비율만큼 imputation target으로 설정</li>
  <li><em>Historical</em> strategy : exploits missing patterns in the training dataset. training과 test의 missing pattern이 highly correlated일 때</li>
  <li><em>Mix</em> strategy : 위 두 가지 방법 mix. Training의 missing pattern에 overfitting되는 것을 방지</li>
  <li><em>Test</em> pattern strategy : test의 missing pattern 알 때</li>
</ul>

<h2 id="5-implementation-of-csdi-for-time-series-imputation">5. Implementation of CSDI for time series imputation</h2>

<p><img src="/assets/img/timeseries/CSDI/fig3.png" alt="그림3" /></p>

<ul>
  <li>\(\mathbf{x}_t^{\mathrm{ta}}\)와 \(\mathbf{x}_0^{\mathrm{co}}\)를 \(\mathbb{R}^{K \times L}\)로 만들어주기 위해 zero-padding
    <ul>
      <li>모델의 input에 conditional mask \(\mathbf{m}^{\mathrm{co}} \in\{0,1\}^{K \times L}\)를 추가</li>
    </ul>
  </li>
  <li>The conditional denoising function \({\epsilon}_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}, \mathbf{m}^{\mathrm{co}}\right)\)은 \({\epsilon}_\theta:\left(\mathbb{R}^{K \times L} \times \mathbb{R} \mid \mathbb{R}^{K \times L} \times\{0,1\}^{K \times L}\right) \rightarrow \mathbb{R}^{K \times L}\)로 표현됨</li>
</ul>

<h3 id="attention-mechanism">Attention mechanism</h3>

<ul>
  <li>Multivariate time series의 temporal and feature dependency를 파악하기 위해
    <ul>
      <li>two dimensional attention mechanism 활용 (conv 대신)</li>
      <li>각각을 temporal Transformer layer and a feature Trans- former layer라 함</li>
    </ul>
  </li>
</ul>

<h2 id="6-experimental-results">6. Experimental results</h2>

<h3 id="61-time-series-imputation">6.1. Time series imputation</h3>

<p><img src="/assets/img/timeseries/CSDI/table2.png" alt="그림12" /></p>

<p><img src="/assets/img/timeseries/CSDI/fig4.png" alt="그림4" /></p>

<p><img src="/assets/img/timeseries/CSDI/table3.png" alt="그림13" /></p>

<h3 id="62-interpolation-of-irregularly-sampled-time-series">6.2. Interpolation of irregularly sampled time series</h3>

<p><img src="/assets/img/timeseries/CSDI/table4.png" alt="그림14" /></p>

<h3 id="63-time-series-forecasting">6.3. Time series Forecasting</h3>

<p><img src="/assets/img/timeseries/CSDI/table5.png" alt="그림15" /></p>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>CSDI : novel approach to impute multivariate time series with conditional diffusion models</li>
  <li>Future works
    <ul>
      <li>improve the computation efficiency</li>
      <li>extend CSDI to downstream tasks such as classifications</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[NeurIPS 2021](https://arxiv.org/pdf/2107.03502)]]></summary></entry><entry><title type="html">Timegrad :Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (ICML 2021)</title><link href="http://localhost:4000/timeseries/2024-07-09-Timegrad/" rel="alternate" type="text/html" title="Timegrad :Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (ICML 2021)" /><published>2024-07-09T00:00:00+09:00</published><updated>2024-07-11T14:35:37+09:00</updated><id>http://localhost:4000/timeseries/Timegrad</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-09-Timegrad/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li><strong>TimeGrad</strong> : auto-regressive model for multivariate time series forecasting, using diffusion
    <ul>
      <li>learns gradients by optimizing a variational bound on the data likelihood</li>
      <li>inference는 white noise에서 학습한 분포의 sample로 convert (though a Markov chain)</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<p>pass</p>

<h2 id="2-diffusion-probabilistic-model">2. Diffusion Probabilistic Model</h2>

<p>pass</p>

<h2 id="3-timegrad-method">3. TimeGrad Method</h2>

<ul>
  <li>Multivariate time series \(x_{i, t}^0 \in \mathbb{R}\) for \(i \in\{1, \ldots, D\}\) where \(t\) is the time index
    <ul>
      <li>at time \(t\), \(\mathbf{x}_t^0 \in \mathbb{R}^D\)</li>
      <li>context window \(\left[1, t_0\right)\), prediction interval \(\left[t_0, T\right]\)</li>
    </ul>
  </li>
  <li>TimeGrad 이전까지는 full joint distribution at each time step을 모델링 했어야 함
    <ul>
      <li>하지만 full covariance matrix을 모델링 하는 것은 computation cost 측면에서 impractical</li>
      <li>그래서 Gaussians with low-rank covariance matrices으로 approximate 하기도 함 (Vec-LSTM)</li>
    </ul>
  </li>
  <li>본 논문에서는 과거 데이터를 보고 미래 시점의 conditional distribution을 학습
    <ul>
      <li>formula : \(q_{\mathcal{X}}\left(\mathbf{x}_{t_0: T}^0 \mid \mathbf{x}_{1: t_0-1}^0, \mathbf{c}_{1: T}\right)=\Pi_{t=t_0}^T q_{\mathcal{X}}\left(\mathbf{x}_t^0 \mid \mathbf{x}_{1: t-1}^0, \mathbf{c}_{1: T}\right)\)​​</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/fig1.png" alt="그림1" /></p>

<h3 id="31-training">3.1. Training</h3>

<ul>
  <li>이전 시점 데이터 \(\mathbf{x}_{t-1}^0\)과 covariates \(\mathbf{c}_{t-1}\)이 들어오면 hidden state \(\mathbf{h}_{t-2}\)를 \(\mathbf{h}_{t-1}\)​로 업데이트
    <ul>
      <li>즉 \(\mathbf{h}_t=\mathrm{RNN}_\theta\left(\operatorname{concat}\left(\mathbf{x}_t^0, \mathbf{c}_t\right), \mathbf{h}_{t-1}\right)\)</li>
    </ul>
  </li>
  <li>그러면 위에 있는 fomula는 \(\Pi_{t=t_0}^T p_\theta\left(\mathbf{x}_t^0 \mid \mathbf{h}_{t-1}\right)\)가 되고
    <ul>
      <li>Negative log-likelihood \(\sum_{t=t_0}^T-\log p_\theta\left(\mathbf{x}_t^0 \mid \mathbf{h}_{t-1}\right)\)를 minimize하도록 학습</li>
    </ul>
  </li>
</ul>

<h3 id="32-inference">3.2. Inference</h3>

<ul>
  <li>Inference할 때에는 한 시점씩 auto-regressive하게 작동
    <ul>
      <li>만약 다음 시점의 sample \(\mathbf{x}_{T+1}^0\)을 얻었다면 위에서 설명한 것처럼 hidden state \(\mathbf{h}_{T+1}\)를 얻고</li>
      <li>같은 과정을 반복. 얻은 sample로 또 다음 sample을 얻고…</li>
    </ul>
  </li>
</ul>

<h3 id="33-scaling">3.3. Scaling</h3>

<ul>
  <li>각 context window를 scale normalizing</li>
  <li>Residual connection은 사용하지 않음</li>
</ul>

<h3 id="34-covariates">3.4. Covariates</h3>

<ul>
  <li>\(\mathbf{c}_t\)는 time-dependent and time- independent embeddings으로 구성되는 embeddings for categorical features</li>
  <li>All covariates are thus known for the periods we wish to forecast !</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>사용한 데이터셋</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/table1.png" alt="그림11" /></p>

<ul>
  <li>Model architecture</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/fig2.png" alt="그림2" /></p>

<ul>
  <li>Results</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/table2.png" alt="그림12" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2021](https://arxiv.org/pdf/2101.12072)]]></summary></entry></feed>