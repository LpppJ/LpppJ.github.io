---
layout: post
related_posts:
  _
title: 
description: >
  [AAAI 2021](https://arxiv.org/pdf/2012.07436)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)

## Abstract

- Transformer: quadratic time complexity, high memory usage, and in- herent limitation of the encoder-decoder architecture
- **Informer** : efficient transformer-based model for LSTF !
  - *ProbSparse* self-attention mechanism
    - $$\mathcal{O}(L \log L)$$ in time complexity and memory usage
  - The self-attention distilling
    - highlights dominating attention by halving cascading layer input
    - and efficiently handles extreme long input sequences
  - The generative style decoder
    - predicts the long time-series sequences at one forward operation, rather than a step-by-step way
    - improves the inference speed of long-sequence predictions

## 1. Introduciton

- The major challenge for LSTF is to enhance the **prediction capacity to meet the increasingly long sequence** demand
- *can we improve Transformer models to be computation, memory, and architecture efficient, as well as maintaining higher prediction capacity?*
- Vanila Transformer의 limitation 3
  - The quadratic computation of self-attention $$\mathcal{O}\left(L^2\right)$$
  - The memory bottleneck in stacking layers for long inputs $$\mathcal{O}\left(J \cdot L^2\right)$$
  - The speed plunge in predicting long outputs

## 2. Preliminary

pass

## 3. Methodology

![그림1](/assets/img/timeseries/Informer/fig2.png)

### Query Sparsity Measurement

- Based on KL divergence
  - : $$K L(q \| p)=\ln \sum_{l=1}^{L_K} e^{\mathbf{q}_i \mathbf{k}_l^{\top} / \sqrt{d}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \mathbf{q}_i \mathbf{k}_j^{\top} / \sqrt{d}-\ln L_K$$​
- $$i$$-th query's sparsity measurement
  - : $$M\left(\mathbf{q}_i, \mathbf{K}\right)=\ln \sum_{j=1}^{L_K} e^{\frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}$$
  - 즉 query별로 key들과의 attention이 uniform distribution과 얼마나 다른지를 측정

### *ProbSparse* Self-attention

- $$\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Softmax}\left(\frac{\overline{\mathbf{Q}} \mathbf{K}^{\top}}{\sqrt{d}}\right) \mathbf{V}$$, where $$\overline{\mathbf{Q}}$$ only contains the Top-u queries under the sparsity measurement $$M(\mathbf{q}, \mathbf{K})$$​
- Query 중에서 특정 key에 대해서 높은 attention을 가지는 query도 있지만 (Active) 모든 key에 대해서 비슷한 attention을 가지는 query도 있음 (Lazy)
  - 굳이 모든 query를 다 볼 필요는 없다. Active = useful query이고 Lazy = trivial query

![그림2](/assets/img/timeseries/Informer/myfig3.png)

![그림4](/assets/img/timeseries/Informer/myfig4.png)

- 하지만 모든 query들 중 query가 active한지 알기 위해서는 또 모든 keys와 attention을 계산해봐야 할 것 같지만,
  - 그렇지 않고 keys를 sampling해서 몇 개만 가져와서 모든 query들과 attention을 계산해도 된다. (증명 : lemma1)

![그림6](/assets/img/timeseries/Informer/lemma1.png)

- 그렇게 찾은 useful query들만 가지고, 이제는 모든 keys와 attention을 계산한다

### Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation

![그림7](/assets/img/timeseries/Informer/fig3.png)

- We use the distilling operation to privilege the superior ones with dominating features and make a focused self-attention fea- ture map in the next layer.
- Distilling procedure : $$\mathbf{X}_{j+1}^t=\operatorname{MaxPool}\left(\operatorname{ELU}\left(\operatorname{Conv1d}\left(\left[\mathbf{X}_j^t\right]_{\mathrm{AB}}\right)\right)\right)$$

### Decoder: Generating Long Sequential Outputs Through One Forward Procedure

- Transformer의 Masked-attention과 Encoder-Decoder Attention 대신 **generative inference**

  - Decoder의 input : $$\mathbf{X}_{\mathrm{de}}^t=\operatorname{Concat}\left(\mathbf{X}_{\text {token }}^t, \mathbf{X}_{\mathbf{0}}^t\right) \in \mathbb{R}^{\left(L_{\text {token }}+L_y\right) \times d_{\text {model }}}$$

  - Start token을 사용하는 대신, Target 직전 시점의 값 몇개를 start token으로 주고, 우리가 원하는 길이의 예측값을 한 번에 decoding

## 4. Experiments

![그림9](/assets/img/timeseries/Informer/table1.png)

## 5. Conclusion

- Informer
  - *ProbSparse* self- attention mechanism
  - Distilling operation : to handle the challenges of quadratic time complexity and quadratic mem- ory usage in vanilla Transformer
  - generative decoder alleviates : alleviates the limitation of tra- ditional encoder-decoder architecture