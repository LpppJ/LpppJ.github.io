---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Minimax Estimator and Stein's Paradox

## 1. Minimax Estimator

- Minimax estimator는 최악이 가장 좋은 estimator이다.
- 최악이라는 말은 true parameter $$\theta$$에 대한 estimator의 risk이다.
  - Risk : $$R(\theta, \widehat{\theta}(X))=\mathbb{E}_{X \sim f_\theta}[(\widehat{\theta}(X) - \theta)^2]$$
- 즉 Minimax estimator $$\hat \theta$$는 아래를 만족하는 estimator이다.
  - : $$\sup _{\theta \in \Theta} R(\theta, \widehat{\theta})=\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta})$$

### Boundig the Minimax Risk

- Minimax Risk의 Upper bound와 Lower bound에 대해서 알아보자

- Upper bound는 그냥 다른 estimator의 maximum risk를 사용하면 된다.

- Lower bound를 찾는 방법은 아래와 같다.

  - Bayes estimator의 Bayes risk는 (어떤 prior $$\pi$$를 사용하더라도) minimax risk의 lower bound가 된다.
  - 수식으로 표현하면 $$B_\pi\left(\widehat{\theta}_{\text {low }}\right) \leq B_\pi\left(\hat{\theta}_{\text {minimax}}\right) \leq \sup _\theta R\left(\theta, \hat{\theta}_{\text {minimax}}\right)=\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta})$$

  - 직관적인 의미는 minimax risk는 risk의 maximum을 고려하지만, bayes risk는 risk를 prior를 가중치로 한 weighted average이다.
  - 평균은 최댓값보다 작다.

### Example : d-dim Gaussian

- $$X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, I_d\right)$$, then the average $$\widehat{\theta}=\frac{1}{n} \sum_{i=1}^n X_i$$가 minimax estimator of $$\theta$$ w.r.t the squared loss임을 보이자.

- Upper bound

  - $$\widehat{\theta} \sim N\left(\theta, I_d / n\right)$$의 Risk는 $$R(\theta, \widehat{\theta})=\mathbb{E}\left[\sum_{i=1}^d\left(\hat{\theta}_i-\theta_i\right)^2\right]=\mathbb{E}\left[\sum_{i=1}^d Z_i^2\right]=\frac{d}{n}$$이므로
  - $$\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta}) \leq R(\theta, \widehat{\theta})=\frac{d}{n}$$이다.

- Lower bound

  - 먼저 Bayes estimator를 구하고 bayes risk를 구할 것이다.
  -  $$\theta \in \mathbb R$$이므로 prior는 $$N(0, c^2I)$$를 사용한다. $$c^2$$가 매우 크면 non-informative prior이다.
  - Bayes estimator는 $$\hat \theta_{\text{bayes}}=\frac{c^2}{c^2+\frac{1}{n}}\hat \theta$$이다.
  - $$R\left(\theta, \widehat{\theta}_{\text {Bayes }}\right)=\mathbb{E}_{X_1, \ldots, X_X \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, I_d\right)}\left\|\frac{c^2}{c^2+1 / n} \widehat{\theta}-\theta\right\|^2 = \frac{\|\theta\|_2^2}{n^2 \beta^2}+\frac{c^4}{\beta^2} \frac{d}{n}$$
    - where $$\widehat{\theta}=\theta+W$$ and $$W \sim N\left(0, I_d / n\right)$$, and $$\beta:=c^2+1 / n$$
  - 이제 Bayes risk는 다음과 같다.
    - $$\begin{aligned}
      B_\pi\left(\frac{c^2}{c^2+1 / n} \widehat{\theta}\right) & =\mathbb{E}_{\theta \sim \pi}\left[R\left(\theta, \widehat{\theta}_{\text {Bayes }}\right)\right] \\
      & =\frac{c^2 d}{n^2 \beta^2}+\frac{c^4}{\beta^2} \frac{d}{n}=\frac{c^2 d}{n \beta}=\frac{d}{n\left(1+1 /\left(n c^2\right)\right)}
      \end{aligned}$$
  - 그러므로 $$\frac{d}{n\left(1+1 /\left(n c^2\right)\right)} \leq R_n \leq \frac{d}{n}$$인데 c는 arbitrary이니까 무한대로 보낼 수 있다.
  - upper bound와 lower bound가 모두 $$\frac{d}{n}$$이므로 $$\hat \theta$$는 minimax estimator이다.

## 2. Stein's Paradox

- 먼저 admissible의 의미를 이해해야 한다.

  - Estimator가 admissible하다는 것은
  - 모든 true parameters에 대해 다른 estimator보다 risk가 작거나 같고,
  - 적어도 하나의 true parameters에 대해 다른 estimator보다 risk가 작다는 뜻이다.

- 모든 Minimax estimator가 admissible한 것은 아니다. 특히 parameter의 dimension이 3 이상일 때 그렇다.

- 예를 들어 $$Y_i \sim N\left(\theta_i, 1\right) \quad \text { for } i \in\{1, \ldots, d\}$$ 세팅에서는 minimax estimator는 $$\widehat{\theta}=Y=\left(Y_1, \ldots, Y_d\right)^{\top}$$이다. 

- 하지만 James-Stein estimator $$\widehat{\theta}_{\mathrm{JS}}=\left(1-\frac{d-2}{\|Y\|^2}\right) Y$$는 $$\mathbb{E}\left[\left\|\widehat{\theta}_{\mathrm{JS}}-\theta\right\|^2\right]=d-(d-2)^2 \mathbb{E}\left[\frac{1}{\|Y\|^2}\right]<d$$​이다.

  - 즉 James-Stein estimator의 risk가 더 작기 때문에 minimax estimator가 admissible하지 않다.

- **Proof**

  - Squared term을 전개하면 $$\mathbb{E}\left[\left\|\widehat{\theta}_{\mathrm{JS}}-\theta\right\|^2\right]=\mathbb{E}\left[\|Y-\theta\|^2\right]+\mathbb{E}\left[\frac{(d-2)^2}{\|Y\|^2}\right]-2(d-2) \sum_{i=1}^d \mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]$$

  - 가장 우측에 있는 $$\mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]$$를 보자

    - $$\|Y-\theta\|^2 \sim \chi^2(d)$$이므로 expectation이 d이다.

    - Gaussian setting이므로 $$\mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]=\int \cdots \int \underbrace{\frac{y_i}{\|y\|^2}}_{=g_i(y)} \underbrace{\frac{y_i-\theta}{(2 \pi)^{d / 2}} e^{-\|y-\theta\|^2 / 2}}_{=-\frac{\partial f(y)}{\partial y_i}} d y_1 \cdots d y_d$$이다.

    - 아래와 같은 계산 과정을 통해 $$\sum_{i=1}^d \mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]=(d-2) \mathbb{E}\left[\frac{1}{\|Y\|^2}\right]$$이다.

      $$\begin{aligned}
      \mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right] & =\int \cdots \int \frac{\partial g_i(y)}{\partial y_i} f(y) d y_1 \cdots d y_d \\
      & =\int \cdots \int \frac{\|y\|^2-2 y_i^2}{\|y\|^4} f(y) d y_1 \cdots d y_d \\
      & =\mathbb{E}\left[\frac{1}{\|Y\|^2}\right]-\mathbb{E}\left[\frac{2 Y_i^2}{\|Y\|^4}\right]
      \end{aligned}$$

    - 이걸 첫 줄의 식에 대입하면 증명 끝이다.