---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2023](https://arxiv.org/pdf/2305.10721.pdf)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---


# Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping

- 이전에 review했던 [DLinear](https://arxiv.org/abs/2205.13504) paper에서는 'Time series의 properties가 무엇인지', 그리고 '왜 Transformer-based model이 완벽할 수 없는지'에 집중했다면, 본 논문은 '왜 linear mapping이 단순한데도 성능이 좋은지'에 집중한다. 

## Abstract

- Linear mapping is critical to prior long-term time series forecasting efforts
- RevIN (reversible normalization) and CI (Channel Independent) play a vital role for performance
- Linear mapping can effectively capture periodic features in TS

## 1. Introduction

- Transformer-based model
  - Non-autoregressive methods (to capture long-term temporal correlations)
  - [Informer(AAAI 2021)](https://arxiv.org/pdf/2012.07436.pdf), [Autoformer(NeurIPS 2021)](https://arxiv.org/pdf/2106.13008.pdf), [Fedformer(PMLR 2022)](https://arxiv.org/pdf/2201.12740.pdf), [Pyraformer(ICLR 2022)](https://openreview.net/pdf?id=0EXmFzUn5I), [Scaleformer(ICLR 2023)](https://arxiv.org/pdf/2206.04038.pdf), [Crossformer(ICLR 2023)](https://openreview.net/pdf?id=vSVLM2j9eie)
  - However, autoregressive에 비해 non-autoregressive의 성능이 좋았던 것이지 transformer가 TS forecasting에 효과적인 것은 아님 [DLinear(AAAI 2022)](https://arxiv.org/pdf/2205.13504.pdf)

- Subsequent approaches (patching)
  - Encoder-decoder(ex.transformer) 구조를 버리고 temporal feature extractor 모델링 (attention을 안쓴 건 아님)
  - [SCINet(NeurIPS 2022)](https://proceedings.neurips.cc/paper_files/paper/2022/file/266983d0949aed78a16fa4782237dea7-Paper-Conference.pdf), [PatchTST(ICLR2023)](https://arxiv.org/pdf/2211.14730.pdf), [MTS-Mixers(Arxiv 2023)](https://arxiv.org/pdf/2302.04501.pdf), [Timesnet(ICLR 2023)](https://openreview.net/pdf?id=ju_Uqw384Oq)
  - But, adjustable hyper-parameters가 너무 많이 필요하다.

- 본 논문의 Questions
  - (1) Are temporal feature extractors effective for long-term time series forecasting?
  - (2) What are the <u>underlying mechanisms</u> explaining the effectiveness of <u>linear mapping</u> in time series forecasting?
  - (3) What are the limits of linear models and how can we improve them?

## 2. Problem Definition and Experimental Setup

- $$\mathbf{X}=\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n\right] \in \mathbb{R}^{c \times n}$$ 으로 $$\mathbf{Y}=\left[\boldsymbol{x}_{n+1}, \boldsymbol{x}_{n+2}, \ldots, \boldsymbol{x}_{n+m}\right] \in \mathbb{R}^{c \times m}$$ 예측(mapping)하는 함수 $$\mathcal{F}: \mathbf{X}^{c \times n} \mapsto \mathbf{Y}^{c \times m}$$ 를 학습
- [PatchTST(ICLR2023)](https://arxiv.org/pdf/2211.14730.pdf)와 동일한 dataset split, Adam optimizer, Nvidia V100 32GB GPU 사용

## 3. Are Temporal Feature Extractors Effective ?

- TSF 일반적인 Framework는 **RevIN** $$\to$$ **temporal feature extractor**(Attention, MLP, Conv, ...) $$\to$$ **linear projection**
  - 다른 모델들의 temporal feature extractor를 살펴보면 PatchTST(attention), MTS-Mixers(MLP), TimesNet(conv), SCINet(conv), ...

![사진1](/assets/img/timeseries/RTSF/fig2.png)

- Fig2 - RLinear : linear projection layer with RevIN
- Fixed random extractor : only initialize the temporal feature extractor randomly and do not update its parameters in the training phase
- Fig2는 RevIN이 예측 성능을 향상시킨다 정도를 보여줄 뿐. simple linear layer가 RevIIN 도움 받으면 PatchTST보다 성능이 좋다.

![사진2](/assets/img/timeseries/RTSF/fig3.png)

- Fig3는 복잡한 모델들이 결국에는 가장 왼쪽에 있는 simple linear layer의 weights와 비슷한 weights를 학습하게 됨을 보여준다.

## 4. Theoretical and Empirical Study on the Linear Mapping

### 4.1. Roles of Linear Mapping in Forecasting

- **Linear mapping learns periodicity**
  : single linear layer는 periodicity를 학습할 수 있다. (trend는 잘 학습하지 못한다.)

  아래 가정들과 정리들의 의미를 이해해보자.

  - single linear layers : $$\mathbf{Y}=\mathbf X \mathbf{W}+\mathbf{b}$$  라 하자.

  - **Assumption 1**. A general time series $$x(t)$$ can be disentangled into seasonality part $$s(t)$$ and trend part $$f(t)$$ with tolerable noise, denoted as $$x(t)=s(t)+f(t)+\epsilon$$

    - 즉 시계열 = seasonality + trend + noise로 분해할 수 있다는 의미이다.

  - **Theorem 1**. Given a seasonal time series satisfying $$x(t)=s(t)=s(t-p)$$ where $$p \leq n$$ is the period, there always exists an analytical solution for the linear model as
    - $$\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n\right] \cdot \mathbf{W}+\mathbf{b}=\left[\boldsymbol{x}_{n+1}, \boldsymbol{x}_{n+2}, \ldots, \boldsymbol{x}_{n+m}\right]$$ 이고
    - $$\mathbf{W}_{i j}^{(k)}=\left\{\begin{array}{ll} 1, & \text { if } i=n-k p+(j \bmod p) \\ 0, & \text { otherwise } \end{array}, 1 \leq k \in \mathbb{Z} \leq\lfloor n / p\rfloor, b_i=0\right.$$ 이다.
    - input historical sequence의 길이가 주기보다 길다면 linear mapping은 periodicity를 예측할 수 있다는 의미이다.
    - 하지만 위 정리는 $$x(t)=s(t)=s(t-p)$$ 즉 trend가 없는 경우에 해당한다.
    - ![사진3](/assets/img/timeseries/RTSF/fig4.png)
    - Fig4는 linear model이 seasonality는 잘 학습하지만 trend가 있을 때에는 성능이 좋지 못함을 보여준다.

  - **Theorem 2**. Let $$x(t)=s(t)+f(t)$$ where $$s(t)$$ is a seasonal signal with period $$p$$ and $$f(t)$$ satisfies $$K$$-Lipschitz continuous. Then there exists a linear model as $$\mathbf{Y}=\mathbf X \mathbf{W}+\mathbf{b}$$ with input horizon size $$n=p+\tau, \tau \geq 0$$ such that $$\mid x(n+j)-\hat x(n+j) \mid \leq K(p+j), j=1, \ldots, m$$.

    - linear model의 trend term에 대한 forecasting error의 upper bound를 제시하는 정리이지만, trend에 대해 성능이 좋지 않다는 건 여전하다.
    - ![사진4](/assets/img/timeseries/RTSF/proof.png)

### **4.2. Disentanglement and Normalization**

- **Problems in Disentanglement**

  - 시계열에서 trend와 seasonality를 분리할 수 있으면 성능을 높일 수 있을 것

    - moving average(by an average pooling layer with a sliding window)로 trend를 분리할 수 있다.

    - 하지만, sliding window의 크기가 seasonality의 최대 주기보다 커야만 효과적이고

      average pooling layer를 사용할 때 input TS의 양 끝에 padding을 해줘야 하는데, 그러면 원본 시퀀스가 왜곡된다.

- **Turning trend into seasonality**
  - Disentanglement의 핵심은 원본 TS에서 movind average를 빼는 것인데, 이건 normalization과 관련이 있다.
    - TS의 statistical information(평균, 분산)은 distribution shift로 인해 계속 바뀌기 때문에 RevIN이 사용된다.
    - RevIN : Normalization the input $$\to$$ Forecasting module $$\to$$ Denormalization the output
  - 하지만 input 데이터에 directly normalization하면 오히려 statistical information을 지우는 것과 같다.
  - ![사진5](/assets/img/timeseries/RTSF/fig5.png)
  - RevIN의 경우에는 scaling을 하지만 periodicity는 바꾸지 않는다. 그리고 reversible하다.
  - ![사진6](/assets/img/timeseries/RTSF/fig6.png)
  - ![사진7](/assets/img/timeseries/RTSF/fig7.png)
  - RevIN은 continuously changing trends를 multiple segments with a fixed and similar trend로 바꾼다.
  - 그러면 accumulated timesteps in the past로 인한  errors in trend prediction이 완화된다.

## 5. Experimental Evaluation

### 5.1 Comparison on Real-world Datasets

![사진8](/assets/img/timeseries/RTSF/table3.png)

- learning of periodicity via linear mapping, 그리고 efficiency of reversible normalization 덕분에 well-designed models보다 RLinear의 성능이 좋다.

### 5.2. When Linear Meets Multiple Periods among Channels

![사진9](/assets/img/timeseries/RTSF/table4.png)

-  Multi-channel datasets의 경우에는 Channel Independent(CI) modeling으로 TS의 각 채널을 독립적으로 처리할 때 성능이 좋다.

![사진10](/assets/img/timeseries/RTSF/fig10.png)

- Channels가 많아지면 channel마다 periodicity가 달라져서 예측이 어려워지는데 input horizon을 늘리면 완화된다. 

## 6. Conclusion

- Linear mapping와 foreacsting methods가 학습하는 것은 비슷하다. (input historical observations에서 periodic patterns)
- RevIN과 Channel Independent는 periodicity를 단순하게 만들어 학습을 용이하게 하므로 성능 향상에 필요하다.
- Linear mapping은 MTS에 대해서도 input horizon만 충분하다면 예측 성능이 뛰어나다.





