---
layout: post
related_posts:
  _
title: 
description: >
  [AAAI 2022](https://arxiv.org/abs/2106.10466)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# TS2Vec: Towards Universal Representation of Time Series (AAAI 2022)

## Abstract
- 본 논문에서 풀고자 하는 문제는 unsupervised TS representation(TS $$\to$$ vector)
- TS를 arbitrary semantic level에서 representation하는 방법을 학습하는 framework
- augmented context views를 hierarchical하게 contrastive learning한다.
- (선행연구([TNC](https://arxiv.org/abs/2106.00750), [T-Loss](https://arxiv.org/abs/1901.10738))를 읽기 전이라면 abstract에서 등장하는 단어들의 의미가 와닿지 않을 수 있다.)

## 1. Introduction
- 이미 instance-level representation을 학습하는 연구들도 있었고([TNC](https://arxiv.org/abs/2106.00750), [T-Loss](https://arxiv.org/abs/1901.10738)),
- contrastive loss를 사용해서 TS의 구조를 학습하는 연구들도 있었다.([TS-TCC](https://arxiv.org/abs/2106.14112), [T-Loss](https://arxiv.org/abs/1901.10738))
- 하지만 지금까지의 방법들의 한계는 아래 3가지로 정리할 수 있다.
- 첫째, instance-level representation은 fine-grained representation(e.g. forecasting, anomaly detection)에 적합하지 않다.
  - 왜냐하면 specific한 timestamp, sub-series를 타겟으로 inference해야 하는데, coarse-grained representation으로는 충분하지 않다.
- 둘째, 다양한 granularities에서의 multi-scale contextual information을 파악하기 어렵다.
  - granularity가 높을수록 더 자세한 정보를 포함 (일별 < 시간별 < 분별 < 초별 ...)
  - multi-scale information은 다양한 granularities에서 나타나는 정보. Scale에 따라 달라질 수는 있지만, representation의 generalization capability를 향상시킨다는 점에서 TS task에서 필수적인 정보이다.
- 셋째, CV, NLP에서의 unsupervised representation은 강한 inductive bias가 있는데, TS는 그렇지 않다.
  - transformation-invariant : 강아지 사진은 거꾸로 뒤집어도 강아지 사진이지만, TS는 거꾸로 뒤집으면 아예 다른 데이터가 된다.
  - cropping-invariant : 사진이나 문장은 일부분을 잘라도 본질적인 정보가 바뀌지 않는 경우가 많아 augmentation 방법으로 쓰이지만, TS는 일부분을 자르면 패턴이나 분포 자체가 달라지게 된다.
- 본 논문에서 제시하는 TS2Vec은 universal contrastive learning framework를 제안
  - maxpooling으로 다양한 granularity의 overall representation을 얻고
  - instance-wise and temporal dim에서의 hierarchically contrastive learning을 통해
  - all-semantic level에서의 representation을 얻는다.

## 2. Method

### 2.1. Problem Definition
- N개의 시계열 $$\mathcal{X}=\left\{x_1, x_2, \cdots, x_N\right\}$$에 대해서 nonlinear embedding function $$f_\theta : x_i \in \mathbb{R}^{T \times F} \to r_i=\left\{r_{i, 1}, r_{i, 2}, \cdots, r_{i, T}\right\} \in \mathbb{R}^{T \times K}$$를 학습한다.

### 2.2. Model Architecture
![사진1](/assets/img/timeseries/ts2vec/fig1.png)
- **sub-series** : input TS $$x_i$$에서 2개의 sub-series를 랜덤하게 sampling한다.
  - 겹치는 부분이 있도록 (겹치는 부분의 contextual representation이 consistent하도록 할거니까)
- **Encoder** $$f_\theta$$는 3개의 modules로 구성
  - **projection layer** : t시점의 input을 high-dim latent vector로 mapping하는 FC layer. $$x_{i,t} \to z_{i,t}$$
  - **timestamp masking module** : latent vectors의 random한 timestamps를 masking(=0)해서 augmented context view를 생성
  - **dilated CNN module** : 각 timestamp의 contextual representation을 extract. 이 때 1d dilated conv layer를 사용하는데, dilation parameter를 다양하게 해서($$2^l$$ for $$l$$-th block) larger receptive field

![사진11](/assets/img/timeseries/ts2vec/dilatedconv1.png)
source : [WAVENET: A GENERATIVE MODEL FOR RAW AUDIO](https://arxiv.org/pdf/1609.03499v2.pdf)
{:.figcaption}

### 2.3. Contextual Consistency
- Contrastive learning을 위한 positive pair를 만드는 전략들을 소개한다.
  ![사진2](/assets/img/timeseries/ts2vec/fig2.jpeg)
- **Subseries consistency** : 서로 sub-series 관계인 segments를 positive pair로 설정하고 representation을 가깝게 학습
- **Temporal consistency** : 인접한 시점의 segments를 positive pair로 설정
- **Transformation consistency** : scaling, permutation과 같은 transformation에 invariant한 representation을 학습
  ![사진3](/assets/img/timeseries/ts2vec/fig3.jpeg)
- 하지만 fig3을 보면 위와 같은 전략들이 시계열 데이터에는 적절하지 않다. sub-series라고 해서, 인접한 시점이라고 해서 패턴이 같은 것은 아니다.
- **Contextual consistency** : 본 논문에서 제시하는 방법으로, 동일한 timestamp에 대해 random masking이나 random cropping으로 생성한 contexts를 positive pair로 설정한다.
  - **Timestamp masking** : 각 timestamp에 대해 latent vector $$z_i = z_{i,t}$$를 $$p=0.5$$ bernoulli masking
  - **Random cropping** : input $$x_i \in \mathbb R^{T \times F}$$에 대해 $$0 < a_1 \le a_2 \le b_1 \le b_2 \le T$$를 만족하는 segments $$[a_1, b_1], [a_2, b_2]$$를 random하게 만들고, 각 segment에 대해 overlapping segment인 $$[a_2, b_1]$$의 contextual representation이 consistent해지도록 학습한다.
  - Masking과 random cropping은 시계열의 magnitude를 바꾸지도 않으면서, 각 timestamp에 대해 복원하도록 학습시키기 때문에 robust한 representation learning 방식이다.

### 2.4. Hierarchical Contrasting
- **Hierarchical contraastive loss** : 본 논문에서 제시하는 학습 방식으로, 다양한 scales에서의 representation을 학습하기 위한 loss이다. (scales가 다양하기 때문에 max-pooling을 사용한다.)
  - instance-wise & temporal contrastive losses 모두 leverage하는데, 이걸 모든 granularity levels에 대해서 hierarchical하게 적용한다.
- **Temporal Contrastive Loss**
  - $$\ell_{t e m p}^{(i, t)}=-\log \frac{\exp \left(r_{i, t} \cdot r_{i, t}^{\prime}\right)}{\sum_{t^{\prime} \in \Omega}\left(\exp \left(r_{i, t} \cdot r_{i, t^{\prime}}^{\prime}\right)+\mathbb{1}_{\left[t \neq t^{\prime}\right]} \exp \left(r_{i, t} \cdot r_{i, t^{\prime}}\right)\right)}, \quad \Omega \text{ is the set of timestamps}$$이다.
  - 동일한 input $$x_i$$에 대해서, timestamp가 같으면 positive이고 다르면 negative이다.
  ![사진5](/assets/img/timeseries/ts2vec/myfig1.jpeg)
  - 위 그림에서 빨강색이 분모에 포함되는 timestamps이고, 파랑색이 분자에 해당하는 timestamp이다. 같은 input에 대해 서로 다른 augmentation의 같은 timestamp가 가깝게 representation되도록 학습한다는 의미이다.
- **Instance-wise Contrastive Loss**
  - $$\ell_{i n s t}^{(i, t)}=-\log \frac{\exp \left(r_{i, t} \cdot r_{i, t}^{\prime}\right)}{\sum_{j=1}^B\left(\exp \left(r_{i, t} \cdot r_{j, t}^{\prime}\right)+\mathbb{1}_{[i \neq j]} \exp \left(r_{i, t} \cdot r_{j, t}\right)\right)}, \quad B \text{ is the batch size}$$이다.
  - 한 시점 t와 모든 instance(input)에 대해 같은 instance이면 positive이고 다른 instance이면 negative이다.
  ![사진6](/assets/img/timeseries/ts2vec/myfig2.jpeg)
  - 역시 빨강색이 분모에 포함되는 instance이고, 파랑색이 분자에 해당하는 instance이다. 한 timestamp에 대해 같은 instance의 augmentation가 가깝게 representation되도록 학습한다는 의미이다.
- 두 losses는 complementary하다. (예를 들어 다수의 전기 사용량 시계열 데이터라면, instance contrast는 user-specifc 정보를, temporal contrast는 시간에 따른 dynamic trends를 학습한다.)
- The overall loss : $$\mathcal{L}_{\text {dual }}=\frac{1}{N T} \sum_i \sum_t\left(\ell_{\text {temp }}^{(i, t)}+\ell_{\text {inst }}^{(i, t)}\right)$$

## 3. Experiments
![사진7](/assets/img/timeseries/ts2vec/table2.jpeg)
![사진8](/assets/img/timeseries/ts2vec/fig5.png)
- Informer는 trends는 학습했지만 주기적 패턴을 학습하지 못했고, TCN은 반대로 주기적 패턴은 학습했지만 trends는 학습하지 못했다. (coarse-grained vs fine-grained 둘 다 잘 학습하기 어려움)

## 4. Analysis
![사진9](/assets/img/timeseries/ts2vec/table5.png)
- Ablation study를 통해 components를 justify하였다.
![사진10](/assets/img/timeseries/ts2vec/fig7.png)
- Heatmap을 통해 급작스러운 spike에 대해서도 적절하게 representation할 수 있음을 확인하였다.

## 5. Conclusion
- TS2Vec : universial representation learning framework
  - hierarchical contrasting을 통해 scale-invariant representation을 학습하였고,
  - instance-wise contrasting과 temporal contrasting으로 loss를 설정하였다.
- Ablation study를 통해 모델의 components가 모두 필요함을 보여주었다.