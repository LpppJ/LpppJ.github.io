---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# MCMC with Implementation (2) : Gibbs Sampling

## 1. Introduction
- 만약 sampling하고자 하는 parameter의 full conditional distribution을 알 수 있다면 Gibbs sampler를 사용할 수 있다.
- Gibbs sampler는 MH-algorithm의 special case라고 할 수 있다.
  - Acceptance rate = 1이 되어 accept/reject 과정이 없다.
- **Full conditional distribution** : 만약 model parameters가 $$\mathbf{\theta}=\left(\theta_1, \cdots, \theta_k\right)$$이라면, full conditional distribution은 $$\pi\left(\theta_i \mid \theta_1, \cdots, \theta_{i-1}, \theta_{i+1}, \cdots, \theta_k, \mathbf{X}\right)$$이다. 즉 다른 parameters는 모두 given일 때 관심있는 parameter의 분포를 의미한다.
- 정확히는 full conditional distribution이 모두 closed-form이어야 한다. 즉 k개의 conditional distributions가 standard한 분포(Normal, Gamma, ...)를 따른다.
- 그러므로 Gibss sampling 방식은 k개의 parameters로 이루어지는 확률변수 $$\mathbf{\theta}$$를 sampling을 할 때 k개 parameters를 한 번에 sampling하는 것이 아니라, 하나씩 sampling한 다음 k개를 모아서 하나의 sample로 만든다.
  - 1. Choose starting values $$\mathbf{\theta}=\left(\theta_1^{(1)}, \cdots, \theta_k^{(1)}\right)$$
  - 2. For $$i=2, \cdots, T$$ sample \
    $$
    \begin{aligned}
    & \theta_1^{(i)} \mid \theta_2^{(i-1)}, \cdots, \theta_k^{(i-1)}, \mathbf{X} \\
    & \theta_2^{(i)} \mid \theta_1^{(i)}, \theta_3^{(i-1)}, \cdots, \theta_k^{(i-1)}, \mathbf{X} \\
    & \theta_k^{(i)} \mid \theta_1^{(i)}, \cdots, \theta_{k-1}^{(i)}, \mathbf{X}
    \end{aligned}
    $$
- 주의할 점은 $$\theta_2^{(i)}$$를 sampling할 때에는 아직 sampling하지 않은 $$\theta_3^{(i)}$$부터 $$\theta_k^{(i)}$$까지는 이전 시점의 값을 사용하지만, 이미 sampling을 한 $$\theta_1^{(i)}$$은 sampling한 값을 사용한다는 점이다.

## 2. Full conditional distribution for Gibb Sampler
- 다음과 같은 간단한 Bayse rule을 보자. $$P(A \mid B)=\frac{P(A, B)}{P(B)}=\frac{P(A, B)}{P(A)} \times \frac{P(A)}{P(B)}=P(B \mid A) \times \frac{P(A)}{P(B)}$$
  - A를 sampling할 parameters, B를 주어진 데이터라고 하면 $$P(B \mid A)$$는 likelihood이고, $$P(A)$$는 prior가 된다. 그리고 $$P(B)$$는 주어진 상수가 되어 고려하지 않아도 된다. 왜냐하면 결국 A가 바뀔 때 posterior $$P(A \mid B)$$의 대소관계가 궁금하기 때문이다.
  - 그러므로 $$\text{posterior} \propto \text{likelihood} \times \text{prior}$$ 관계가 성립한다.

## 3. R Implementation of Gibb Sampler

### 3.1. Ex.1 : Bayesian Linear Regression
$$\begin{gathered}
Y_i=\beta_0+\beta_1 X_{1, i}+\beta_2 X_{2, i}+\beta_3 X_{1, i} X_{2, i}+\epsilon_i  \\
\text{where } \epsilon_i \sim N\left(0, \sigma^2\right) \text{independently for } i=1, \cdots, 300,000 \\
\text{We will use independent priors as} \\
\beta_j \sim N(0,10) \text { for } j=0,1,2,3 \\
\sigma^2 \sim \operatorname{IG}(0.01,0.01)
\end{gathered}$$
- 위 모형에서 임의의 parameters로 데이터를 생성하고, Gibbs sampler가 생성한 parameters samples와 임의로 정한 parameters를 비교한다.
- **Simulation the dataset**
  ~~~r
  # the size of dataset
  n = 300000

  # Simulating (X_1,1, ..., X_1,n) and (X_2,1, ..., X_2,n) ~ N(0,1)
  X <- mvrnorm(n =n, mu = rep(0, 2), Sigma = diag(2))
  
  # Setting the true parameters (beta)
  beta.true <- c(0.5, 1, 2, -1)

  # Simulating (Y_1, ..., Y_n) using the design matrix for above model
  X_design <- cbind( rep(1,n), X, X[,1]*X[,2] )
  Y <- X_design %*% beta.true + rnorm(n, 0, 1)
  ~~~
- **Calculation the full conditional distribution**
  - For $$\beta$$, the kernel of posterior for Gibbs sampler is following :
  $$\begin{aligned}
  & P\left(\beta \mid Y, X, \sigma^2\right) \propto L\left(Y, X \mid \beta, \sigma^2\right) P(\beta) \\
  & \propto\left(\frac{1}{\sqrt{2 \pi}}\right)^n\left(\frac{1}{\operatorname{det}\left(\sigma^2 I\right)}\right)^{\frac{1}{2}} \exp \left(-\frac{(Y-X \beta)^{\top}(Y-X \beta)}{2 \sigma^2}\right)\left(\frac{1}{\sqrt{2 \pi}}\right)^p\left(\frac{1}{\operatorname{det}(10 I)}\right)^{\frac{1}{2}} \exp \left(-\frac{\beta^{\top} \beta}{20}\right) \\
  & \propto \exp \left(-\frac{(Y-X \beta)^{\top}(Y-X \beta)}{2 \sigma^2}-\frac{\beta^{\top} \beta}{20}\right) \\
  & =\exp \left(-\frac{1}{2 \sigma^2} Y^{\top} Y+\frac{1}{2 \sigma^2} Y^{\top} X \beta+\frac{1}{2 \sigma^2} \beta^{\top} X Y-\frac{\beta^{\top} X^{\top} X \beta}{2 \sigma^2}-\frac{\beta^{\top} \beta}{20}\right) \\
  & =\exp \left(-\frac{1}{2}\left(\beta^{\top}\left(\frac{X^{\top} X}{\sigma^2}+\frac{I}{10}\right) \beta-\beta^{\top}\left(\frac{X^{\top} Y}{\sigma^2}\right)-\left(\frac{Y^{\top} X}{\sigma^2}\right) \beta+\frac{Y^{\top} Y}{6^2}\right)\right) \\
  & \therefore V_\beta=\left[\frac{X^{\top} X}{\sigma^2}+\frac{I}{10}\right]^{-1}, \quad m_\beta=V_\beta \frac{X^{\top} Y}{\sigma^2} \\
  & \beta \sim N\left(m_\beta, V_\beta\right) \\
  \end{aligned}$$
- For $$\sigma^2$$, the kernel of posterior for Gibbs sampler is following :
  $$\begin{aligned}
  P\left(\sigma^2 \mid, \beta\right) & \propto L\left(Y, X \mid \beta, \sigma^2\right) P\left(\sigma^2\right) \\
  & \propto\left(\frac{1}{\sigma \sqrt{2 \pi}}\right)^n \exp \left(-\frac{(Y-X \beta)^{\top}(Y-X \beta)}{2 \sigma^2}\right) \frac{\beta^\alpha}{\Gamma(\alpha)}\left(\frac{1}{\sigma^2}\right)^{\alpha+1} \exp \left(-\frac{\beta}{\sigma^2}\right) \\
  & \propto\left(\frac{1}{\sigma^2}\right)^{\frac{n}{2}+\alpha+1} \exp \left(-\frac{\beta+\frac{1}{2}(Y-X \beta)^{\top}(Y-X \beta)}{\sigma^2}\right) \\
  & \\
  & \therefore \alpha_{\sigma^2}=\frac{n}{2}+\alpha, \quad \beta_{\sigma^2}=\beta+\frac{1}{2}(Y-X \beta)^{\top}(Y-X \beta) \\
  & \sigma^2 \sim I G\left(\alpha_{\sigma^2}, \beta_{\sigma^2}\right)
  \end{aligned}$$
  - 위 결과를 바탕으로 sampling하는 R 코드를 작성한다.
  
  ~~~r
  # the number of variates
  p = ncol(X_design)

  # The prior parameters
  m.beta <- 0; v.beta <- 10 a.s2 <- 0.01; b.s2 <- 0.01
  
  # 각각의 parameters를 1000개씩 sampling한다.
  B = 1000

  # sample space with initialization for each parameters
  beta.samps <- matrix(NA, nrow = p, ncol = B)
  s2.samps <- matrix(NA, nrow = 1, ncol = B)
  beta.samps[,1] <- rep(1,p)
  s2.samps[1] <- 1

  # Gibbs sampler
  for(i in 2:B){
    
    ## beta[i] | s2[i-1]
    V <- solve( t(X_design) %*% X_design / s2.samps[i-1]+ diag(p) / v.beta )
    m <- V %*% ( t(X_design) %*% Y / s2.samps[i-1] )
    beta.samps[,i] <- rmvnorm(1, mean = m, sigma = V, method = "svd")
    
    # s2[i] | beta[i]
    a <- n/2 + a.s2
    b <- b.s2 + t( Y - X_design %*% beta.samps[,i] ) %*% ( Y - X_design %*% beta.samps[,i] ) / 2
    s2.samps[i] <- rinvgamma(1, shape=a, scale=b)
    s2.samps <- t(s2.samps)
  }
    # burn-in
    beta.samps <- beta.samps[,51:B]
    s2.samps <- s2.samps[51:B]
  ~~~
- **Checking the Samples**
  - density plot, trace plot 등 다양한 tools를 활용하여 결과를 확인해야 하지만, posterior mean(samples의 평균)만 확인한다. 상당히 정확하게 sampling 되었다는 것을 알 수 있다.

  ~~~r
  # posterior mean
  cat("posterior mean of beta0:", mean(beta.samps[1,]), "\n",
      "posterior mean of beta1:", mean(beta.samps[2,]), "\n",
      "posterior mean of beta2:", mean(beta.samps[3,]), "\n",
      "posterior mean of beta3:", mean(beta.samps[4,]), "\n",
      "posterior mean of sigma2:", mean(s2.samps) )

  ##### result #####
  ## posterior mean of beta0: 0.4976068
  ##  posterior mean of beta1: 0.9989775
  ##  posterior mean of beta2: 2.00346
  ##  posterior mean of beta3: -1.00101
  ##  posterior mean of sigma2: 0.9986912
  ~~~
  - Gibbs Sampler는 proposed sample이 항상 accept된다는 점에서 MH-algorithm의 speecial case라고 할 수 있다. samples의 unique value의 개수와 B=1000의 비율을 출력한다.

  ~~~r
  # acceptance ratio
  # (Because I used Gibbs sampler, samples was always accepted !)
  cat("Acceptance ratio of beta0 :", length(unique(beta.samps[1,]))/B ,"\n",
      "Acceptance ratio of beta1 :", length(unique(beta.samps[2,]))/B ,"\n",
      "Acceptance ratio of beta2 :", length(unique(beta.samps[3,]))/B ,"\n",
      "Acceptance ratio of beta3 :", length(unique(beta.samps[4,]))/B ,"\n",
      "Acceptance ratio of sigma2 :", length(unique(s2.samps))/B )
  
  ##### result #####
  ## Acceptance ratio of beta0 : 0.95
  ##  Acceptance ratio of beta1 : 0.95
  ##  Acceptance ratio of beta2 : 0.95
  ##  Acceptance ratio of beta3 : 0.95
  ##  Acceptance ratio of sigma2 : 0.95
  ~~~
  - 앞서 burn-in으로 각 parameters에서 50개의 samples를 버린 것을 감안하면 sample가 항상 accept되었음을 알 수 있다. (acceptance probabilties = 1)
    - Acceptance probabilies는 각 proposed sample이 accept될 확률이고
    - Acceptance ratio는 전체 sampling 횟수 중에서 accept된 비율을 의미한다.

### 3.2. Ex.2 : Truncated Exponential distribution
- to be continued...