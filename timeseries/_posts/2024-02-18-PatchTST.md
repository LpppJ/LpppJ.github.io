---
layout: post
related_posts:
  _
title: 
description: >
  ICLR 2023
sitemap: false
hide_last_modified: true
---

# (PatchTST) A Time Series Is Worth 64 Words: Long-term Forecasting With Transformers

## Abstract
- 논문에서 요약을 잘 해놔서 굳이 번역하지 않고 그대로 가져왔다.
- 2개의 Key components
  - Segmentation of time series into **subseries-level patches** which are served as input tokens to Transformer
  - **Channel-independence** where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series.
- Patching design의 장점 3가지
  - **local semantic information** is retained in the embedding
  - **computation and memory usage** of the attention maps are quadratically reduced
  - the model can attend **longer history**

## 1. Introduction
- Patching : 단일 time-step을 token으로 만들면 (`point-wise input token`) 시계열의 포괄적인 의미 정보를 파악할 수 없기 때문에, time-steps를 합쳐서 subseries-level patchs를 만들어 locality를 강화하고 포괄적인 의미 정보를 파악한다.
- Channel-independence : 각 token이 오직 하나의 채널(feature)의 정보만 담는 것이다. (반대로 `channel-mixing`은 token이 모든 채널(features)를 embedding space에 projection해서 정보를 섞는 방식이다.)
- PatchTST의 장점 3
  - Reduction on time and space complexity
  - Longer look-back window
  - Capability of representation learning

## 2. Related work
- Patching의 milestone은 ViT(2021)
- LogTrans(2019)
  - key, query는 point-wise 내적 안하지만 여전히 value는 single time step에 기반한다.
- Autoformer(2021)
  - patch level connection을 얻기 위해 auto-correlation을 사용하지만 handcrafted design이라서 패치 내 의미 정보를 모두 파악하기 어렵다.
- Triformer(2022)
  - patch attention을 제안하긴 하지만 patch를 input으로 사용하지 않는다는 점에서 의미 정보를 파악하기 어렵다.
- Unlabelled data로 인해 self-supervised learning이 많이 떴는데, transformer를 통해 time series에 적용하기 위한 representation을 학습하는 시도는 아직 완전하지 않다.

## 3. Proposed Method

### 3.1. Model Structure
- $$(\boldsymbol{x_{1}}, ..., \boldsymbol x_L)$$ 를 보고 $$(\boldsymbol x_{L+1}, ..., \boldsymbol x_{L+T})$$를 예측하는 문제이고, PatchTST는 transformer의 encoder를 핵심으로 한다.
![사진1](/assets/img/timeseries/PatchTST/fig1.jpeg)
- **Forward Process** : 시계열에 있는 M개의 변수가 있고 길이가 L이라고 할 때, $$i$$번째 series는 $$\boldsymbol{x}_{1:L}^{(i)}=(x_{1}^{(i)}, ... , x_{L}^{(i)})$$이다.
- M개의 $$\boldsymbol{x}^{(i)} \in \mathbb R^{1 \times L}$$ 가 각각 transformer backbone으로 들어가고 (channel-independence) 각각의 transformer는 $$\boldsymbol{\hat x}^{(i)} =(\hat x_{L+1}^{(i)}, ..., \hat x_{L+T}^{(i)})\in \mathbb R^{1 \times L}$$를 output으로 한다.
- **Patching** : 아래 그림처럼 univariate time series $$\boldsymbol{x}^{(i)}$$를 $$\boldsymbol{x}_p^{(i)} \in \mathbb R^{P \times N}$$으로 patching한다.
![사진2](/assets/img/timeseries/PatchTST/myfig1.jpeg)
- Input token의 개수가 $$L$$에서 $$N=\left\lfloor\frac{(L-P)}{S}\right\rfloor+2$$로 줄어들기 때문에, 사용할 수 있는 memory와 complexity가 확보되면서 더 긴 historical sequence를 볼 수 있어 성능이 향상된다.
- **Transformer Encoder** :
  - 1) Mapping to the transformer latent space : $$\boldsymbol{x}_d^{(i)}= \mathbf W_p\boldsymbol{x}_p^{(i)}+ \mathbf W_{pos}$$
    - where trainable linear projection $$\mathbf W_p \in \mathbb R^{D \times P}$$, learnable addictive position encoding $$\mathbf W_{pos} \in \mathbb R^{D \times N}$$)
  - 2) Multi-head attention (with Batchnorm and Residual connection)
  : `Query` $$Q_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^Q$$, `Key` $$K_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^K$$ and `Value` $$V_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^V$$
    - where $$\mathbf{W}_h^Q, \mathbf{W}_h^K \in \mathbb{R}^{D \times d_k}$$ and $$\mathbf W_h^V \in \mathbb R^{D \times D}$$
  - 3) Getting attention : $$\mathbf O_h^{(i)} \in \mathbb R^{D \times N}$$
    - where $$\left(\mathbf{O}_h^{(i)}\right)^T=\operatorname{Attention}\left(Q_h^{(i)}, K_h^{(i)}, V_h^{(i)}\right)=\operatorname{Softmax}\left(\frac{Q_h^{(i)} K_h^{(i)^T}}{\sqrt{d_k}}\right) V_h^{(i)}$$
  - 4) Flatten and Linear head : $$\boldsymbol{\hat x}^{(i)}=(\boldsymbol{\hat x}_{L+1}^{(i)}, ..., \boldsymbol{\hat x}_{L+T}^{(i)}) \in \mathbb R^{1 \times T}$$
- **Loss function** : MSE. $$\mathcal{L}=\mathbb{E}_{\boldsymbol{x}} \frac{1}{M} \sum_{i=1}^M\left\|\hat{\boldsymbol{x}}_{L+1: L+T}^{(i)}-\boldsymbol{x}_{L+1: L+T}^{(i)}\right\|_2^2$$
- **Instance Normalization** : Pathcing 전에 각 univariate time series에 `mean=0`, `std=1`하고, output prediction 전에 다시 더해준다.

### 3.2. Representation Learning
- Self-supervised representation learning 방법 중에서 masked autoencoder를 사용했다. (input sequence의 일부를 0으로 masking하고 recover하도록 모델링)
- 다만 이걸 그대로 Multivariate time series에 가져오면 두 가지 문제가 발생한다.
	- 첫째, single time step에 masking하면 맞추기가 너무 쉽다. (interpolating 하면 끝) 그래서 다양한 크기의 group of time series를 랜덤하게 masking하는 기존의 방법을 사용했다.
    - 둘째, 각 time step을 D차원으로 representation하면 $$z_t \in \mathbb R^D$$가 되니, parameter matrix $$\mathbf W$$의 차원이 $$(L\cdot D) \times (M\cdot T)$$이 되어 $$L, D, M, T$$ 중 하나만 커지더라도 oversieze가 된다. 그래서 PatchTST에서는 $$D \times P$$ size의 linear layer를 사용하였고, patch 단위로 masking을 했다.

## 4. Experiments

- 사용한 데이터셋은 9개(ETTm1/2, ETTh1/2, ILI, Weather, Traffic, Exchange, Electrictiy)이고 비교한 모델은 6개(FEDformer, Autoformer, Informer, Pyraformer, LogTrans +LTSF-Linear)이다.
![사진3](/assets/img/timeseries/AreTF/table12.jpeg)
![사진4](/assets/img/timeseries/PatchTST/table34.jpeg)
- PatchTST/64는 input patches 64개, look-back window size L=512이다.
	PatchTST/42는 input patches 42개, look-back window size L=336이다.
    두 버전 모두 patch length P = 16, stride S = 8이다.
    Masking ratio = 40%이다.
- 실험 결과 long-term forecasting에서 다른 transformer-based models 및 DLinear보다 성능이 뛰어났다.
![사진5](/assets/img/timeseries/PatchTST/table56.jpeg)
- Transfer learning task에서도 다른 모델들보다 성능이 뛰어났다.
![사진6](/assets/img/timeseries/PatchTST/table7.jpeg)
- Ablation study 결과 patching과 channel-independence 모두 성능에 중요한 역할을 하고 있음을 알 수 있다. 특히 patching의 motivation은 앞서 언급한 것처럼 직관적이다.
![사진7](/assets/img/timeseries/PatchTST/fig2.jpeg)
- 논문 `Are Transformer Effective for Time Series Forecasting?`에서 transformer-based models의 경우 look-back windows size가 커져도 예측 성능이 좋아지지 않는다고 주장했고, 이는 temporal information을 잘 못잡아내는 것이 맞다. 하지만 PatchTST는 look-back windows가 길어질수록 성능이 좋아지므로 해당사항이 없다.

## 5. Conclusion
- PatchTST의 key components 2가지는 : Patching과 Channel-independence이다.
- PatchTST는 longer look-back windows의 benefit을 가질 수 있으면서 local semantic information을 파악할 수 있다.