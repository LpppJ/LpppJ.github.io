---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Inequalities for Expectation and Variance

## 1. Inequalities for Expectation

## 1.1. Jensen's Inequality

- Suppose $$g$$ is a convex function such that $$\lambda g(x)+(1-\lambda) g(y) \geq g(\lambda x+(1-\lambda) y)$$ for all $$\lambda \in(0,1)$$ and $$x, y \in \mathbb{R}$$.
- Then $$\mathbb{E}[g(X)] \geq g(\mathbb{E}[X]),$$ provided that both expectations exist, i.e., $$\mathbb{E}{\left\vert X \right\vert}<\infty$$ and $$\mathbb{E}{\left\vert g(X) \right\vert}<\infty$$.

### Proof

- Assumption : $$g^{\prime \prime}(x)\ge0 \ \forall x$$
- Using Taylor series about $$\mu = \mathbb E[X]$$ of order 2
- $$g(x)=g(\mu)+g^{\prime}(\mu)(x-\mu)+\frac{g^{\prime \prime}(\zeta)}{2}(x-\mu)^2 \text { where } \zeta \text { is between } x \text { and } \mu$$이고
- $$g(x) \geq g(\mu)+g^{\prime}(\mu)(x-\mu)\quad (\because g^{\prime \prime}(x)\ge0 \ \forall x)$$. 양변에 expectation을 취하면
- $$\mathbb E[g(x)] \geq g(\mu)$$이 된다.

![그림1](/assets/img/stat/ineq1/fig1.png)

### Examples : $$AM\ge GM \ge HM$$

- $$\begin{aligned} \text { Arithmetic Mean (AM) }&=\frac{\sum_{i=1}^n x_i}{n} \\ \text { Geometric Mean }(\mathrm{GM}) & =\left(\prod_{i=1}^n x_i\right)^{1 / n} \\ \text { Harmonic Mean }(\mathrm{HM}) & =\frac{1}{\frac{1}{n} \sum_{i=1}^n \frac{1}{x_i}} \end{aligned}$$이다.

- $$-\log(HM) = \log \left(\frac{1}{n} \sum_{i=1}^n \frac{1}{x_i}\right) \ \geq \ -\log(GM)=\frac{1}{n} \sum_{i=1}^n\left\{\log \left(\frac{1}{x_i}\right)\right\}\ \geq \ -\log \left(\frac{1}{n} \sum_{i=1}^n x_i\right)=-\log (\mathrm{AM})$$를 보이면 된다.

### Examples : KL-divergence $$\ge 0$$

- $$D_{\mathrm{KL}}(P \mid\mid Q)=\sum_x p(x) \log \left(\frac{p(x)}{q(x)}\right)=\mathbb{E}\left[\log \left(\frac{p(X)}{q(X)}\right)\right]=\mathbb{E}\left[-\log \left(\frac{q(X)}{p(X)}\right)\right]$$이다.

- $$D_{\mathrm{KL}}(P \mid\mid Q)=\mathbb{E}[-\log (Z)] \geq-\log \mathbb{E}[Z]=-\log \sum_x p(x) \frac{q(x)}{p(x)}=-\log (1)=0$$이므로 KL-divergence는 non-negative이다.

### Examples : Chi-square divergence $$D_{\chi^2} \ge D_{KL}$$

- using $$\log x\le x-1$$
- $$\begin{aligned}D_{\mathrm{KL}}(P \mid\mid Q) &= \sum_x p(x) \log \left(\frac{p(x)}{q(x)}\right) \\ &\le \sum_x\left(\frac{p(x)}{q(x)}\right)^2 q(x)-1 \\ & = \mathrm{D}_{\chi^2}(P \mid\mid Q)\end{aligned}$$이다.

## 1.2. Holder's inequality

- If $$p, q \in(1, \infty)$$ with $$1 / p+1 / q=1$$,
- Then $$\left\vert\mathbb{E}[X Y]\right\vert \leq \mathbb{E}\left\vert X Y\right\vert \leq\left(\mathbb{E}{\left\vert X \right\vert}^p\right)^{1 / p}\left(\mathbb{E}{\left\vert Y\right\vert}^q\right)^{1 / q} .$$

### Proof

- The first inequality follows from :
- .$$\begin{aligned}-{\left\vert XY \right\vert} \leq X Y \leq{\left\vert XY \right\vert}\end{aligned}$$
- The second inequality follows from :
- .$$\begin{aligned}
  a=\frac{|X|}{\left(\mathbb{E}|X|^p\right)^{1 / p}} \quad \text { and } \quad b=\frac{|Y|}{\left(\mathbb{E}|Y|^q\right)^{1 / q}}\\ \frac{1}{p} \frac{|X|^p}{\mathbb{E}|X|^p}+\frac{1}{q} \frac{|Y|^q}{\mathbb{E}|Y|^q} \geq \frac{|X Y|}{\left(\mathbb{E}|X|^p\right)^{1 / p}\left(\mathbb{E}|Y|^q\right)^{1 / q}}
  \end{aligned}$$

- c.f. Lemma) Let $$a$$ and $$b$$ be any positive numbers, and let $$p$$ and $$q$$ be any positive numbers satisfying $$1 / p+1 / q=1$$. Then $$\frac{1}{p} a^p+\frac{1}{q} b^q \geq a b$$ with equality if and only if $$a^p=b^q$$​.
  - Note : $$1 / p+1 / q=1 \Longleftrightarrow p-1=p / q \text {. }$$

### Corollary : Cauchy–Schwarz inequality

- $$\left\vert\mathbb{E}[X Y]\right\vert \leq \mathbb{E}[{\left\vert XY\right\vert}] \leq\left(\mathbb{E}{\left\vert X \right\vert}^2\right)^{1 / 2}\left(\mathbb{E}{\left\vert Y\right\vert}^2\right)^{1 / 2}$$이다.

### Example : Covariance inequality

- $$\mathbb{E}\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right] \leq\left\{\mathbb{E}\left[\left(X-\mu_X\right)^2\right]\right\}^{1 / 2}\left\{\mathbb{E}\left[\left(Y-\mu_Y\right)^2\right]\right\}^{1 / 2}$$ 이다.
- 즉 $$\operatorname{Cov}(X, Y)^2 \leq \sigma_X^2 \sigma_Y^2$$
- Cramér–Rao Lower Bound : $$\frac{\operatorname{Cov}(Y, Z)}{\sqrt{\operatorname{Var}(Y) \operatorname{Var}(Z)}} \leq 1 \quad \Longleftrightarrow \quad \operatorname{Var}(Y) \geq \frac{\operatorname{Cov}(Y, Z)^2}{\mathcal{I}(\theta)}=\frac{1}{\mathcal{I}(\theta)}$$
  - $$\begin{aligned}
    \operatorname{Cov}(Y, Z) & =\mathbb{E}[Y Z]-\mathbb{E}[Y] \mathbb{E}[Z] \\
    & =\frac{d}{d \theta} \underbrace{\int u(x) f_X(x ; \theta) d x}_{=\theta}-\theta \frac{d}{d \theta} \underbrace{\int f_X(x ; \theta) d x}_{=1}=1
    \end{aligned}$$ under some regularity conditions.

### Example

- $$\mathbb{E}{\left\vert X \right\vert} \leq\left\{\mathbb{E}\left[{\left\vert X \right\vert}^p\right]\right\}^{1 / p} \leq\left\{\mathbb{E}\left[{\left\vert X \right\vert}^s\right]\right\}^{1 / s}, \quad \text { for } 1<p<s<\infty$$이다.

- **즉 higher moment $$\mathbb E[{\left\vert X \right\vert}^N]<\infty$$이면 lower moment $$\mathbb E[{\left\vert X \right\vert}^n]<\infty$$이다.**

## 1.3. Minkowski's inequality

- $$1<p<\infty, \quad\left(\mathbb{E}{\left\vert X+Y \right\vert}^p\right)^{1 / p} \leq\left(\mathbb{E}{\left\vert X \right\vert}^p\right)^{1 / p}+\left(\mathbb{E}{\left\vert Y \right\vert}^p\right)^{1 / p}$$이다.

### Proof

- Using the Hölder's inequality, $$\begin{aligned}
  \mathbb{E}|X+Y|^p & =\mathbb{E}\left[|X+Y||X+Y|^{p-1}\right] \\
  & \leq \mathbb{E}\left[|X||X+Y|^{p-1}\right]+\mathbb{E}\left[|Y||X+Y|^{p-1}\right] \\
  & \leq\left(\mathbb{E}|X|^p\right)^{1 / p}\left(\mathbb{E}|X+Y|^{q(p-1)}\right)^{1 / q}+\left(\mathbb{E}|Y|^p\right)^{1 / p}\left(\mathbb{E}|X+Y|^{q(p-1)}\right)^{1 / q} \\ & \text { where } q \text { satisfies } 1 / p+1 / q=1
  \end{aligned}$$
  
- Then, $$\begin{aligned}
  \frac{\mathbb{E}|X+Y|^p}{\left(\mathbb{E}|X+Y|^{q(p-1)}\right)^{1 / q}} = \frac{\mathbb{E}|X+Y|^p}{\left(\mathbb{E}|X+Y|^{p}\right)^{1 / q}} = \left(\mathbb{E}|X+Y|^p\right)^{1/p}\leq\left(\mathbb{E}|X|^p\right)^{1 / p}+\left(\mathbb{E}|Y|^p\right)^{1 / p}
  \end{aligned}$$

## 1.4. Association inequality

- $$f, g$$ non-decreasing implies $$\mathbb{E}[f(X) g(X)] \geq \mathbb{E}[f(X)] \mathbb{E}[g(X)]$$.
- $$f, g$$ non-increasing implies $$\mathbb{E}[f(X) g(X)] \geq \mathbb{E}[f(X)] \mathbb{E}[g(X)]$$.
- $$f$$ non-decreasing and $$g$$ non-increasing implies $$\mathbb{E}[f(X) g(X)] \leq \mathbb{E}[f(X)] \mathbb{E}[g(X)]$$

### Proof

Let the pair of random variables $$X^{\prime}$$ be distributed as the pair $$X$$ and independent of it. If $$f$$ and $$g$$ are non-decreasing, it holds $$\left(f(X)-f\left(X^{\prime}\right)\right)\left(g(X)-g\left(X^{\prime}\right)\right) \geq 0$$ and therefore $$\mathbb{E}\left[\left(f(X)-f\left(X^{\prime}\right)\right)\left(g(X)-g\left(X^{\prime}\right)\right)\right] \geq 0$$

### Example

$$\begin{aligned}
& \mathbb{E}\left[X^4\right] \geq \mathbb{E}[X] \mathbb{E}\left[X^3\right] \\
& \mathbb{E}\left[X e^{-X}\right] \leq \mathbb{E}[X] \mathbb{E}\left[e^{-X}\right] \\
& \mathbb{E}[X \mathbb{1}(X \geq a)] \geq \mathbb{E}[X] P(X \geq a)
\end{aligned}$$​

---

## 2. Inequalities for Variance

## 2.1. Variance upper bound

- Suppose that $$a \leq X \leq b$$ for some $$a, b \in \mathbb{R}$$.
- Then $$\operatorname{Var}[X] \leq \frac{(b-a)^2}{4}$$

### Proof

- $$\operatorname{Var}[X]=\mathbb{E}\left[(X-\mathbb{E}[X])^2\right] \leq \mathbb{E}\left[(X-c)^2\right] \quad \text { for any } c \in \mathbb{R}$$이다.

- $$c=(b+a) / 2$$이면 $$\operatorname{Var}[X] \leq \mathbb{E}\left[(X-(b+a) / 2)^2\right] \leq(b-a)^2 / 4$$

## 2.2. Efron–Stein inequality

- <u>Simple but surprisingly powerful !</u>
- Suppose that $$X_1, \ldots, X_n, X_1^{\prime}, \ldots, X_n^{\prime}$$ are independent with $$X_i$$ and $$X_i^{\prime}$$ having the same distribution for all $$i \in\{1, \ldots, n\}$$. Let $$X=\left(X_1, \ldots, X_n\right)$$ and $$X^{(i)}=$$ $$\left(X_1, \ldots, X_{i-1}, X_i^{\prime}, X_{i+1}, \ldots, X_n\right)$$.

- Then$$\operatorname{Var}[f(X)] \leq \frac{1}{2} \sum_{i=1}^n \mathbb{E}\left[\left\{f(X)-f\left(X^{(i)}\right)\right\}^2\right]$$

### Proof

- **Step 1**: Define $$V$$
- Let $$V_i=\mathbb{E}\left[f(X) \mid X_1, \ldots, X_i\right]-\mathbb{E}\left[f(X) \mid X_1, \ldots, X_{i-1}\right], \quad \text { for } i=1, \ldots, n$$
- Then $$V=\sum_{i=1}^n V_i$$
- .$$\begin{aligned}
  \operatorname{Var}[V] & =\mathbb{E}\left[\left(\sum_{i=1}^n V_i\right)^2\right] \\
  & =\sum_{i=1}^n \mathbb{E}\left[V_i^2\right]+2 \sum_{1 \leq i<j \leq n} \mathbb{E}\left[V_i V_j\right]
  \end{aligned}$$
- **Step 2** : cross product = 0
- For any $$i>j,$$ $$\begin{aligned}
  \mathbb{E}\left[V_i V_j\right]&=\mathbb{E}\left[\mathbb{E}\left[V_i V_j \mid X_1, \ldots, X_j\right]\right] \\& =\mathbb{E}\left[V_j \mathbb{E}\left[V_i \mid X_1, \ldots, X_j\right]\right]=0\end{aligned}$$

- Because $$\begin{aligned}
  \mathbb{E}\left[V_i \mid X_1, \ldots, X_j\right] & =\mathbb{E}\left[\mathbb{E}\left[f(X) \mid X_1, \ldots, X_i\right] \mid X_1, \ldots, X_j\right]-\mathbb{E}\left[\mathbb{E}\left[f(X) \mid X_1, \ldots, X_{i-1}\right] \mid X_1, \ldots, X_j\right] \\
  & =\mathbb{E}\left[f(X) \mid X_1, \ldots, X_j\right]-\mathbb{E}\left[f(X) \mid X_1, \ldots, X_j\right]=0
  \end{aligned}$$
  - c.f. $$\mathbb{E}[X Y]=\mathbb{E}[\mathbb{E}[X Y \mid X]]=\mathbb{E}[X \mathbb{E}[Y \mid X]]$$ and $$\mathbb{E}[\mathbb{E}[X \mid Y, f(X)] \mid f(X)]=\mathbb{E}[X \mid f(X)]$$​
- Therefore, $$\operatorname{Var}[V]=\sum_{i=1}^n \mathbb{E}\left[V_i^2\right]$$

- **Step 3** by Jensen's inequality
- .$$\begin{aligned}
  V_i^2 & =\left(\mathbb{E}\left[f(X) \mid X_1, \ldots, X_i\right]-\mathbb{E}\left[f(X) \mid X_1, \ldots, X_{i-1}\right]\right)^2 \\
  & =\left(\mathbb{E}\left[\mathbb{E}\left[f(X) \mid X_1, \ldots, X_n\right]-\mathbb{E}\left[f(X) \mid X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n\right] \mid X_1, \ldots, X_i\right]\right)^2 \\
  & \leq \mathbb{E}\left[\left(\mathbb{E}\left[f(X) \mid X_1, \ldots, X_n\right]-\mathbb{E}\left[f(X) \mid X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n\right]\right)^2 \mid X_1, \ldots, X_i\right] \\
  & =\mathbb{E}\left[\left(f(X)-\mathbb{E}[f(X^{(i)})]\right)^2 \mid X_1, \ldots, X_i\right] \end{aligned}$$
- . $$\begin{aligned} \mathbb{E}[V_i^2]&\le\mathbb{E}\left[\mathbb{E}\left[\left(f(X)-\mathbb{E}[f(X^{(i)})]\right)^2 \mid X_1, \ldots, X_i\right]\right]\\&=\mathbb{E}\left[\left(f(X)-\mathbb{E}(f(X^{(i)}))\right)^2\right] \\ & =\mathbb{E}\left[\operatorname{Var}\left(f(X) \mid X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n\right)\right] \\ & =\frac{1}{2} \mathbb{E}\left[\left(f(X)-f(X^{(i)})\right)^2\right] \end{aligned}$$
- Note : $$f(X^{(i)})$$ is $$Z=f\left(X_1, \ldots, X_{i-1}, X_i^{\prime}, X_{i+1}, \ldots, X_n\right)$$
- **결론** : samples $$X = (X_1, \ldots, X_n),\quad X^{\prime} = (X_1^{\prime}, \ldots, X_n^{\prime})$$이 있을 때, estimator $$f(X)$$의 variance를 알기 위해서는 sample $$X_1, \ldots, X_n$$를 하나씩 $$X_1^{\prime}, \ldots, X_n^{\prime}$$으로 바꿔보면서, $$f(X)$$의 변화량$$^2$$ 의 expectation을 다 더하고 2로 나누어주면 upper bound를 구할 수 있다.

### Example : Sample mean

- Let $$f(X)=\frac{1}{n} \sum_{i=1}^n X_i$$ where $$X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} P$$.
- Then the EfronStein inequality yields $$\operatorname{Var}[f(X)] \leq \sum_{i=1}^n \frac{1}{n^2} \mathbb{E}\left[\frac{\left(X_i-X_i^{\prime}\right)^2}{2}\right]=\frac{\sigma^2}{n}$$
- We know that $$\operatorname{Var}[f(X)]=\sigma^2 / n$$. In this regard, the Efron-Stein inequality is not improvable.

### Example : Bounded Differences

- $$\sup _{x_1, \ldots, x_n, x_i^{\prime} \in \mathcal{X}}{\left\vert g\left(x_1, \ldots, x_n\right)-g\left(x_1, \ldots, x_{i-1}, x_i^{\prime}, x_{i+1}, \ldots, x_n\right) \right\vert} \leq c_i, $$이면
- the Efron-Stein inequality shows $$\operatorname{Var}\left[g\left(X_1, \ldots, X_n\right)\right] \leq \frac{1}{2} \sum_{i=1}^n c_i^2, \quad \text { for } 1 \leq i \leq n$$