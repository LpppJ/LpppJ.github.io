---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Convergence of Random Variables

## 1. Almost sure Convergence

- Definition : $$P\left(\displaystyle\lim _{n \rightarrow \infty} X_n=X\right)=1$$
- pass

## 2. Convergence in Probability

- Definition : $$\displaystyle\lim _{n \rightarrow \infty} P\left(\left\vert X_n-X\right\vert \geq \epsilon\right)=0$$
- Notation : $$X_n \xrightarrow{p} X$$
- Convergence in Probability가 Almost sure Convergence를 의미하지는 않는다.

### 2.1. Weak Law of Large Numbers (WLLN)

- Let $$X, X_1, X_2, \ldots$$ be a sequence of independent, identically distributed (i.i.d.) random variables with $$\operatorname{Var}[X]=\sigma^2<\infty$$. Then
  $$
  \bar{X}_n:=\frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{p} \mathbb{E}[X]
  $$

### Proof

- By Chebyshev's inequality, $$P\left(\left\vert \bar{X}_n-\mathbb{E}[X]\right\vert  \geq \epsilon\right) \leq \frac{\sigma^2}{n \epsilon^2}$$

- This in turn implies that, $$\lim _{n \rightarrow \infty} P\left(\left\vert \bar{X}_n-\mathbb{E}[X]\right\vert  \geq \epsilon\right)=0,$$

- 사실 finite variance $$\operatorname{Var}[X]=\sigma^2<\infty$$​ 가정 없이도 증명할 수 있다. (only finite **first** moment)
- (skip the proof)

### 2.2. Strong Law of Large Numbers

- Let $$X_1, X_2, \ldots$$ be pairwise independent identically distributed random variables with $$\mathbb{E}\left[\left\vert X_i\right\vert \right]<\infty$$. Let $$\mathbb{E}\left[X_i\right]=\mu$$. Then
  $$
  \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{a . s .} \mu \quad \text { as } n \rightarrow \infty .
  $$

### Consistency

- Estimator가 convergence in Probability이면 Consistent하다.
- Estimator의 분산이 0으로 수렴하면 Consistent하다.

## 3. Convergence in Quadratic mean

- Definition : $$\mathbb{E}\left[\left(X_n-X\right)^2\right] \rightarrow 0$$
- Notation : $$X_n \xrightarrow{q m} X$$
- $$b>a$$일 때, $$\mathbb{E}\left[\left(X_n-X\right)^b\right] \rightarrow 0$$이면 $$\mathbb{E}\left[\left(X_n-X\right)^a\right] \rightarrow 0$$ 이다.

## 4. Convergence in Distribution

- Definitnion : $$\lim _{n \rightarrow \infty} F_{X_n}(t)=F_X(t), \text { for all points } t \text { where the } \mathrm{CDF} F_X \text { is continuous. }$$
- Notation : $$X_n \xrightarrow{d} X$$
- Target CDF $$F_X$$​​가 continuous한 points에서만 ! 같으면 된다.
  - 만약 limiting distribution이 모든 $$x$$에서 continuous라면, 모든 $$x$$​에서 Convergence in Distribution이다. (Polya's theorem) 

- Convergence in Distribution은 Convergence in Proability를 imply하지 않는다.
  - Ex. $$X, X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} N(0,1) \text {이면} X_n \xrightarrow{d} X$$이다.
  - However, since $$X_n-X \sim N(0,2)$$
  - $$P\left(\left\vert X_n-X\right\vert>\epsilon\right)=2 P(X \leq-\epsilon / \sqrt{2})$$​는 0으로 수렴하지 않는다.
  - 만약 $$X_n$$이 constant이면 Convergence in Distribution이 Convergence in Probability를 imply한다. 

### 4.1. **Polya's theorem**

- Suppose that $$X_n \xrightarrow{d} X$$ for a random variable $$X$$ with a continuous distribution function. Then $$\sup _{x \in \mathbb{R}}\left\vert P\left(X_n \leq x\right)-P(X \leq x)\right\vert  \rightarrow 0$$

- **Proof**
  - By the continuity, $$-\infty=x_0<x_1<\cdots<x_k=\infty \text { with } F\left(x_i\right)=i / k$$가 존재하고
  - $$x_{i-1} \leq x \leq x_i$$에 대해 $$F_n(x_{i-1}) \leq F_n(x) \leq F_n(x_i)$$이고 $$F(x_{i-1}) \leq F(x) \leq F(x_i)$$이므로
  - $$\begin{aligned}
    F_n(x)-F(x) & \leq F_n\left(x_i\right)-F\left(x_{i-1}\right)=F_n\left(x_i\right)-F\left(x_i\right)+1 / k \\
    & \geq F_n\left(x_{i-1}\right)-F\left(x_i\right)=F_n\left(x_{i-1}\right)-F\left(x_{i-1}\right)-1 / k
    \end{aligned}$$​이고,
  - $$\left\vert F_n(x)-F(x)\right\vert \text { 의 upper bound가 } \sup _i\left\vert F_n\left(x_i\right)-F\left(x_i\right)\right\vert +1 / k$$이다.
  - $$k$$는 arbitrary fixed number이므로 $$n \to \infty$$일 때 upper bound가 0이 된다.

### 4.2. Lévy’s continuity theorem

- Let $$X_n$$ and $$X$$ be random vectors in $$\mathbb{R}^d$$. Then $$X_n \xrightarrow{d} X$$ if and only if $$\lim _{n \rightarrow \infty} \mathbb{E}\left[e^{i t^{\top} X_n}\right]=\mathbb{E}\left[e^{i t^{\top} X}\right]$$ for every $$t \in \mathbb{R}^d$$​​
- $$X_n$$이 $$X$$로 Convergence in Distribution하는 것은 characteristic functions의 수렴과 같다.
- Cramér–Wold device : highier-dim problem을 one-dim problem으로 생각할 수 있다.

![그림1](/assets/img/stat/convergence/fig1.jpeg)