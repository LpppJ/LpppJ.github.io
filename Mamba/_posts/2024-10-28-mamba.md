---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2023](https://arxiv.org/pdf/2312.00752)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Arxiv 2023)

## Abstract

- 많은 subquadratic-time architectures (linear attention, gated convolution and recurrent models, and structured state space models (SSMs))가 Transformer의 연산 효율성을 해결하기 위해 제안되었지만
  - content-based reasoning에서는 여전히 약한 모습
- 그래서 본 논문에서 제시하는 Mamba는
  - **SSM parameters를 input의 함수 형태**로 놓아서 모델이 selectively propagate or forget information 할 수 있도록 함
  - 그리고 recurrent 모드로 학습을 진행하게 되면 중간 Hidden State 크기가 매우 커질 수 있기 때문에
    - **hardware-aware parallel algorithm**을 사용하여 hidden State를 메모리에 저장하지 않고 병렬적으로 scan 연산함

## 1. Introduction

- **Selection Mechanism**
  - parameterizing the SSM parameters based on the input
    - $$\to$$ 필요한 정보만 기억하고 필요없는 정보 filter out
- **Hardware-aware Algorithm**
  - 연산 커널을 결합하는 방식(커널 융합)으로 메모리 입출력 과정을 최적화하고 오버헤드를 줄임
  - 고속 메모리(SRAM)를 활용해 느린 GPU 메모리(HBM) 의존도를 줄여 연산 속도를 높이겠다는 것
  - backpropagation 할 때에는 hidden state를 저장하지 않고 필요할 때마다 재계산함으로써 메모리 사용량을 최소화

![그림1](/assets/img/mamba/mamba/fig1.png)

- **Architecture**
  - 기존 SSM architectures와 Transformer의 MLP blocks을 합쳐서 Mamba를 만듬

## 2. State Space Models

- Structured state space sequence models (S4)
  - inspired by a particular continuous system :
    - 1-dimensional function or sequence $x(t) \in \mathbb{R} \mapsto y(t) \in \mathbb{R}$ through an implicit latent state $h(t) \in \mathbb{R}^N$.
    - $$\begin{aligned} h^{\prime}(t) & =A h(t)+B x(t) \\ y(t) & =C h(t)\end{aligned}$$ (1)
    - 4개의 parameters $$(\Delta, A, B, C)$$로 정의됨 (아직 input의 함수 형태가 아님)
- **Discretization**
  - 첫번째 단계는 "continuous parameters" $$(\Delta, A, B)$$를 "discrete parameters" $$(\bar{A}, \bar{B})$$로 바꾸는 것
    - fixed formulas $$\overline{A}=f_A(\Delta, A)$$ and $$\overline{B}=f_B(\Delta, A, B)$$를 사용
    - $$\left(f_A, f_B\right)$$는 discretization rule
- **Computation**
  - $$(\Delta, A, B, C) \mapsto(\bar{A}, \bar{B}, C)$$ 변환이 끝났으면 그 다음에는 다음 두 가지 형태의 computation 가능
    - a linear recurrence :
      - $$\begin{aligned} h_t & =\overline{A} h_{t-1}+\overline{B} x_t \\ y_t & =C h_t\end{aligned}$$ (2) 또는
    - a global convolution :
      - $$\begin{aligned} \bar{K} & =\left(C \bar{B}, C \overline{A B}, \ldots, C \bar{A}^k \bar{B}, \ldots\right) \\ y & =x * \bar{K}\end{aligned}$$ (3)
- **Linear Time Invariance (LTI)**
  - 위 (1) (2) (3) 모델들은 model’s dynamics가 time-invariant
  - 하지만 본 논문에서는 이러한 LTI property가 근본적인 한계가 있음을 밝히고
    - LTI를 제거하면서도 efficiency bottlenecks를 극복함을 제시함
- **Structure and Dimensions**
  - $$A$$ matrix를 사용하기 때문에 SSM인거고, 이때 $$A \in \mathbb{R}^{N \times N}, B \in \mathbb{R}^{N \times 1}, C \in \mathbb{R}^{1 \times N}$$
  - total hidden state has dimension은: $$𝐷𝑁$$ per input
  - the sequence length requires $$𝑂(𝐵𝐿𝐷𝑁)$$

## 3. Selective State Space Models

- 3.1절에서는 selection mechanism을 소개하고
- 3.2절에서는 어떻게 selection mechanism이 SSM과 같이 쓰일 수 있는지 보고
- 3.3절에서는 hardware-aware algorithm을 알아보고
- 3.4절에서는 simple SSM을 attention이나 MLP없이 알아보고
- 3.5절에서는 additional properties of selection mechanisms를 논의한다

### 3.1. Motivation: Selection as a Means of Compression

- sequence modeling의 본질적인 문제는 **small state에 context를 압축**하는 것
  - Trade off : 압축을 안하면 inefficient하고(Transformer) efficient하면 압축을 너무 많이 하고(RNN)

![그림1](/assets/img/mamba/mamba/fig2.png)

- Synthetic task 2가지
  - **Selective Copying** 
    - 필요한 tokens와 아닌 것들을 구별하기 위해 content-aware reasoning하는 task
  - **Induction Heads**
    - 다음에 뭐가 올지 추론하기 위해 context-aware reasoning하는 task
  - 이 두 가지는 위에서 소개한 LTI mode로는 하기 어렵다
- 결국에는 efficient하려면 small state를 가져야 하는데, 그걸 "잘" 하려면 selectivity를 "잘" 해야 함

### 3.2.  Improving SSMs with Selection

- 본 논문에서 소개하는 selection mechanism은 model의 parameters를 input-dependent하게 만드는 것
  - $$\Delta, B, C$$을 length dimension $$L$$로 만듬 (즉 time-invariant에서 time-varying으로)

![그림1](/assets/img/mamba/mamba/algorithm12.png)

### 3.3 Efficient Implementation of Selective SSMs

- Convolution이나 attention처럼 GPU-friendly하게 Selective SSM을 만들고 싶은 거고
- 즉 시간에 따라 필요한 정보를 선택적으로 처리하겠다는 것. 그러면 더 빠르게 긴 시퀀스를 처리

**3.3.1 Motivation of Prior Models**

- paying speed and memory costs 없이 maximize hidden state dimension하고 싶음
- Recurrent mode는 hidden이 input보다 훨씬 커서 메모리 사용량이 많음
  - 그래서 input의 shape (=output의 shape)과 같은 conv를 쓰겠다
- 기존 LTI는 데이터 특성을 잘 반영 못했지만 Mamba는 순환적 요소와 컨볼루션적 요소를 동시에 사용해 모델의 효율성을 극대화 !

**3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion**

- LTI의 한계를 극복하는 selection mechanism을 소개:
- 문제는 (1) the sequential nature of recurrence, and (2) the large memory usage
  - (2) the large memory usage는 kernel fusion으로 해결
    - scan input $$(\bar{A}, \bar{B})$$ of size $$(B, L, D, N)$$을 HBM에 저장하는 것이 아니라
    - the SSM parameters $$(\triangle, A, B, C)$$의 final output $$(B, L, D)$$만 저장
    - discretization이랑 recurrence는 SRAM에서 수행
  - (1) the sequential nature of recurrence는 recomputation으로 해결
    - intermediate states를 저장하지 않는데 이건 backpropagation에서 필요하니까
    - 그냥 다시 계산함 (recomputation)
    - 그 결과 FlashAttention과 유사한 memory efficiency

### 3.4 A Simplified SSM Architecture

- Mamba는 linear attention과 MLP를 결합해서 gated attention unit(GAP)처럼 만듬
- model dimension을 D에서 expansion factor E를 사용해서 늘려줌
  - 대부분의 model parameters $$3ED^2$$개 $$2ED^2$$개가 input projection에, $$ED^2$$개가 output projection에 있음
  - 반면 SSM 안에는 parameters가 별로 없는데, Mamba는 이걸 반복해서 사용하기 때문에 효율적이다

### 3.5 Properties of Selection Mechanisms

- The selection mechanism은 RNN이나 CNN에 쓸 수 있는 broader concept임

**3.5.1 Connection to Gating Mechanisms**

- SSM의 게이트 역할을 하는 $$\Delta$$가 RNN의 게이트와 유사하게 작동
  - 입력된 정보 중 어떤 것을 유지하고 어떤 것을 버릴지 결정하는 역할인 점도 비슷
  - When $N=1, A=-1, B=1, s_{\Delta}=\operatorname{Linear}(x)$, and $\tau_{\Delta}=$ softplus,
  - $$\begin{aligned} & g_t=\sigma\left(\operatorname{Linear}\left(x_t\right)\right) \\ & h_t=\left(1-g_t\right) h_{t-1}+g_t x_t\end{aligned}$$ .
  - 이런 식으로 $$g_t$$가 현재 입력 $$x_t$$가 얼마나 중요한지 표현하게 하고
    - $$g_t$$가 1에 가까울수록 $$x_t$$를 많이 반영, 0에 가까울수록 이전 state $$h_{t-1}$$을 많이 반영

**3.5.2 Interpretation of Selection Mechanisms**

- **Variable Spacing**
  - Selectivity의 역할은 filtering out irrelevant noise tokens that may occur between inputs of interest
- **Filtering Context**
  - context가 길어진다고 성능이 좋아지는 것이 아님. 대부분의 모델이 너무 긴 sequence에서 불필요한 정보를 제거하지 못해서 성능 저하가 발생
  - selective model은 state를 언제든 초기화 할 수 있으니 긴 sequence가 들어왔을 때 성능이 더 좋아지도록 작동
- **Boundary Resetting**
  - LTI는 sequence의 경계에서 정보가 섞이는 문제가 있었는데, selective SSM은 그런 문제 없음
    - 언제든지 state를 초기화할 수 있으니 그냥 boundaries에서 초기화 하면 됨 ($$g_t=1$$)

- **Interpretation of $$\Delta$$**
  - $$\Delta$$가 크면 다 잊고 현재 정보를 위주로 state를 만드는거고, 작으면 이전 state를 유지
- **Interpretation of A**
  - 사실 $$\bar{A}=\exp (\Delta A)$$도 $$\Delta$$를 통해 만들어지니 크게 건드리지 말고 단순하게 둔다
- **Interpretation of 𝑩 and 𝑪.**
  - 결국 Selectivity의 역할은 filtering out.
  - B와 C는 입력을 상태로 전달할지, 상태를 출력으로 내보낼지를 결정
  - 모델이 state(context)를 더 세밀하게 제어할 수 있음

### 3.6 Additional Model Details

pass

## 4. Empirical Evaluation

### 4.1 Synthetic Tasks

**4.1.1 Selective Copying**

**4.1.2 Induction Heads**

![그림1](/assets/img/mamba/mamba/table12.png)

### 4.2. Language Modeling

![그림1](/assets/img/mamba/mamba/table3.png)

### 4.5 Speed and Memory Benchmarks

![그림1](/assets/img/mamba/mamba/fig8.png)

### 4.6. Model Ablations

![그림1](/assets/img/mamba/mamba/table6.png)

![그림1](/assets/img/mamba/mamba/table78.png)

## 5. Discussion

Pass

## 6. Conclusion

- a selection mechanism to structured state space models
  - to perform context-dependent reasoning
  - Without attention ! (simple attention-free architecture)