<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-08-15T13:12:40+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-05-MG-TSD/" rel="alternate" type="text/html" title="MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)" /><published>2024-08-05T00:00:00+09:00</published><updated>2024-08-05T00:58:12+09:00</updated><id>http://localhost:4000/timeseries/MG-TSD</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-05-MG-TSD/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>어떻게 Diffusion model의 성능을 time series forecasting에 활용할 수 있는가</li>
  <li><strong>M</strong>ulti-<strong>G</strong>ranularity <strong>T</strong>ime <strong>S</strong>eries <strong>D</strong>iffusion <strong>(MG- TSD)</strong>
    <ul>
      <li>leveraging the inherent granularity levels</li>
      <li>intuition: diffusion step에서 점차 gaussian noise로 만드는 것을 fine \(\to\) coarse로 이해</li>
      <li>novel multi-granularity guidance diffusion loss function</li>
      <li>method to effectively utilize coarse-grained data across various granularity levels</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>최근에는 Time series predictive 목적으로 conditional generative model을 활용
    <ul>
      <li>처음에는 Auto-regressive 방식으로 하다가 CSDI도 했었음</li>
    </ul>
  </li>
  <li>하지만 문제는 Diffusion이 instability하다는 점
    <ul>
      <li>Image에서 diffusion은 다양한 이미지를 만들 수 있어서 장점이었는데</li>
      <li>시계열 예측 관점에서는 그것이 성능 하락의 원인이 될 수 있음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/MG-TSD/fig1.png" alt="그림1" /></p>

<ul>
  <li>Diffusion step에서 점차 gaussian noise로 만드는 것을 fine \(\to\) coarse로 이해한다면
    <ul>
      <li>Diffusion model이 <strong>labels을 the source of guidance</strong>로 필요로 하는 문제에서</li>
      <li>Time series의 fine feature가 그 labels as the source of guidance 역할을 할 수 있을 것</li>
    </ul>
  </li>
  <li>MG-TSD에서는 coarse-grained data를 denoising process 학습의 guide로 준다.
    <ul>
      <li>\(\to\) intermediate latent states에서의 constraints로 작용</li>
      <li>\(\to\) coarser feature는 더 빠르게 생성할 수 있기 때문에, 그만큼 finer feature recovery도 용이</li>
      <li>\(\to\) coarse-grained data의 trend와 pattern을 보존하는 sampling을 만듬</li>
      <li>\(\to\)​ reduces variability and results in high-quality predictions</li>
    </ul>
  </li>
</ul>

<h2 id="2-background">2. Background</h2>

<ul>
  <li>TimeGrad Model
    <ul>
      <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad Paper</a> <a href="https://lpppj.github.io/timeseries/2024-07-09-Timegrad">TimeGrad Review</a></li>
    </ul>
  </li>
  <li>\(\boldsymbol{X}^{(1)}=\left[\boldsymbol{x}_1^1, \ldots, \boldsymbol{x}_t^1, \ldots, \boldsymbol{x}_T^1\right]\) is the original observed data, where \(t \in[1, T]\) and \(\boldsymbol{x}_t \in \mathbb{R}^D\)
    <ul>
      <li>Mathematical expressions: \(q_{\mathcal{X}}\left(\boldsymbol{x}_{t_0: T}^1 \mid\left\{\boldsymbol{x}_{1: t_0-1}^1\right\}\right)=\prod_{t=t_0}^T q_{\mathcal{X}}\left(\boldsymbol{x}_t^1 \mid\left\{\boldsymbol{x}_{1: t-1}^1\right\}\right)\)</li>
    </ul>
  </li>
</ul>

<h2 id="3-method">3. Method</h2>

<h3 id="31-mg-tsd-model-architecture">3.1. MG-TSD Model Architecture</h3>

<p><img src="/assets/img/timeseries/MG-TSD/fig2.png" alt="그림2" /></p>

<h3 id="multi-granularity-data-generator">Multi-granularity Data Generator</h3>

<p>: for generating multi-granularity data from observations</p>

<ul>
  <li>historical sliding windows with different sizes를 통해 fine \(\to\) coase로 smoothing out</li>
  <li>즉 \(\boldsymbol{X}^{(g)}=f\left(\boldsymbol{X}^{(1)}, s^g\right)\) with pre-defined sliding window size \(s^g\)</li>
  <li>이 때 non-overlapping하게 window를 slicing하고, \(\boldsymbol{X}^{(g)}\)는 \(s^g\)번 복제해서 \([1, T]\)로 맞춤</li>
</ul>

<h3 id="temporal-process-module">Temporal Process Module</h3>

<p>: designed to capture the temporal dynamics of the multi-granularity time series data</p>

<ul>
  <li>각각의 granularity level \(g\)에서 GRU와 같은 방식으로 timestep \(t\)를 \(\mathbf{h}_t^g\)로 encoding</li>
</ul>

<h3 id="guided-diffusion-process-module">Guided Diffusion Process Module</h3>

<p>: designed to generate stable time series predictions at each timestep \(t\)</p>

<ul>
  <li>multi-granularity data를 활용하여 diffusion learning process의 guide로 제공</li>
</ul>

<h3 id="32-multi-granularity-guided-diffusion">3.2. Multi-Granularity Guided Diffusion</h3>

<p>: Guided Diffusion Process Module에 대한 details</p>

<h3 id="321-coarse-grained-guidance">3.2.1. Coarse-grained Guidance</h3>

<p>: the derivation of a heuristic guidance loss for the two- granularity case</p>

<ul>
  <li>consider two granularities at a fixed timestep \(t\)
    <ul>
      <li>: \(\text { finest-grained data } \boldsymbol{x}_t^{g_1}\left(g_1=1\right) \text { from } \boldsymbol{X}^{\left(g_1\right)}\) &amp; \(\text { coarse-grained data } \boldsymbol{x}_t^g \text { from } \boldsymbol{X}^{(g)}\)​</li>
    </ul>
  </li>
  <li>먼저 coarse-grained targets \(x^g\)를 intermediate diffusion step \(N_*^g \in[1, N-1]\)에 introduce
    <ul>
      <li>즉 objective function이 \(\log p_\theta\left(\boldsymbol{x}^g\right)\)</li>
    </ul>
  </li>
  <li>그러면 denoising process에서 recover된 coarser features는 실제 coarse-grained sample의 정보를 많이 가지고 있을테니
    <ul>
      <li>fine-grained feature를 recover하기도 쉬워질 것</li>
    </ul>
  </li>
  <li>\(\theta\)-parameterized: \(p_\theta\left(\boldsymbol{x}_{N_*^g}\right)=\int p_\theta\left(\boldsymbol{x}_{N_*^g: N}\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}=\int p\left(\boldsymbol{x}_N\right) \prod_{N_*^g+1}^N p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}\)​
    <ul>
      <li>where \(\boldsymbol{x}_N \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I}), p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right)=\mathcal{N}\left(\boldsymbol{x}_{n-1} ; \boldsymbol{\mu}_\theta\left(\boldsymbol{x}_n, n\right), \boldsymbol{\Sigma}_\theta\left(\boldsymbol{x}_n, n\right)\right)\)​</li>
    </ul>
  </li>
  <li>이건 \(N_*^g\)번째 diffusion step에서 \(N\)번째까지 총 \(N-N_*^g\) steps의 forward process이므로
    <ul>
      <li>the guidance objective: \(\log p_\theta\left(\boldsymbol{x}^g\right)=\log \int p_\theta\left(\boldsymbol{x}_{N_*^g}^g, \boldsymbol{x}_{N_*^g+1}^g, \ldots, \boldsymbol{x}_N^g\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}^g\)​</li>
    </ul>
  </li>
  <li>sample에 대한 loss 대신 noise에 대한 loss 사용
    <ul>
      <li>loss: \(\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}^g, n}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_n^g, n\right)\right\|^2\right]\)​</li>
      <li>where \(\boldsymbol{x}_n^g=\left(\prod_{i=N_{\boldsymbol{z}}^g}^n \alpha_i^1\right) \boldsymbol{x}^g+\sqrt{ } \mathbf{1}-\prod_{i=N^g}^n \alpha_i^1 \boldsymbol{\epsilon} \text { and } \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})\)</li>
    </ul>
  </li>
</ul>

<h3 id="322-multi-granularity-guidance">3.2.2. Multi-granularity Guidance</h3>

<ul>
  <li>
    <p>Multi-granularity Data Generator가 G개의 granularity levels마다 data 생성: \(\boldsymbol{X}^{(1)}, \boldsymbol{X}^{(2)}, \ldots, \boldsymbol{X}^{(G)}\)</p>
  </li>
  <li>Share ratio: \(r_g:=1-\left(N_*^g-1\right) / N\)
    <ul>
      <li>: the shared percentage of variance schedule between the gth granularity data and the finest-grained data</li>
      <li>ex. finest-grained data에서는 \(N_*^1=1 \text { and } r^1=1\)​
        <ul>
          <li>variance schedule for granularity \(g\) is, \(\alpha_n^g\left(N_*^g\right)= \begin{cases}1 &amp; \text { if } n=1, \ldots, N_*^g \\ \alpha_n^1 &amp; \text { if } n=N_*^g+1, \ldots, N\end{cases}\)​</li>
          <li>and \(\left\{\beta_n^g\right\}_{n=1}^N=\left\{1-\alpha_n^g\right\}_{n=1}^N\)​</li>
          <li>accordingly, \(a_n^g\left(N_*^g\right)=\prod_{k=1}^n \alpha_k^g \text {, and } b_n^g\left(N_*^g\right)=1-a_n^g\left(N_*^g\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이 때 \(N^g_*\)는 : represents the diffusion index for starting sharing the variance schedule across granularity level \(g \in\{1, \ldots, G\}\)</li>
  <li>이렇게 되면 larger coarser granularity level일수록 \(N^g_*\)가 커진다는 뜻
    <ul>
      <li>즉 coarser할수록 fine한 정보는 줄어들테니 이전 diffusion step과 차이가 크지 않을 것</li>
      <li>그러니까 \(N^g_*\)를 크게 해서 fine-grained feature를 생성할 steps를 많이 줌</li>
    </ul>
  </li>
  <li>Then the guidance loss function \(L^{(g)}(\theta)\) for \(g\)-th granularity \(x^g_{n,t}\) at timestep \(t\) and diffusion step \(n\),
    <ul>
      <li>can be expressed as: \(L^{(g)}(\theta)=\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, n} \|\left(\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon}, n, \mathbf{h}_{t-1}^g\right) \|_2^2\right.\)</li>
      <li>where \(\mathbf{h}_t^g=\mathrm{RNN}_\theta\left(\boldsymbol{x}_t^g, \mathbf{h}_{t-1}^g\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="training">Training</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm1.png" alt="그림41" /></p>

<ul>
  <li>최종적인 training objectives는 모든 granularities에서의 Loss의 weighted sum
    <ul>
      <li>: \(L^{\text {final }}=\omega^1 L^{(1)}(\theta)+L^{\text {guidance }}(\theta)=\sum_{q=1}^G \omega^g \mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, t}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_{n, t}^g, n, \mathbf{h}_{t-1}^g\right)\right\|^2\right]\)</li>
      <li>where \(\boldsymbol{x}_{n, t}^g=\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon} \text { and } \sum_{g=1}^G \omega^g=1\)</li>
      <li>이 때 denoising network의 parameters는 shared across all granularities</li>
    </ul>
  </li>
</ul>

<h3 id="inference">Inference</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm2.png" alt="그림42" /></p>

<ul>
  <li>우리의 목표는 특정한 prediction steps에 대한  finest-grained data에 대한 예측
    <ul>
      <li>\(t_0-1\) 시점까지 주어졌다면 아래 algorithm 2를 따라 \(t_0\)시점에 대한 데이터 생성,</li>
      <li>우리가 원하는 forecast horizon이 될 때까지 반복</li>
      <li>hidden states에 conditional inputs으로 무엇을 넣는지에 따라서 그에 해당하는 granularity levels로 샘플링</li>
    </ul>
  </li>
</ul>

<h3 id="selection-of-share-ratio">Selection of share ratio</h3>

<ul>
  <li>위에서는 share ratio \(r_g:=1-\left(N_*^g-1\right) / N\)를 heuristic하게 \(N^g_*\)에 따라 결정되도록 했음
    <ul>
      <li>Diffusion step \(N^g_*\)는 \(q\left(\boldsymbol{x}^g\right) \text { and } p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\)의 거리가 가장 작을 때로 설정 !</li>
      <li>: \(\to\) \(N_*^g:=\arg \min _n \mathcal{D}\left(q\left(\boldsymbol{x}^g\right), p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\right)\)​
        <ul>
          <li>\(\mathcal{D}\)는 두 분포의 거리를 측정하는 metric이 됨 (KL-divergence, …)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/MG-TSD/table12.png" alt="그림112" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig3.png" alt="그림3" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig4.png" alt="그림4" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Multi-Granularity Time Series Diffusion (MG-TSD)
    <ul>
      <li>leverages the inherent granularity levels within the data, as given targets at intermediate diffusion steps to guide the learning process of diffusion models</li>
      <li>to effectively utilize coarse-grained data across various granularity levels.</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=CZiY6OLktd)]]></summary></entry><entry><title type="html">Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/" rel="alternate" type="text/html" title="Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)" /><published>2024-08-04T00:00:00+09:00</published><updated>2024-08-04T18:52:22+09:00</updated><id>http://localhost:4000/timeseries/Diffusion-TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Diffusion-TS: uses an encoder-decoder transformer with disentangled temporal representations</li>
  <li>train the model to directly reconstruct the <strong>sample</strong> instead of the <strong>noise</strong> in each diffusion step</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Synthesizing realistic time series data는 데이터 공유가 개인정보 침해로 이어질 수 있는 사례에서의 솔루션</li>
  <li>지금까지 Diffusion을 활용한 time series generation은 대부분 task-agnostic generation
    <ul>
      <li>첫번째 문제는 RNN-based Autoregressive 방식: limited long-range performance due to error accumulation and slow inference speed</li>
      <li>두번째 문제는 diffusion process에서 noise를 추가할 때 시계열의 combinations of independent components(trend, seasonal, …)이 망가지는 문제 (특히 주기성이 뚜렷한 경우 interpretability가 부족 <a href="https://openreview.net/pdf?id=rdjeCNUS6TG">Liu et al., (2022)</a>)</li>
    </ul>
  </li>
  <li><strong>본 논문에서는 Transformer를 활용하여 trend와 seasonal을 non-autoregressive하게 생성</strong>
    <ul>
      <li>by imposing different forms of constraints on different representations.</li>
    </ul>
  </li>
  <li>For Reconstruct the <strong>samples</strong> rather than the <strong>noises</strong> in each diffusion step, Fourier-based loss 사용</li>
</ul>

<h2 id="2-problem-statement">2. Problem Statement</h2>

<ul>
  <li>N개로 이루어진 데이터셋 \(D A=\left\{X_{1: \tau}^i\right\}_{i=1}^N\)​
    <ul>
      <li>where \(X_{1: \tau}=\left(x_1, \ldots, x_\tau\right) \in \mathbb{R}^{\tau \times d}\)</li>
    </ul>
  </li>
  <li>목표는 Gaussian vectors \(Z_i=\left(z_1^i, \ldots, z_t^i\right) \in \mathbb{R}^{\tau \times d \times T}\)를 DA와 비슷한 \(\hat{X}_{1: \tau}^i=G\left(Z_i\right)\)로 바꾸는 Generator \(G\)를 학습하는 것</li>
  <li>Time series model은 trend와 여러 개의 seasonality로 구성 : \(x_j=\zeta_j+\sum_{i=1}^m s_{i, j}+e_j\)
    <ul>
      <li>where \(j=0,1, \ldots, \tau-1\)</li>
      <li>\(x_j\) : observed time series</li>
      <li>\(\zeta_j\): trend component</li>
      <li>\(s_{i,j}\): \(i\)-th seasonal component</li>
      <li>\(e_j\): remainder part (contatins the noise and some outliers at time t)</li>
    </ul>
  </li>
</ul>

<h2 id="3-diffusion-ts-interpretable-diffusion-for-time-series">3. Diffusion-TS: Interpretable Diffusion for Time Series</h2>

<ul>
  <li>이러한 interpretable decomposition architecture의 근거는 3가지
    <ul>
      <li>첫째, disentangled patterns in the diffusion model은 아직 연구되지 않음</li>
      <li>둘째, specific designs of architecture and objective 덕분에 interpretable</li>
      <li>셋째, explainable disentangled representations 덕분에 complex dynamics 파악</li>
    </ul>
  </li>
</ul>

<h3 id="31-diffusion-framework">3.1. Diffusion Framework</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig1.png" alt="그림1" /></p>

<ul>
  <li>Forward process
    <ul>
      <li>\(x_0 \sim q(x)\)에서 점점 noisy into Gaussian noise \(x_T \sim \mathcal{N}(0, \mathbf{I})\)</li>
      <li>Parameterization: \(q\left(x_t \mid x_{t-1}\right)=\mathcal{N}\left(x_t ; \sqrt{ } 1-\beta_t x_{t-1}, \beta_t \mathbf{I}\right) \text { with } \beta_t \in(0,1)\)</li>
    </ul>
  </li>
  <li>
    <p>Reverse process</p>

    <ul>
      <li>
        <p>반대로 \(p_\theta\left(x_{t-1} \mid x_t\right)=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)\)</p>
      </li>
      <li>
        <p>MSE: \(\mathcal{L}\left(x_0\right)=\sum_{t=1}^T \underset{q\left(x_t \mid x_0\right)}{\mathbb{E}}\left\|\mu\left(x_t, x_0\right)-\mu_\theta\left(x_t, t\right)\right\|^2\)</p>
        <ul>
          <li>where \(\mu\left(x_t, x_0\right) \text { is the mean of the posterior } q\left(x_{t-1} \mid x_0, x_t\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="32-decomposition-model-architecture">3.2. Decomposition Model Architecture</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig2.png" alt="그림2" /></p>

<ul>
  <li>Noisy sequence가 encoder 통과해서 decoder로 들어옴 (초록색)</li>
  <li>Decoder는 multilayer structure, 각 layer에는 <strong>Transformer Block</strong>, <strong>FFN</strong>, <strong>Trend and Fourier synthetic layer</strong>가 포함됨</li>
  <li>각 layer는 시계열의 각 component를 생성하는 역할
    <ul>
      <li>component에 해당하는 inductive bias를 각 layer에 반영해줌으로써 학습이 쉬워짐</li>
      <li>Trend representation captures the intrinsic trend which changes gradually and smoothly</li>
      <li>Seasonality representation illustrates the periodic patterns of the signal</li>
      <li>Error representation characterizes the remaining parts after removing trend and periodicity</li>
    </ul>
  </li>
  <li>\(w_{(\cdot)}^{i, t}\) where \(i \in 1, \ldots, D\)는 \(i\)번째 decoder block에서의 diffusion step \(t\)를 의미</li>
</ul>

<h3 id="trend-synthesis">Trend Synthesis</h3>

<ul>
  <li>smooth underlying mean of the data, which aims to model slow-varying behavior</li>
  <li>그러므로 Trend \(V_{t r}^t\)를 위해 Polynomial regressor 사용
    <ul>
      <li>\(V_{t r}^t=\sum_{i=1}^D\left(C \cdot \operatorname{Linear}\left(w_{t r}^{i, t}\right)+\mathcal{X}_{t r}^{i, t}\right)\) where \(C=\left[1, c, \ldots, c^p\right]\)</li>
      <li>\(\mathcal{X}_{t r}^{i, t}\)는 the mean value of the output of the \(i\)​-th decoder block</li>
      <li>\(C\)는 slow-varying poly space인데, matrix of powers of vector \(c=[0,1,2, \ldots, \tau-2, \tau-1]^T / \tau\)</li>
      <li>\(p\)는 small degree (e.g. \(p\)​=3) to model low frequency behavior</li>
    </ul>
  </li>
</ul>

<h3 id="seasonality--error-synthesis">Seasonality &amp; Error Synthesis</h3>

<ul>
  <li>이제 Trend, Seasonality, Error 모두 생각해보자.</li>
  <li><strong>결국 문제는 noisy input \(x_t\)에서 seasonal patterns를 구분해내는 것 !</strong></li>
  <li>푸리에 시리즈의 trigonometric representation of seasonal components를 기반으로 Fourier bases를 활용한 Fourier synthetic layers에서 seasonal component 파악</li>
</ul>

<p><img src="/assets/img/timeseries/Diffusion-TS/fomula456.png" alt="그림456" /></p>

<ul>
  <li>\(A_{i, t}^{(k)}, \Phi_{i, t}^{(k)}\) are the phase, amplitude of the \(k\)-th frequency after the DFT \(\mathcal F\) repectively</li>
  <li>\(f_k\)는 Fourier frequency of the corresponding index \(k\)</li>
  <li>결국 the Fourier synthetic layer는 진폭(amplitude)이 큰 frequency를 찾고, 그 frequency들만 IDFT.
    <ul>
      <li>그걸 seasonality로 본다. (Pathformer랑 같은 방식)</li>
    </ul>
  </li>
  <li>최종적으로 original signal: \(\hat{x}_0\left(x_t, t, \theta\right)=V_{t r}^t+\sum_{i=1}^D S_{i, t}+R\)​
    <ul>
      <li>\(R\): output of the last decoder block, which can be regarded as the sum of residual periodicity and other noise.</li>
    </ul>
  </li>
</ul>

<h3 id="33-fourier-based-traning-objective">3.3 Fourier-based Traning Objective</h3>

<ul>
  <li>\(\hat{x}_0\left(x_t, t, \theta\right)\)를 directly estimate
    <ul>
      <li>Reverse process: \(x_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \hat{x}_0\left(x_t, t, \theta\right)+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} x_t+\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t z_t\)</li>
      <li>where \(z_t \sim \mathcal{N}(0, \mathbf{I}), \alpha_t=1-\beta_t \text { and } \bar{\alpha}_t=\prod_{s=1}^t \alpha_s\)​</li>
    </ul>
  </li>
  <li>Reweighting strategy: \(\mathcal{L}_{\text {simple }}=\mathbb{E}_{t, x_0}\left[w_t\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2\right], \quad w_t=\frac{\lambda \alpha_t\left(1-\bar{\alpha}_t\right)}{\beta_t^2}\)​
    <ul>
      <li>where \(\lambda\) is constant (i.e. 0.01)</li>
      <li>즉 small t에서 down-weighted, 모델이 larger diffusion step에 집중하도록 만듬</li>
    </ul>
  </li>
  <li>Fourier-based loss term이 time serie reconstruction에서는 더 좋다 <a href="https://arxiv.org/pdf/2208.05836">Fons et al. (2022)</a>
    <ul>
      <li>: \(\mathcal{L}_\theta=\mathbb{E}_{t, x_0}\left[w_t\left[\lambda_1\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2+\lambda_2\left\|\mathcal{F} \mathcal{F} \mathcal{T}\left(x_0\right)-\mathcal{F F} \mathcal{T}\left(\hat{x}_0\left(x_t, t, \theta\right)\right)\right\|^2\right]\right]\)</li>
    </ul>
  </li>
</ul>

<h3 id="34-conditional-generation-for-time-series-applications">3.4. Conditional Generation for Time Series Applications</h3>

<ul>
  <li><strong>Conditional extensions of the Diffusion-TS</strong>, in which the modeled \(x_0\) is conditioned on targets \(y\)​</li>
  <li>목표는 pre-trained diffusion model과 the gradients of a classifier를 활용하여
    <ul>
      <li>Posterior \(p\left(x_{0: T} \mid y\right)=\prod_{t=1}^T p\left(x_{t-1} \mid x_t, y\right)\)에서 sampling하는 것</li>
    </ul>
  </li>
  <li>\(p\left(x_{t-1} \mid x_t, y\right) \propto p\left(x_{t-1} \mid x_t\right) p\left(y \mid x_{t-1}, x_t\right)\)이므로 bayse theorem을 통해 gradient update
    <ul>
      <li>Score function \(\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t, y\right)=\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t\right)+\nabla_{x_{t-1}} \log p\left(y \mid x_{t-1}\right)\)</li>
      <li>\(\log p\left(x_{t-1} \mid x_t\right)\)은 diffusion model에서 정의됨.</li>
      <li>\(\log p\left(y \mid x_{t-1}\right)\)는 classifier에서 parametrize되며, \(\nabla_{x_{t-1}} \log p\left(y \mid x_{0 \mid t-1}\right)\)로 근사됨</li>
    </ul>
  </li>
  <li>즉 classifier가 높은 likelihood를 가진 영역에서 sample이 생성되도록 하는 것
    <ul>
      <li>: \(\tilde{x}_0\left(x_t, t, \theta\right)=\hat{x}_0\left(x_t, t, \theta\right)+\eta \nabla_{x_t}\left(\left\|x_a-\hat{x}_a\left(x_t, t, \theta\right)\right\|_2^2+\gamma \log p\left(x_{t-1} \mid x_t\right)\right)\)</li>
      <li>where Conditional part \(x_a\), generative part \(x_b\)</li>
      <li>gradient term은 reconstruction-based guidance, \(\eta\)로 강도 조절</li>
    </ul>
  </li>
  <li>각 diffusion step에서 이 gradient update를 여러 번 반복하여 quality 높인다</li>
  <li>Replacing: \(\tilde{x}_a\left(x_t, t, \theta\right):=\sqrt{\bar{\alpha}_t} x_a+\sqrt{ } 1-\bar{\alpha}_t \epsilon\)을 통해, \(\tilde{x}_0\)를 사용한 sample \(x_{t-1}\)가 생성됨</li>
</ul>

<h2 id="4-empirical-evaluaiton">4. Empirical Evaluaiton</h2>

<h3 id="42-metrics">4.2. Metrics</h3>

<ul>
  <li>Discriminative score (Yoon et al., 2019): measures the similarity using a classification model to distinguish between the original and synthetic data as a supervised task;</li>
  <li>Predictive score (Yoon et al., 2019):  measures the usefulness of the synthesized data by training a post-hoc sequence model to predict next-step temporal vectors using the train-synthesis-and-test-real (TSTR) method;</li>
  <li>Context-Frechet Inception Distance (Context-FID) score ´ (Paul et al., 2022):  quantifies the quality of the synthetic time series samples by computing the difference between representations of time series that fit into the local context;</li>
  <li>Correlational score (Ni et al., 2020): uses the absolute error between cross correlation matrices by real data and synthetic data to assess the temporal dependency</li>
</ul>

<h3 id="43-interpretability-results">4.3. Interpretability Results</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig3.png" alt="그림3" /></p>

<ul>
  <li>the corrupted samples (shown in (a)) with 50 steps of noise added as input</li>
  <li>outputs the signals (shown in (c)) that try to restore the ground truth (shown in (b))</li>
  <li>with the aid of the decomposition of temporal trend (shown in (d)) and season &amp; error (shown in (e)).</li>
  <li>Result: As would be expected, the trend curve follows the overall shape of the signal, while the season &amp; error oscillates around zero !</li>
</ul>

<h3 id="44-unconditional-time-series-generation">4.4. Unconditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table1.png" alt="그림21" /></p>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig4.png" alt="그림4" /></p>

<h3 id="45-conditional-time-series-generation">4.5. Conditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig6.png" alt="그림6" /></p>

<h3 id="46-ablaction-study">4.6. Ablaction Study</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table2.png" alt="그림22" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Diffusion-TS, a DDPM-based method for general time series generation
    <ul>
      <li>TS-specific loss design and transformer-based deep decomposition architecture</li>
    </ul>
  </li>
  <li>Unconditional로 훈련된 model이 쉽게 conditional로 확장될 수 있음
    <ul>
      <li>by combining gradients into the sampling !</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/pdf/2403.01742)]]></summary></entry><entry><title type="html">Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting (ICLR 2022)</title><link href="http://localhost:4000/timeseries/2024-07-26-pyraformer/" rel="alternate" type="text/html" title="Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting (ICLR 2022)" /><published>2024-07-26T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/pyraformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-26-pyraformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Pyraformer: a flexible but parsimonious model that can capture a wide range of temporal dependencies, by exploring the multi-resolution representation of the time series
    <ul>
      <li>Pyramidal attention module (PAM)</li>
      <li>the inter-scale tree structure summarizes features at different resolutions</li>
      <li>the intra-scale neighboring connections model the temporal dependencies of different ranges</li>
      <li>the maximum length of the signal traversing path in Pyraformer is a constant with regard to the sequence length L (i.e. \(\mathcal{O}(1)\))</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>시계열 예측에서 Challenge는 powerful but parsimonious model</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/fig1.png" alt="그림1" /></p>

<p><img src="/assets/img/timeseries/pyraformer/table1.png" alt="그림11" /></p>

<ul>
  <li>Pyraformer: to simultaneously capture temporal dependencies of different ranges in a compact multi-resolution fashion</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-method">3. Method</h2>

<h3 id="31-pyramidal-attention-module-pam">3.1. Pyramidal Attention Module (PAM)</h3>

<ul>
  <li>The inter-scale connections form a C-ary tree, in which each parent has C children.
    <ul>
      <li>the nodes at coarser scales can be regarded as the daily, weekly, and even monthly features of the time series</li>
    </ul>
  </li>
  <li>\(\to\) The pyramidal graph offers a multi-resolution representation of the original time series !
    <ul>
      <li>long-range dependencies 파악이 쉬워짐. 그냥 이웃 노드 연결하기만 하면 되니까 (intra-scale)</li>
    </ul>
  </li>
  <li>Original Attention mechanism
    <ul>
      <li>input \(X\), output \(Y\)</li>
      <li>Query \({Q}={X} {W}_Q\), key \({K}={X} {W}_K\), value \({V}={X} {W}_V\)</li>
      <li>where \({W}_Q, {W}_K, {W}_V \in \mathbb{R}^{L \times D_K}\)</li>
      <li>Then, \({y}_i=\sum_{\ell=1}^L \frac{\exp \left({q}_i {k}_{\ell}^T / \sqrt{D_K}\right) {v}_{\ell}}{\sum_{\ell=1}^L \exp \left({q}_i {k}_{\ell}^T / \sqrt{D_K}\right)}\)</li>
      <li>time and space complexity \(\mathcal{O}(L^2)\)</li>
    </ul>
  </li>
  <li>Pyramidal Attention Module (PAM)</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/fomula2.png" alt="그림21" /></p>

<p><img src="/assets/img/timeseries/pyraformer/myfig1.png" alt="그림31" /></p>

<ul>
  <li>Then, \({y}_i=\sum_{\ell \in \mathbb{N}_{\ell}^{(s)}} \frac{\exp \left({q}_i {k}_{\ell}^T / \sqrt{d_K}\right) {v}_{\ell}}{\sum_{\ell \in \mathbb{N}_l^{(s)}} \exp \left({q}_i {k}_{\ell}^T / \sqrt{d_K}\right)}\)</li>
  <li>모든 시점끼리 attention을 하지 않고 conv filter로 nodes를 만들고 이웃 노드끼리 attention !</li>
</ul>

<h3 id="32-coarser-saleㄴ-construvtion-module-cscm">3.2. Coarser-saleㄴ Construvtion Module (CSCM)</h3>

<p><img src="/assets/img/timeseries/pyraformer/fig3.png" alt="그림3" /></p>

<ul>
  <li>PAM이 작동할 수 있도록 pyramidal 구조를 initialize하는 역할</li>
</ul>

<h3 id="33-prediction-module">3.3. Prediction Module</h3>

<ul>
  <li>input embedding 할 때에 예측하고자 하는 길이만큼 붙여서 CSCM, PAM을 통과하면</li>
  <li>예측 시점에 대한 representation을 얻을 수 있음</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/myfig3.png" alt="그림33" /></p>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/pyraformer/table2.png" alt="그림12" /></p>

<p><img src="/assets/img/timeseries/pyraformer/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/pyraformer/fig4.png" alt="그림14" /></p>

<h2 id="5-conclusion-and-outlook">5. Conclusion and Outlook</h2>

<ul>
  <li>Pyraformer: a novel model based on pyramidal attention
    <ul>
      <li>effectively describe both short and long temporal dependencies with low time and space complexity</li>
      <li>CSCM to construct a C-ary tree, and then design the PAM to pass messages in both the inter-scale and the intra-scale fashion</li>
      <li>Pyraformer can achieve the theoretical \(\mathcal{O}(L)\) complexity and \(\mathcal{O}(1)\) maximum signal traversing path length (L: input sequence length)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2022](https://arxiv.org/pdf/2012.07436)]]></summary></entry><entry><title type="html">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)</title><link href="http://localhost:4000/timeseries/2024-07-26-Informer/" rel="alternate" type="text/html" title="Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)" /><published>2024-07-26T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/Informer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-26-Informer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer: quadratic time complexity, high memory usage, and in- herent limitation of the encoder-decoder architecture</li>
  <li><strong>Informer</strong> : efficient transformer-based model for LSTF !
    <ul>
      <li><em>ProbSparse</em> self-attention mechanism
        <ul>
          <li>\(\mathcal{O}(L \log L)\) in time complexity and memory usage</li>
        </ul>
      </li>
      <li>The self-attention distilling
        <ul>
          <li>highlights dominating attention by halving cascading layer input</li>
          <li>and efficiently handles extreme long input sequences</li>
        </ul>
      </li>
      <li>The generative style decoder
        <ul>
          <li>predicts the long time-series sequences at one forward operation, rather than a step-by-step way</li>
          <li>improves the inference speed of long-sequence predictions</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>The major challenge for LSTF is to enhance the <strong>prediction capacity to meet the increasingly long sequence</strong> demand</li>
  <li><em>can we improve Transformer models to be computation, memory, and architecture efficient, as well as maintaining higher prediction capacity?</em></li>
  <li>Vanila Transformer의 limitation 3
    <ul>
      <li>The quadratic computation of self-attention \(\mathcal{O}\left(L^2\right)\)</li>
      <li>The memory bottleneck in stacking layers for long inputs \(\mathcal{O}\left(J \cdot L^2\right)\)</li>
      <li>The speed plunge in predicting long outputs</li>
    </ul>
  </li>
</ul>

<h2 id="2-preliminary">2. Preliminary</h2>

<p>pass</p>

<h2 id="3-methodology">3. Methodology</h2>

<p><img src="/assets/img/timeseries/Informer/fig2.png" alt="그림1" /></p>

<h3 id="query-sparsity-measurement">Query Sparsity Measurement</h3>

<ul>
  <li>Based on KL divergence
    <ul>
      <li>: \(K L(q \| p)=\ln \sum_{l=1}^{L_K} e^{\mathbf{q}_i \mathbf{k}_l^{\top} / \sqrt{d}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \mathbf{q}_i \mathbf{k}_j^{\top} / \sqrt{d}-\ln L_K\)​</li>
    </ul>
  </li>
  <li>\(i\)-th query’s sparsity measurement
    <ul>
      <li>: \(M\left(\mathbf{q}_i, \mathbf{K}\right)=\ln \sum_{j=1}^{L_K} e^{\frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}\)</li>
      <li>즉 query별로 key들과의 attention이 uniform distribution과 얼마나 다른지를 측정</li>
    </ul>
  </li>
</ul>

<h3 id="probsparse-self-attention"><em>ProbSparse</em> Self-attention</h3>

<ul>
  <li>\(\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Softmax}\left(\frac{\overline{\mathbf{Q}} \mathbf{K}^{\top}}{\sqrt{d}}\right) \mathbf{V}\), where \(\overline{\mathbf{Q}}\) only contains the Top-u queries under the sparsity measurement \(M(\mathbf{q}, \mathbf{K})\)​</li>
  <li>Query 중에서 특정 key에 대해서 높은 attention을 가지는 query도 있지만 (Active) 모든 key에 대해서 비슷한 attention을 가지는 query도 있음 (Lazy)
    <ul>
      <li>굳이 모든 query를 다 볼 필요는 없다. Active = useful query이고 Lazy = trivial query</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Informer/myfig3.png" alt="그림2" /></p>

<p><img src="/assets/img/timeseries/Informer/myfig4.png" alt="그림4" /></p>

<ul>
  <li>하지만 모든 query들 중 query가 active한지 알기 위해서는 또 모든 keys와 attention을 계산해봐야 할 것 같지만,
    <ul>
      <li>그렇지 않고 keys를 sampling해서 몇 개만 가져와서 모든 query들과 attention을 계산해도 된다. (증명 : lemma1)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Informer/lemma1.png" alt="그림6" /></p>

<ul>
  <li>그렇게 찾은 useful query들만 가지고, 이제는 모든 keys와 attention을 계산한다</li>
</ul>

<h3 id="encoder-allowing-for-processing-longer-sequential-inputs-under-the-memory-usage-limitation">Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation</h3>

<p><img src="/assets/img/timeseries/Informer/fig3.png" alt="그림7" /></p>

<ul>
  <li>We use the distilling operation to privilege the superior ones with dominating features and make a focused self-attention fea- ture map in the next layer.</li>
  <li>Distilling procedure : \(\mathbf{X}_{j+1}^t=\operatorname{MaxPool}\left(\operatorname{ELU}\left(\operatorname{Conv1d}\left(\left[\mathbf{X}_j^t\right]_{\mathrm{AB}}\right)\right)\right)\)</li>
</ul>

<h3 id="decoder-generating-long-sequential-outputs-through-one-forward-procedure">Decoder: Generating Long Sequential Outputs Through One Forward Procedure</h3>

<ul>
  <li>
    <p>Transformer의 Masked-attention과 Encoder-Decoder Attention 대신 <strong>generative inference</strong></p>

    <ul>
      <li>
        <p>Decoder의 input : \(\mathbf{X}_{\mathrm{de}}^t=\operatorname{Concat}\left(\mathbf{X}_{\text {token }}^t, \mathbf{X}_{\mathbf{0}}^t\right) \in \mathbb{R}^{\left(L_{\text {token }}+L_y\right) \times d_{\text {model }}}\)</p>
      </li>
      <li>
        <p>Start token을 사용하는 대신, Target 직전 시점의 값 몇개를 start token으로 주고, 우리가 원하는 길이의 예측값을 한 번에 decoding</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/Informer/table1.png" alt="그림9" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Informer
    <ul>
      <li><em>ProbSparse</em> self- attention mechanism</li>
      <li>Distilling operation : to handle the challenges of quadratic time complexity and quadratic mem- ory usage in vanilla Transformer</li>
      <li>generative decoder alleviates : alleviates the limitation of tra- ditional encoder-decoder architecture</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2021](https://arxiv.org/pdf/2012.07436)]]></summary></entry><entry><title type="html">Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)</title><link href="http://localhost:4000/timeseries/2024-07-15-timediff/" rel="alternate" type="text/html" title="Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)" /><published>2024-07-15T00:00:00+09:00</published><updated>2024-07-23T18:18:28+09:00</updated><id>http://localhost:4000/timeseries/timediff</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-15-timediff/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>TimeDiff : non-autoregressive diffusion model, w/ two novel conditioning mechanisms
    <ul>
      <li>future mixup : future prediction의 ground-truth의 일부를 conditioning하는 것을 허용</li>
      <li>autoregressive initialization : time series의 basic pattern (short term trends 등)을 모델 initialization에 사용</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Diffusion model (iterative denoising)은 이미지 생성에서 뛰어난 quality
    <ul>
      <li>하지만 time series prediction을 위해 어떻게 쓸지에 대한 연구는 아직</li>
      <li>time series는 <strong>complex dynamics, nonlinear patterns, long-temporal dependencies</strong></li>
    </ul>
  </li>
  <li>기존 diffusion model들은 decoding strategy에 따라 구분됨
    <ul>
      <li><strong>Autoregressive</strong> : future prediction이 one by one으로 generated (ex. Timegrad)
        <ul>
          <li>하지만 error accumulation 때문에 long range prediction 성능이 떨어지고</li>
          <li>하나씩 예측하다보니 inference가 느리다는 단점이 있음</li>
        </ul>
      </li>
      <li><strong>Non-autoregressive</strong> : CSDI, SSSD처럼 denoising networks에 intermediate layers를 conditioning으로 넣고 the denoising objective에 inductive bias를 introduce
        <ul>
          <li>하지만 long-range prediction performance는 Fedformer, NBeats보다 떨어짐</li>
          <li>왜냐하면 conditioning 전략이 image, textf를 위한 것이지 time series를 위한 것이 아니기 때문</li>
          <li>inductive bias를 위해 denoising objective를 사용하는 것만으로는 lookback window에서 유용한 정보를 알아내기 어렵다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>본 논문에서는 long time series prediction을 위한 conditional non-autoregressive diffusion model인 TimeDiff 제안
    <ul>
      <li>CSDI, SSSD와 다르게 conditioning module에 time series를 위한 additional inductive bias 도입
        <ul>
          <li><strong>future mixup</strong>: randomly reveals parts of the ground-truth future pre- dictions during training</li>
          <li><strong>autoregressive initialization</strong>: better initializes the model with basic components in the time series</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-preliminaries">2. Preliminaries</h2>

<h3 id="21-diffusion-models">2.1. Diffusion Models</h3>

<p>pass</p>

<h3 id="22-conditional-ddpms-for-time-series-prediction">2.2. Conditional DDPMs for Time Series Prediction</h3>

<ul>
  <li>\(\mathbf{x}_{-L+1: 0}^0 \in \mathbb{R}^{d \times L}\)를 보고 \(\mathbf{x}_{1: H}^0 \in \mathbb{R}^{d \times H}\)를 예측하는 문제</li>
  <li>\(p_\theta\left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}\right)=p_\theta\left(\mathbf{x}_{1: H}^K\right) \prod_{k=1}^K p_\theta\left(\mathbf{x}_{1: H}^{k-1} \mid \mathbf{x}_{1: H}^k, \mathbf{c}\right)\),
    <ul>
      <li>where \(\mathbf{x}_{1: H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)</li>
    </ul>
  </li>
  <li>아직 efficient denoising network \(\mu_{\theta}\)와 conditioning network \(\mathcal F\) in time series diffusion models를 어떻게 디자인할 것인지 명확하지 않음</li>
  <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad (ICML 2021)</a>
    <ul>
      <li>autoregressive manner :
 \(\begin{aligned}
p_\theta &amp; \left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\right) \\
&amp; =\prod_{t=1}^H p_\theta\left(\mathbf{x}_t^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right) \\
&amp; =\prod^H p_\theta\left(\mathbf{x}_t^K\right) \prod^K p_\theta\left(\mathbf{x}_t^{k-1} \mid \mathbf{x}_t^k, \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right)
\end{aligned}\)</li>
      <li>training objectives : \(\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_t^k, k \mid \mathbf{h}_t\right)\right\|^2\right]\)</li>
      <li>autoregressive decoding 때문에 error accumulation이 발생하고 inference가 느리고 부정확함</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/2107.03502">CSDI(NeurIPS 2021)</a>
    <ul>
      <li>time series \(\mathbf{x}_{-L+1: H}^0\) 전체를 한 번에 diffusing and denoising</li>
      <li>binary mask \(\mathbf{m} \in\{0,1\}^{d \times(L+H)}\)를 사용하여 self-supervised strategy 제안</li>
      <li>training objectives : \(\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_{\text {target }}^k, k \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{\text {observed }}^k\right)\right)\right\|^2\right]\)</li>
    </ul>
  </li>
  <li>하지만 CSDI의 한계는
    <ul>
      <li>Denoising networks가 2개의 transformers를 사용해서 complexity가 높다.</li>
      <li>conditioning에 사용되는 masking은 vision의 inpainting이랑 비슷한데
        <ul>
          <li><a href="https://arxiv.org/abs/2201.09865">(Lugmayr et al., 2022)</a>에서는 이 방식이 masking과 observed사이의 부조화 발생한다고 밝힘</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/2208.09399">SSSD (TMLR 2022)</a>
    <ul>
      <li>Transfermer를 structured state space model로 대체</li>
      <li>하지만 여전히 non-autoregressive strategy이라서 boundary disharmony가 발생할 수 있음</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-model">3. Proposed Model</h2>

<ul>
  <li>Conditional Diffusion의 conditioning은 semantic similarities across modalities 파악에 중점</li>
  <li>하지만 현실에서의 non-stationary time series는 complex temporal dependencies 파악이 중요함</li>
</ul>

<h3 id="31-forward-diffusion-process">3.1. Forward Diffusion Process</h3>

<ul>
  <li>Forward process : \(\mathbf{x}_{1: H}^k=\sqrt{\bar{\alpha}_k} \mathbf{x}_{1: H}^0+\sqrt{1-\bar{\alpha}_k} \epsilon\)
    <ul>
      <li>where \(\epsilon\) is sampled from \(\mathcal{N}(0, \mathbf{I})\) with the same size as \(\mathbf{x}_{1: H}^0\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-conditioning-the-backward-denoising-process">3.2. Conditioning the Backward Denoising Process</h3>

<ul>
  <li>Illustration</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/fig1.png" alt="그림1" /></p>

<h4 id="321-future-mixup">3.2.1. FUTURE MIXUP</h4>

<ul>
  <li>먼저 <em>future mixup</em>으로 \(\mathbf{z}_{\text {mix }}=\mathbf{m}^k \odot \mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)+\left(1-\mathbf{m}^k\right) \odot \mathbf{x}_{1: H}^0\)를 만든다.
    <ul>
      <li>past information’s mapping \(\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)과 the future ground-truth \(\mathbf{x}_{1: H}^0\)를 combine</li>
      <li>training에서 적용되는 것이고, inference에서는 \(\mathbf{z}_{\text {mix }}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)</li>
    </ul>
  </li>
</ul>

<h4 id="322-autoregressive-model">3.2.2. AUTOREGRESSIVE MODEL</h4>

<ul>
  <li>Non-autoregressive models는 masked와 observed의 경계에서 disharmony
    <ul>
      <li>그래서 linear autoregressive (AR) model \(\mathcal{M}_{a r}\) 사용. \(\mathbf{z}_{a r}=\sum_{i=-L+1}^0 \mathbf{W}_i \odot \mathbf{X}_i^0+\mathbf{B}\)
        <ul>
          <li>\(\mathbf{X}_i^0 \in \mathbb{R}^{d \times H}\) is a matrix containing \(H\) copies of \(\mathbf{x}_i^0\),</li>
          <li>\(\mathbf{W}_i\) s \(\in \mathbb{R}^{d \times H}, \mathbf{B} \in \mathbb{R}^{d \times H}\) are trainable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>complex nonlinear time series는 approximate 못하는 건 사실이지만
    <ul>
      <li>simple patterns (short-term trends) 정도는 잘 잡으니까</li>
      <li>그리고 one by one으로 하는 것이 아니라 \(\mathbf{z}_{a r}\)의 모든 columns는 동시에 계산됨</li>
    </ul>
  </li>
</ul>

<h4 id="33-denoising-network">3.3. Denoising Network</h4>

<ul>
  <li>먼저 the transformer’s sinusoidal position embedding으로 the diffusion-step embedding \(\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}\)​ 얻음
    <ul>
      <li>즉 \(\begin{aligned}
k_{\text {embedding }}= &amp; {\left[\sin \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \sin \left(10^{\frac{w \times 4}{w-1}} t\right),\right.} \\
&amp; \left.\cos \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \cos \left(10^{\frac{w \times 4}{w-1}} t\right)\right],
\end{aligned}\)​
        <ul>
          <li>where \(w=\frac{d^{\prime}}{2}\)</li>
        </ul>
      </li>
      <li>그리고 \(\mathbf{p}^k=\operatorname{SiLU}\left(\mathrm{FC}\left(\operatorname{SiLU}\left(\mathrm{FC}\left(k_{\text {embedding }}\right)\right)\right)\right) \in \mathbb{R}^{d^{\prime} \times 1}\)</li>
    </ul>
  </li>
  <li>그 다음 \(\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}\)는 diffused input \(\mathbf{x}_{1: H}^k\)의 embedding \(\mathbf{z}_1^k \in \mathbb{R}^{d^{\prime} \times H}\)에 합쳐짐 (\(2 d^{\prime} \times H\)이 됨)
    <ul>
      <li>\(\mathbf{z}_1^k\)는 여러 개의 convolution layers로 이루어진 input projection block을 통과시켜 얻음</li>
    </ul>
  </li>
  <li>그 다음 multilayer convolution-based <strong>encoder</strong> 통과하면 \(\mathbf{z}_2^k \in \mathbb{R}^{d^{\prime \prime} \times H}\)로 representation</li>
  <li>그 다음 \(\mathbf{c}\)와 \(\mathbf{z}_2^k\)를 fuse해서 \(\left(2 d+d^{\prime \prime}\right) \times H\)로 만들고
    <ul>
      <li>multiple convolution layers <strong>decoder</strong>에 넣어서 \(\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right)\in \mathbb{R^{d \times H}}\)로 만듬 (\(\mathbf{x}_{1: H}^k\)와 같은 size)</li>
    </ul>
  </li>
  <li>마지막으로 \(\mu_{\mathbf{x}}\left(\mathbf{x}_\theta\right)=\frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}^k, k \mid \mathbf{c}\right)\)을 통해 denoised output \(\begin{aligned}
\hat{\mathbf{x}}_{1: H}^{k-1}= &amp; \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
+\sigma_k \epsilon
\end{aligned}\)얻음</li>
  <li>흔히 아는 Diffusion에서는 noise \(\epsilon_\theta\left(\mathbf{x}_{1: H}^k, k\right)\)를 예측하지만, time series에서는  highly irregular noisy components라서 데이터 \(\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k\right)\)를 예측</li>
</ul>

<h3 id="34-training">3.4. Training</h3>

<ul>
  <li>Pesudo code</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/code.png" alt="그림3" /></p>

<ul>
  <li>각각의 \(\mathbf{x}_{1: H}^0\)에 대해 batch of diffusion steps \(k\)’s를 sampling하고
    <ul>
      <li>conditioned variant of loss를 minimize: \(\min _\theta \mathcal{L}(\theta)=\min _\theta \mathbb{E}_{\mathbf{x}_{1 . H}^0, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), k} \mathcal{L}_k(\theta)\)</li>
    </ul>
  </li>
</ul>

<h3 id="35-inference">3.5. Inference</h3>

<ul>
  <li>Pesudo code</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/code.png" alt="그림4" /></p>

<ul>
  <li>먼저 noise vector \(\mathbf{x}_{1 \cdot H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \in \mathbb{R}^{d \times H}\)를 생성하고</li>
  <li>\(\begin{aligned}
\hat{\mathbf{x}}_{1: H}^{k-1}= &amp; \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
+\sigma_k \epsilon
\end{aligned}\)을 반복 (\(k=1\) 까지)
    <ul>
      <li>when \(k=1\), \(\epsilon=0\)이므로 \(\hat{\mathbf{x}}_{1: H}^0\)를 final prediction으로 얻을 수 있음</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/timediff/table2.png" alt="그림12" /></p>

<p><img src="/assets/img/timeseries/timediff/table3.png" alt="그림13" /></p>

<p><img src="/assets/img/timeseries/timediff/fig2.png" alt="그림2" /></p>

<h3 id="43-ablation-study">4.3. Ablation study</h3>

<ul>
  <li><strong>The Effectiveness of Future mixup</strong></li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table4.png" alt="그림14" /></p>

<ul>
  <li>
    <p>특히 ETTh1 데이터셋에서 future mixup을 안썼을 때 성능이 많이 떨어진다.</p>
  </li>
  <li>
    <p><strong>The Mixup Strategies in Future mixup</strong></p>
    <ul>
      <li>Hard mixup : The sampled values in \(\mathbf{m}^k\) are binarized by a threshold \(\tau \in (0,1)\)</li>
      <li>Segment mixup : The mask  \(\mathbf{m}^k\), Each masked segment has a length following the geometric distribution with a mean of 3. This is then followed by an unmasked segment with mean length \(3(1 − \tau)/\tau\)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table5.png" alt="그림15" /></p>

<ul>
  <li><strong>Predicting \(\mathbf{x}_\theta\) vs Predicting \(\epsilon_\theta\)</strong></li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table6.png" alt="그림16" /></p>

<h3 id="44-integration-into-existing-diffusion-models">4.4. Integration into Existing Diffusion Models</h3>

<p><img src="/assets/img/timeseries/timediff/table7.png" alt="그림17" /></p>

<h3 id="45-inference-efficiency">4.5. Inference Efficiency</h3>

<p><img src="/assets/img/timeseries/timediff/table8.png" alt="그림18" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Timediff : diffusion model for time series prediction,
    <ul>
      <li>1)future mixup과 2)autoregressive initialization이라는 conditioning mechanisms으로</li>
      <li>conditioning network에 useful inductive bias를 추가</li>
      <li>한계점으로 변수의 개수가 많을 때 multivariate dependencies를 학습하기 어렵다
        <ul>
          <li>graph 사용 ?</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2023](https://arxiv.org/pdf/2306.05043)]]></summary></entry><entry><title type="html">CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation (NeurIPS 2021)</title><link href="http://localhost:4000/timeseries/2024-07-15-CSDI/" rel="alternate" type="text/html" title="CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation (NeurIPS 2021)" /><published>2024-07-15T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/CSDI</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-15-CSDI/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Imputation에서 autoregressive models보다 score-based diffusion models의 성능이 좋음</li>
  <li>Conditional Score-based Diffusion models for Imputation (CSDI)
    <ul>
      <li>explicitly trained for imputation</li>
      <li>can exploit correlations between observed values</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<p><img src="/assets/img/timeseries/CSDI/fig1.png" alt="그림1" /></p>

<ul>
  <li>Conditional diffusion modeld을 위해서 필요한 것들
    <ul>
      <li>observed values (i.e., conditional information)</li>
      <li>ground-truth missing values (i.e., imputation targets)</li>
      <li>하지만 실제로는 ground-truth missing values를 모르기 때문에</li>
      <li>masked language modeling처럼 self-supervised training
        <ul>
          <li>separates observed values into conditional information and imputation targets</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>RNN \(\to\) GAN and self-training(deterministic) \(\to\) GP-VAE(probabilistic)</li>
  <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad(ICML 2021)</a>에서 diffusion probabilistic models 사용하여 SOTA
    <ul>
      <li>but 과거를 보는 RNNs을 썼기 때문에 time series imputation으로 활용되기는 어려움</li>
    </ul>
  </li>
</ul>

<h2 id="3-background">3. Background</h2>

<h3 id="31-multivariate-time-series-imputation">3.1. Multivariate time series imputation</h3>

<ul>
  <li>\(\mathbf{X}=\left\{x_{1: K, 1: L}\right\} \in \mathbb{R}^{K \times L}\), \(K\)는 the number of features, \(L\)은 length of sequence</li>
  <li>observation mask : \(\mathbf{M}=\left\{m_{1: K, 1: L}\right\} \in\{0,1\}^{K \times L}\)
    <ul>
      <li>\(x_{k, l}\)가 missing이면 \(m_{k, l}=0\), observed이면 \(m_{k, l}=1\)</li>
    </ul>
  </li>
  <li>Timestamps of the time series \(\mathbf{s}=\left\{s_{1: L}\right\} \in \mathbb{R}^L\)</li>
  <li>즉 각각의 time series는 \(\{\mathbf{X}, \mathbf{M}, \mathbf{s}\}\)로 표현됨</li>
</ul>

<h3 id="32-denoising-diffusion-probabilistic-models">3.2. Denoising diffusion probabilistic models</h3>

<ul>
  <li>forward process : \(q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right):=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right) \text { where } q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right):=\mathcal{N}\left(\sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right)\)</li>
  <li>reverse process : \(\begin{aligned}
&amp; p_\theta\left(\mathbf{x}_{0: T}\right):=p\left(\mathbf{x}_T\right) \prod_{t=1}^T p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right), \quad \mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&amp; p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ; {\mu}_\theta\left(\mathbf{x}_t, t\right), \sigma_\theta\left(\mathbf{x}_t, t\right) \mathbf{I}\right)
\end{aligned}\)</li>
</ul>

<h3 id="33-imputation-with-diffusion-models">3.3 Imputation with diffusion models</h3>

<ul>
  <li>conditional observation \(\mathbf{x}_0^{\mathrm{co}} \in \mathcal{X}^{\mathrm{co}}\)를 활용해서 Imputation target \(\mathbf{x}_0^{\mathrm{ta}} \in \mathcal{X}^{\mathrm{ta}}\)을 생성</li>
  <li>Reverse process에 conditional 추가 : modeling \(p_\theta\left(\mathbf{x}_{t-1}^{\mathrm{ta}} \mid \mathbf{x}_t^{\mathrm{ta}}, \mathbf{x}_0^{\mathrm{co}}\right)\)</li>
</ul>

<h2 id="4-conditional-score-based-diffusion-model-for-imputation-csdi">4. Conditional score-based diffusion model for imputation (CSDI)</h2>

<ul>
  <li>Reverse process of the conditional diffusion model, and self-supervised training method</li>
</ul>

<h3 id="41-imputation-with-csdi">4.1. Imputation with CSDI</h3>

<ul>
  <li>all observed values of \(\mathbf{x}_0\) as conditional observations \(\mathbf{x}_0^{\mathrm{co}}\),
    <ul>
      <li>all missing values as imputation targets \(\mathbf{x}_0^{\mathrm{ta}}\)</li>
    </ul>
  </li>
  <li>Parameterization with \({\epsilon}_\theta\): \({\mu}_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}\right)={\mu}^{\mathrm{DDPM}}\left(\mathbf{x}_t^{\mathrm{ta}}, t, {\epsilon}_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}\right)\right), \quad \sigma_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}\right)=\sigma^{\mathrm{DDPM}}\left(\mathbf{x}_t^{\mathrm{ta}}, t\right)\)</li>
</ul>

<h3 id="42-training-of-csdi">4.2. Training of CSDI</h3>

<p><img src="/assets/img/timeseries/CSDI/fig2.png" alt="그림2" /></p>

<p><img src="/assets/img/timeseries/CSDI/fig6.png" alt="그림6" /></p>

<ul>
  <li>Train \({\epsilon}_\theta\) by minimizing the loss function : \(\min _\theta \mathcal{L}(\theta):=\min _\theta \mathbb{E}_{\mathbf{x}_0 \sim q\left(\mathbf{x}_0\right), {\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), t}\left\|\left({\epsilon}-{\epsilon}_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}\right)\right)\right\|_2^2\)</li>
  <li>fig2는 Masked modeling에서 아이디러를 얻은 CSDI의 self-supervised learning method
    <ul>
      <li>흰색은 missing, 파랑색은 observed이다.</li>
      <li>observed의 일부를 imputation target(빨강색)으로 분리하고 noise를 씌운다.</li>
      <li>남은 observed와 noisy target을 보고 imputation target을 맞추도록 학습한다.</li>
      <li>학습 할 때에만 이렇게 하고 실제 sampling(imputation)은 missing(흰색)에 하는 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/CSDI/table1.png" alt="그림11" /></p>

<h3 id="43-choice-of-imputation-targets-in-self-supervised-learning">4.3. Choice of imputation targets in self-supervised learning</h3>

<ul>
  <li><em>Random</em> strategy : missing patterns 모를 때 일정 비율만큼 imputation target으로 설정</li>
  <li><em>Historical</em> strategy : exploits missing patterns in the training dataset. training과 test의 missing pattern이 highly correlated일 때</li>
  <li><em>Mix</em> strategy : 위 두 가지 방법 mix. Training의 missing pattern에 overfitting되는 것을 방지</li>
  <li><em>Test</em> pattern strategy : test의 missing pattern 알 때</li>
</ul>

<h2 id="5-implementation-of-csdi-for-time-series-imputation">5. Implementation of CSDI for time series imputation</h2>

<p><img src="/assets/img/timeseries/CSDI/fig3.png" alt="그림3" /></p>

<ul>
  <li>\(\mathbf{x}_t^{\mathrm{ta}}\)와 \(\mathbf{x}_0^{\mathrm{co}}\)를 \(\mathbb{R}^{K \times L}\)로 만들어주기 위해 zero-padding
    <ul>
      <li>모델의 input에 conditional mask \(\mathbf{m}^{\mathrm{co}} \in\{0,1\}^{K \times L}\)를 추가</li>
    </ul>
  </li>
  <li>The conditional denoising function \({\epsilon}_\theta\left(\mathbf{x}_t^{\mathrm{ta}}, t \mid \mathbf{x}_0^{\mathrm{co}}, \mathbf{m}^{\mathrm{co}}\right)\)은 \({\epsilon}_\theta:\left(\mathbb{R}^{K \times L} \times \mathbb{R} \mid \mathbb{R}^{K \times L} \times\{0,1\}^{K \times L}\right) \rightarrow \mathbb{R}^{K \times L}\)로 표현됨</li>
</ul>

<h3 id="attention-mechanism">Attention mechanism</h3>

<ul>
  <li>Multivariate time series의 temporal and feature dependency를 파악하기 위해
    <ul>
      <li>two dimensional attention mechanism 활용 (conv 대신)</li>
      <li>각각을 temporal Transformer layer and a feature Trans- former layer라 함</li>
    </ul>
  </li>
</ul>

<h2 id="6-experimental-results">6. Experimental results</h2>

<h3 id="61-time-series-imputation">6.1. Time series imputation</h3>

<p><img src="/assets/img/timeseries/CSDI/table2.png" alt="그림12" /></p>

<p><img src="/assets/img/timeseries/CSDI/fig4.png" alt="그림4" /></p>

<p><img src="/assets/img/timeseries/CSDI/table3.png" alt="그림13" /></p>

<h3 id="62-interpolation-of-irregularly-sampled-time-series">6.2. Interpolation of irregularly sampled time series</h3>

<p><img src="/assets/img/timeseries/CSDI/table4.png" alt="그림14" /></p>

<h3 id="63-time-series-forecasting">6.3. Time series Forecasting</h3>

<p><img src="/assets/img/timeseries/CSDI/table5.png" alt="그림15" /></p>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>CSDI : novel approach to impute multivariate time series with conditional diffusion models</li>
  <li>Future works
    <ul>
      <li>improve the computation efficiency</li>
      <li>extend CSDI to downstream tasks such as classifications</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[NeurIPS 2021](https://arxiv.org/pdf/2107.03502)]]></summary></entry><entry><title type="html">Timegrad :Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (ICML 2021)</title><link href="http://localhost:4000/timeseries/2024-07-09-Timegrad/" rel="alternate" type="text/html" title="Timegrad :Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (ICML 2021)" /><published>2024-07-09T00:00:00+09:00</published><updated>2024-07-11T14:35:37+09:00</updated><id>http://localhost:4000/timeseries/Timegrad</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-09-Timegrad/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li><strong>TimeGrad</strong> : auto-regressive model for multivariate time series forecasting, using diffusion
    <ul>
      <li>learns gradients by optimizing a variational bound on the data likelihood</li>
      <li>inference는 white noise에서 학습한 분포의 sample로 convert (though a Markov chain)</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<p>pass</p>

<h2 id="2-diffusion-probabilistic-model">2. Diffusion Probabilistic Model</h2>

<p>pass</p>

<h2 id="3-timegrad-method">3. TimeGrad Method</h2>

<ul>
  <li>Multivariate time series \(x_{i, t}^0 \in \mathbb{R}\) for \(i \in\{1, \ldots, D\}\) where \(t\) is the time index
    <ul>
      <li>at time \(t\), \(\mathbf{x}_t^0 \in \mathbb{R}^D\)</li>
      <li>context window \(\left[1, t_0\right)\), prediction interval \(\left[t_0, T\right]\)</li>
    </ul>
  </li>
  <li>TimeGrad 이전까지는 full joint distribution at each time step을 모델링 했어야 함
    <ul>
      <li>하지만 full covariance matrix을 모델링 하는 것은 computation cost 측면에서 impractical</li>
      <li>그래서 Gaussians with low-rank covariance matrices으로 approximate 하기도 함 (Vec-LSTM)</li>
    </ul>
  </li>
  <li>본 논문에서는 과거 데이터를 보고 미래 시점의 conditional distribution을 학습
    <ul>
      <li>formula : \(q_{\mathcal{X}}\left(\mathbf{x}_{t_0: T}^0 \mid \mathbf{x}_{1: t_0-1}^0, \mathbf{c}_{1: T}\right)=\Pi_{t=t_0}^T q_{\mathcal{X}}\left(\mathbf{x}_t^0 \mid \mathbf{x}_{1: t-1}^0, \mathbf{c}_{1: T}\right)\)​​</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/fig1.png" alt="그림1" /></p>

<h3 id="31-training">3.1. Training</h3>

<ul>
  <li>이전 시점 데이터 \(\mathbf{x}_{t-1}^0\)과 covariates \(\mathbf{c}_{t-1}\)이 들어오면 hidden state \(\mathbf{h}_{t-2}\)를 \(\mathbf{h}_{t-1}\)​로 업데이트
    <ul>
      <li>즉 \(\mathbf{h}_t=\mathrm{RNN}_\theta\left(\operatorname{concat}\left(\mathbf{x}_t^0, \mathbf{c}_t\right), \mathbf{h}_{t-1}\right)\)</li>
    </ul>
  </li>
  <li>그러면 위에 있는 fomula는 \(\Pi_{t=t_0}^T p_\theta\left(\mathbf{x}_t^0 \mid \mathbf{h}_{t-1}\right)\)가 되고
    <ul>
      <li>Negative log-likelihood \(\sum_{t=t_0}^T-\log p_\theta\left(\mathbf{x}_t^0 \mid \mathbf{h}_{t-1}\right)\)를 minimize하도록 학습</li>
    </ul>
  </li>
</ul>

<h3 id="32-inference">3.2. Inference</h3>

<ul>
  <li>Inference할 때에는 한 시점씩 auto-regressive하게 작동
    <ul>
      <li>만약 다음 시점의 sample \(\mathbf{x}_{T+1}^0\)을 얻었다면 위에서 설명한 것처럼 hidden state \(\mathbf{h}_{T+1}\)를 얻고</li>
      <li>같은 과정을 반복. 얻은 sample로 또 다음 sample을 얻고…</li>
    </ul>
  </li>
</ul>

<h3 id="33-scaling">3.3. Scaling</h3>

<ul>
  <li>각 context window를 scale normalizing</li>
  <li>Residual connection은 사용하지 않음</li>
</ul>

<h3 id="34-covariates">3.4. Covariates</h3>

<ul>
  <li>\(\mathbf{c}_t\)는 time-dependent and time- independent embeddings으로 구성되는 embeddings for categorical features</li>
  <li>All covariates are thus known for the periods we wish to forecast !</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>사용한 데이터셋</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/table1.png" alt="그림11" /></p>

<ul>
  <li>Model architecture</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/fig2.png" alt="그림2" /></p>

<ul>
  <li>Results</li>
</ul>

<p><img src="/assets/img/timeseries/Timegrad/table2.png" alt="그림12" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2021](https://arxiv.org/pdf/2101.12072)]]></summary></entry><entry><title type="html">TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis (ICLR 2023)</title><link href="http://localhost:4000/timeseries/2024-07-08-TimesNet/" rel="alternate" type="text/html" title="TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis (ICLR 2023)" /><published>2024-07-08T00:00:00+09:00</published><updated>2024-07-09T00:34:54+09:00</updated><id>http://localhost:4000/timeseries/TimesNet</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-08-TimesNet/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Temporal variation modeling을 위해서 multi-periodicity in time series를 파악
    <ul>
      <li>complex temporal variations into the multiple intraperiod- and interperiod-variations</li>
      <li>각각이 2D tensor의 행과 열이 된다.</li>
    </ul>
  </li>
  <li>TimesBlock : task-general backbone for TS analysis</li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>다른 타입의 데이터(language, video, …) Time series는 시점마다 하나의 숫자가 기록되고, 연속적으로 기록되기 때문에 충분한 semantic information을 제공하지 않는다.
    <ul>
      <li>그래서 더 informative하고 inherent properties(continuity, periodicity, trend, …)를 reflect하는 temporal variations를 파악하고자 함</li>
      <li>하지만 temporal patterns는 복잡하고 다양한 variations가 mix and overlap 되어있음</li>
    </ul>
  </li>
  <li>복잡한 temporal variations를 파악하기 위해
    <ul>
      <li>RNN-based approach는 sequential computation paradigm 때문에 어렵고</li>
      <li>TCN-based approach는 1d conv의 locality property 때문에 long-term dependencies 파악 어려움</li>
      <li>Transformer-based approach도 directly find out하기는 어려움
        <ul>
          <li>그래서 intricate temporal variations을 찾기 위해 <strong>multi-periodicity</strong>를 활용</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>multiple periods는 overlap and interact with each other. 그래서 intractable
    <ul>
      <li>또한 각 periods에서 variation of each time point는 adjacent area뿐만 아니라 adjacent periods의 variation에도 많은 영향을 받음
        <ul>
          <li>전자가 intraperiod-variation (short-term temporal patterns within a period),</li>
          <li>후자가 interperiod-variation (long-term trends of consecutive different periods)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>1d time series에는 두 가지 variations를 동시에 표현하기 어려움 그래서 2d tensor로 만들고
    <ul>
      <li>각 columns에는 time points within a period,</li>
      <li>각 row에는 time points at the same phase among different periods</li>
    </ul>
  </li>
  <li>TimesNet은 learned periods로 multi-periodicity를 발견하고 intraperiod- and interperiod-variations를 capture</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>Classical methods : ARIMA, Holt-Winter, Prophet
    <ul>
      <li>temporal variations가 pre-defined pattern을 따른다고 가정하지만 실제로는 X</li>
    </ul>
  </li>
  <li>MLP-based
    <ul>
      <li>adopt the MLP along the temporal dimension</li>
      <li>encode the temporal dependencies into the <strong>fixed</strong> parameter of MLP layers</li>
    </ul>
  </li>
  <li>TCN-based
    <ul>
      <li>convolutional kernels that slide along the temporal dimension</li>
    </ul>
  </li>
  <li>RNN-based
    <ul>
      <li>state transitions among time steps</li>
    </ul>
  </li>
  <li>Autoformer
    <ul>
      <li>the series-wise temporal dependencies based on the learned periods</li>
      <li>deep decomposition architecture to obtain the seasonal and trend parts of input series</li>
    </ul>
  </li>
  <li>FEDformer
    <ul>
      <li>mixture-of-expert design to enhance the seasonal-trend decomposition</li>
      <li>sparse attention within the frequency domain</li>
    </ul>
  </li>
  <li><strong>TimesNet : temporal 2D-variations derived by periodicity</strong></li>
</ul>

<h2 id="3-timesnet">3. TimesNet</h2>

<ul>
  <li>TimesBlock
    <ul>
      <li>transform the 1D time series into 2D space</li>
      <li>simultaneously model the two types of variations by a parameter-efficient inception block</li>
    </ul>
  </li>
</ul>

<h3 id="31-transform-1d-variations-into-2d-variations">3.1. Transform 1d-variations into 2d-variations</h3>

<p><img src="/assets/img/timeseries/TimesNet/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p>Two Temporal variations, simultaneously !</p>

    <ul>
      <li>
        <p>with its adjacent area (intraperiod-variations)</p>
      </li>
      <li>
        <p>with the same phase among different periods (interperiod-variations)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>원래 Time series는 \(\mathbf{X}_{1 \mathrm{D}} \in \mathbb{R}^{T \times C}\)</p>
  </li>
  <li>
    <p>먼저  Fast Fourier Transform (FFT)으로 periods를 찾음</p>

    <p>\(\mathbf{A}=\operatorname{Avg}\left(\operatorname{Amp}\left(\operatorname{FFT}\left(\mathbf{X}_{1 \mathrm{D}}\right)\right)\right),\left\{f_1, \cdots, f_k\right\}=\underset{f_* \in\left\{1, \cdots,\left[\frac{T}{2}\right]\right\}}{\arg \operatorname{Topk}}(\mathbf{A}), p_i=\left\lceil\frac{T}{f_i}\right\rceil, i \in\{1, \cdots, k\}\)</p>
    <ul>
      <li>\(FFT(\cdot)\)은 Fast Fourier Transform, \(\text{Amp}(\cdot)\)은 amplitude 값 계산</li>
      <li>\(\mathbf{A} \in \mathbb{R}^T\)는 각 frequency에서 계산된 amplitude (averaged from C dimensions by \(\text{Avg}(\cdot)\))
        <ul>
          <li>즉 \(\mathbf{A}_j\)는 intensity of the frequency-j periodic basis function (period length는 \(\left\lceil\frac{T}{j}\right\rceil\))</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>top k개의 amplitude에 해당하는 most significant frequencies \(\left\{f_1, \cdots, f_k\right\}\)만 사용</p>

    <ul>
      <li>불필요한 high frequencies는 필요 없으니까</li>
      <li>해당하는 period length는 \(\left\{p_1, \cdots, p_k\right\}\)</li>
      <li>즉 \(\mathbf{A},\left\{f_1, \cdots, f_k\right\},\left\{p_1, \cdots, p_k\right\}=\operatorname{Period}\left(\mathbf{X}_{1 \mathrm{D}}\right)\)</li>
    </ul>
  </li>
  <li>
    <p>이제 \(\mathbf{X}_{1 \mathrm{D}} \in \mathbb{R}^{T \times C}\)를 여러 개의 2d-tensors로 바꿈</p>

    <ul>
      <li>\(\mathbf{X}_{2 \mathrm{D}}^i=\operatorname{Reshape}_{p_i, f_i}\left(\text { Padding }\left(\mathbf{X}_{1 \mathrm{D}}\right)\right), i \in\{1, \cdots, k\}\)​</li>
      <li>Padding은 \(\operatorname{Reshape}_{p_i, f_i}(\cdot)\)을 위해 수행</li>
      <li>즉 \(\mathbf{X}_{2 \mathrm{D}}^i \in \mathbb{R}^{p_i \times f_i \times C}\)는 time series based on frequency-\(f_i\)
        <ul>
          <li>열은 intraperiod-variation, 행은 interperiod-variation, under the corresponding period length \(p_i\)​</li>
        </ul>
      </li>
      <li>최종적으로는 서로 다른 주기 k개에 대한 2d-tensors \(\left\{\mathbf{X}_{2 \mathrm{D}}^1, \cdots, \mathbf{X}_{2 \mathrm{D}}^k\right\}\)​​를 얻음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TimesNet/fig2.png" alt="그림2" /></p>

<h3 id="32-timesblock">3.2. TimesBlock</h3>

<p><img src="/assets/img/timeseries/TimesNet/fig3.png" alt="그림3" /></p>

<ul>
  <li>처음에는 \(\mathbf{X}_{1 \mathrm{D}}^0=\operatorname{Embed}\left(\mathbf{X}_{1 \mathrm{D}}\right)\)하고 \(l=1\)부터는 \(\mathbf{X}_{1 \mathrm{D}}^l=\text { TimesBlock }\left(\mathbf{X}_{1 \mathrm{D}}^{l-1}\right)+\mathbf{X}_{1 \mathrm{D}}^{l-1}\)</li>
  <li>각 TimesBlock은 two successive parts:
    <ul>
      <li>capturing temporal 2D-variations</li>
      <li>adaptively aggregating representations from different periods</li>
    </ul>
  </li>
</ul>

<h3 id="capturing-temporal-2d-variations">Capturing temporal 2D-variations</h3>

\[\begin{aligned}
\mathbf{A}^{l-1},\left\{f_1, \cdots, f_k\right\},\left\{p_1, \cdots, p_k\right\} &amp; =\operatorname{Period}\left(\mathbf{X}_{1 \mathrm{D}}^{l-1}\right) \\
\mathbf{X}_{2 \mathrm{D}}^{l, i} &amp; =\operatorname{Reshape}_{p_i, f_i}\left(\operatorname{Padding}\left(\mathbf{X}_{1 \mathrm{D}}^{l-1}\right)\right), i \in\{1, \cdots, k\} \\
\widehat{\mathbf{X}}_{2 \mathrm{D}}^{l, i} &amp; =\operatorname{Inception}\left(\mathbf{X}_{2 \mathrm{D}}^{l, i}\right), i \in\{1, \cdots, k\} \\
\widehat{\mathbf{X}}_{1 \mathrm{D}}^{l, i} &amp; =\operatorname{Trunc}\left(\operatorname{Reshape}_{1,\left(p_i \times f_i\right)}\left(\widehat{\mathbf{X}}_{2 \mathrm{D}}^{l, i}\right)\right), i \in\{1, \cdots, k\},
\end{aligned}\]

<ul>
  <li>\(\mathbf{X}_{2 \mathrm{D}}^{l, i} \in \mathbb{R}^{p_i \times f_i \times d_{\text {model }}}\)은 \(i\)-번째 transformed 2d-tensor</li>
  <li>그 다음  parameter-efficient inception block \(\text{Inception}(·)\)을 거쳐 \(\widehat{\mathbf{X}}_{2D}^{l, i}\)를 얻고</li>
  <li>
    <p>Aggregation을 위해 \(\text{Trunc}(\cdot)\)을 사용하여  \(\widehat{\mathbf{X}}_{\mathrm{1D}}^{l, i} \in \mathbb{R}^{T \times d_{\mathrm{model}}}\)로 되돌린다.</p>
  </li>
  <li>intraperiod-variation (columns) : cover adjacent time points</li>
  <li>interperiod-variation (rows) : cover adjacent periods</li>
  <li>서로 다른 여러 개의 2d-tensors \(\left\{\mathbf{X}_{2 \mathrm{D}}^{l, 1}, \cdots, \mathbf{X}_{2 \mathrm{D}}^{l, k}\right\}\)에 대해 shared inception block
    <ul>
      <li>for parameter efficiency, invariant to the selection of hyper-parameter k</li>
    </ul>
  </li>
</ul>

<h3 id="adaptive-aggregation">Adaptive Aggregation</h3>

<ul>
  <li>이제 k개의 서로 다른 1d-representations \(\left\{\widehat{\mathbf{X}}_{1 \mathrm{D}}^{l, 1}, \cdots, \widehat{\mathbf{X}}_{1 \mathrm{D}}^{l, k}\right\}\)를 fuse해서 다음 layer에 전달해야 함
    <ul>
      <li>amplitudes \(\mathbf{A}\)는 각 frequencies and periods의 상대적인 중요도라 할 수 있으므로</li>
      <li>\(\begin{aligned}
\widehat{\mathbf{A}}_{f_1}^{l-1}, \cdots, \widehat{\mathbf{A}}_{f_k}^{l-1} &amp; =\operatorname{Softmax}\left(\mathbf{A}_{f_1}^{l-1}, \cdots, \mathbf{A}_{f_k}^{l-1}\right) \\
\mathbf{X}_{1 \mathrm{D}}^l &amp; =\sum_{i=1}^k \widehat{\mathbf{A}}_{f_i}^{l-1} \times \widehat{\mathbf{X}}_{1 \mathrm{D}}^{l, i}
\end{aligned}\)로 aggregate</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/TimesNet/table1.png" alt="그림11" /></p>

<p><img src="/assets/img/timeseries/TimesNet/fig4.png" alt="그림4" /></p>

<h3 id="main-results">Main results</h3>

<p><img src="/assets/img/timeseries/TimesNet/table2.png" alt="그림12" /></p>

<h3 id="imputation-task">Imputation task</h3>

<p><img src="/assets/img/timeseries/TimesNet/table4.png" alt="그림14" /></p>

<h3 id="classification-task">Classification task</h3>

<p><img src="/assets/img/timeseries/TimesNet/fig5.png" alt="그림5" /></p>

<h3 id="anomaly-detection-task">Anomaly detection task</h3>

<p><img src="/assets/img/timeseries/TimesNet/table5.png" alt="그림15" /></p>

<h3 id="representation-analysis">Representation analysis</h3>

<p><img src="/assets/img/timeseries/TimesNet/fig6.png" alt="그림6" /></p>

<h2 id="5-conclusion-and-future-work">5. Conclusion and Future Work</h2>

<ul>
  <li>TimeNet은 multi-periodicity를 기반으로 복잡한 temporal variations를 파악
    <ul>
      <li>intraperiod- and interperiod-variations in 2D space by a parameter-efficient inception block.</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://arxiv.org/pdf/2210.02186)]]></summary></entry><entry><title type="html">MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting (Arxiv 2023)</title><link href="http://localhost:4000/timeseries/2024-07-06-MultiResFormer/" rel="alternate" type="text/html" title="MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting (Arxiv 2023)" /><published>2024-07-06T00:00:00+09:00</published><updated>2024-07-06T12:23:19+09:00</updated><id>http://localhost:4000/timeseries/MultiResFormer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-06-MultiResFormer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer-based models는 TS를 segments로 나눠서 encode (<strong>patches</strong>)
    <ul>
      <li>다양한 <strong>scale</strong>(i.e.. <strong>resolutions</strong>)에서의 TS를 모델링</li>
    </ul>
  </li>
  <li>하지만 pre-defined scale(patch의 길이)은…
    <ul>
      <li>variety of intricate temporal dependencies를 찾기 어려움</li>
    </ul>
  </li>
  <li>그래서 MultiResFormer는 adaptive time scale.
    <ul>
      <li>patch length를 주어진 데이터의 주기성을 보고 찾겠다</li>
      <li>그 다음 intraperiod and interperiod dependencies 학습</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>TSF를 위한 vanila Transformer의 modification 3:</li>
  <li>첫째. Efficient attention mechanisms for sub-quadratic attention computation
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2012.07436">Informer(2021)</a>, <a href="https://openreview.net/pdf?id=0EXmFzUn5I">Pyraformer(2022)</a></li>
    </ul>
  </li>
  <li>둘째. Breaking the point-wise nature of dot-product attention for segment or series level dependency modeling
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2106.13008">Autoformer(2021)</a>, <a href="https://arxiv.org/pdf/2201.12740">Fedformer(2022)</a>, <a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>, <a href="https://openreview.net/pdf?id=vSVLM2j9eie">Crossformer(2023)</a></li>
    </ul>
  </li>
  <li>셋째. Modeling sequences at multiple time scales with hierarchical representation learning
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2204.13767">Triformer(2022)</a>, <a href="https://arxiv.org/pdf/2206.04038">Scaleformer(2023)</a>, <a href="https://openreview.net/pdf?id=lJkOCMP2aW">Pathformer(2024)</a></li>
    </ul>
  </li>
  <li>하지만 세 번째 multi-scale methods의 경우 pre-defined resolution으로 인해 generalization이 안된다.</li>
  <li>그래서 본 논문에서는 데이터의 underlying periodicities을 파악해서 데이터에 맞게 multi-resolution view</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig1.png" alt="그림1" /></p>

<ul>
  <li>본 논문에서는 two core Transformer sublayers를 repurpose:
    <ul>
      <li><strong>Multi-headed attention</strong> for “interperiod” variation modeling
        <ul>
          <li>MHA는 전역적인 패치 간 의존성을 모델링하는 데 강점을 가지고 있어 interperiod 변동을 모델링하는 데 적합</li>
        </ul>
      </li>
      <li><strong>Position-wise Feed-Forward network</strong> for “intraperiod” variation modeling
        <ul>
          <li>FFN은 각 위치 내의 복잡한 의존성을 모델링하는 데 강점을 가지고 있어 intraperiod 변동을 모델링하는 데 적합</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>고려해야 할 사항들
    <ul>
      <li>어떻게 different resolution branches끼리 parameter-sharing을 할까 ?
        <ul>
          <li>일단 parameter-sharing을 해야 특정 scale에 overfitting되는 걸 방지하는 건 맞음</li>
          <li>patch length 모르니까 linear projection 못 쓰고, 그냥 padding하는 건 모델 학습을 방해함</li>
          <li>그래서 각 scale에서의 patches의 길이를 맞추기 위한 <strong>interpolation scheme</strong> 사용</li>
          <li>그리고 <strong>resolution embedding</strong>으로 scale-awareness</li>
        </ul>
      </li>
      <li>계산 복잡도를 어떻게 줄일까 ?
        <ul>
          <li>이미 interpolation scheme으로 patches의 길이를 맞춰줬으니 별도의 embedding 필요 없음
            <ul>
              <li>Dlinear model의 성공 사례에서 영감 받음</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="time-series-transformer">Time series Transformer</h3>

<ul>
  <li>Transformer의 quadratic complexity 때문에 longer series는 모델링하기 어려웠음
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a> : sparse attention blocks where each token attends to others with an exponential step size</li>
      <li><a href="https://arxiv.org/pdf/2012.07436">Informer(2021)</a> : entropy-based measurement to filter out uninformative keys for \(O(N)\)</li>
      <li><a href="https://arxiv.org/pdf/2204.13767">Triformer(2022)</a>, <a href="https://openreview.net/pdf?id=0EXmFzUn5I">Pyraformer(2022)</a> : adopt CNN-like approaches for local attention operations</li>
      <li><a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a> : patch단위로 attention 연산 하니까 \(O(N^2/S^2)\)</li>
    </ul>
  </li>
  <li>단일 시점의 데이터는 정보가 별로 없음 (<a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>)
    <ul>
      <li>단일 시점을 토큰으로 하는 transformer는 localized patterns을 간과할 수 있음</li>
    </ul>
  </li>
  <li>channel-mixing embedding은 over-fitting 발생시킬 수 있음 (<a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>)</li>
</ul>

<h3 id="multi-resolution-time-series-modeling">Multi-Resolution Time Series Modeling</h3>

<ul>
  <li><a href="https://arxiv.org/pdf/2210.02186">TimesNet(2023)</a>에서 adaptive multi-resolution modeling 하긴 함
    <ul>
      <li>하지만 input length를 맞춰줘야 해서 flatten해야 하고 longer series 예측 못함</li>
      <li>channel-mixing embedding이 불가피해서 overfitting</li>
    </ul>
  </li>
</ul>

<h2 id="3-adaptive-multi-resolution-time-series-modeling-with-transformers">3. Adaptive Multi-Resolution Time Series Modeling with Transformers</h2>

<ul>
  <li>\(\mathbf{X}_{1 \ldots I}=\left(\mathbf{x}_1, \ldots, \mathbf{x}_I\right) \in \mathbb{R}^{I \times V}\)로 \(\mathbf{X}_{I+1 \ldots I+O}=\left(\mathbf{x}_{I+1}, \ldots, \mathbf{x}_{I+O}\right) \in \mathbb{R}^{O \times V}\)​ 예측</li>
</ul>

<h3 id="31-multiresformer">3.1. MultiResFormer</h3>

<p><img src="/assets/img/timeseries/MultiResFormer/fig2.png" alt="그림2" /></p>

<ul>
  <li>The periodicity-aware patching module, detecting salient periodicities (section 3.2.)</li>
  <li>The Transformer Encoder block, shared across all resolution branches (section 3.3.)</li>
  <li>aggregate the representations derived within each resolution branch into \(\mathbf{X}^{(l)} \in \mathbf{R}^{I \times V}\)​ (section 3.4.)</li>
</ul>

<h3 id="32-salient-periodicity-detection">3.2. Salient Periodicity Detection</h3>

<ul>
  <li>
    <p>salient periodicites of the input series는 Fast Fourier Transform (FFT)으로 찾음</p>

\[\begin{aligned}
\mathbf{A} &amp; =\operatorname{Avg}(\operatorname{Amp}(\operatorname{FFT}(\mathbf{X}))) \\
\left\{f_1, \ldots, f_k\right\} &amp; =\underset{f_* \in\left\{1, \ldots,\left\lfloor\frac{I}{2}\right\rfloor\right\}}{\operatorname{argTopk}}(\mathbf{A}) \\
\text { Period }_i &amp; =\left\lceil\frac{I}{f_i}\right\rceil
\end{aligned}\]
  </li>
  <li>
    <p>Gradient-based 방식이 아니라서 미분이 필요없음</p>
  </li>
</ul>

<h3 id="33-multi-resolution-modeling-with-a-shared-transformer-block">3.3. Multi-Resolution Modeling with a Shared Transformer Block</h3>

<ul>
  <li>PatchTST처럼 fixed patch length 사용할 땐 high-dimensional embeddings을 위한 linear transformations가 필요했는데, 이제는 patch embedding layers가 필요없어짐 (efficient)</li>
  <li>다양한 patch length를 사용한다고 해서 길이를 맞춰주기 위해 padding을 한다고 하더라도, MHA와 FFN에는 masking mechanism이 없기 때문에 모델 성능 저하
    <ul>
      <li>그러므로 interpolation으로 original patches의 temporal characteristics 보존</li>
    </ul>
  </li>
  <li>one resolution branch의 Periodi가 주어지면 길이 Periodi의 겹치치 않는 patch로 분할
    <ul>
      <li>그 다음 길이 d가 되도록 linearly interpolate</li>
      <li>shape of the patch-based representation of the input series : \(V \times\left\lceil\frac{I}{\text { Period }_i}\right\rceil \times d\)</li>
    </ul>
  </li>
  <li>각 resolution branch에서 same-sized patches로 연산이 이루어지기 때문에 resolution embedding을 linearly interpolate에 더해줌 (transformer block에게 해상도 알려주기 위해)</li>
  <li>MHA to capture patch-wise dependencies (interperiod variation modeling)
    <ul>
      <li>FFN layers for capturing dependencies within each patch (intraperiod variation modeling)</li>
    </ul>
  </li>
</ul>

<h3 id="34-adaptive-aggregation">3.4. Adaptive Aggregation</h3>

<ul>
  <li>i 번째 resolution에서 representation의 shape은 \(V \times\left\lceil\frac{I}{\text { Period }_i}\right\rceil \times d\)​</li>
  <li>interpolation으로 \(I \times V\)로 representation</li>
  <li>
    <p>\(I \times V\)으로 표현된 모든 k개의 resolution에서의 representation을 adaptive aggregation:</p>

\[\begin{aligned}
  &amp; \left\{A_1, \ldots, A_k\right\}=\underset{f_* \in\left\{1, \ldots,\left\lfloor\frac{I}{2}\right\rfloor\right\}}{\operatorname{Topk}}(\mathbf{A}) \\
  &amp; \left\{w_1, \ldots, w_k\right\}=\operatorname{Softmax}\left(\left\{A_1, \ldots, A_k\right\}\right)
  \end{aligned}\]
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>Main Results</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/table12.png" alt="그림3" /></p>

<ul>
  <li>Ablation Study</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/table4.png" alt="그림4" /></p>

<ul>
  <li>Varying Look-back Window Size</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig3.png" alt="그림5" /></p>

<ul>
  <li>Representation analysis</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig4.png" alt="그림6" /></p>

<ul>
  <li>Efficiency Comparison</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig5.png" alt="그림7" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>각 transformer blocks 내에서 FFT로 데이터의 underlying periodicities를 파악하고 resolution 결정</li>
  <li>각 transformer blocks 내에서 interpolation 덕분에 resolution끼리 parameter sharing</li>
  <li>블록 내의 encoder는 interpolation으로 input size와 output representation의 size가 같아서
    <ul>
      <li>embedding layer가 필요없고 final linear prediction head layer에서의 parameters 개수가 적음</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2311.18780)]]></summary></entry><entry><title type="html">Neyman-Pearson Hypothesis Testing</title><link href="http://localhost:4000/stat/2024-07-02-NPtest/" rel="alternate" type="text/html" title="Neyman-Pearson Hypothesis Testing" /><published>2024-07-02T00:00:00+09:00</published><updated>2024-07-04T09:50:22+09:00</updated><id>http://localhost:4000/stat/NPtest</id><content type="html" xml:base="http://localhost:4000/stat/2024-07-02-NPtest/"><![CDATA[<h2 id="basics-of-hypothesis-testing">Basics of Hypothesis Testing</h2>

<ul>
  <li>일반적으로 가설 검정을 construction하는 과정은 아래와 같다.
    <ul>
      <li>Test statistic \(T_n=T_n\left(X_1, \ldots, X_n\right)\) 선택</li>
      <li>Rejection region (기각역) 설정</li>
      <li>만약 \(T_n \in R\)이면 귀무가설 기각, 그렇지 않으면 기각할 수 없다.</li>
    </ul>
  </li>
  <li>가설 검정을 하는 이유는 귀무가설이 참인지 거짓인지 판단하기 위함이 아니다.
    <ul>
      <li>정확히는 <strong>귀무가설을 기각할 충분한 evidence</strong>가 있는지를 판단하기 위함이다.</li>
    </ul>
  </li>
</ul>

<h2 id="neyman-pearson-paradigm">Neyman-Pearson paradigm</h2>

<ul>
  <li>
    <p>\(H_0: \theta \in \Theta_0 \quad\) versus \(\quad H_1: \theta \in \Theta_1\)</p>
  </li>
  <li>Pick an \(\alpha \in(0,1)\)​</li>
  <li>Then try to maximize \(\beta(\theta)\) over \(\Theta_1\) subject to \(\sup _{\theta \in \Theta_0} \beta(\theta) \leq \alpha\)
    <ul>
      <li>where Power function : \(\beta(\theta)=P_\theta\left(\left(X_1, \ldots, X_n\right) \in R\right)\)​</li>
    </ul>
  </li>
  <li>가설 검정할 때 test statistic이 \(Z_{1-\alpha}\)보다 크면 귀무가설 기각하고… 어쩌고 그게 어디서 나온 것인지를 아래 예시를 통해 알아보자.</li>
</ul>

<h3 id="example--one-sided-test">example : One sided test</h3>

<ul>
  <li>Suppose \(X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, \sigma^2\right)\) with \(\sigma^2\) known</li>
  <li>\(H_0: \theta=\theta_0 \quad\) versus \(\quad H_1: \theta&gt;\theta_0\) (one sided)</li>
  <li>natural test statistic은 \(T_n\left(X_1, \ldots, X_n\right)=\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta_0}{\sigma / \sqrt{n}}\)이다.</li>
  <li>Power function : \(\beta(\theta)=P_\theta\left(T_n&gt;t\right)=P_\theta\left(\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta}{\sigma / \sqrt{n}}&gt;t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\)에서
    <ul>
      <li>Thresholdld \(t\)를 결정해야 한다.</li>
    </ul>
  </li>
  <li>\(\theta\)는 true parameter라는 점에서 \(\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta}{\sigma / \sqrt{n}} \sim N(0,1)\)이므로 \(\beta(\theta)=P\left(Z&gt;t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)=1-\Phi\left(t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\)이다.</li>
  <li>이제 Neyman-Pearson paradigm을 implement한다.
    <ul>
      <li>\(\sup _{\theta \in \Theta_0}\left\{1-\Phi\left(t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\right\} \leq \alpha\)인데 귀무가설에서는 \(\theta = \theta_0\)이므로</li>
      <li>\(1-\Phi(t) \leq \alpha\)이고 \(t\)에 대해 정리하면 \(t=\Phi^{-1}(1-\alpha)\)이다.</li>
    </ul>
  </li>
</ul>

<h2 id="neyman-pearson-procedure">Neyman-Pearson Procedure</h2>

<ul>
  <li>먼저 test function을 정의한다.
    <ul>
      <li>\(\phi(\boldsymbol{x})= \begin{cases}1, &amp; \text { if } \boldsymbol{x} \in R \\ 0, &amp; \text { if } \boldsymbol{x} \notin R\end{cases}\) , 즉 \(\phi(\boldsymbol{x})=1\)은 귀무가설 기각을 의미한다.</li>
      <li>이 notation에 따르면 \(\text { power }=\int \phi(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x}\) 이고 \(\text { size }=\int \phi(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}\)이다.</li>
    </ul>
  </li>
  <li>Neyman–Pearson test statistic은 likelihood ratio이다.
    <ul>
      <li>\(\Lambda(\boldsymbol{x})=\frac{L\left(\theta_0 \mid \boldsymbol{x}\right)}{L\left(\theta_1 \mid \boldsymbol{x}\right)}=\frac{f_0(\boldsymbol{x})}{f_1(\boldsymbol{x})}\)로 setting하고 \(P_0\left(\Lambda(\boldsymbol{X}) \leq t^*\right)=\alpha\)가 되도록 \(t^*\)를 결정한다.</li>
      <li>이렇게 likelihood ratio로 test function을 결정하는 방식이 유의수준이 \(\alpha\)인 모든 test 중에서 가장 power가 높다.</li>
      <li>이것을 증명하는 것이 The Neyman–Pearson Lemma이다.</li>
    </ul>
  </li>
</ul>

<h2 id="the-neymanpearson-lemma">The Neyman–Pearson Lemma</h2>

<ul>
  <li>
    <p>한 마디로 말하자면 귀무가설 하에서 type 1 error를 유의수준 \(\alpha\)와 같게 했을 때, type 2 error의 확률이 최소화된다는 것 (most powerful)</p>
  </li>
  <li>Consider a test with hypotheses \(H_0: \theta=\theta_0\) and \(H_1: \theta=\theta_1\)
    <ul>
      <li>where the pdf (or pmf) is \(f_i(\boldsymbol{x})\) for \(i=0,1\).</li>
    </ul>
  </li>
  <li>Consider the Neyman-Pearson test \(\phi_{\mathrm{NP}}(\boldsymbol{x})=\mathbb{1}\left(\frac{f_0(\boldsymbol{x})}{f_1(\boldsymbol{x})} \leq t^*\right)\),
    <ul>
      <li>where \(t^*\) is chosen such that \(\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}=\alpha\).</li>
    </ul>
  </li>
  <li>Consider any arbitrary test \(\phi_A(\boldsymbol{x})\) such that \(\int \phi_A(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x} \leq \alpha\),
    <ul>
      <li>i.e. level \(\alpha\) test.</li>
    </ul>
  </li>
  <li>Then the power of \(\phi_A(\boldsymbol{x})\) is at most the power of the Neyman-Pearson test,
    <ul>
      <li>that is \(\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x} \geq \int \phi_A(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x}\)</li>
    </ul>
  </li>
</ul>

<h3 id="proof">Proof</h3>

<ul>
  <li>
    <p>먼저 \(\int \underbrace{\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right)}_{T_1} \underbrace{\left(f_1(\boldsymbol{x})-\frac{f_0(\boldsymbol{x})}{t^*}\right)}_{T_2} d \boldsymbol{x} \geq 0\)​를 보인다.</p>

    <ul>
      <li>NP와 arbitrary test가 같은 결정을 내렸다면 \(T_1=0\)이므로 성립한다.</li>
      <li>NP와 arbitrary test가 다른 결정을 내렸다면 \(T_1\)과 \(T_2\)의 부호가 같으므로 성립한다.</li>
    </ul>
  </li>
  <li>
    <p>이제 다음과 같이 전개가 가능하다.</p>

    <p>\(\begin{aligned}
\int\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right) f_1(\boldsymbol{x}) d \boldsymbol{x} &amp; \geq \frac{1}{t^*} \int\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right) f_0(\boldsymbol{x}) d \boldsymbol{x} \\
&amp; =\frac{1}{t^*}(\underbrace{\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}}_{=\alpha}-\underbrace{\int \phi_A(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}}_{\leq \alpha} \geq 0
\end{aligned}\)​</p>
  </li>
  <li>
    <p>위 결과를 통해 the power of the NP test가 the power of any other test보다 크다는 것을 알 수 있다.</p>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[Basics of Hypothesis Testing]]></summary></entry></feed>