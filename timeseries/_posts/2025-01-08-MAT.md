---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2409.08530)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# MAT: Integration of Mamba and Transformer for Long-Short Range TSF (Arxiv 2024)

## Abstract

- Time series는 sparse semantic features를 가짐
  - Transformer만으로 다루기에는 어려움이 있고
  - Mamba의 selective input and parallel computing 필요
- Mamba and Transformer models의 장단점을 이해하고 combined approach 제안
- the long-range dependency capabilities of **Mamba**
  - and the short-range characteristics of **Transformers**
  - called **MAT**

## 1. Introduction

- **Linear** scalability를 가지는 모델들은 성능이 아쉬움
  - SciNet
    - relying solely on time points
    - obscure context-based choices and overlook long-range dependencies
    - 그러므로 iTransformer처럼 entire window로 다루는 것이 맞음
- **Transformer**-based models는 high computational complexity
  - Informer, FEDformer,and Autoformer
    - mixed-channel approach (2d matrix defined by the **number of channels and the length of histories**)
    - beneficial when channels exhibit significant correlations,
- SSM
  - handle very long sequences with linear complexity
  - context-aware selectivity through hidden attention mechanisms

- 본 논문에서는
  - Mamba coupled with attentionmechanisms called Transformer
    - (with multi-scale contexts)
  - **long**-term forecasting capabilities and **short**-range dependencies in MTS data
    - while maintaining **linear** scalability and minimal memory usage
  - bottom-up strategy
    - generating contextual cues at two distinct scales through linear mapping
    - At each of these levels four MAT modules

## 2. Related Work

pass

## 3. Methodology

- input sequence $$\mathbf{x}=\left[x_1, \ldots, x_L\right]$$ 보고 future values $$\left[x_{L+1}, \ldots, x_{L+T}\right]$$ 예측

### A~C. Preliminary, Transformer, Mamba

pass

### D. MAT

![그림1](/assets/img/timeseries/MAT/fig3.png)

- Four combined Mambas and Transformer
  - to extract **long-short** range contextual information
    - **long**-term forecasting capability of **Mamba**
    - and **short** range dependency of **Transformer**.

- **Normalizing**(RevIN)
- **Embedding** : $$\mathbf{x}^{(1)}=E M B_1\left(\mathbf{x}^{(0)}\right), \mathbf{x}^{(2)}=E M B_2\left(\mathbf{x}^{(1)}\right)$$.
  - $$E M B_1: \mathbb{R}^{M \times L} \rightarrow \mathbb{R}^{M \times n_1}$$ and $$E M B_2: \mathbb{R}^{M \times n 1} \rightarrow \mathbb{R}^{M \times n_2}$$
  - $$n_1$$ and $$n_2$$, are chosen from the set $$\{512,256,128,64,32\}$$ s.t. $$n_1>n_2$$.
- 사실 [TimeMachine](https://lpppj.github.io/timeseries/2024-10-29-timemachine)이랑 비슷한데
  - **not only** the **long**-term prediction capability of the **Mamba**,
  - **but also** the **short**-range dependency learned from the **Transformer**
  - 이 구조가 pivotal component in the modern LLM
- **Output Prediction** : $$\overline{\mathbf{x}}^{(1)}=\operatorname{Proj}_1\left(\overline{\mathbf{x}}^{\left(F_1\right)}\right), \overline{\mathbf{x}}=\operatorname{Proj}_2\left(\overline{\mathbf{x}}^{(1)} \oplus \hat{\mathbf{x}}^{\left(F_2\right)}\right)$$
  - $$\text{Proj}_1$$:  $$\mathbb{R}^{M \times n_2} \rightarrow \mathbb{R}^{M \times n_1}$$ to obtained $$\overline{\mathrm{x}}^1$$,
  - $$\mathrm{Proj}_2$$:  $$\mathbb{R}^{M \times n_1} \rightarrow \mathbb{R}^{M \times T}$$ to yield $$\overline{\mathbf{x}}$$.

## 4. Experiments

- fixed $$L=96$$ and $$T=\{96,192,336,720\}$$
- Default parameters for all Mambas were set as follows:
  - Dimension factor $$D=$$ 256,
  - local convolutional width =2,
  - and state expand factor $$N=$$ 1,
- For the transformer module,
  - the multi head number is set $$H=$$ 8,
  - the Batch Size in the training process is set as Batch =32.

![그림1](/assets/img/timeseries/MAT/table1.png)


## 5. Conclusion

- Transformers struggle with long-term dependencies and sparse semantic features,
  - Mamba excels through selective input handling and parallel computing.
- MAT, a combined approach leveraging Mamba’s long-range capabilities
  - and Transformers’ short-range strengths