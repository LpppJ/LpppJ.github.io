---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# MGF(Moment Generating Function) and Characteristic Function에 대해서

## Moments

- Definition
  - For each integer $$n$$, the $$n$$th moment of $$X$$ is $$\mu_n=\mathbb{E}\left[X^n\right]$$.
  - The $$n$$th central moment of $$X$$ is $$\mu_n^{\prime}=\mathbb{E}\left[(X-\mu)^n\right]$$ where $$\mu=\mu_1^{\prime}=\mathbb{E}[X]$$

- <u>확률변수의 moments는 tail probability와 밀접한 연관이 있다.</u>
  - Let $$X$$ is non-negative random variable
  
    : $$\begin{aligned}\mathbb{E}[X]&=\mathbb{E}\left[\int_0^{\infty} \mathbb{1}(t<X) d t\right]\\&=\int_0^{\infty} \mathbb{E}[\mathbb{1}(t<X)] d t\\&=\int_0^{\infty} P(t<X) d t\\&=\int_0^{\infty}\left[1-F_X(x)\right] d x \end{aligned}$$ 

  - 위 결과를 확장하면 아래와 같다.
  
    : $$\begin{aligned} \mathbb{E}\left[\mid X\mid ^p\right] & =\int_0^{\infty}\left[1-F_{\mid X\mid ^p}(y)\right] d y \\ P\left(\mid X\mid ^p>y\right) & =P\left(\mid X\mid >y^{\frac{1}{p}}\right) \\ & =1-F_{\mid X\mid }\left(y^{\frac{1}{p}}\right) \\ \text{Let } y&=x^p, \text{then } \frac{d y}{d x}=p x^{p-1} \\ \mathbb{E}\left[\mid X\mid ^p\right] & =\int_0^{\infty}\left[1-F_{\mid X\mid }(x)\right] p x^{p-1} d x \\ & =\int_0^{\infty} p x^{p-1} P(\mid X\mid >x) d x \end{aligned}$$

## Moment Generating Function

- Definition : $$M_X(t)=\mathbb{E}\left[e^{t X}\right]$$ 

- MGF가 왜 중요할까 ?

  - <u>첫째, 확률변수 X의 MGF는 X의 모든 moments를 생성한다.</u>
  - <u>둘째, 확률변수 X의 MGF는 X의 분포를 uniquely 결정한다.</u>
  - <u>셋째, MGF의 수렴은 분포수렴(convergence in distribution)을 의미한다.</u>
  - <u>넷째, MGF로 probability tail bound를 구한다. (Hoeffding's inequality)</u>

### 첫째, 확률변수 X의 MGF는 X의 모든 moments를 생성한다.

  - Step1) MGF가 존재한다고 가정했다면 $$\mathbb{E}\left[e^{\mid t X\mid }\right] \leq \mathbb{E}\left[e^{t X}\right]+\mathbb{E}\left[e^{-t X}\right]<\infty \quad \text { for all }\mid t\mid <\epsilon$$​

  - c.f. **(Dominated Convergence Theorem)**. If $$\left\{f_n: \mathbb{R} \mapsto \mathbb{R}\right\}$$ is a sequence of measurable functions which converge pointwise almost everywhere to $$f$$, and if there exists an integrable function $$g$$ (that is $$\left.\int_{-\infty}^{\infty}\mid g(x)\mid  d x<\infty\right)$$ such that $$\mid f_n(x)\mid  \leq\mid g(x)\mid $$ for all $$n$$ and for all $$x$$, then $$f$$ is integrable and 

    - $$\int_{-\infty}^{\infty} f(x) d x=\int_{-\infty}^{\infty} \lim _{n \rightarrow \infty} f_n(x) d x=\lim _{n \rightarrow \infty} \int_{-\infty}^{\infty} f_n(x) d x$$​​
    - 즉 $$\mid f_n(x)\mid $$의 bound 역할을 하는 $$\mid g(x)\mid $$가 $$\int_{-\infty}^{\infty}\mid g(x)\mid  d x<\infty$$이면 $$\int_{-\infty}^{\infty} \lim _{n \rightarrow \infty} f_n(x) d x$$의 $$\int$$와 $$\lim$$의 위치를 바꿀 수 있다.

  - Step2) DCT에 의해 X의 MGF는 아래와 같다.

    - MGF
    
    : $$\begin{aligned}\int_{-\infty}^{\infty} e^{t x} f_X(x) d x & =\lim _{n \rightarrow \infty} \int_{-\infty}^{\infty} \sum_{j=0}^n t^j \frac{x^j}{j !} f_X(x) d x \\ & =\lim _{n \rightarrow \infty} \sum_{j=0}^n \frac{t^j}{j !} \underbrace{\int_{-\infty}^{\infty} x^j f_X(x) d x}_{=\mu_j} \\ & =\sum_{j=0}^{\infty} \frac{t^j \mu_j}{j !} \end{aligned}$$

    - moment
    
    : $$\begin{aligned} M_X^{(m)}(t) & =\frac{d^m}{d t^m}\left(\sum_{k=0}^{\infty} \frac{t^k \mu_k}{k !}\right) \\ & =\sum_{k=0}^{\infty} \frac{\mu_k}{k !} \frac{d^m\left(t^k\right)}{d t^m} \\ & =\sum_{k=m}^{\infty} \mu_k \frac{t^{k-m}}{(k-m) !} \\ & =\sum_{j=0}^{\infty} \mu_{m+j} \frac{t^j}{j !} \quad(j \equiv k-m) \end{aligned}$$
    - Thus, $$M_X^{(m)}(0)=\mathbb{E}\left[X^m\right] \text { by noting } 0^0=1,0^j=0 \text { for } j \geq 1$$

### 둘째, 확률변수 X의 MGF는 X의 분포를 uniquely 결정한다.

  - 확률변수 X와 Y의 분포가 같다면 $$\to$$ MGF가 같다는 것을 보이는 것은 쉽다.

  - 반대로 두 확률변수 X와 Y의 MGF가 같으면 같은 분포를 따름은 아래와 같이 증명한다.

  - Suppose that $$X$$ and $$Y$$ have the same MGF for all $$t$$ : $$\sum_{x=0}^n e^{t x} f_X(x)=\sum_{y=0}^n e^{t y} f_Y(y)$$
  Let $$s=e^t$$ and $$c_i=f_X(i)-f_Y(i)$$ for $$i=0,1, \ldots, n$$

  : $$\begin{aligned} \sum_{x=0}^n e^{t x} f_X(x)-\sum_{y=0}^n e^{t y} f_Y(y)=0 \\ \Rightarrow \sum_{x=0}^n s^x f_X(x)-\sum_{y=0}^n s^y f_Y(y)=0 \\ \Rightarrow \sum_{x=0}^n s^x f_X(x)-\sum_{x=0}^n s^x f_Y(x)=0 \\ \Rightarrow \sum_{x=0}^n s^x\left[f_X(x)-f_Y(x)\right]=0 \\ \Rightarrow \sum_{x=0}^n s^x c_x=0 \quad \forall s>0 \end{aligned}$$

  - The above is simply a polynomial in $$\mathrm{s}$$ with coefficients $$c_0, c_1, \ldots, c_n$$. The only way it can be zero for all values of $$\mathrm{s}$$ is if $$c_0=c_1=\cdots=c_n=0$$
    
  - Therefore, $$0=c_i=f_X(i)-f_Y(i)$$ for $$i=0,1, \ldots, n$$, which means $$f_X(i)=f_Y(i)$$ for $$i=0,1, \ldots, n$$.

### 셋째, MGF의 수렴은 분포수렴(convergence in distribution)을 의미한다.

  - 즉 $$\lim _{n \rightarrow \infty} M_{X_n}(t)=M_X(t)$$이면 $$\lim _{n \rightarrow \infty} F_{X_n}(x)=F_X(x)$$이다.
  - **(Portmanteau lemma)** For random vector $$Y_n$$ and $$Y$$ with $$Y_n \sim P_n$$ and $$Y \sim P$$, the following are equivalent :
    - $$Y_n \xrightarrow{d} Y$$;
    - $$P\left(Y_n \leq t\right) \rightarrow P(Y \leq t)$$ for all continuous points $$t$$ of $$t \mapsto P(Y \leq t)$$;
    - $$\mathbb{E}\left[g\left(Y_n\right)\right] \rightarrow \mathbb{E}[g(Y)]$$ for all bounded, continuous $$g: \mathbb{R}^d \rightarrow \mathbb{R}$$​;
    - (Skip the proof)
    - 그러므로 아래 (Lévy’s continuity theorem)가 성립한다.
  - **(Lévy's continuity theorem)** Let $$X_n$$ and $$X$$ be random vectors in $$\mathbb{R}^d$$. Then $$X_n \xrightarrow{d} X$$ if and only if $$\lim _{n \rightarrow \infty} \mathbb{E}\left[e^{i t^{\top} X_n}\right]=\mathbb{E}\left[e^{i t^{\top} X}\right]$$ for every $$t \in \mathbb{R}^d$$

  - 위의 high-dimension에서의 증명은 (Cramér–Wold device)를 통해 1-dimension에서도 성립된다.
  - **(Cramér-Wold device)** If $$t^{\top} X_n \xrightarrow{d} t^{\top} X$$ for all $$t \in \mathbb{R}^d$$, then $$X_n \xrightarrow{d} X$$

### 넷째, MGF로 probability tail bound를 구한다. (Hoeffding's inequality)
  - [Inequalities for Probabilities](https://lpppj.github.io/stat/2024-04-18-ineq2) 
  - pass

## Characteristic Function

- Definitnion : $$\phi_X(t)=\mathbb{E}[\exp (i t X)]=\mathbb{E}[\cos (t X)+i \sin (t X)], \quad t \in \mathbb{R},$$
  - where $$i=\sqrt{-1}$$ is the imaginary unit (note Euler's formula: $$e^{i x}=\cos (x)+i \sin (x)$$​)

- characteristic function은 언제 사용할까

  - MGF가 존재하지 않는 분포(ex. Cauchy)에 대한 특성을 파악할 때
  - Lévy's continuity theorem 증명할 때

- Element properties:

  - If $$Y=a X+b, \phi_Y(t)=e^{i b t} \phi_X(a t)$$.

  - If $$X$$ and $$Y$$ are independent, then $$\phi_{X+Y}(t)=\phi_X(t) \phi_Y(t)$$​

- 아래 4가지 properties를 하나씩 증명한다.

  - 첫째, $$\phi_X(0)=1$$ and $$\mid \phi_X(t)\mid \leq 1$$ for all $$t \in \mathbb{R}$$​

  - 둘째, $$\phi_X(t)$$ is uniformly continuous on  $$\mathbb{R}$$​​
    - 즉 $$h \to 0$$에 따라 $$\psi(h) \rightarrow 0$$ s.t.  $$\mid \phi_X(t+h)-\phi_X(t)\mid \leq \psi(h)$$

  - 셋째, The characteristic function of a symmetric random variable, that is  $$X \stackrel{d}{=}-X$$, is real-valued

  - 넷째, MGF처럼 두 확률변수의 characteristic function이 같으면 같은 분포를 따른다.

### 첫째, $$\phi_X(0)=1$$ and $$\mid \phi_X(t)\mid \leq 1$$ for all $$t \in \mathbb{R}$$

  - $$\mid e^{i t x}\mid =\sqrt{\cos ^2(t x)+\sin ^2(t x)}=1$$ for all $$t$$ and $$x$$. Therefore $$\mid \phi_X(t)\mid =\mid \mathbb{E}\left[e^{i t X}\right]\mid  \leq \mathbb{E}\left[\mid e^{i t X}\mid \right]=1$$

### 둘째, $$\phi_X(t)$$ is uniformly continuous on  $$\mathbb{R}$$

  - $$\mid \underbrace{\mathbb{E}\left[e^{i(t+h) X}\right]}_{=\phi_X(t+h)}-\underbrace{\mathbb{E}\left[e^{i t X}\right]}_{=\phi_X(t)}\mid =\mid \mathbb{E}\left[e^{i t X}\left(e^{i h X}-1\right)\right]\mid  \leq \mathbb{E}\left[\mid e^{i h X}-1\mid \right]$$ 이고
  - Let $$g_X(h)=\mid e^{i h X}-1\mid , \text{ Then } g_X(h) \rightarrow 0 \text{ as } h \to 0$$ 이므로

  : $$\begin{aligned} g_X(h) & =\mid e^{i h X}-1\mid  \\ & =\mid \{\cos (h X)-1\}+i \sin (h X)\mid  \\ & =\sqrt{\{\cos (h X)-1\}^2+\sin ^2(h X)} \\ & =\sqrt{2-2 \cos (h X)} \leq 2 \end{aligned}$$

  - $$g_X(h)$$가 uniformly bounded by 2이므로 DCT에 의해 $$\mathbb E[g_X(h)] \rightarrow 0 \text{ as } h \to 0$$

### 셋째, $$X \stackrel{d}{=}-X$$, characteristic function of $$X$$는 real-valued

  - : $$\begin{aligned} \mathbb{E}\left[e^{i t X}\right] & =\mathbb{E}[\cos (t X)]+i \mathbb{E}[\sin (t X)] \stackrel{(\mathrm{i})}{=} \mathbb{E}[\cos (t X)]+i \mathbb{E}[\sin (-t X)] \\ & \stackrel{(\mathrm{ii})}{=} \mathbb{E}[\cos (t X)]-i \mathbb{E}[\sin (t X)] \end{aligned}$$.

  - (i)는 $$X \stackrel{d}{=}-X$$ 때문이고, (ii)는 sin의 성질 $$\sin (-x)=-\sin (x)$$ 때문이다.

### 넷째, MGF처럼 두 확률변수의 characteristic function이 같으면 같은 분포를 따른다.

  - 두 확률변수가 characteristic function이 같으면 같은 분포를 따름만 증명한다.
  - 본 증명에는 다양한 기법이 사용되는데 그 중 몇 가지를 소개한다. 아래와 같다.
    - Convolution : $$X$$와 $$Y$$가 독립이면 $$Z=X+Y$$의 density function은 $$f_Z(z)=\int_{-\infty}^{\infty} f_X(x) f_Y(z-x) d x$$​
    - Characteristic function of $$Z \sim N(0, \sigma^2)$$ : $$\mathbb{E}\left[e^{i t Z}\right]=e^{-\frac{\sigma^2 t^2}{2}} \text { for all } t \in \mathbb{R}$$​
    - Fubini's theorem : $$\int_{X \times Y}\mid f(x, y)\mid  d(x, y)<\infty$$ 이면 $$\int$$의 순서를 바꿀 수 있다.

  -  $$X+Z$$와 $$Y+Z$$가 같은 분포를 따른다는 것을 증명하고 Slutsky's theorem으로  $$X$$와 $$Y$$가 같은 분포를 따름을 증명한다.
  - $$\begin{aligned}
    f_{X+Z}(t) & =\int_{-\infty}^{\infty} f_Z(w) f_X(t-w) d w \\
    & =\int_{-\infty}^{\infty} f_Z(-w) f_X(t-w) d w \quad \text { since } f_Z(-w)=f_Z(w) \\
    & =\int_{-\infty}^{\infty} f_Z(w) f_X(t+w) d w \quad \text { by the change of variables }(-w \rightarrow w) \\
    & =\frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\infty} e^{-\frac{w^2}{2 \sigma^2}} f_X(t+w) d w \\
    & =\frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\infty} \mathbb{E}_Z\left[e^{i w Z \sigma^{-2}}\right] f_X(t+w) d w \\ & \text {by Characteristic function of } Z \sim N(0, \sigma^2) \text { and } Z \sigma^{-2} \sim N\left(0, \sigma^{-2}\right) \\
    & =\frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[\int_{-\infty}^{\infty} e^{i w Z \sigma^{-2}} f_X(t+w) d w\right] \quad \text { by Fubini's theorem } \\
    & =\frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[e^{-i t Z \sigma^{-2}} \int_{-\infty}^{\infty} e^{i(t+w) Z \sigma^{-2}} f_X(t+w) d w\right] \\
    & =\frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[e^{-i t Z \sigma^{-2}} \int_{-\infty}^{\infty} e^{i w Z \sigma^{-2}} f_X(w) d w\right] \\ &\text { by the change of variables }(t+w \rightarrow w) \\ & =\frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[e^{-i t Z \sigma^{-2}} \phi_X\left(Z \sigma^{-2}\right)\right] = \frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[e^{-i t Z \sigma^{-2}} \phi_Y\left(Z \sigma^{-2}\right)\right] \\ &( \because \phi_X = \phi_Y ) \\ &=f_{Y+Z}(t)
    \end{aligned}$$​
  - By Slutsky's theorem, $$X$$와 $$Y$$는 같은 분포를 따른다.

