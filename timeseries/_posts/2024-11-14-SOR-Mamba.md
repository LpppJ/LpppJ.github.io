---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2410.23356)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# SEQUENTIAL ORDER-ROBUST MAMBA FOR TIME SERIES FORECASTING (Arxiv 2024)

## Abstract

- Mamba : near-linear complexity in processing sequential data
  - **하지만 일반적으로 Time series에서 변수의 순서는 존재하지 않기 때문에**
  - **Mamba에서 channel dependencies (CD)를 잡다보면 sequential order bias가 발생**

- 그러므로 본 논문에서는 **SOR-Mamba**를 제안
  - **Regularization strategy**
    - to minimize the discrepancy btw two embedding vectors (reversed channel orders)
    - $$\to$$  robustness to channel order
  - **Eliminates the 1D-convolution**
    - (originally designed to capture local information in sequential data)
  - **Channel correlation modeling (CCM)**
    - pretraining task aimed at preserving **correlations between channels**
      - from the data space to the latent space
      - in order to enhance the ability to capture CD

## 1. Introduction

- Transformer  
  - ability to capture long-term dependencies but, quadratic computational complexity
  - reduce the complexity 하려다보니 performance degradations
- **SSM**
  - employing **convolutional** operations to process sequences with linear complexity
- **Mamba**
  - incorporating a **selective** mechanism to prioritize important information
  - balance btw performance and computational efficiency
- Temporal dependencies (TD), channel dependencies (CD) 둘 다 잡아야 하는데,
  - iTransformer에서는 CD는 complex attention mechanisms으로,
  - TD는 simple multi-layer perceptrons (MLPs)으로 했었음
- Mamba는 **sequential order bias**가 있다보니 **Bidirectional Mamba로는 충분하지 않음**
- 그렇다고 MambaTS처럼 channel을 섞자니 추가적인 작업이 소요됨

![그림1](/assets/img/timeseries/SOR-Mamba/fig1.png)

- 그래서 **Sequential Order-Robust Mamba for TS forecasting (SOR-Mamba)**를 제안
  - (간단한 설명은 abstract에 잘 정리되어 있으므로 pass)

## 2. Related Works

- **CD-Mamba Block**
  - 1D Conv 제거
  - Time series의 channels는 애초에 inherent sequential order가 존재하지 않기 때문
- **Regulararization with CD-Mamba Block**
  - Reversed channel order로 발생하는 두 embedding vectors의 차이를 줄이도록 학습
  - $$\to$$ Enhancing robustness to channel order !
- **Channel correlation modeling**
  - Data space에서의 correlation (btw channels)과
  - Embedding space에서의 correlation (btw channels)의 차이를 줄이도록 학습

## 3. Preliminaries

pass

## 4. Methodology

![그림1](/assets/img/timeseries/SOR-Mamba/fig2.png)

### 4.1. Architecture of SOR-Mamba

- Embedding layer: $$\mathbf{x} \in \mathbb{R}^{L \times C}$$ into $$\mathbf{Z} \in \mathbb{R}^{C \times D}$$ using a single linear layer.
- Mamba for CD: CD-Mamba block (*1D-conv* 제거)
- MLP for TD: (with layer normalization (LN))
- Prediction head: linear prediction head, resulting in $$\hat{\mathbf{y}} \in \mathbb{R}^{H \times C}$$

### 4.2. Regularization with CD-Mamba Block

- $$L_{\mathrm{reg}}(\mathbf{z})=d\left(\mathbf{z}_1, \mathbf{z}_2\right)$$, 여기서는 $$d$$를 MSE 사용
- 최종적으로 $$L(\mathbf{x}, \mathbf{y})=L_{\mathrm{fcst}}(\mathbf{x}, \mathbf{y})+\lambda \cdot \sum_{i=1}^m L_{\mathrm{reg}}\left(\mathbf{z}^{(i)}\right)$$

### 4.3. Channel Correlation modeling

- Temporal Dependencies보다는 Channel Dependencies를 강조하는 pre-training task
- CCM: preserve the (Pearson) correlation between channels from the data space to the latent space

![그림1](/assets/img/timeseries/SOR-Mamba/fig3.png)

- input token on the data space에서의 correlation matrices가 있고
  - output token on the latent space에의 correlation matrices가 있으면
  - loss function for CCM은 distance between these two matrices:
    - $$L_{\mathrm{CCM}}(\mathbf{x})=d\left(\mathbf{R}_{\mathbf{x}}, \mathbf{R}_{\mathbf{z}}\right)$$.

## 5. Experiments

### 5.1. Experimental settings

pass

### 5.2. Time series Forecasting

![그림1](/assets/img/timeseries/SOR-Mamba/table2.png)

### 5.3. Transfer Learning

![그림1](/assets/img/timeseries/SOR-Mamba/table4.png)

- SimMTM에 transfer learning 한 걸 S-Mamba와 비교

### 5.4. Ablation Study

![그림1](/assets/img/timeseries/SOR-Mamba/table5.png)

## 6. Analysis

### 6.1. Sequential order bias

![그림1](/assets/img/timeseries/SOR-Mamba/fig4.png)

- Channels가 많을수록, correlated 되어있을수록, Sequential order bias 큼

### 6.2. Effect of regularization

![그림1](/assets/img/timeseries/SOR-Mamba/table6.png)

### 6.3. Effect of 1D-conv

![그림1](/assets/img/timeseries/SOR-Mamba/table7.png)
