---
layout: post
related_posts:
  _
title: 
description: >
  [SIGIR'24 Best Paper](https://arxiv.org/pdf/2407.11245)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIR'24 Best Paper)

## Abstract

- Cross-Domain Sequential Recommendation (CDSR)은 multiple domain에서의 정보를 활용하여 Single-Domain Sequential Recommendation (SDSR)보다 좋은 성능을 보여주었음
- 하지만 **negative transfer** : lack of relation btw domains은 성능 저하의 원인
- 그래서 본 논문에서는
  1. estimates the degree of **negative transfer** of each domain
  2. adaptively assigns it as a **weight factor** to the prediction loss
     - to control gradient flows through domains with significant negative transfer !
  3. developed **auxiliary loss** that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis

- 이러한 CDSR과 SDSR의 cooperative learning은 collaborative dynamics between pacers and runners in a marathon와 유사함

## 1. Introduction

- Single-Domain Sequential Recommendation (SDSR)
  - focuses on **recommending the next item** within a **specific** domain using **only** the **single**-domain sequence
- Cross-Domain Sequential Recommendation (CDSR)
  - **predicts** the **next item** a user will interact with, by leveraging their historical **interaction** sequences across **multiple** domains
- 둘의 차이는 결국 다른 domains의 정보를 활용하는지 여부
- CDSR은 성능 향상을 위해 다른 domains의 정보를 활용하지만 항상 성능이 향상되는 건 아님
  - 만약 그것 때문에 성능이 더 안좋아진다면, 그건 **negative transfer**가 있었기 때문

![그림1](/assets/img/timeseries/SyNCRec/fig1.png)

- 본 논문에서는 SyNCRec: Asymmetric Cooperative Network for Cross-Domain Sequential Recommendation을 제안

- 1. assess the degree of **negative transfer** of each domain
     - by comparing the performance of CDSR and SDSR
  2. adaptively assign this value as **weight to the prediction loss** corresponding to a specific domain
     - to reduces its flow in domains with significant negative transfer !

  3. developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis
     - to exploit the effective correlation signals inherent in the representation pairs of SDSR and CDSR tasks within a specific domain

- SDSR은 negative transfer를 줄이기 위한 pacer의 역할을 함
  - (마라톤에서 runner가 너무 빠르거나 느리게 하지 않게 해주는 pacer)
- 특히 CDSR이 SDSR보다 성능이 안좋았던 (=negative transfer가 발생한) 도메인에서 성능 향상됨
- 이러한 방법으로 여러 개의 domain-specific models를 만들 필요가 없을 것을 기대함

## 2. Related Work

### 2.1. Single-Domain Sequential Recommendation

- SDSR : temporal dynamics in user-item interactions를 디자인
  - GRU-based models : GRU4Rec, STAMP, NARM
  - Attention-mechanism : SASRec, BERT4Rec, SINE, LightSANs
  - Others : NextItNet(CNN), TransRec(Markov chain), ...

### 2.2 Cross-Domain Sequential Recommendation

- CDSR : information from various other domains를 leverage
  - Matrix factorization : CMF, CLFM, ...
  - Multi-task learning : DTCDR, DeepAPF, BiTGCF, CAT-ART
  - $$\pi-Net$$ :  introduced gating mechanisms designed to transfer information from a single domain to another paired domain
  - $$C^2DSR$$ : employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations
  - $$MIFN$$ :  introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains
  - $$MAN$$ : designed group-prototype attention mechanisms to capture domainspecific and cross-domain relationships
- However... 결국에는 모두 domain pair 끼리의 관계를 모델링
  - 3개 이상의 domains의 관계를 파악할 때, domains이 엄청 많을 때에는 어려움
  - 그래서 CGRec에서 CDSR을 제안하면서 negative transfer 개념을 제안
    - high negative transfer를 가지는 domain에 panalty를 주는 방식
    - 하지만 여전히 SDSR보다 성능이 안좋은 domain이 꽤 있음
- 그러므로 본 논문에서의 목표는 3개 이상의 **모든** 도메인에서 negative transfer를 **효율적**으로 줄이는 것

## 3. Preliminary

- Domains : $$\mathcal{D}=\{A, B, C, \ldots\}$$ where $$\mid \mathcal{D}\mid  \geq 3$$
  - $$d \in \mathcal D$$ 는 하나의 특정 도메인을 의미,
  - $$V^d$$는 set of items specific to the domain $$d$$, $$V$$는 total item set across all domains

### Definition 1. Single- and Cross-Domain Sequential Recommendation

-  The single-domain sequences of domain $$d$$ : $$X^d=\left[(\mathrm{SOS}), x_1^d, x_2^d, \ldots, x_{\mid X^d\mid -1}^d\right]$$​
  - $$x_t^d$$ :  interaction occurring at time $$t$$
- 그러므로 cross-domain sequence는 $$X=\left(X^A, X^B, X^C, \ldots\right)$$로 표현할 수 있음
- 예를 들어, $$X=\left[(\mathrm{SOS}), x_1^A, x_2^B, x_3^A, x_4^B, x_5^A, x_6^C, x_7^C\right]$$은 $$
  X^A=\left[(\mathrm{SOS}), x_1^A, x_3^A, x_5^A\right], X^B=\left[(\mathrm{SOS}), x_2^B, x_4^B\right], \text { and } X^C=[(\mathrm{SOS})\left., x_6^C, x_7^C\right]$$으로 split 가능
- SDSR은 하나의 domain 안에서 recommending, CDSR은 전체 도메인에서 recommending

### Definition 2. Negative Transfer Gap (NTG)

- $$\mathcal{L}_\pi^d$$는 domain $$d$$에서의 model $$\pi$$의 loss를 의미 (SDSR 또는 CDSR)
- 그러므로 Negative transfer는 $$\phi_\pi(d) = \mathcal{L}_\pi^d\left(X^d\right)-\mathcal{L}_\pi^d(X)$$

### Problem Statement

- historical cross-domain sequences $$X_{1:t}$$가 주어졌을 때, 목표는 다음 item $$x_{t+1}^d = \underset{x_{t+1}^d \in V^d}{\operatorname{argmax}} P\left(x_{t+1}^d \mid X_{1: t}\right)$$를 예측하는 것
- 만약 $$\mid \mathcal{D}\mid $$개의 single-domain sequences (for SDSR)과 1개의 sequence (for CDSR)가 있다면
  - multi-tasking learning manner의 모델 하나는 $$\mid \mathcal{D}\mid +1$$개의 next item prediction tasks를 수행하는 것이다.

## 4. Model

![그림1](/assets/img/timeseries/SyNCRec/fig2.png)

### 4.1. Shared Embedding Layer

- 여기서는 **initialized representations** of items를 얻는다.
  - for $$\mid \mathcal{D}\mid $$ single-domain sequences $$X^d$$, and one cross-domain sequence $$X$$
- Item embedding matrix $$M^d \in \mathbb R^{\mid V^d\mid \times r}$$​이고
  - $$\mid V^d\mid $$는 domain d의 items 개수, r은 embedding dimension
- 모든 domains에 대해 concat하면 $$M \in \mathbb R^{\mid V\mid \times r}$$
  - $$\mid V\mid $$는 모든 도메인에서 items 개수
- 여기서 최근 T개만을 사용 (T개보다 적다면 앞쪽에 padding으로 맞춰줌)
  - 그러면 $$\mathbf{E}^d \in \mathbb{R}^{T \times r} \text { and } \mathbf{E} \in \mathbb{R}^{T \times r}$$를 얻음 (각각 Fig2(c-1), (c-2))
  - $$\mid \mathcal D\mid $$개의 $$\mathbf{E}^d$$를 aggregation한 것이 $$\mathbf{E}^{\text {single }}$$ (Fig2(c-1))
  - 참고로 $$\mathbf E, \mathbf E^d$$에는 learnable positional embedding 더해져있음
  - $$t$$-th step에서의 $$\mathbf E, \mathbf E^d$$는 각각 $$\mathbf e, \mathbf e^d$$로 정의

### 4.2. Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMoE)

- Negative Transfer (NTG)는 **loss of the SDSR**과 **the loss of CDSR**의 차이로 정의
  - NTG가 작으면 다른 domains의 정보가 도움이 안되는 거고 크면 도움이 되는 것
- 그러므로 weight for the prediction loss in the domain로 사용할 수 있다
  - gradient flow를 작게 만들기 위해서다
- Multi-gate Mixture of Sequential Experts (MoE) architecture를 사용하여 SDSR과 CDSR를 수행하고
  - **models** relationships between different tasks and **learns** task-specific functionalities
  - enabling it to effectively leverage shared representations

- SDSR과 CDSR은 서로 간섭하지 않고, experts로는 Transformer를 사용

### 4.2.1. Architecture

- **먼저 SDSR을 보자**

- shared embedding layer로부터 initialized representations of single- and cross-domain sequences,

  - 즉 $$\mathbf E, \mathbf E^d$$가 주어져있을 때, 각 expert는 many-to-many sequence learning을 수행

- domain $$d$$의 output : $$\begin{aligned}
  & \left(\mathbf{Y}^d\right)^{\text {single }}=h^d\left(f^d\left(\mathbf{E}^d\right)\right) \\
  & f^d\left(\mathbf{E}^d\right)=\sum_{k=1}^j g^d\left(\mathbf{E}^d\right)_k \mathrm{SG}\left(f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)\right)+\sum_{k=j+1}^K g^d\left(\mathbf{E}^d\right)_k f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)
  \end{aligned}$$

  - $$h^d$$ : the tower network for domain $$d$$​ (Fig. 2(c-7))
    - feed-forward network with layer normalization
  - $$f^d$$ : the multi-gated mixture of the sequential experts layer
  - $$SG$$​ :  the stopgradient operation (Fig. 2(c-4))
    - forward pass에서는 identity function
    - backward pass에서는 SG 안에 있는 것들의 gradient는 drop
    - 위 식에서는 $$j+1 \sim K$$번째 experts만 unique sequential pattern of single-domain sequences를 학습
  - $$f_{\text {TRM }}^k$$​ :  the 𝑘-th transformerbased sequential expert (Fig. 2(c-3))
  - $$g^d$$ :  gating network for domain $$d$$ (Fig. 2(c-6))
    - $$g^d\left(\mathbf{E}^d\right)=\operatorname{softmax}\left(W_g^d \mathbf{E}^d\right)$$ where $$W_g^d \in \mathbb{R}^{K \times d T}$$ is trainable FC
  - The $$t$$-th element of $$\mathrm{Y}^{\text {single }}$$는 $$\left(y_t^d\right)^{\text {single }}$$

- **다음으로 CDSR을 보자**

- ACMoE module : $$\begin{aligned}
  & \mathbf{Y}^{\text {cross }}=h^{\text {cross }}\left(f^{\text {cross }}(\mathbf{E})\right) \\
  & f^{\text {cross }}(\mathbf{E})=\sum_{k=1}^j g^{\text {cross }}(\mathbf{E})_k f_{\mathrm{TRM}}^k(\mathbf{E})+\sum_{k=j+1}^K g^{\text {cross }}(\mathbf{E})_k \operatorname{SG}\left(f_{\mathrm{TRM}}^k(\mathbf{E})\right)
  \end{aligned}$$

  - $$h^{cross}$$ :  the tower network (Fig. 2(c-9))
  - $$f^{\text {cross }}$$ : the multi-gated mixture of sequential experts layer for a cross-domain sequence
  - $$SG$$는 $$j+1\sim K$$-th $$f^k_{TRM}$$에만 사용
    - 그러면 $$1\sim j$$​번째 experts가cross-domain sequences에서 the distinct sequential patterns present를 학습

  - $$g^{\text {cross }}(\mathbf{E})=\operatorname{softmax}\left(W_a^{c r o s s} \mathbf{E}\right)$$​ : gating network for the crossdomain sequence (Fig. 2(c-8))

- $$\left(y_t^d\right)^{\text {single }} \text { and }\left(y_t\right)^{\text {cross }}$$는 two representations of different views for the same item

### 4.2.2. Transformer Experts

- 각각의 Multi-head Self-Attention에 $$Z \in \mathbb{R}^{T \times r}$$ 가 linear transformation
  - $$\to$$ $$\text { queries } Q_i \in \mathbb{R}^{T \times r / p} \text {, keys } K_i \in \mathbb{R}^{T \times r / p} \text {, } \text { values } V_i \in \mathbb{R}^{T \times r / p}$$가 됨
- $$\begin{aligned}
  & \operatorname{Attn}\left(Q_i, K_i, V_i\right)=\operatorname{softmax}\left(\frac{Q_i K_i^{\top}}{\sqrt{r / p}}\right) V_i, Q_i=Z \mathrm{~W}_i^Q, K_i=Z \mathrm{~W}_i^K, V_i=Z \mathrm{~W}_i^V
  \end{aligned}$$ 거쳐 final output은 $$\mathbf{H} \in \mathbb{R}^{T \times r}$$ 

- 마지막으로 $$\operatorname{FFN}(\mathbf{H})=\left[\mathrm{FC}\left(\mathbf{H}_1\right)\left\\mid \mathrm{FC}\left(\mathbf{H}_2\right)\right\\mid , \ldots, \\mid  \mathrm{FC}\left(\mathbf{H}_T\right)\right]$$
  - where $$\mathrm{FC}\left(\mathbf{H}_t\right)=\operatorname{GELU}\left(\mathbf{H}_t \mathrm{~W}_1+b_1\right) \mathrm{W}_2+b_2$$
  - $$\mathbf{H}_t$$ : 𝑡-th representation of $$\mathbf{H}$$

### 4.3. Loss Correction with Negative Transfer Gap (LC-NTG)

### 4.3.1.  Single-Domain Item Prediction

- Fig 2(e-1)
- single domoin sequence $$X_{1: t}^d$$가 주어졌을 때 다음 아이템 $$x_{t+1}^d$$를 예측하는 것은 pairwise ranking loss를 사용
  - 즉 $$l_t^d=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}^d\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}^d\right)\right), \mathcal{L}_{\text {single }}^d=\sum_{t=1}^T l_t^d$$
    - where $$x^{d+}$$ : ground-truth item paired with a negative item $$x^{d-}$$ sampled froem Unif
    - $$P\left(x_{t+1}^d=x^d \mid X_{1: t}^d\right)$$ = $$\sigma\left(\left(y_t^d\right)^{\text {single }} \cdot M\left(x^d\right)\right)$$

### 4.3.2. Cross-Domain Item Prediction

- CDSR $$l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t$$
  - where $$P\left(x_{t+1}^d=x^d \mid X_{1: t}\right) \text { is obtained by } \sigma\left(\left(y_t\right)^{\text {cross }} \cdot M\left(x^d\right)\right)$$

### 4.3.3.  Calculating the Negative Transfer Gap

- 이제 NTG를 구할 수 있다. $$\phi_\pi(d)=\sum_{t=1}^T\left(l_t^d-l_t\right)$$
  - where $$l_t^d$$ and $$l_t$$ are losses of the SDSR and CDSR tasks in time step $$t$$ for the domain $$d$$, respectively, calculated with our model $$\pi$$
- $$\lambda=\left(\lambda_1, \lambda_2, \ldots, \lambda_{\mid \mathcal{D}\mid }\right)$$를 각 domain에서의 NTG라고 하면 $$\lambda_d \leftarrow \operatorname{softmax}\left(\alpha * \lambda_d+\beta * \phi_\pi(d) ; \delta\right)$$로 계산
  - where $$\alpha \text { and } \beta$$ are learnable parameters

### 4.3.4. Loss Correction

- NTG는 weight for the cross-domain item prediction loss로 활용됨 
  - loss는 $$l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t$$
- re-aggregate : multiplying the relative NTG for each domain separately 
  - $$\mathcal{L}_{\text {cross }}^{l c} = =\sum_{t=1}^T \sum_{d=1}^{\mid \mathcal{D}\mid } \lambda_d \log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right)$$

- 이렇게 하면 NTG가 발생하는 domain에서의 gradient flow를 줄이는 것

## 4.4. Single-Cross Mutual Information Maximization (SC-MIM)

- SC-MIM: SDSR and CDSR tasks 사이의 정보를 잘 transfer하기 위한 방법
  - mutual information으로 두 tasks의 correlation signals를 파악
  - mutual information: $$I(X, Y)=D_{K L}(p(X, Y) \\mid  p(X) p(Y))=\mathbb{E}_{p(X, Y)}\left[\log \frac{p(X, Y)}{p(X) p(Y)}\right]$$​
- 하지만 이 mutual information을 high-dimd에서 구하는 건 어렵기 때문에 lower bound로 InfoNCE를 사용
  - lower bound : $$I(X, Y) \geq \mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\mathbb{E}_{q(\hat{Y})}\left(\log \sum_{\hat{y} \in \hat{Y}} \exp \rho_\theta(x, \hat{y})\right)\right]+\log \mid \hat{Y}\mid $$
    - where $$x, y$$는 같은 input의 서로 다른 view points
    - $$\rho_\theta$$ 는 similarity function,
  - InfoNCE를 maximizing하는 것은 standard cross-entropy loss를 maximizing하는 것과 같음
    - : $$$\mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\log \sum_{\hat{y} \in Y} \exp \rho_\theta(x, \hat{y})\right]$$

- 아무튼 돌아와서 우리는 $$$\mathbf{Y}^{\text {single }}$ and $\mathbf{Y}^{\text {cross }}$$의 mutual information을 maximizing하고 싶음
  - 그러므로 cross-domain representation $$\mathbf{Y}^{\text {ross }}$$를 domain별로 split해서 $$(\mathbf{Y^d})^{\text {ross }}$$ 구하고
  - 아래 식처럼 계산
    - : $$\begin{aligned} & \mathcal{L}_{S C-M I M}^d=\rho\left(\left(\mathbf{Y}^d\right)^{\text {single }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)-\log \sum_{u-} \exp \left(\rho\left(\left(\mathbf{Y}^d\right)^{\text {single- }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)\right)\end{aligned}$$
    - where $$u-$$는 other users in a training batch,
    - $$\left(\mathbf{Y}^d\right)^{\text {single- }}$$는 subsequence of domain $$𝑑$$ of user $$𝑢−$$​ 
    - $$\rho(\cdot, \cdot)$$는 $$\rho(U, V)=\sigma\left(U^{\top} \cdot W^H \cdot V\right)$$

### 4.5. Model Training and Evaluation

- Total training loss : $$\mathcal{L}=\eta\left(\sum_{d=1}^{\mid \mathcal{D}\mid }\left(\mathcal{L}_{\text {single }}^d\right)+\mathcal{L}_{\text {cross }}^{l c}\right)+(1-\eta) \sum_{d=1}^{\mid \mathcal{D}\mid } \mathcal{L}_{S C-M I M}^d$$
  - where $$\eta$$ is the harmonic factor
  - evaluation할 때에는 cross-domain representation만 사용

## 5. Experiments

### 5.1. Dataset

### 5.2. Experimental Setting

- 먼저 Amazon dataset과 Telco dataset에 대한 성능

![그림1](/assets/img/timeseries/SyNCRec/table23.png)

- Research Questions:

  - (RQ1): Does the performance of our model surpass the current stateof-the-art baselines in practical applications that involve more than three domains?

  - (RQ2): Can our model effectively address the challenge of negative transfer across all domains in the CDSR task?

  - (RQ3): What is the impact of various components of our model on its performance in CDSR tasks?

  - (RQ4): How do variations in hyper-parameter settings influence the performance of our model?

  - (RQ5): How does the model perform when deployed online ?

### 5.3. Performance Evaluation (RQ1)

- First, The effectiveness of our model can be observed.
  - 다른 baseline models보다 성능이 뛰어남
- Second, Integrating information from all domains simultaneously in a model can improve performance in each domain compared to modeling a pairwise domain-domain relationship.
  - 본 논문에서 제시하는 방법을 사용할 경우에는 CDSR task에서 domain끼리의 정보를 결합해서 사용하는 것이 더 효율적이다.

### 5.4. Discussion of the negative transfer (RQ2)

- 기존 baseline models는 SDSR보다 CDSR의 성능이 더 안좋았지만 본 논문에서 제시하는 모델은 그렇지 않다

![그림1](/assets/img/timeseries/SyNCRec/table4.png)

### 5.5 Discussion of Model Variants (RQ3)

- LC-NTG, SC-MIM, ACMoE 세 가지 components 모두 성능 향상을 위해 필요하다

![그림1](/assets/img/timeseries/SyNCRec/table5.png)

### 5.6. Hyperparameter Analysis (RQ4)

![그림1](/assets/img/timeseries/SyNCRec/fig3.png)

## 6. Online A/B Test (RQ5)

pass

## 7. Conclusion

- Negative transfer를 다루는 CDSR framework를 제안
  - Negative transfer를 측정하고 prediction loss의 weight로 활용
- SDSR and CDSR tasks의 정보를 교환시키는 Auxiliary loss 제안