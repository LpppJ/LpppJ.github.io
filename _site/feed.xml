<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-10-27T05:37:21+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)</title><link href="http://localhost:4000/timeseries/2024-10-27-TPatchGNN/" rel="alternate" type="text/html" title="T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)" /><published>2024-10-27T00:00:00+09:00</published><updated>2024-10-27T05:37:21+09:00</updated><id>http://localhost:4000/timeseries/TPatchGNN</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-27-TPatchGNN/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>transforms each univariate irregular time series into a series of transformable patches</li>
      <li>local semantics captureì™€, inter-time series correlation modelingëŠ” í•˜ë©´ì„œ</li>
      <li>avoiding sequence <strong>length explosion</strong> in aligned IMTS (ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€ 1. introduction (3)ì—ì„œ ì„¤ëª…)</li>
    </ul>
  </li>
  <li>Time-adaptive graph neural networksìœ¼ë¡œ time-varying adaptive graphsë¥¼ í•™ìŠµí•´ì„œ
    <ul>
      <li>dynamic intertime series correlationë¥¼ í‘œí˜„</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Multivariate Time Series (IMTS)ì˜ íŠ¹ì§•ì€ irregular sampling intervals and missing data</li>
  <li>Irregularity within the series and asynchrony ë•Œë¬¸ì— ë‹¤ë£¨ê¸° ì–´ë ¤ì›€
    <ul>
      <li>ODEë¡œ í’€ë ¤ê³  í•œ ì ì€ ìˆì§€ë§Œ numerical integration processìœ¼ë¡œ ì¸í•´ computationally expensive</li>
    </ul>
  </li>
  <li>IMTS forecastingì˜ ì–´ë ¤ì›€ì—ëŠ” 3ê°€ì§€ ì´ìœ ê°€ ìˆìŒ</li>
  <li>ì²«ë²ˆì§¸ëŠ” (1) irregularity in intra-time series dependency modeling
    <ul>
      <li><strong>varying time intervals</strong> between adjacent observationsì´ the consistent flow of time series dataë¥¼ ë°©í•´</li>
    </ul>
  </li>
  <li>ë‘ë²ˆì§¸ëŠ” (2) asynchrony in intertime series correlation modeling
    <ul>
      <li><strong>misaligned at time</strong> due to irregular sampling or missing data.</li>
    </ul>
  </li>
  <li>ê°€ì¥ ì¤‘ìš”í•œ ê±´ (3) sequence length explosion with the increase of variables
    <ul>
      <li>ì•„ë˜ fig1ì²˜ëŸ¼ â€œë‹¨ í•˜ë‚˜ì˜ ë³€ìˆ˜ë¼ë„ ê¸°ë¡ëœ time stampâ€ëŠ” ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê±¸ë¡œ í•´ë²„ë¦¬ë©´, ë³€ìˆ˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚¨ì— ë‹¤ë¼ time stampsì˜ ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„ì§€ëŠ” ë¬¸ì œ. (ì´ëŸ¬í•œ ë°©ë²•ì„ canonical pre-alignment representationì´ë¼ê³  ë¶€ë¦„)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TPatchGNN/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” T-PATCHGNNì˜ ì¥ì ì€
    <ul>
      <li>ì²«ì§¸ë¡œ The independent patching process for each univariate irregular time seriesìœ¼ë¡œ representationì—ì„œ sequence length explosionì˜ riskë¥¼ ì—†ì• ê³ </li>
      <li>ë‘˜ì§¸ë¡œ local semanticsë¥¼ ì˜ ì¡ê¸° ìœ„í•´ putting each individual observation into patches with richer context</li>
      <li>ì…‹ì§¸ë¡œ transformable patching í›„ì— IMTS is naturally aligned in a consistent patch-level temporal resolution</li>
    </ul>
  </li>
  <li>ë³¸ ë…¼ë¬¸ì˜ contributionì€ :
    <ul>
      <li>New transformable patching method to transform each univariate irregular time series of IMTS into a series of variable-length yet time-aligned patches</li>
      <li>transformable patching outcomesì„ ë°”íƒ•ìœ¼ë¡œ,  time-adaptive graph neural networksë¥¼ ì œì•ˆ</li>
      <li>building a benchmark for IMTS forecasting evaluation</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2, Related Works</h2>

<h3 id="21-irregular-multivariate-time-series-forecasting">2.1. Irregular Multivariate Time Series Forecasting</h3>

<p>pass</p>

<h3 id="22-irregular-multivariate-time-series-representation">2.2. Irregular Multivariate Time Series Representation</h3>

<ul>
  <li>ê¸°ì¡´ì—ëŠ” time-aligned mannerë¡œ IMTSë¥¼ representation (pre-alignment representation method)
    <ul>
      <li>ì¦‰ í•˜ë‚˜ì˜ ë³€ìˆ˜ë¼ê³  ê¸°ë¡ëœ time stampëŠ” ì¡´ì¬í•˜ëŠ” ê±¸ë¡œ ìƒê°í•˜ë‹ˆ</li>
      <li>sequence length that equals the number of all unique time stamps in IMTS</li>
      <li>ì˜ˆë¥¼ ë“¤ì–´ ë³€ìˆ˜ 1ì€ 1,3,5 ì‹œì ì— ê¸°ë¡ë˜ê³  ë³€ìˆ˜ 2ëŠ” 2,4,6 ì‹œì ì— ê¸°ë¡ë˜ë©´ unique time stampsì˜ ê°œìˆ˜ëŠ” 6ì´ ë¨</li>
      <li>sequence length explosion problem ë°œìƒ</li>
    </ul>
  </li>
</ul>

<h3 id="23-graph-neural-networks-for-multivariate-time-series">2.3. Graph Neural Networks for Multivariate Time Series</h3>

<ul>
  <li>
    <p>2018ë…„ DCRNN, STGCNì€ pre-defined graph structuresë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤ì œë¡œ ì“°ê¸° ì–´ë ¤ì› ê³ </p>
  </li>
  <li>2019ë…„ë¶€í„° dataë¡œë¶€í„° graph structuresë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©
    <ul>
      <li>í•˜ì§€ë§Œ IMTSì—ì„œëŠ” ì˜ ì‘ë™ì„ ì•ˆ í•¨. mimisalignment at timesìœ¼ë¡œ ì¸í•´ inter-time series correlation modelingì´ ì˜ ì•ˆ ë¨</li>
    </ul>
  </li>
  <li>Raindrop(2021)[<a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">paper review</a>]
    <ul>
      <li>ì´ ë¬¸ì œë¥¼ propagation the asynchronous observations at all the timestampsë¡œ í•´ê²°í•˜ë ¤ê³  í–ˆì§€ë§Œ  sequence length explosion problemì„ í”¼í•  ìˆ˜ ì—†ìŒ</li>
    </ul>
  </li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<h3 id="31-problem-definition">3.1. Problem Definition</h3>

<h3 id="definition-1">Definition 1</h3>

<ul>
  <li>Irregular Multivariate Time Series
    <ul>
      <li>\(\mathcal{O}=\left\{\mathbf{o}_{1: L_n}^n\right\}_{n=1}^N=\left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\), where</li>
      <li>\(N\)ê°œì˜ ë³€ìˆ˜ê°€ ìˆê³  \(n\)ë²ˆì§¸ ë³€ìˆ˜ëŠ” \(L_n\)ê°œì˜ observationsê°€ ìˆê³ , \(n\)ë²ˆì§¸ ë³€ìˆ˜ì˜ \(i\)ë²ˆì§¸ ë³€ìˆ˜ì˜ ê°’ì€ \(t_i^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="definition-2">Definition 2</h3>

<ul>
  <li>Forecasting Query \(q_j^n\)
    <ul>
      <li>\(j\)-th query on \(n\)-th variable to predict its corresponding value at a future time \(q_j^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="problem-1">Problem 1</h3>

<ul>
  <li>Irregular Multivariate Time Series Forecasting
    <ul>
      <li>IMTS \(\mathcal{O} =  \left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\)ì™€ Forecasting query \(\mathcal{Q}=\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)ê°€ ìˆì„ ë•Œ,</li>
      <li>problemì€ accurately forecast recorded values \(\hat{\mathcal{X}}=\left\{\left[\hat{x}_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\) in correspondence to the forecasting queries</li>
      <li>\(\mathcal{F}(\mathcal{O}, \mathcal{Q}) \longrightarrow \hat{\mathcal{X}}\)ë¡œ í‘œí˜„ë¨</li>
    </ul>
  </li>
</ul>

<h3 id="32-canonical-pre-alignment-representation-for-imts">3.2. Canonical Pre-Alignment Representation for IMTS</h3>

<ul>
  <li>2.2. Irregular Multivariate Time Series Representation ì°¸ê³ </li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<p><img src="/assets/img/timeseries/TPatchGNN/fig2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="41-irregular-time-series-patching">4.1. Irregular Time Series Patching</h3>

<ul>
  <li>ëª¨ë“  univariate TSì— ê°™ì€ patching operationì„ í•˜ë‹ˆê¹Œ ë³€ìˆ˜ index í‘œê¸°ëŠ” ìƒëµ</li>
</ul>

<h3 id="411-transformable-patching">4.1.1. TRANSFORMABLE PATCHING</h3>

<ul>
  <li>Time series patchingì´ forecastingì— ì¢‹ì€ ë°©ë²•ì´ë¼ëŠ” ê±´ ì•Œë ¤ì§„ ì‚¬ì‹¤. benefits in :
    <ul>
      <li>capturing local semantic information,</li>
      <li>reducing computation and memory usage,</li>
      <li>modeling longer-range historical observations</li>
    </ul>
  </li>
  <li>ì¼ë°˜ì ìœ¼ë¡œ time series patchingì€ í•˜ë‚˜ì˜ patchì— ê°™ì€ ìˆ«ìì˜ observationsê°€ ìˆëŠ”ë°,
    <ul>
      <li>IMTSì—ì„œ time intervalsëŠ” ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë°©ì‹ì´ ì ì ˆí•˜ì§€ ì•ŠìŒ</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ patchì— ê°™ì€ ê°œìˆ˜ì˜ observataionsê°€ ì•„ë‹ˆë¼, unified time horizonì´ ë“¤ì–´ê°€ë„ë¡ í•¨
    <ul>
      <li>patch ì•ˆì— ë“¤ì–´ê°€ëŠ” observationsì˜ ê°œìˆ˜ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ, ex) 2ì‹œê°„ì¸ ê±´ ë™ì¼í•˜ë„ë¡</li>
    </ul>
  </li>
  <li>patchëŠ” \(\left[\mathbf{o}_{l_p: r_p}\right]_{p=1}^P\)ë¡œ í‘œí˜„ë˜ê³  \(P\)</li>
</ul>

<h3 id="412-patch-encoding">4.1.2. PATCH ENCODING</h3>

<ul>
  <li><strong>Continuous time embedding</strong>
    <ul>
      <li>\(\phi(t)[d]=\left\{\begin{array}{lll}
\omega_0 \cdot t+\alpha_0, &amp; \text { if } &amp; d=0 \\
\sin \left(\omega_d \cdot t+\alpha_d\right), &amp; \text { if } &amp; 0&lt;d&lt;D_t
\end{array}\right.\).
        <ul>
          <li>where the \(\omega_d\) and \(\alpha_d\) are learnable parameters and \(D_t\) is embeddingâ€™s dimension</li>
        </ul>
      </li>
      <li>Concatenationí•˜ë©´ observations in the patch:
        <ul>
          <li>\(\mathbf{z}_{l_p: r_p}=\left[z_i\right]_{i=l_p}^{r_p}=\left[\phi\left(t_i\right) \| x_i\right]_{i=l_p}^{r_p}\).</li>
          <li>ì´ê±´ í•˜ë‚˜ì˜ patchì— ëŒ€í•œ í‘œí˜„ì´ ë˜ëŠ” ê²ƒ !</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Transformable time-aware convolution</strong>
    <ul>
      <li>input sequenceì˜ ê¸¸ì´ì— ë§ê²Œ (adaptively), generated parametersì™€ transformable filter sizeë¥¼ ì‚¬ìš©</li>
      <li>\(\mathbf{f}_d=\left[\frac{\exp \left(\mathbf{F}_d\left(z_i\right)\right)}{\sum_{j=1}^{L_p} \exp \left(\mathbf{F}_d\left(z_j\right)\right)}\right]_{i=1}^{L_p}\)ìœ¼ë¡œ í‘œí˜„ë¨
        <ul>
          <li>where \(L_p\) is the sequence length of patch \(\mathbf{z}_{l_p: r_p}, \mathbf{f}_d \in \mathbb{R}^{L_p \times D_{i n}}\) is the derived filter for \(d\)-th feature map, \(D_{i n}\) is dimension of inputs, and \(\mathbf{F}_d\) denotes the meta-filter that can be instantiated by learnable neural networks</li>
          <li>ì´ê±´ filterì˜ parametersë¥¼ along the temporal dimensionìœ¼ë¡œ normalizaingí•´ì„œ consistent scaling í•˜ê² ë‹¤ëŠ” ê²ƒ</li>
        </ul>
      </li>
      <li>ìœ„ ì‹ìœ¼ë¡œ \(D-1\)ê°œì˜ filtersë¥¼ ì‚¬ìš©í•´ì„œ <strong>latent patch embedding</strong> \(h_p^c \in \mathbb{R}^{D-1}\)ë¥¼ ì–»ìŒ :
        <ul>
          <li>\(h_p^c=\left[\sum_{i=1}^{L_p} \mathbf{f}_d[i]^{\top} \mathbf{z}_{l_p: r_p}[i]\right]_{d=1}^{D-1}\).</li>
          <li>ì´ê±´  encoded transformable patches:
            <ul>
              <li>variable-length sequencesì— ë”°ë¼ flexibilityë¥¼ ê°€ì§€ê³ </li>
              <li>parameterization for varying time intervalsì„ í•˜ë©´ì„œ</li>
              <li>additional learnable filter parameters ì—†ì´ ë” ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ</li>
            </ul>
          </li>
          <li>ë§ˆì§€ë§‰ìœ¼ë¡œ \(h_p=\left[h_p^c \| m_p\right]\) ì´ë ‡ê²Œ patchì— maskingì„ ë§ë¶™ì—¬ì£¼ëŠ”ë°,
            <ul>
              <li>\(m_p\)ëŠ” ì´ patch ì•ˆì— observationsê°€ í•˜ë‚˜ ì´ìƒ ìˆë‹¤~ë¥¼ indicatorë¡œ í‘œí˜„</li>
            </ul>
          </li>
          <li>ìµœì¢…ì ìœ¼ë¡œ \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)ë¥¼ ì–»ëŠ”ë‹¤.</li>
          <li>ì´ê±´ \(P\)ê°œì˜ patchë¥¼ \(D-1\)ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•˜ê³  ë§ˆì§€ë§‰ì—ëŠ” maskingìœ¼ë¡œ indicatorë¥¼ ë¶™ì¸ ê²ƒ</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-intra--and-inter-time-series-modeling">4.2. Intra- and Inter-Time Series Modeling</h3>

<ul>
  <li>ì´ì œ ì´  transformable patchingì„ irregular time seriesë¥¼ intra- and inter-time series modelingí•˜ëŠ”ì§€ ì•Œì•„ë³´ì</li>
</ul>

<h3 id="421-transformer-to-model-sequential-patches">4.2.1. TRANSFORMER TO MODEL SEQUENTIAL PATCHES</h3>

<ul>
  <li>ìœ„ì—ì„œ êµ¬í•œ \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)ë¥¼ Transformerì— ë„£ëŠ”ë‹¤.</li>
  <li>ë¨¼ì € positional encodingì„ í•˜ê³ 
    <ul>
      <li>\(\mathbf{x}_{1: P}^{t f, n}=\mathbf{h}_{1: P}^n+\mathbf{P E}_{1: P}\).</li>
    </ul>
  </li>
  <li>Q, K, Vë¥¼ ë§Œë“¤ì–´ì„œ MHAë¥¼ í†µê³¼í•œë‹¤.
    <ul>
      <li>\(\mathbf{q}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^Q\) / \(\mathbf{k}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^K\) / \(\mathbf{v}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^V\) where \(\mathbf{W}_h^Q, \mathbf{W}_h^K, \mathbf{W}_h^V \in \mathbb{R}^{D \times(D / H)}\)</li>
      <li>\(\mathbf{h}_{1: P}^{t f, n}=\|_{h=1}^H \operatorname{Softmax}\left(\frac{\mathbf{q}_h^n \mathbf{k}_h^{n T}}{\sqrt{D / H}}\right) \mathbf{v}_h^n \in \mathbb{R}^{P \times D}\),</li>
    </ul>
  </li>
</ul>

<h3 id="422-time-varying-adaptive-graph-structure-learning">4.2.2. TIME-VARYING ADAPTIVE GRAPH STRUCTURE LEARNING</h3>

<ul>
  <li>í•œ ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ë‹¤ë¥¸ ë³€ìˆ˜ì˜ ì •ë³´ëŠ” ë§¤ìš° ìœ ìš©í•  ìˆ˜ê°€ ìˆìŒ</li>
  <li>í•˜ì§€ë§Œ IMTSì—ì„œëŠ” misaligned at timesìœ¼ë¡œ ì¸í•´ correlation modelingì´ ì–´ë ¤ì›€
    <ul>
      <li>ê·¸ë ‡ë‹¤ê³  Raindropì²˜ëŸ¼ í•˜ê¸°ì—” e sequence length explosion problemì´ ë°œìƒ</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ <strong>transformable patching</strong>ìœ¼ë¡œ í•´ê²°
    <ul>
      <li>patchë¥¼ observationsì˜ ê°œìˆ˜ê°€ ì•„ë‹ˆë¼ ì‹œê°„ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŠë‹¤ë³´ë‹ˆ</li>
      <li>ê° ë³€ìˆ˜ëŠ” ê°™ì€ ìˆ«ìì˜ patchesë¡œ ì´ë£¨ì–´ì§€ë‹ˆê¹Œ</li>
      <li>time-adaptive graph neural networksë¡œ inter-time series correlationë¥¼ modelingí•  ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
  <li>ì¦‰ IMTSì˜  dynamic correlationsë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œëŠ”
    <ul>
      <li>series of time-varying adaptive graphsë¥¼ í•™ìŠµí•˜ê² ë‹¤ëŠ” ê²ƒì´ê³ </li>
      <li>ì§€ê¸ˆ ë¬¸ì œëŠ” variable embeddingì´ trainingì—ì„œëŠ” update ê°€ëŠ¥í•˜ì§€ë§Œ inferenceì—ì„œëŠ” static</li>
      <li>ê·¸ëŸ¬ë‹ˆ learnable \(\mathbf{E}_1^s, \mathbf{E}_2^s \in \mathbb{R}^{N \times D_g}\)ë¥¼ ì‚¬ìš©í•´ì„œ</li>
      <li>ìš°ë¦¬ê°€ ì§€ê¸ˆê¹Œì§€ ë§Œë“¤ì—ˆë˜  time-varying patch embedding \(\mathbf{H}_p^{t f}=\left[\mathbf{h}_p^{t f, n}\right]_{n=1}^N \in \mathbb{R}^{N \times D}\)ì„
        <ul>
          <li>static variable embeddingìœ¼ë¡œ ë§Œë“¤ë©´ ë¨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ê·¸ gated adding operationì€ ë‹¤ìŒê³¼ ê°™ìŒ
    <ul>
      <li>\(\begin{gathered}
\mathbf{E}_{p, k}=\mathbf{E}_k^s+g_{p, k} * \mathbf{E}_{p, k}^d, \\
\mathbf{E}_{p, k}^d=\mathbf{H}_p^{t f} \mathbf{W}_k^d, \\
g_{p, k}=\operatorname{ReLU}\left(\tanh \left(\left[\mathbf{H}_p^{t f} \| \mathbf{E}_k^s\right] \mathbf{W}_k^g\right)\right) \\
k=\{1,2\}
\end{gathered}\), where
        <ul>
          <li>\(\mathbf{W}_k^d \in \mathbb{R}^{D \times D_g}, \mathbf{W}_k^g \in \mathbb{R}^{\left(D+D_g\right) \times 1}\) are learnable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ì´ì œ time-varying adaptive graph structureë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì–»ìŒ : \(\mathbf{A}_p=\operatorname{Softmax}\left(\operatorname{ReLU}\left(\mathbf{E}_{p, 1} \mathbf{E}_{p, 2}^T\right)\right)\)</li>
</ul>

<h3 id="423-gnns-to-model-inter-time-series-correlation">4.2.3. GNNS TO MODEL INTER-TIME SERIES CORRELATION</h3>

<ul>
  <li>ë‹¤ìŒìœ¼ë¡œ dynamic inter-time series correlation at a patch-level resolutionì„ ì–»ìŒ
    <ul>
      <li>\(\mathbf{H}_p=\operatorname{ReLU}\left(\sum_{m=0}^M\left(\mathbf{A}_p\right)^m \mathbf{H}_p^{t f} \mathbf{W}_m^{g n n}\right) \in \mathbb{R}^{N \times D}\).</li>
      <li>where $M$ is the number of layers for GNNs, and $\mathbf{W}_m^{g n n} \in$ $\mathbb{R}^{D \times D}$ are learnable parameters at $m$-th layer.</li>
    </ul>
  </li>
</ul>

<h3 id="43-imts-forecasti">4.3. IMTS Forecasti</h3>

<ul>
  <li>ì´ì œ final latent representationì„ ì–»ëŠ”ë‹¤ :
    <ul>
      <li>\(\mathbf{H}=\text { Flatten }\left(\left[\mathbf{H}_p\right]_{p=1}^P\right) \mathbf{W}^f \in \mathbb{R}^{N \times D_o}\), where  \(\mathbf{W}^f \in \mathbb{R}^{P D \times D_o}\) are learnable parameters.</li>
      <li>ê° ë³€ìˆ˜ë§ˆë‹¤ ì´ representationì„ ì–»ëŠ”ë‹¤</li>
    </ul>
  </li>
  <li>n-ë²ˆì§¸ ë³€ìˆ˜ì˜ final latent representation \(\mathbf{H}^n \in \mathbf{H}\)ê³¼, forecasting query \(\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)ë¥¼ ê°€ì§€ê³  MLPì— ë„£ëŠ”ë‹¤</li>
  <li>
    <p>\(\hat{x}_j^n=\operatorname{MLP}\left(\left[\mathbf{H}^n \| \phi\left(q_j^n\right)\right]\right)\).</p>
  </li>
  <li>ëª¨ë¸ì€ ê° ë³€ìˆ˜ì˜ ì˜ˆì¸¡ì˜ MSEë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
    <ul>
      <li>\(\mathcal{L}=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2\).</li>
    </ul>
  </li>
</ul>

<h3 id="44-analysis-on-scalabil">4.4. Analysis on Scalabil</h3>

<ul>
  <li>The average sequence length : \(L_{t p}=L_{a v g} \leq L_{\max } \leq L_{c p r} \leq N \times L_{a v g}\), where
    <ul>
      <li>\(L_{\text {avg }}=\frac{1}{N} \sum_{n=1}^N L_n\).</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-experimental-setup">5.1. Experimental Setup</h3>

<ul>
  <li>Dataset :
    <ul>
      <li>PhysioNet, MIMIC, Human Activity, and USHCN</li>
      <li>training, validation, and test sets adhering to ratios of 60%, 20%, and 20%</li>
    </ul>
  </li>
  <li>Evaluation Metric :
    <ul>
      <li>\(\begin{aligned}
\text { MSE }&amp;=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2, \\\text { MAE }&amp;=
 \frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left|\hat{x}_j^n-x_j^n\right| .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h4 id="52-main-results">5.2. Main Results</h4>

<p><img src="/assets/img/timeseries/TPatchGNN/table1.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="53-ablation-study">5.3. Ablation Study</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="54-scalability-and-efficiency-analysis">5.4. Scalability and Efficiency Analysis</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table3.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="55-effect-of-patch-size">5.5. Effect of Patch Size</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/fig4.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>achieved the alignment between asynchronous IMTS
        <ul>
          <li>by transforming each univariate irregular time series into a series of transformable patches with varying observation counts but maintaining unified time horizon resolution.</li>
          <li>without a canonical pre-alignment representation process, preventing the aligned sequence length from explosively growing</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2024](https://openreview.net/pdf?id=UZlMXUGI6e)]]></summary></entry><entry><title type="html">Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)</title><link href="http://localhost:4000/timeseries/2024-10-03-Mamba360/" rel="alternate" type="text/html" title="Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)" /><published>2024-10-03T00:00:00+09:00</published><updated>2024-10-03T20:01:49+09:00</updated><id>http://localhost:4000/timeseries/Mamba360</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-03-Mamba360/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Sequence modelingì—ì„œ RNN, LSTMì„ ì‚¬ìš©í–ˆì—ˆìŒ</li>
  <li>Transformerê°€ í›Œë¥­í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ
    <ul>
      <li>but \(O(N^2)\) complexity,inductive bias handlingì´ ì–´ë ¤ì›€</li>
    </ul>
  </li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” State Space Model (SSM)ë¥¼ í¬ê²Œ 3ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
    <ul>
      <li>Gating architectures</li>
      <li>Structural architectures</li>
      <li>Recurrent architectures</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>RNN
    <ul>
      <li>look at only the last state and current input for predicting the next state</li>
      <li>gradient calculations being limited to the hidden state and current input</li>
      <li>exploding or vanishing gradient problem</li>
      <li>lack sufficient memory for long sequences</li>
    </ul>
  </li>
  <li>LSTM
    <ul>
      <li>complexity with their gating mechanisms</li>
      <li>exhibit challenges in transfer learning</li>
    </ul>
  </li>
  <li>Transformer
    <ul>
      <li>enable each token to interact with every other token in the input sequence</li>
      <li>but \(O(N^2)\) complexity</li>
    </ul>
  </li>
  <li>State Space Model (SSM)
    <ul>
      <li>Understanding of State Space Models (SSMs) : mathematical fundamentals</li>
      <li>Categorization and Recent Advances of SSMs : systematic categorization</li>
      <li>Application of SSMs Across Domains : utility in domains</li>
      <li>Performance Comparison of SSMs with Transformers : SSMê³¼ Transformer ë¹„êµ</li>
    </ul>
  </li>
</ul>

<h2 id="2-basics-of-state-space-model">2. Basics of State Space Model</h2>

<ul>
  <li>High-orderë¥¼ first-order derivativesì™€ vector quantitiesë¡œ representation</li>
  <li>Dynamics of  damped mass-spring system : \(m \frac{d^2 y(t)}{d t^2}+c \frac{d y(t)}{d t}+k y(t)=u(t)\)
    <ul>
      <li>\(u(t)\) : ì§ˆëŸ‰ì— ì‘ìš©í•˜ëŠ” ì™¸ë¶€ í˜</li>
      <li>\(y(t)\) : ìˆ˜ì§ ìœ„ì¹˜</li>
      <li>\(x(t)\) : ì´ ë°©ì •ì‹ì„ 1ì°¨ ë¯¸ë¶„ê³¼ ë²¡í„° ì–‘ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ ë„ì…í•˜ëŠ” ë²¡í„°</li>
    </ul>
  </li>
</ul>

<h3 id="21-spring-mass-damper-system">2.1. Spring Mass-Damper system</h3>

<ul>
  <li>State Variables
    <ul>
      <li>\(x_1\) : equilibriumìœ¼ë¡œë¶€í„° ì§ˆëŸ‰ì˜ ìœ„ì¹˜</li>
      <li>\(\dot{x_1}\) : ì§ˆëŸ‰ì˜ ì†ë„</li>
    </ul>
  </li>
  <li>System Dynamics
    <ul>
      <li>ë‰´í„´ì˜ ì œ 2ë²•ì¹™ìœ¼ë¡œ í‘œí˜„í•˜ë©´ \(m \ddot{x}_1=-k x_1-c \dot{x}_1\)</li>
      <li>\(\ddot{x_1}\)ëŠ” ì§ˆëŸ‰ì˜ ê°€ì†ë„, \(-kx_1\)ì€ ìœ„ì¹˜ì— ë¹„ë¡€í•˜ëŠ” ìŠ¤í”„ë§ì˜ í˜,</li>
      <li>\(c\dot{x_1}\)ì€ ì†ë„ì— ë¹„ë¡€í•˜ëŠ” damping force (ìš´ë™ ì—ë„ˆì§€ ê°ì‡ ì‹œí‚¤ëŠ” í˜)</li>
    </ul>
  </li>
  <li>State-Space Formulation
    <ul>
      <li>State vector \(x \in \mathbb R^n\) : ì‹œìŠ¤í…œì˜ ë‚´ë¶€ ìƒíƒœ ë³€ìˆ˜</li>
      <li>Input vector \(u\in \mathbb R^m\) : ì‹œìŠ¤í…œì— ëŒ€í•œ ì œì–´ ë˜ëŠ” ì™¸ë¶€ ì…ë ¥</li>
      <li>Output vector \(y \in \mathbb R^p\) : ê´€ì‹¬ ìˆëŠ” ì¸¡ì • ê°€ëŠ¥í•œ ì–‘</li>
      <li>System dynamics : ì¼ì°¨ ë¯¸ë¶„ ë°©ì •ì‹ìœ¼ë¡œ í‘œí˜„ \(\dot{\mathbf x}=\mathbf A \mathbf x+\mathbf B \mathbf u\)
        <ul>
          <li>\(\mathbf x=\left[x_1, \dot{x}_1\right]^T\)ëŠ” state vector, \(\mathbf u\)ëŠ” input,</li>
          <li>\(\mathbf A\in\mathbb R^{n \times n}\)ëŠ” dynamic matrix \(\mathbf A=\left[\begin{array}{cc}
0 &amp; 1 \\
-\frac{k}{m} &amp; -\frac{c}{m}
\end{array}\right]\)</li>
          <li>\(\mathbf B \in \mathbb R^{n \times m}\)ì€ input matrix \(\mathbf B=\left[\begin{array}{c}
0 \\
\frac{1}{m}
\end{array}\right] \dot{\mathbf x}\)</li>
          <li>Output equation : \(\mathbf{y}=\mathbf{C x}+\mathbf{D u}\)
            <ul>
              <li>\(\mathbf{C} \in \mathbb{R}^{p \times n}\)ëŠ” output or sensor matrix</li>
              <li>\(\mathbf{D} \in \mathbb{R}^{p \times m}\)ëŠ” feedthrough matrix</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-state-space-model">2.2. State Space Model</h3>

<ul>
  <li>Definition
    <ul>
      <li>Discrete-time dynamical system :
        <ul>
          <li>\(x(t+1)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t=0,1,2,\)â€¦</li>
          <li>\(x(t) \in \mathbb{R}^n\) : tì‹œì ì—ì„œ state</li>
          <li>\(u(t) \in\mathbb{R}^p\) : control variables</li>
          <li>\(y(t) \in\mathbb{R}^k\)â€‹ : specific outputs of interest</li>
        </ul>
      </li>
      <li>Continuous-time model
        <ul>
          <li>\(\frac{d}{d t} x(t)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t \geq 0\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/fig3.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>Model Formulation
    <ul>
      <li>Complexity ë•Œë¬¸ì— Multi-head self-attention ëŒ€ì‹  SSM ì‚¬ìš©</li>
      <li>Continuous-time Latent State spaceëŠ” linear ordinary differential equationìœ¼ë¡œ í‘œí˜„
        <ul>
          <li>\(\begin{aligned} \dot{x}(t) &amp; =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
y(t) &amp; =\boldsymbol{C} x(t)+\boldsymbol{D} u(t) \end{aligned}\),</li>
          <li>evolution parameter \(A \in \mathcal{R}^{N \times N}\)</li>
          <li>projection parameter \(B \in \mathcal{R}^{N \times 1} \text { and } C \in \mathcal{R}^{N \times 1}\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Discrete-time SSM
    <ul>
      <li>continuous parameters \(A, B, C\) ë¥¼ discreteí•˜ê²Œ ë°”ê¾¸ê¸° ìœ„í•´ time-scale parameter \(\Delta\) ì‚¬ìš©</li>
      <li>ì¦‰ \(\bar{A}=f_A(\Delta, A), \bar{B}=f_B(\Delta, A, B)\)â€‹</li>
      <li>\(\begin{array}{lll}
x_k=\overline{\boldsymbol{A}} x_{k-1}+\overline{\boldsymbol{B}} u_k &amp; \overline{\boldsymbol{A}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1}(\boldsymbol{I}+\Delta / 2 \cdot \boldsymbol{A}) &amp; \\
y_k=\overline{\boldsymbol{C}} x_k &amp; \overline{\boldsymbol{B}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1} \Delta \boldsymbol{B} &amp; \overline{\boldsymbol{C}}=\boldsymbol{C}\end{array}\)â€‹.</li>
      <li>ì›ë˜ëŠ” ìœ„ì²˜ëŸ¼ ìƒê²¼ìŒ</li>
    </ul>
  </li>
  <li>Convolutional Kernel Representation
    <ul>
      <li>í•˜ì§€ë§Œ ìœ„ ì‹ì€ sequential nature ë•Œë¬¸ì— trainableí•˜ì§€ ì•ŠìŒ</li>
      <li>ê·¸ë˜ì„œ ì•„ë˜ì²˜ëŸ¼ continuous convolutionì„ ì‚¬ìš©</li>
      <li>\(\begin{array}{lll}
x_0=\overline{\boldsymbol{B}} u_0 &amp; x_1=\overline{\boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{B}} u_1 &amp; x_2=\overline{\boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{B}} u_2 \\
y_0=\overline{\boldsymbol{C} B} u_0 &amp; y_1=\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{C} B} u_1 &amp; y_2=\overline{\boldsymbol{C} \boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{C} B} u_2
\end{array}\).</li>
      <li>vectorizeí•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ</li>
      <li>\(\begin{aligned}
y_k &amp; =\overline{\boldsymbol{C A}}^k \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C A}}^{k-1} \overline{\boldsymbol{B}} u_1+\cdots+\overline{\boldsymbol{C} \boldsymbol{A B}} u_{k-1}+\overline{\boldsymbol{C} \boldsymbol{B}} u_k \\
y &amp; =\overline{\boldsymbol{K}} * u \\
\overline{\boldsymbol{K}} \in \mathbb{R}^L: &amp; =\mathcal{K}_L(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}}):=\left(\overline{\boldsymbol{C}}^i \overline{\boldsymbol{B}}\right)_{i \in[L]}=\left(\overline{\boldsymbol{C B}}, \overline{\boldsymbol{C} \boldsymbol{A B}}, \ldots, \overline{\boldsymbol{C}}^{L-1} \overline{\boldsymbol{B}}\right) .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h2 id="3-recent-advances-in-state-space-models">3. Recent Advances in State Space Models</h2>

<ul>
  <li>Transformerì˜ limitations :
    <ul>
      <li>Computational Complexity</li>
      <li>Large Memory Requirements : for storing embeddings and intermediate actiavations</li>
      <li>Fixed Sequence Length : du to positional embeddings</li>
      <li>Attention Mechanism Scalability : quadratic scaling with input length</li>
      <li>Lack of Causality in Standard Attention : not inherently capture causality</li>
    </ul>
  </li>
  <li>SSMì˜ categorization : ì–´ë–»ê²Œ long sequenceë¥¼ ë‹¤ë£° ê²ƒì¸ê°€
    <ul>
      <li>Structured SSMs : based on S4 and variants</li>
      <li>Recurrent SSMs : based on RNNs and variants</li>
      <li>Gated SSMs : leveraging gating techniques</li>
      <li>Miscellaneous SSMs : ê¸°íƒ€ ë‹¤ì–‘í•œ ë°©ë²•ë“¤</li>
    </ul>
  </li>
</ul>

<h3 id="31-structured-ssms">3.1. Structured SSMs</h3>

<ul>
  <li>
    <p>S4, HiPPO, H3, Liquid-S4 ë“±â€¦</p>
  </li>
  <li>
    <p>long-range dependencyë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²• ì‚¬ìš© :</p>

    <ul>
      <li>
        <p>polynomial projection operators</p>
      </li>
      <li>
        <p>multi-input multi-output systems</p>
      </li>
      <li>
        <p>and convolutional kernels</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="311-structured-state-space-sequence-s4">3.1.1. Structured State Space Sequence (S4)</h3>

<ul>
  <li>Higher-Order Polynomial Project Operator (HiPPO)
    <ul>
      <li>State and input transition matricesë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ memorize</li>
    </ul>
  </li>
  <li>Diagonal Plus Low-Rank Parametrization</li>
  <li>SSM matrix (A)ì˜ rankë¥¼ ë‚®ê²Œ í•´ì„œ diagonalizability and stability ë³´ì¥</li>
  <li>Efficient (convolutional) Kernel Computation
    <ul>
      <li>FFTì™€ iFFT ì‚¬ìš©í•´ì„œ complexityë¥¼ \(ğ‘‚(ğ‘ log(ğ‘))\)ë¡œ ë§Œë“¬</li>
    </ul>
  </li>
</ul>

<h3 id="312-high-order-polynomial-projection-operators-hippo">3.1.2. High-Order Polynomial Projection Operators (HiPPO)</h3>

<ul>
  <li>S4ì— ì‚¬ìš©ëœ í–‰ë ¬ì˜ ìˆ˜í•™ì ì¸ í•´ì„ì„ ì œê³µ</li>
  <li>4ê°€ì§€ì˜ ë³€í˜•ì„ ì‚¬ìš©í•˜ëŠ”ë°,
    <ul>
      <li>the truncated Fourier basis polynomial (Hippo-FouT)</li>
      <li>based on Lagurre polynomials(LagT)</li>
      <li>based on Legendre polynomials(LegT)</li>
      <li>based on Legendre polynomials with a sliding window(LegS)</li>
    </ul>
  </li>
</ul>

<h3 id="313-hungry-hungry-hippo-h3">3.1.3. Hungry Hungry HiPPO (H3)</h3>

<ul>
  <li>SSMì—ì„œì˜ 2ê°œì˜ challenges
    <ul>
      <li>ì²«ì§¸, difficulty in recalling earlier tokens
        <ul>
          <li>ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ì´ì „ í† í°ì„ ê¸°ì–µí•˜ëŠ” ë° ì–´ë ¤ì›€</li>
        </ul>
      </li>
      <li>ë‘˜ì§¸, difficult in comparing the tokens across different sequences
        <ul>
          <li>ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ì—ì„œ í† í°ì„ ë¹„êµí•˜ëŠ” ë° ì–´ë ¤ì›€</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ê·¹ë³µí•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì˜ 3ê°€ì§€ í•µì‹¬ ìš”ì†Œ
    <ul>
      <li>Multiplicative Interactionsê°€ ìˆëŠ” Stacked SSMs
        <ul>
          <li>stacking two SSMs with multiplicative interactions between their input and output projections</li>
        </ul>
      </li>
      <li>í•™ìŠµ íš¨ìœ¨ì„±ì„ ìœ„í•œ FlashConv
        <ul>
          <li>FFTë¥¼ ì‚¬ìš©í•˜ì—¬  training efficiency í–¥ìƒ</li>
        </ul>
      </li>
      <li>Scalingì„ ìœ„í•œ State-Passing
        <ul>
          <li>effectively splits the input into the largest possible chunks that can fit</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="314-global-convolution">3.1.4. Global Convolution</h3>

<ul>
  <li>ì›ë˜ëŠ” inputë§Œí¼ ê¸´ conv kernelì„ hidden state matrixì— ê³±í–ˆëŠ”ë° - ë¶ˆì•ˆì •í•¨</li>
  <li>ì´ conv kernelì„ parametrizingí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆ</li>
  <li>ì¼ë°˜ì ìœ¼ë¡œ conv kernelì€ FFTë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ëŠë¦´ ìˆ˜ê°€ ìˆì–´ì„œ IO-aware algorithm ì‚¬ìš©</li>
</ul>

<h3 id="317-ldstack">3.1.7. LDStack</h3>

<ul>
  <li>RNNì´ ë‹¤ì¤‘ ì…ë ¥ ë‹¤ì¤‘ ì¶œë ¥(MIMO)  Linear Dynamical System(LDS)ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆìŒ</li>
  <li>ì´ ë•Œ Parallel scanì´ ì‚¬ìš©ë¨</li>
  <li>ì¦‰  Single Input Multiple Outputs (SIMO) LDSë¥¼ í•©ì³ì„œ  MIMO LDSë¥¼ approximate
    <ul>
      <li>essential characteristicsë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ê³„ì‚°ì€ simpleí•´ì§</li>
    </ul>
  </li>
  <li>LDSë¥¼ time-varying state space modelsë¡œ ë³¼ ìˆ˜ ìˆìŒ</li>
</ul>

<h3 id="318-s5">3.1.8 S5</h3>

<ul>
  <li>RNNì„ ë‹¤ì¤‘ ì…ë ¥ ë‹¤ì¤‘ ì¶œë ¥ ì„ í˜• ë™ì  ì‹œìŠ¤í…œ(LDS)ìœ¼ë¡œ ëª¨ë¸ë§í•œ LDStackì„ state space models (SSMs)ìœ¼ë¡œ í™•ì¥</li>
  <li>LDStackê³¼ ë‹¬ë¦¬, S5 ê³„ì¸µì€ ì—¬ëŸ¬ ì…ë ¥ ë° ì¶œë ¥ì„ ë™ì‹œì— ì²˜ë¦¬</li>
</ul>

<h2 id="32-gated-ssms">3.2. Gated SSMs</h2>

<ul>
  <li>FFT ì—°ì‚° ìµœì í™”ë¥¼ ìœ„í•´ gating unitsë¥¼ ì‚¬ìš©</li>
  <li>Toepliz NNì€ position-encoded Toeplitz matrixë¡œ token mixing</li>
  <li>MambaëŠ” gated MLPë¡œ SSMì˜ compoutational inefficiency ê·¹ë³µí•˜ê³ ì í•¨</li>
  <li>(ë¬´ìŠ¨ ë§ ?) ë” ì½ì–´ë³´ì</li>
</ul>

<h3 id="323-toeplitz-neural-network-tnn">3.2.3. Toeplitz Neural Network (TNN)</h3>

<ul>
  <li>Transformerì˜ <strong>attention-mechanism</strong>ê³¼ <strong>positional embedding</strong>ì„ ê°œì„ </li>
  <li>position-encoded Toeplitz matrixë¥¼ ì‚¬ìš©í•˜ì—¬ token-pair ê´€ê³„ íŒŒì•…
    <ul>
      <li>space-time complexityë¥¼ \(O(NlogN)\)ìœ¼ë¡œ ì¤„ì„</li>
      <li>Relative Position Encoder (RPE)ë¡œ ìƒëŒ€ì  ìœ„ì¹˜ë¥¼ ìƒì„±í•´ì„œ parametersê°€ input lengthì— ë…ë¦½ì ì´ê²Œ í•¨</li>
    </ul>
  </li>
</ul>

<h3 id="324-mamba">3.2.4. Mamba</h3>

<ul>
  <li>Transformerì˜ quadratic computational and memory complexityì— ì£¼ëª©</li>
  <li>íŠ¹íˆ SSMì€  addressing tasks (selective copying, induction head)ì—ì„œ ë¹„íš¨ìœ¨ì ì´ì—ˆìŒ</li>
  <li>Mambaê°€ ì´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì€ :
    <ul>
      <li>novel parametrization approach for SSMs based on input characteristics</li>
      <li>incorporating a simple selection mechanism</li>
      <li>efficient hardware-aware algorithm based on selective scan</li>
      <li>gated technique to reduce the dimensionality of global kernel operations</li>
      <li>combine gated MLP[93] with the SSM module</li>
    </ul>
  </li>
</ul>

<h2 id="4-applications-of-state-space-models">4. Applications of State Space Models</h2>

<h3 id="41-language-domain-long-sequence">4.1. Language Domain (long sequence)</h3>

<ul>
  <li>ì›ë˜ëŠ” Transformer ë§ì´ ì¼ëŠ”ë° \(O(N^2)\) quadratic complexity \(\to\) long sequence ë¶ˆê°€ëŠ¥</li>
  <li>ê·¸ë˜ì„œ  State Space Models (SSMs)ì´ ë“±ì¥
    <ul>
      <li>input dataë¥¼ fixed-size latent stateì— í‘œí˜„</li>
      <li>í•˜ì§€ë§Œ ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ capability to retrieve and copyì—ì„œ trade-off</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="42-vision-domain">4.2. Vision domain</h3>

<ul>
  <li>Vision Mambaë‚˜ SiMBAì™€ ê°™ì€ Vision-specific Mamba
    <ul>
      <li>utilize bidirectional and visual state space models</li>
    </ul>
  </li>
  <li>SiMBA
    <ul>
      <li>sequence length and channel dimensionsì´ ê¼­ perfect square dimensionsì´ ì•„ë‹ˆì–´ë„ ë¨</li>
      <li>pyramid version of the transformer architecture (ì„±ëŠ¥ í–¥ìƒ)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table3.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="47-time-series-domain">4.7. Time Series Domain</h3>

<ul>
  <li>ì˜›ë‚ ì—ëŠ” ARIMA ì“°ë‹¤ê°€ Transformer ë“±ì¥í•˜ë©´ì„œ variantsê°€ ë§ì´ ë‚˜ì˜´
    <ul>
      <li>Informer, FEDFormer, PatchTSTâ€¦</li>
      <li>í•˜ì§€ë§Œ ì—¬ì „íˆ attention complexity ë•Œë¬¸ì— long-range dependency ëª»ì¡ìŒ</li>
    </ul>
  </li>
  <li>ê·¸ë˜ì„œ SSM ëª¨ë¸ì¸ Timemachine, SiMBA, MambaMix ë“±ì¥</li>
</ul>

<p><img src="/assets/img/timeseries/Mamba360/table11.png" alt="ê·¸ë¦¼1" /></p>

<p><img src="/assets/img/timeseries/Mamba360/table14.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>SSMì€ 3ê°€ì§€ ë²”ì£¼ë¡œ ë¶„ë¥˜ ê°€ëŠ¥ (structured, gated, and recurrent)</li>
  <li>ì•„ì§ Transformerê°€ ë” ì˜í•˜ëŠ” ì˜ì—­ì´ ìˆê¸´ í•˜ì§€ë§Œ (ë§¥ë½ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‘ì—… ë“±)
    <ul>
      <li>SiMBAëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì™€ Mamba ì•„í‚¤í…ì²˜ë¥¼ ê²°í•©í•´ì„œ Time seriesì—ì„œ SOTA</li>
    </ul>
  </li>
  <li>SSMì„ large networkë¡œ ì•ˆì •ì ìœ¼ë¡œ í™•í•˜ëŠ” ê²ƒì´ ì•„ì§ í•´ê²°ë˜ì§€ ì•Šì€ ë¬¸ì œ</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/abs/2404.16112)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2022) Raindrop</title><link href="http://localhost:4000/pytorch/2024-09-24-raindrop/" rel="alternate" type="text/html" title="(Code Review, ICLR 2022) Raindrop" /><published>2024-09-24T00:00:00+09:00</published><updated>2024-09-24T18:34:03+09:00</updated><id>http://localhost:4000/pytorch/raindrop</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-24-raindrop/"><![CDATA[<p><a href="https://arxiv.org/abs/2110.05357">(Paper) Graph-Guided Network for Irregularly Sampled Multivariate Time Series</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">(Paper Review, ICLR 2022) Raindrop</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig1.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig2.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë¨¼ì € í„°ë¯¸ë„ì— git cloneê³¼ requirementsë¥¼ ì…ë ¥í•˜ì—¬ install í•œë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">python Raindrop.py</code>ë¡œ P19, P12, PAM ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ì„ ë³¼ ìˆ˜ ìˆë‹¤.</li>
</ul>

<h2 id="2-raindroppy">2. Raindrop.py</h2>

<h3 id="21-data-preparing">2.1. Data Preparing</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig3.png" alt="ì‚¬ì§„1" />
<img src="/assets/img/pytorch/raindrop_code/fig4.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>parserë¥¼ í†µí•´ aurgmentsë¥¼ ë§Œë“¤ê³ </li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” modelì€ irregular time seriesë¥¼ ë‹¤ë£¬ë‹¤.
    <ul>
      <li>ê·¸ëŸ¬ë¯€ë¡œ <code class="language-plaintext highlighter-rouge">missing ratio</code>, ì¦‰ featureë¥¼ maskingí•˜ëŠ” ë¹„ìœ¨ì„ ë¯¸ë¦¬ ê²°ì •í•´ì¤€ë‹¤. (option)</li>
      <li>ì¼ë‹¨ì€ 0(no missing)ìœ¼ë¡œ ë‘ê³  ì½”ë“œë¥¼ ì´í•´í•´ë³´ì</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig5.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì‚¬ì „ì— ì •í•œ <code class="language-plaintext highlighter-rouge">missing ratio</code>ë¥¼ ì‚¬ìš©í•œë‹¤.</li>
  <li>epoch ìˆ˜ì™€ learning rateë„ ë¯¸ë¦¬ ì •í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig6.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>P19 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">d_static</code>ê³¼ <code class="language-plaintext highlighter-rouge">d_inp</code>ëŠ” ì‹œê°„ì— ë”°ë¼ ë³€í•˜ì§€ ì•ŠëŠ”(ì •ì ) / ë³€í•˜ëŠ”(ë™ì ) ë³€ìˆ˜ì˜ ê°œìˆ˜</li>
      <li><code class="language-plaintext highlighter-rouge">static_info</code>ëŠ” <code class="language-plaintext highlighter-rouge">d_static</code> ë³€ìˆ˜ê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ (bool)</li>
      <li><code class="language-plaintext highlighter-rouge">max_len</code>ì€, batch ë‚´ ìƒ˜í”Œë§ˆë‹¤ ì‹œê³„ì—´ì˜ ê¸¸ì´ê°€ ë‹¤ë¥¸ë° ìµœëŒ€ ê¸¸ì´
        <ul>
          <li>ë§Œì•½ <code class="language-plaintext highlighter-rouge">max_len</code>ë³´ë‹¤ ì§§ë‹¤ë©´ ê·¸ ë¶€ë¶„ì€ ë‹¤ 0ìœ¼ë¡œ ê¸°ë¡ë˜ì–´ìˆë‹¤.</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">n_classes</code>ëŠ” ìƒ˜í”Œì— ì†í•˜ëŠ” classì˜ ê°œìˆ˜</li>
    </ul>
  </li>
  <li>
    <p>ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤ë©´ ìœ„ì˜ ë³€ìˆ˜ë“¤ì€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">d_ob</code>ëŠ” ê° ë³€ìˆ˜ë¥¼ ëª‡ ì°¨ì›ìœ¼ë¡œ í‘œí˜„í• ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.</li>
  <li>ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">d_model</code>ì€ ë™ì  ë³€ìˆ˜ì˜ ê°œìˆ˜ì¸ <code class="language-plaintext highlighter-rouge">d_inp</code>ì— <code class="language-plaintext highlighter-rouge">d_ob</code>ë¥¼ ê³±í•œ ê°’ì´ ëœë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">nhid</code>ëŠ” FFNì˜ dimensionì¸ë° <code class="language-plaintext highlighter-rouge">d_model</code>ì˜ 2ë°°ë¥¼ ì‚¬ìš©</li>
  <li><code class="language-plaintext highlighter-rouge">nlayers</code>ëŠ” layerì˜ ê°œìˆ˜, <code class="language-plaintext highlighter-rouge">nhead</code>ëŠ” MHA(multi-head attention)ì—ì„œ heads ê°œìˆ˜ì´ê³  ëª¨ë‘ 2ê°œë¥¼ ì‚¬ìš©</li>
  <li><code class="language-plaintext highlighter-rouge">dropout</code>ì€ TransformerEncoderLayerì—ì„œ ì‚¬ìš©í•˜ëŠ” dropout ratio</li>
  <li><code class="language-plaintext highlighter-rouge">aggreg</code>ëŠ” ë‚˜ì¤‘ì— ê° ë°°ì¹˜ë§ˆë‹¤, ê° ì‹œì ì„ vectorë¡œ í‘œí˜„í• í…ë° ê·¸ê±¸ ëª¨ë“  ì‹œì ì— ëŒ€í•´ í•©ì¹  ë•Œ <strong>í‰ê· </strong>ì„ ì‚¬ìš©</li>
  <li><code class="language-plaintext highlighter-rouge">MAX</code>ëŠ” positional encoderì— ë“¤ì–´ê°€ëŠ” MAX parameterì¸ë°
    <ul>
      <li>ë§‰ìƒ positional encoder ì½”ë“œë¥¼ ë³´ë©´ <code class="language-plaintext highlighter-rouge">MAX</code>ë¼ëŠ” ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë‹ˆ ì‹ ê²½ ì•ˆì¨ë„ ëœë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig7.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n_run</code>ì€ ë°ì´í„°ì…‹ì— ëŒ€í•´ ëª‡ ë²ˆì„ ì‹¤í—˜í•´ì„œ ê¸°ë¡í• ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">n_splits</code>ëŠ” ë°ì´í„°ê°€ 5ë“±ë¶„ ë˜ì–´ìˆì–´ì„œ 5ë¥¼ ì‚¬ìš©í•œë‹¤.</li>
  <li>ê·¸ë¦¬ê³  ë³¸ modelì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì„±ëŠ¥ ì§€í‘œë¥¼ ê¸°ë¡í•  arraysë¥¼ ë§Œë“¤ì–´ë†“ëŠ”ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig8.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ê·¸ë¦¬ê³  ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ì…‹ì„ train(0.8) / valid(0.1) / test(0.1)ë¡œ ë‚˜ëˆ„ê³  label(y)ë„ ë”°ë¡œ ì¤€ë¹„í•œë‹¤.</li>
  <li>P19 ë°ì´í„°ì…‹ì˜ ê²½ìš° trainì—ëŠ” 31042ê°œì˜ ìƒ˜í”Œì´ ìˆë‹¤. (ìƒ˜í”Œì€ í•œ ëª…ì˜ í™˜ì ì •ë„ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.)
    <ul>
      <li>ê·¸ë¦¬ê³  ê° ìƒ˜í”Œì€ <code class="language-plaintext highlighter-rouge">torch.size([# of timesetps,  # of features])</code>ì¸ tensorì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig9.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>TëŠ” time stepsì˜ ìˆ˜, FëŠ” (ë™ì ) ë³€ìˆ˜ì˜ ê°œìˆ˜ì´ê³ , DëŠ” (ì •ì ) ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ëœë‹¤.
    <ul>
      <li>ë™ì  ë³€ìˆ˜ëŠ” <code class="language-plaintext highlighter-rouge">Ptrain</code>ì˜ <code class="language-plaintext highlighter-rouge">arr</code>ì—, ì •ì  ë³€ìˆ˜ëŠ” <code class="language-plaintext highlighter-rouge">Ptrain</code>ì˜ <code class="language-plaintext highlighter-rouge">extended_static</code>ì— ë”°ë¡œ ì¤€ë¹„í•˜ê³  ìˆë‹¤.</li>
    </ul>
  </li>
  <li>ê·¸ë¦¬ê³  normalizationì„ ìœ„í•´ ëª¨ë“  ë³€ìˆ˜ë“¤ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì–»ëŠ”ë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">getStats</code> í•¨ìˆ˜ì—ëŠ” ì‚¬ìš©í•˜ëŠ”ë° íŠ¹ì´ì‚¬í•­ ì—†ìœ¼ë¯€ë¡œ skip</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig10.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ê°ê°ì˜ shapeì€ ì•„ë˜ì™€ ê°™ë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>ì˜ ë™ì  ë³€ìˆ˜ë“¤ì˜ ê°œìˆ˜ê°€ 34ê°œì˜€ëŠ”ë° 68ì´ ëœ ì´ìœ ëŠ” :
    <ul>
      <li>ê°™ì€ í¬ê¸°ì˜ Maskë¥¼ ì˜†ì— ì´ì–´ë¶™ì˜€ê¸° ë•Œë¬¸ì´ë‹¤.</li>
      <li>MaskëŠ” <code class="language-plaintext highlighter-rouge">M = 1*(input_tensor &gt; 0) + 0*(input_tensor &lt;= 0)</code>ì´ë‹¤.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>ì€ 31042ê°œì˜ í…ì„œì¸ë°, ê° tensorëŠ” í•´ë‹¹ ìƒ˜í”Œì˜ ê¸¸ì´ë¥¼ ì•Œë ¤ì¤€ë‹¤.
    <ul>
      <li>ì¦‰ ë§Œì•½ 0ë²ˆì§¸ ìƒ˜í”Œì˜ ê¸¸ì´ê°€ 40ì´ë¼ë©´, 0ë²ˆì§¸ tensorëŠ”<code class="language-plaintext highlighter-rouge">[1, 2, ..., 40, 0, 0, ...]</code>ì´ë‹¤.</li>
      <li>ì¼ë‹¨ ìˆ«ìëŠ” <code class="language-plaintext highlighter-rouge">max_len</code>ê°œì¸ë° í•´ë‹¹ ìƒ˜í”Œì˜ ê¸¸ì´ê¹Œì§€ë§Œ indexë¥¼ ê¸°ë¡í•˜ê³  ë’·ë¶€ë¶„ì€ zero padding</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">y_train</code>ì€ ê° ìƒ˜í”Œì˜ ì •ë‹µ labelì´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig11.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ <code class="language-plaintext highlighter-rouge">global_structure</code>ë¥¼ ì •ì˜í•˜ëŠ”ë°, ê°ê°ì˜ ë™ì  ë³€ìˆ˜ê°€ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€ë¥¼ 0, 1ë¡œ í‘œí˜„
    <ul>
      <li>adjacency matrixì˜ ì—­í• ì„ í•œë‹¤.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">missing_ratio</code>ê°€ ì¡´ì¬í–ˆë‹¤ë©´ ëª‡ëª‡ featureë¥¼ maskingí•œë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>ì´ <code class="language-plaintext highlighter-rouge">sample</code>ì´ë©´ ê° ìƒ˜í”Œ(í™˜ì)ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ íŠ¹ì„±ì„ ë¬´ì‘ìœ„ ì œê±°</li>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>ì´ <code class="language-plaintext highlighter-rouge">set</code>ì´ë©´ ë¯¸ë¦¬ ê³„ì‚°ëœ density scoresë¥¼ ì‚¬ìš©í•˜ì—¬ ì œê±°í•  íŠ¹ì„±ì„ ê²°ì •í•˜ê³  ëª¨ë“  ìƒ˜í”Œì—ì„œ ì œê±°</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig12.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>ì˜ shapeì„ <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps,  batch_size,  # of features(w/masking)])</code>ìœ¼ë¡œ,</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>ì˜ shapeì„ <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps, batch_size])</code>ë¡œ setting</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig13.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì•ì„œ ì†Œê°œí•œ parametersë¥¼ í•œ ë²ˆ ì¶œë ¥í•´ë³´ì•˜ë‹¤.</li>
  <li>ì§€ê¸ˆì€ masking ratioê°€ 0ì´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig14.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>parametersì˜ descriptionsëŠ” ìœ„ì™€ ê°™ë‹¤.</li>
</ul>

<h3 id="22-model-setting">2.2. Model setting</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig15.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ model, criterion, optimiazer, schedulerë¥¼ ì •ì˜í•œë‹¤.</li>
  <li>modelì€ 2d tensorë¡œ í‘œí˜„ëœ ìƒ˜í”Œë§ˆë‹¤ classificationí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë¯€ë¡œ CrossEntropyLossë¥¼ ì‚¬ìš©</li>
  <li>ì•„ì§ inputì„ modelì— ë„£ì€ ê±´ ì•„ë‹˜.
    <ul>
      <li>inputì´ modelì— ë“¤ì–´ê°€ë©´ ì–´ë–¤ ê³¼ì •ì„ ê±°ì¹˜ëŠ”ì§€ëŠ” ì•„ë˜ 3. <code class="language-plaintext highlighter-rouge">models_rd.py</code>ì—ì„œ ë³´ë„ë¡ í•œë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig16.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">idx_0</code>ì€ <code class="language-plaintext highlighter-rouge">y</code>ê°€ 0ì¸ samplesì˜ index, <code class="language-plaintext highlighter-rouge">idx_1</code>ì€ ë°˜ëŒ€</li>
  <li>labelì´ 1ì¸ ìƒ˜í”Œì˜ ê°œìˆ˜ê°€ ì ì€ unbalancing ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 3ë°°ë¡œ ëŠ˜ë¦¼ (ì™œ <strong>3</strong>ë°°ì¸ì§€ëŠ” ëª¨ë¦„)</li>
  <li>batch_sizeê°€ 128ì¸ë° labelì´ 0ê³¼ 1ì¸ samplesë¥¼ ì ˆë°˜ì”© ì±„ìš¸í…Œë‹ˆ
    <ul>
      <li>n_batchesëŠ” ê°œìˆ˜ê°€ ë” ì ì€ label ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  samplesë¥¼ í•œ ë²ˆì”© ë‹¤ ë³¼ ìˆ˜ ìˆë„ë¡ ì„¤ì •í–ˆë‹¤.</li>
      <li>ì‚¬ì‹¤ labelì´ 1ì¸ samplesë¥¼ 3ë°° í–ˆìœ¼ë‹ˆ labelì´ 1ì¸ ìƒ˜í”Œì„ 3ë²ˆì”© ë³´ëŠ” ê¼´ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig17.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig18.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ epochë¥¼ ì‹œì‘í•˜ëŠ”ë°, labelì´ 0ì¸ ìƒ˜í”Œê³¼ 1ì¸ ìƒ˜í”Œì—ì„œ ë¬´ì‘ìœ„ë¡œ <code class="language-plaintext highlighter-rouge">batch_size/2</code>ê°œì”© ê°€ì ¸ì˜¨ë‹¤.</li>
  <li>ì‚¬ì‹¤ labelì´ 1ì¸ samplesë¥¼ 3ë°° í–ˆìœ¼ë‹ˆ ì—¬ê¸°ì„œëŠ” ì¤‘ë³µëœ ìƒ˜í”Œì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ìˆë‹¤.</li>
  <li>modelì— ë“¤ì–´ê°ˆ input tensorsì˜ shapeì„ ë¯¸ë¦¬ í™•ì¸í•´ë‘ì</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig19.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ modelì— ë“¤ì–´ê°€ê³  í†µìƒì ì¸ backpropagationì„ ê±°ì¹œë‹¤.</li>
  <li>modelì— ë“¤ì–´ê°€ë©´ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì•Œì•„ë³´ì.</li>
</ul>

<h2 id="3-models_rdpy">3. models_rd.py</h2>

<h3 id="31-init">3.1. <strong>init</strong></h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>ì´ ìƒë‹¹íˆ ë§ì§€ë§Œ ì§€ê¸ˆ ë‹¤ ì•Œ í•„ìš”ëŠ” ì—†ë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>ì—ì„œ ì‚¬ìš©í•  ë•Œ ë‹¤ì‹œ ì˜¬ë¼ì™€ì„œ ë³´ë©´ ë  ë“¯</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig20.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig21.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig22.png" alt="ì‚¬ì§„1" /></p>

<h2 id="32-forward">3.2. forward</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig23.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>P19 ë°ì´í„°ì…‹ì˜ ê²½ìš° input shapeì€ ì£¼í™©ìƒ‰ ì£¼ì„ê³¼ ê°™ë‹¤.</li>
  <li>srcë¡œ ë“¤ì–´ì˜¤ëŠ” Pì˜ ê²½ìš° 34ê°œì˜ ë³€ìˆ˜ì˜€ëŠ”ë° ê°™ì€ í¬ê¸°ì˜ Maskë¥¼ ì˜†ì— ì´ì–´ë¶™ì¸ ê²ƒì´ë‹ˆ ë‹¤ì‹œ ë¶„ë¦¬
    <ul>
      <li>ê°ê°ì„ missing_mask, srcë¼ê³  ë¶€ë¦„</li>
    </ul>
  </li>
  <li>ê·¸ ë‹¤ìŒ 34ê°œì˜ ë³€ìˆ˜ë¥¼ <code class="language-plaintext highlighter-rouge">d_ob</code>(ì—¬ê¸°ì„  4)ë²ˆ ë°˜ë³µí•´ì„œ srcì˜ representation capacityë¥¼ í‚¤ì›Œì£¼ê³ 
    <ul>
      <li>ReLuë¥¼ í†µê³¼ì‹œì¼œì„œ non-linearityë¥¼ í‘œí˜„í•  ìˆ˜ ìˆê²Œ í•œë‹¤.</li>
      <li>ê·¸ ë‹¤ìŒ dropoutì„ ê±°ì¹œë‹¤.</li>
    </ul>
  </li>
  <li>ê²°êµ­ <code class="language-plaintext highlighter-rouge">h</code>ëŠ” srcë¥¼ í™•ì¥ì‹œí‚¤ê³  learnable weightsì™€ ReLuë¥¼ ê³±í•´ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë§Œë“  ê²ƒ</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig24.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ batchì— ìˆëŠ” ê° sampleë§ˆë‹¤ maskë¥¼ ë§Œë“ ë‹¤.</li>
  <li>sampleì— ê°’ì´ ìˆìœ¼ë©´ maskì—ëŠ” Falseê°€ ë˜ê³  ê°’ì´ ì—†ìœ¼ë©´ maskê°€ Trueê°€ ëœë‹¤.</li>
  <li>maskì˜ ê¸¸ì´ëŠ” 60ìœ¼ë¡œ ê³ ì •ì´ì§€ë§Œ sampleë§ˆë‹¤ ê¸¸ì´ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì–´ë””ê¹Œì§€ Falseì´ê³  ì–¸ì œë¶€í„° Trueì¸ì§€ëŠ” sampleë§ˆë‹¤ ë‹¤ë¥´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig25.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë‹¤ìŒìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">global_structure</code>ë¥¼ adjacency matrixë¡œ ì‚¬ìš©í•œë‹¤.
    <ul>
      <li>shapeì€ ë™ì  ë³€ìˆ˜ì˜ ê°œìˆ˜ <code class="language-plaintext highlighter-rouge">d_inp</code> x <code class="language-plaintext highlighter-rouge">d_inp</code>ê°€ ë˜ë¯€ë¡œ ê° ë™ì  ë³€ìˆ˜ë¥¼ ì—°ê²° ì—¬ë¶€ë¥¼ (0,1)ë¡œ í‘œí˜„í•œë‹¤.</li>
      <li>epochê°€ ì§„í–‰ë˜ë©´ì„œ ë°”ë€” ìˆ˜ë„ ìˆìœ¼ë‹ˆ ëŒ€ê°ì„±ë¶„ì€ í•­ìƒ 1ë¡œ updateí•´ì¤€ë‹¤.</li>
    </ul>
  </li>
  <li>ê·¸ ë‹¤ìŒ edge_indexì™€ edge_weightsë¥¼ ë¯¸ë¦¬ êµ¬í•´ë†“ëŠ”ë‹¤.
    <ul>
      <li>ì—°ê²°ëœ nodesì˜ indexì™€ ê·¸ weightsë¥¼ ì˜ë¯¸í•¨</li>
    </ul>
  </li>
  <li>ê·¸ ë‹¤ìŒ batchì— ìˆëŠ” ê° sampleë§ˆë‹¤ (ë™ì ) ë³€ìˆ˜ë“¤ì˜ global structure(edge)ë¥¼ ê³ ë ¤í•œ representationì„ ì €ì¥í•  ê³µê°„ <code class="language-plaintext highlighter-rouge">output</code>ì„ ë¯¸ë¦¬ ë§Œë“¤ì–´ë†“ëŠ”ë‹¤.
    <ul>
      <li>ê° sampleë§ˆë‹¤ <code class="language-plaintext highlighter-rouge">torch([# of time steps,  d_inp x d_ob])</code> shapeì˜ tensorê°€ ë“¤ì–´ê°ˆ ì˜ˆì •ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig26.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ ì•„ê¹Œ ë§Œë“  <code class="language-plaintext highlighter-rouge">h</code>ë¥¼ <code class="language-plaintext highlighter-rouge">x</code>ë¡œ ë°›ì•„ì„œ (<code class="language-plaintext highlighter-rouge">x=h</code>) í•˜ë‚˜ì˜ sampleì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">h</code>ë¥¼ <code class="language-plaintext highlighter-rouge">stepdata</code> ê°€ì ¸ì˜¨ë‹¤</li>
  <li><code class="language-plaintext highlighter-rouge">p_t</code>ëŠ” ê° timestepì„ <code class="language-plaintext highlighter-rouge">d_pe = 16</code>ì°¨ì› vectorë¡œ embeddingí•œ ê²ƒì´ë‹¤. (init ì°¸ê³ )</li>
  <li>ì´ì œ <code class="language-plaintext highlighter-rouge">stepdata</code>ë¥¼ <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code>ë¡œ reshapeí•œë‹¤.
    <ul>
      <li>ì™œëƒí•˜ë©´ featureë¼ë¦¬ attentionì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ê° featureë¥¼ í•˜ë‚˜ì˜ vectorë¡œ ë§Œë“¤ í•„ìš”ê°€ ìˆê¸° ë•Œë¬¸</li>
    </ul>
  </li>
  <li>ì´ì œ ê° featureë¥¼ vectorë¡œ ë§Œë“  ê±¸ <code class="language-plaintext highlighter-rouge">ob_propagation</code>ìœ¼ë¡œ ì •ì˜ëœ attention layerì— ë„£ëŠ”ë‹¤.
    <ul>
      <li>ê·¸ëŸ¬ë©´ ê°™ì€ shape <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code> tensorê°€ returnë˜ì§€ë§Œ</li>
      <li>í•´ë‹¹ sampleì˜ ê°ê°ì˜ featuresë¥¼ Observation Propagationì„ ê±°ì³ representationí•œ ê²°ê³¼ì´ë‹¤.</li>
      <li><code class="language-plaintext highlighter-rouge">Ob_propagation.py</code>ì— ìˆê³ , ì½”ë“œë¥¼ ë”°ë¡œ ì²¨ë¶€í•˜ì§€ëŠ” ì•Šê² ìœ¼ë‚˜ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ì„ ê±°ì¹œë‹¤.
        <ul>
          <li>1) Message Passing: node ê°„ì— ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” mechanism êµ¬í˜„</li>
          <li>2) Attention Mechanism: ê° nodeê°€ ì´ì›ƒ nodeë¡œë¶€í„° ë°›ëŠ” ë©”ì‹œì§€ì˜ ì¤‘ìš”ë„ë¥¼ í•™ìŠµ</li>
          <li>3) Egde weights: graphì˜ edgeì— weightë¥¼ ì ìš©í•˜ì—¬ ì •ë³´ ì „ë‹¬ì˜ ê°•ë„ë¥¼ ì¡°ì ˆ</li>
          <li>4) Edge prune: ì¤‘ìš”ë„ê°€ ë‚®ì€ edgeë¥¼ ì œê±°í•˜ì—¬ computation efficiency ë†’ì„</li>
          <li>5) Feature Transform: linear Transformê³¼ activation ftnìœ¼ë¡œ nodeì˜ featureë¥¼ ë³€í™˜</li>
          <li>6) Aggregation: ì´ì›ƒ nodeë¡œë¶€í„° ë°›ì€ ë©”ì‹œì§€ë¥¼ í•©ì¹¨</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig27.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ob_propagation-layer</code>ë¥¼ í•œ ë²ˆ ë” í†µê³¼ì‹œí‚¤ê³  shapeì„ ë§ì¶°ì„œ <code class="language-plaintext highlighter-rouge">output</code>ì˜ sample index ìë¦¬ì— ë„£ëŠ”ë‹¤.
    <ul>
      <li>ê·¸ë¦¬ê³  alpha_allì—ëŠ” ê·¸ attention weightsë¥¼ ë„£ëŠ”ë‹¤.
        <ul>
          <li>34ê°œì˜ featuresë¼ë¦¬ì˜ attentionì´ë‹ˆ 34\(\times\)34\(=\)1156ê°œì˜ ìˆ«ìê°€ ëœë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ëª¨ë“  samplesì— ëŒ€í•´ì„œ ì™„ë£Œí•˜ì—¬ <code class="language-plaintext highlighter-rouge">output</code>ì´ ì™„ì„±ë˜ë©´ distanceë¥¼ êµ¬í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig28.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë‹¤ìŒìœ¼ë¡œ time embeddingì„ concatí•œë‹¤.</li>
  <li>ì´ëŸ¬ë©´ shapeì´ <code class="language-plaintext highlighter-rouge">torch.size([60, 128, 152])</code>ê°€ ë˜ëŠ”ë°, ê° sampleë§ˆë‹¤(128), í•˜ë‚˜ì˜ ì‹œì ì„ 152ì°¨ì› vectorë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤.
    <ul>
      <li>ì´ 152ëŠ” (ë™ì ) ë³€ìˆ˜ 34ê°œë¥¼ 34\(\times\)4 = 136ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•˜ê³ , time embedding 16ì°¨ì›ì„ ë¶™ì¸ ê²ƒ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig29.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ê±¸ transformer encoderì— í†µê³¼ì‹œí‚¤ê³ </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig30.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>aggregate í•˜ëŠ”ë°, ì´ ë•Œ ëª¨ë“  ì‹œì ì— ëŒ€í•´ í‰ê· ì„ ë‚´ì¤€ë‹¤. (<code class="language-plaintext highlighter-rouge">aggreg == mean</code>)</li>
  <li>ê·¸ëŸ¬ë©´ ê° sampleì€ ëª¨ë“  ì‹œì ê³¼ ëª¨ë“  ë³€ìˆ˜ë¥¼ í†µí•©í•˜ì—¬ 152ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ëœ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig31.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë§ˆì§€ë§‰ìœ¼ë¡œ (ì •ì ) ë³€ìˆ˜ë¥¼ embeddingí•œ embë¥¼ ë¶™ì—¬ì„œ 2-layer MLPì— ë„£ìœ¼ë©´</li>
  <li>ê° sampleì— ëŒ€í•œ classificationì´ ì™„ë£Œëœë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig32.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>Trainingì— ë”°ë¥¸ validation set acccuracyê°€ ì¶œë ¥ëœë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig33.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ê·¸ë¦¬ê³  classification reportê°€ ì¶œë ¥ëœë‹¤.</li>
</ul>

<p>ë !</p>

<ul>
  <li>ì°¸ê³ ë¡œ ë‚˜ì˜ ê²½ìš°ì—ëŠ” <code class="language-plaintext highlighter-rouge">from torch_scatter import gather_csr, scatter, segment_csr</code>ê°€ ì•ˆë˜ì–´ì„œ ì•„ë˜ì™€ ê°™ì´ ì£¼ì„ ì²˜ë¦¬í•˜ê³ 
    <ul>
      <li>pytorchë¥¼ ë³´ê³  í•¨ìˆ˜ë¥¼ ì§ì ‘ ì‘ì„±í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ë‹¤.</li>
      <li><a href="https://github.com/rusty1s/pytorch_scatter/blob/master/torch_scatter/scatter.py">pytorch_scatter ì°¸ê³ </a></li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># from torch_scatter import gather_csr, scatter, segment_csr
</span><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">+</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
            <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">(),</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">other</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">src</span>

<span class="k">def</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">index</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">dim_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">dim_size</span>
        <span class="k">elif</span> <span class="n">index</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">dim_size</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="n">index_dim</span> <span class="o">=</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">index_dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index_dim</span> <span class="o">+</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">index_dim</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">size</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">index_dim</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">count</span><span class="p">[</span><span class="n">count</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="p">.</span><span class="nf">is_floating_point</span><span class="p">():</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">true_divide_</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">div_</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">floor</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="nb">reduce</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">
    |

    .. image:: https://raw.githubusercontent.com/rusty1s/pytorch_scatter/
            master/docs/source/_figures/add.svg?sanitize=true
        :align: center
        :width: 400px

    |

    Reduces all values from the :attr:`src` tensor into :attr:`out` at the
    indices specified in the :attr:`index` tensor along a given axis
    :attr:`dim`.
    For each value in :attr:`src`, its output index is specified by its index
    in :attr:`src` for dimensions outside of :attr:`dim` and by the
    corresponding value in :attr:`index` for dimension :attr:`dim`.
    The applied reduction is defined via the :attr:`reduce` argument.

    Formally, if :attr:`src` and :attr:`index` are :math:`n`-dimensional
    tensors with size :math:`(x_0, ..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})`
    and :attr:`dim` = `i`, then :attr:`out` must be an :math:`n`-dimensional
    tensor with size :math:`(x_0, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})`.
    Moreover, the values of :attr:`index` must be between :math:`0` and
    :math:`y - 1`, although no specific ordering of indices is required.
    The :attr:`index` tensor supports broadcasting in case its dimensions do
    not match with :attr:`src`.

    For one-dimensional tensors with :obj:`reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, the operation
    computes

    .. math::
        \mathrm{out}_i = \mathrm{out}_i + \sum_j~\mathrm{src}_j

    where :math:`\sum_j` is over :math:`j` such that
    :math:`\mathrm{index}_j = i`.

    .. note::

        This operation is implemented via atomic operations on the GPU and is
        therefore **non-deterministic** since the order of parallel operations
        to the same value is undetermined.
        For floating-point variables, this results in a source of variance in
        the result.

    :param src: The source tensor.
    :param index: The indices of elements to scatter.
    :param dim: The axis along which to index. (default: :obj:`-1`)
    :param out: The destination tensor.
    :param dim_size: If :attr:`out` is not given, automatically create output
        with size :attr:`dim_size` at dimension :attr:`dim`.
        If :attr:`dim_size` is not given, a minimal sized output tensor
        according to :obj:`index.max() + 1` is returned.
    :param reduce: The reduce operation (:obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">mul</span><span class="sh">"</span><span class="s">`,
        :obj:`</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="s">` or :obj:`</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="s">`). (default: :obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`)

    :rtype: :class:`Tensor`

    .. code-block:: python

        from torch_scatter import scatter

        src = torch.randn(10, 6, 64)
        index = torch.tensor([0, 1, 0, 1, 2, 1])

        # Broadcasting in the first and last dim.
        out = scatter(src, index, dim=1, reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">)

        print(out.size())

    .. code-block::

        torch.Size([10, 3, 64])
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">sum</span><span class="sh">'</span> <span class="ow">or</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">add</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mul</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">min</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">max</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span>

</code></pre></div></div>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Raindrop github](https://github.com/mims-harvard/Raindrop)]]></summary></entry><entry><title type="html">(Code Review, ICLR 2024) Pathformer</title><link href="http://localhost:4000/pytorch/2024-09-09-pathformer/" rel="alternate" type="text/html" title="(Code Review, ICLR 2024) Pathformer" /><published>2024-09-09T00:00:00+09:00</published><updated>2024-09-24T18:29:39+09:00</updated><id>http://localhost:4000/pytorch/pathformer</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-09-09-pathformer/"><![CDATA[<p><a href="https://openreview.net/pdf?id=lJkOCMP2aW">(Paper) Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-05-23-Pathformer">(Paper Review, ICLR 2024) Pathformer</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig1.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig2.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë¨¼ì € í„°ë¯¸ë„ì— git cloneê³¼ requirementsë¥¼ ì…ë ¥í•˜ì—¬ install í•œë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">bash scripts/multivariate/ETTm2.sh</code>ë¡œ ETTm2 ë°ì´í„°ì…‹ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h2 id="2-sh">2. .sh</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig3.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ETTm2.sh</code> íŒŒì¼ì—ëŠ” <code class="language-plaintext highlighter-rouge">run.py</code>ë¥¼ ì‹¤í–‰í•˜ë„ë¡ ë˜ì–´ìˆë‹¤.</li>
</ul>

<h2 id="3-runpy">3. run.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig4.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>parserë¥¼ í†µí•´ argumentsë¥¼ ë§Œë“ ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig5.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">Exp_Main</code>ì— ìˆëŠ” trainì— argumentsë¥¼ ë„£ì–´ì¤€ë‹¤.</li>
</ul>

<h2 id="4-exp_mainpy">4. exp_main.py</h2>

<h3 id="41-train">4.1. train</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig6.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>model, data, optimizer, criterionì„ ì„¤ì •í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë“¤ê³¼, <code class="language-plaintext highlighter-rouge">vali</code>, <code class="language-plaintext highlighter-rouge">train</code>, <code class="language-plaintext highlighter-rouge">test</code>, <code class="language-plaintext highlighter-rouge">predict</code> í•¨ìˆ˜ê°€ ìˆë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig7.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Exp_main</code> : <code class="language-plaintext highlighter-rouge">train</code>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">_get_data</code>ë¡œ train, valid, test ë°ì´í„°ì…‹ì„ load</li>
      <li><code class="language-plaintext highlighter-rouge">sum(p.numel() for p in self.model.parameters())</code>ëŠ” parameters ê°œìˆ˜</li>
      <li>time, early sipping, optimizer, criterion, learning rate scheduler ì •ì˜</li>
      <li><code class="language-plaintext highlighter-rouge">lr_scheduler.OneCycleLR</code>ëŠ” learning rateë¥¼ ë¹ ë¥´ê²Œ ìµœëŒ€ í•™ìŠµë¥ ê¹Œì§€ ì¦ê°€ì‹œì¼°ë‹¤ê°€ ë‹¤ì‹œ ê°ì†Œì‹œí‚¤ë©´ì„œ ìµœì í™” ê³¼ì •
        <ul>
          <li><code class="language-plaintext highlighter-rouge">optimizer</code> : ì‚¬ìš©í•˜ëŠ” optimizer</li>
          <li><code class="language-plaintext highlighter-rouge">steps_per_epoch</code> : 1 epochê°€ ëª‡ ë²ˆì˜ updateê°€ ë°œìƒí•˜ëŠ”ì§€ (mini-batch)</li>
          <li><code class="language-plaintext highlighter-rouge">pct_start</code> : learning rateê°€ ì¦ê°€í•˜ëŠ” êµ¬ê°„ì˜ ë¹„ìœ¨ / <code class="language-plaintext highlighter-rouge">epochs</code> : ì „ì²´ epoch ìˆ˜</li>
          <li><code class="language-plaintext highlighter-rouge">max_lr</code> : ìµœëŒ€ learning rate</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig8.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>each epochì—ì„œëŠ” train loderì—ì„œ batch ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë°›ê³ </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig9.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">with torch.cuda.amp.autocast():</code>ëŠ” <code class="language-plaintext highlighter-rouge">float16</code>ê³¼ <code class="language-plaintext highlighter-rouge">float32</code>ë¥¼ ìë™ìœ¼ë¡œ ìºìŠ¤íŒ…</li>
  <li>ëª¨ë¸ì´ ì˜ˆì¸¡í•œ <code class="language-plaintext highlighter-rouge">outputs</code>ì™€ ì •ë‹µ <code class="language-plaintext highlighter-rouge">batch_y</code>ë¥¼ ë¹„êµí•˜ì—¬ loss ê³„ì‚°</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig10.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>epochì— ê±¸ë¦° ì‹œê°„ê³¼ lossë¥¼ ì¶œë ¥í•˜ê³  backwardë¡œ parametersë¥¼ update</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig11.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>Validation setì— ëŒ€í•œ lossë¡œ early stopping ì—¬ë¶€ë¥¼ ê²°ì •í•˜ê³  í•™ìŠµì´ ì¢…ë£Œë˜ë©´ ëª¨ë¸ ì €ì¥</li>
  <li>vali í•¨ìˆ˜ëŠ” íŠ¹ì´ ì‚¬í•­ ì—†ìœ¼ë¯€ë¡œ pass</li>
</ul>

<h3 id="42-test">4.2. test</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig12.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>test datasetê³¼, í•™ìŠµë˜ì–´ ì €ì¥ëœ modelì„ loadí•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig13.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>trainê³¼ ë¹„ìŠ·í•˜ê²Œ batch ë‹¨ìœ„ë¡œ ëª¨ë¸ì— ë„£ì–´ì„œ ì˜ˆì¸¡ê°’ì„ ì–»ëŠ”ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig14.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>batch 20ê°œë§ˆë‹¤ ë¬¶ì–´ì„œ visualizaitonì„ í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig15.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ìµœì¢…ì ì¸ ì˜ˆì¸¡ê³¼ lossë¥¼ <code class="language-plaintext highlighter-rouge">results.txt</code>ì— ì €ì¥í•œë‹¤.</li>
</ul>

<h3 id="43-predict">4.3. predict</h3>

<p>pass</p>

<h2 id="5-modelspathformerpy">5. models/Pathformer.py</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">from models import PathFormer</code> ì´ë¯€ë¡œ í•´ë‹¹ ê²½ë¡œë¡œ ê°€ì„œ pathformerì˜ archtectureë¥¼ ë³´ì</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">forward</code>ëŠ” normalization \(\to\) <code class="language-plaintext highlighter-rouge">start_fc</code> \(\to\) for <code class="language-plaintext highlighter-rouge">layer</code> in <code class="language-plaintext highlighter-rouge">self.AMS_lists</code> \(\to\) de-normalizationë¡œ êµ¬ì„±ëœë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>ì— ë“¤ì–´ì˜¨ xì˜ shapeì€ <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes])</code>ì´ë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">seq_len</code>ì€ ê´€ì¸¡í•˜ëŠ” ê³¼ê±° ì‹œì  ìˆ˜, <code class="language-plaintext highlighter-rouge">num_nodes</code>ëŠ” multivariateì—ì„œ variates ê°œìˆ˜</li>
    </ul>
  </li>
  <li>xê°€ unsqueezeë˜ì–´ normalization, <code class="language-plaintext highlighter-rouge">start_fc</code>ë¥¼ í†µê³¼í•˜ë©´ <code class="language-plaintext highlighter-rouge">torch.size([batch_size, seq_len, num_nodes, d_model])</code>ì´ ëœë‹¤. (ì•„ë˜ <code class="language-plaintext highlighter-rouge">__init__</code> ì°¸ê³ )</li>
  <li>ì´ì œ <code class="language-plaintext highlighter-rouge">AMS_list</code>ì˜ <code class="language-plaintext highlighter-rouge">layers</code>ë¥¼ í†µê³¼í•˜ê³  denormalizationì„ í†µê³¼í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig17.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>ì„ ë³´ë©´ <code class="language-plaintext highlighter-rouge">self.AMS_lists</code>ëŠ” <code class="language-plaintext highlighter-rouge">layers.AMS</code>ì—ì„œ importí•œë‹¤.</li>
  <li>AMS layerê°€ pathformerëŠ” ì „ë¶€ì´ë‹ˆ ì‚´í´ë³´ì</li>
</ul>

<h2 id="6-layersamspy">6. Layers/AMS.py</h2>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">self.seasonality_and_trend_decompose</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.noisy_top_k_gating</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.cv_squared</code></li>
  <li><code class="language-plaintext highlighter-rouge">SparseDispatcher</code>ì™€ <code class="language-plaintext highlighter-rouge">SparseDispatcher.dispatch</code>, <code class="language-plaintext highlighter-rouge">SparseDispatcher.combine</code></li>
  <li><code class="language-plaintext highlighter-rouge">self.experts</code></li>
  <li>ê°ê°ì— ëŒ€í•´ì„œ í•˜ë‚˜ì”© ì‚´í´ë³´ë„ë¡ í•œë‹¤.</li>
</ul>

<h3 id="61-selfseasonality_and_trend_decompose">6.1. self.seasonality_and_trend_decompose</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig19.png" alt="ì‚¬ì§„1" /></p>

<p><img src="/assets/img/pytorch/pathformer_code/fig20.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>AMS class ì•ˆì—ì„œ ì •ì˜ëœ í•¨ìˆ˜</li>
  <li><strong>seasonalityì™€ trendë¥¼ xì—ì„œ ê°ê° ê³„ì‚°</strong>í•˜ê¸° ë•Œë¬¸ì— \(seasonal + trend = x\)ê°€ ì•„ë‹˜
    <ul>
      <li>í•´ë‹¹ í•¨ìˆ˜ì˜ ê²°ê³¼ëŠ” xì— seasonalityì™€ trendë¥¼ ë”í•œ ê²°ê³¼ì´ë‹¤.</li>
    </ul>
  </li>
  <li>ì²˜ìŒì— <code class="language-plaintext highlighter-rouge">x = x[:, :, :, 0]</code>ì€ <code class="language-plaintext highlighter-rouge">d_model</code> ì°¨ì›ìœ¼ë¡œ í‘œí˜„ëœ xì—ì„œ ì²« ë²ˆì§¸ dimensionë§Œ ì‚¬ìš©í•´ì„œ decomposeí•œë‹¤ëŠ” ì˜ë¯¸</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig21.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>seasonality_modelì€ <code class="language-plaintext highlighter-rouge">FourierLayer</code>
    <ul>
      <li>í‘¸ë¦¬ì— ë³€í™˜(fft) í›„ amplitudeê°€ ë†’ì€ frequency \(k\)â€‹â€‹ê°œë¥¼ inverse í‘¸ë¦¬ì— ë³€í™˜(extrapolate)</li>
    </ul>
  </li>
  <li>trend_modelì€ <code class="language-plaintext highlighter-rouge">series_decomp_multi</code>
    <ul>
      <li>ë‹¤ì–‘í•œ í¬ê¸°ì˜ kernel sizeë¡œ moving averageë¥¼ softmax</li>
    </ul>
  </li>
</ul>

<h3 id="62-selfnoisy_top_k_gating">6.2. self.noisy_top_k_gating</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig22.png" alt="ì‚¬ì§„1" />
<img src="/assets/img/pytorch/pathformer_code/fig23.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">start_linear.squeeze</code>ì™€ <code class="language-plaintext highlighter-rouge">w_gate</code>ë¡œ <code class="language-plaintext highlighter-rouge">torch.Size([batch, seq_len, num_node])</code>ê°€ <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>ê°€ ëœë‹¤.</li>
  <li>ê°™ì€ í¬ê¸° <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_expert])</code>ì˜ <code class="language-plaintext highlighter-rouge">logit</code>ì„ ë§Œë“¤ê³  \(top-k\) logitì„ <code class="language-plaintext highlighter-rouge">gates</code>ì— ë„£ëŠ”ë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">scatter</code>ëŠ” íŠ¹ì • ì¸ë±ìŠ¤ ìœ„ì¹˜ì— ê°’ì„ í• ë‹¹í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.</li>
      <li><code class="language-plaintext highlighter-rouge">gate</code>ì˜ shapeì€ <code class="language-plaintext highlighter-rouge">torch.Size([batch, num_experts])</code>ê°€ ë˜ëŠ”ë°, ê° í–‰(batch)ì—ì„œ kê°œë¥¼ ì œì™¸í•˜ê³ ëŠ” ë‹¤ 0ì´ë‹¤.</li>
      <li>ê·¸ë¦¬ê³  ê° í–‰(batch)ë§ˆë‹¤ ê·¸ kê°œê°€ ì–´ë–¤ expertsì¸ì§€ëŠ” ë‹¤ë¥´ë‹¤.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">load</code>ëŠ” ê° expertê°€ ë°°ì¹˜ ì „ì²´ì—ì„œ ì–¼ë§ˆë‚˜ ì„ íƒë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤.
    <ul>
      <li>shapeì€ <code class="language-plaintext highlighter-rouge">torch.Size([num_experts])</code>ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="63-selfcv_squared">6.3. self.cv_squared</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ë‹¤ì‹œ AMS.forwardë¡œ ëŒì•„ì˜¤ì</li>
  <li>ê° expertë§ˆë‹¤ ëª¨ë“  ë°°ì¹˜ì— ëŒ€í•´ sumì„ í•´ì„œ <code class="language-plaintext highlighter-rouge">importance</code>ë¥¼ ê³„ì‚°í•˜ë©´ <code class="language-plaintext highlighter-rouge">num_experts</code>ê°œì˜ ìˆ«ìê°€ ëœë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">cv_squared</code>ë¥¼ í†µí•´ <code class="language-plaintext highlighter-rouge">num_experts</code>ê°œì˜ ìˆ«ìì˜ ë³€ë™ê³„ìˆ˜ë¥¼ êµ¬í•´ì„œ <code class="language-plaintext highlighter-rouge">balance_loss</code>ë¥¼ êµ¬í•œë‹¤.
    <ul>
      <li>ë³€ë™ê³„ìˆ˜ëŠ” \(\frac{\sigma^2}{\mu^2}\)ì´ë‹¤.</li>
      <li>ì´ ê°’ì´ í¬ë©´ íŠ¹ì • expertsì— importanceê°€ ëª°ë ¤ìˆìŒì„ ì˜ë¯¸í•œë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig24.png" alt="ì‚¬ì§„1" /></p>

<h3 id="64-sparsedispatcher-ì–´ë ¤ì›€-ì£¼ì˜">6.4. SparseDispatcher (*ì–´ë ¤ì›€ ì£¼ì˜)</h3>

<p><img src="/assets/img/pytorch/pathformer_code/fig25.png" alt="ì‚¬ì§„1" />
<img src="/assets/img/pytorch/pathformer_code/fig26.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>ì—ì„œ ì¤€ë¹„í•´ë†“ëŠ” ê²ƒë“¤ì´ ë§ìœ¼ë‹ˆ í•˜ë‚˜í•˜ë‚˜ ë³´ë„ë¡ í•œë‹¤. \(k=2\), <code class="language-plaintext highlighter-rouge">num_experts</code>=4ì¸ ê²½ìš°ì´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig27.png" alt="ì‚¬ì§„1" /></p>
<ul>
  <li>ê° í–‰ì€ batchë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— í–‰ì˜ ê°œìˆ˜ëŠ” batch size (ì—¬ê¸°ì„  512)</li>
  <li>ê° í–‰ì—ëŠ” <code class="language-plaintext highlighter-rouge">num_experts</code>ê°œì˜ ìˆ«ìê°€ ìˆê³  ê·¸ ì¤‘ \(k\)ê°œë§Œ non-negative, ë‚˜ë¨¸ì§€ëŠ” 0</li>
  <li>ì²« í–‰ì—ì„œ 2, 3ë²ˆì§¸ ìˆ«ìê°€ ì–‘ìˆ˜ë¼ëŠ” ê²ƒì€, ì²«ë²ˆì§¸ ë°°ì¹˜ì—ì„œ 2, 3ë²ˆì§¸ expertsê°€ ì„ íƒë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸</li>
  <li>ë°”ë¡œ ì•„ë˜ì— ìˆëŠ” <code class="language-plaintext highlighter-rouge">torch.nonzero(gates)</code>ì—ì„œë„ ê·¸ ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.
    <ul>
      <li>ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œëŠ” index 1, 2ì¸ expertsê°€, ë§ˆì§€ë§‰ ë°°ì¹˜ì—ì„œëŠ” index 1, 2ì¸ expertsê°€ ì„ íƒë¨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig28.png" alt="ì‚¬ì§„1" /></p>
<ul>
  <li>ì´ì œ sortë¥¼ í•˜ëŠ”ë° ì²«ë²ˆì§¸ ì—´ì€ ì–´ì°¨í”¼ indexë¼ì„œ ì •ë ¬ë˜ì–´ìˆê³ 
    <ul>
      <li>(ë‘ ë²ˆì§¸ ì—´ì´ ì •ë ¬ë˜ë©´ì„œ ì„ì´ê¸° ë•Œë¬¸ì— ë‘ ë²ˆì§¸ ì—´ì˜ ìˆ«ìê°€ ì²« ë²ˆì§¸ ì—´ì˜ ë°°ì¹˜ indexì™€ ìƒê´€ ì—†ê²Œ ëœë‹¤)</li>
    </ul>
  </li>
  <li>ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">index_sorted_experts</code>ëŠ” ì •ë ¬ëœ ìˆ«ìê°€ ëª‡ ë²ˆì§¸ indexì— ìˆë˜ ìˆ«ìì¸ì§€ë¥¼ í‘œì‹œí•´ì¤€ë‹¤.
    <ul>
      <li>(ì—¬ê¸°ì„œë¶€í„° í—·ê°ˆë¦¬ê¸° ì‹œì‘í•¨)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig29.png" alt="ì‚¬ì§„1" /></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">self._expert_index</code>ëŠ” ê° ë°°ì¹˜ì—ì„œ ì„ íƒëœ expertsì˜ indexë¥¼ ì •ë ¬í•œ ê²ƒì´ë‹¤.
    <ul>
      <li>ê° ë°°ì¹˜ë§ˆë‹¤ \(k\)ê°œì”© ìˆìœ¼ë‹ˆ ì´ batch_size \(\times k\)ê°œì˜ ìˆ«ìê² ë‹¤.</li>
    </ul>
  </li>
  <li>ê·¸ë¦¬ê³  ê·¸ê±¸ ë‹¤ì‹œ batch indexë¡œ ë˜ëŒë¦´ ìˆ˜ê°€ ìˆì„ ê²ƒì´ë‹¤.
    <ul>
      <li>ì¦‰ <code class="language-plaintext highlighter-rouge">self._batch_index</code>ê°€ 1, 3, 6,â€¦ì´ë¼ëŠ” ê²ƒì€ expert 0ì´ ì„ íƒë˜ì—ˆë˜ batchê°€ 1, 3, â€¦ì´ê³ </li>
      <li>ê·¸ ë‹¤ìŒ expert 1ì´ ì„ íƒëœ batchë“¤ì´ ëª‡ ë²ˆì§¸ batchì¸ì§€ ì­‰ ë‚˜ì—´ì´ ëœë‹¤. (ì´ê±¸ ë§ˆì§€ë§‰ expertê¹Œì§€ ë°˜ë³µ)</li>
    </ul>
  </li>
  <li>ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">self._part_sizes</code>ëŠ” ëª¨ë“  batches í†µí‹€ì–´ì„œ ê° expertê°€ ëª‡ ë²ˆ ì„ íƒë˜ì—ˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig30.png" alt="ì‚¬ì§„1" /></p>
<ul>
  <li>ì´ì œ <code class="language-plaintext highlighter-rouge">gates_exp</code>ëŠ” expert 0ì´ ì„ íƒë˜ì—ˆë˜ batchesë¥¼ ì­‰ ë‚˜ì—´í•˜ê³ , ê·¸ ë‹¤ìŒì— expert 1ì´ ì„ íƒë˜ì—ˆë˜ batchesë¥¼ ì­‰ ë‚˜ì—´í•˜ê³ â€¦ ë§ˆì§€ë§‰ expertê°€ ì„ íƒë˜ì—ˆë˜ batchesê¹Œì§€ ë‚˜ì—´í•œ ê²ƒì´ë‹¤.</li>
  <li>ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">self._nonzero_gates</code>ëŠ” expert \(i\) (\(i = 1, ...,\) <code class="language-plaintext highlighter-rouge">num_experts</code>)ê°€ ì„ íƒëœ ë°°ì¹˜ì—ì„œ expert \(i\)ì˜ gatesë¥¼ ë‚˜ì—´í•œ ê²ƒì´ë‹¤.</li>
</ul>

<h3 id="641-sparsedispatcherdispatch">6.4.1. SparseDispatcher.dispatch</h3>

<ul>
  <li>ì´ì œ dispatchì—ì„œëŠ” ê° expertì— ì²˜ë¦¬í•´ì•¼ í•  batchesë¥¼ í• ë‹¹í•œë‹¤.</li>
  <li>ë§Œì•½ ì§€ê¸ˆì²˜ëŸ¼ inpì˜ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">torch.Size([512, 96, 7, 16])</code>, <code class="language-plaintext highlighter-rouge">self._batch_index</code>ì˜ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">torch.Size([1024])</code>, ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">self._part_sizes</code>ê°€ <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>ë¼ê³  ê°€ì •í•˜ë©´:</li>
  <li><code class="language-plaintext highlighter-rouge">inp[self._batch_index]</code>ì—ì„œëŠ” inp í…ì„œì—ì„œ 1024ê°œì˜ ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬, í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">torch.Size([1024, 96, 7, 16])</code>ì¸ ìƒˆë¡œìš´ í…ì„œë¥¼ ìƒì„±í•œë‹¤.</li>
  <li>ê·¸ë¦¬ê³  ì²« ë²ˆì§¸ ì°¨ì›(batch ì°¨ì›, 1024ê°œ)ì„ <code class="language-plaintext highlighter-rouge">self._part_sizes</code> = <code class="language-plaintext highlighter-rouge">[262, 348, 249, 165]</code>ë¡œ ë‚˜ëˆˆë‹¤.</li>
  <li>ê²°ê³¼ëŠ” ê° expertì—ê²Œ í• ë‹¹ëœ batchesì˜ ë¦¬ìŠ¤íŠ¸ì´ë©°, ê° í…ì„œì˜ í¬ê¸°ëŠ”:
    <ul>
      <li>ì²« ë²ˆì§¸ expert: [262, 96, 7, 16]</li>
      <li>ë‘ ë²ˆì§¸ expert: [348, 96, 7, 16]</li>
      <li>ì„¸ ë²ˆì§¸ expert: [249, 96, 7, 16]</li>
      <li>ë„¤ ë²ˆì§¸ expert: [165, 96, 7, 16]</li>
    </ul>
  </li>
  <li>ì´ê±¸ ë¦¬ìŠ¤íŠ¸ë¡œ returní•œë‹¤.</li>
</ul>

<h3 id="642-sparsedispatchercombine">6.4.2. SparseDispatcher.combine</h3>

<ul>
  <li>ì´ì œ ê°ê°ì„ í•´ë‹¹ expertì— í†µê³¼ì‹œí‚¨ë‹¤.</li>
  <li>expertëŠ” <code class="language-plaintext highlighter-rouge">TransformerLayer</code>ì´ë‹¤. (Pathformer.pyì˜ __init__ì°¸ê³ )</li>
  <li>ê·¸ë¦¬ê³  ê·¸ ê²°ê³¼ë¥¼ ë‹¤ì‹œ combineí•œë‹¤.
    <ul>
      <li>ê·¸ëŸ°ë° ìœ„ì—ì„œ combine í•¨ìˆ˜ë¥¼ ì˜ ë³´ë©´ ì²˜ìŒì— <code class="language-plaintext highlighter-rouge">.exp()</code>ë¥¼ í•˜ê³  ë‹¤ì‹œ <code class="language-plaintext highlighter-rouge">.log()</code>ë¥¼ í•´ì£¼ëŠ”ë°,</li>
      <li><code class="language-plaintext highlighter-rouge">.exp()</code>ì—ì„œ NaNì´ ë‚˜ì˜¬ ìˆ˜ê°€ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì.</li>
      <li>(ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œëŠ” í•´ë‹¹ì‚¬í•­ ì—†ì§€ë§Œ ë‚´ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì—ì„œëŠ” ë°œìƒí–ˆë‹¤.)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig18.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì´ì œ residual_connectionë§Œ ì ìš©í•´ì£¼ë©´ ëë‚œë‹¤.</li>
  <li>ì—¬ê¸°ê¹Œì§€ê°€ í•˜ë‚˜ì˜ <code class="language-plaintext highlighter-rouge">AMS</code> layerì´ë‹¤.</li>
</ul>

<p><img src="/assets/img/pytorch/pathformer_code/fig16.png" alt="ì‚¬ì§„1" /></p>

<ul>
  <li>ì—¬ê¸°ì„œ for ì•ˆì— ìˆëŠ” layerê°€ AMS layerì´ë‹¤.</li>
  <li>
    <p>ë§ˆì§€ë§‰ìœ¼ë¡œ de-normalizationì„ í•˜ë©´ ëì´ë‹¤.</p>
  </li>
  <li>ë‚˜ë¨¸ì§€ëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì†Œê°œí•œ <code class="language-plaintext highlighter-rouge">3. run.py</code>ì™€ <code class="language-plaintext highlighter-rouge">4. exp_main.py</code>ê°€ ì „ë¶€ì´ë‹¤.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Pathformer github](https://github.com/decisionintelligence/pathformer)]]></summary></entry><entry><title type="html">Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIRâ€™24 Best Paper)</title><link href="http://localhost:4000/timeseries/2024-09-03-SyNCRec/" rel="alternate" type="text/html" title="Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (SIGIRâ€™24 Best Paper)" /><published>2024-09-03T00:00:00+09:00</published><updated>2024-09-03T15:04:04+09:00</updated><id>http://localhost:4000/timeseries/SyNCRec</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-09-03-SyNCRec/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Cross-Domain Sequential Recommendation (CDSR)ì€ multiple domainì—ì„œì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ Single-Domain Sequential Recommendation (SDSR)ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ</li>
  <li>í•˜ì§€ë§Œ <strong>negative transfer</strong> : lack of relation btw domainsì€ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸</li>
  <li>ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ”
    <ol>
      <li>estimates the degree of <strong>negative transfer</strong> of each domain</li>
      <li>adaptively assigns it as a <strong>weight factor</strong> to the prediction loss
        <ul>
          <li>to control gradient flows through domains with significant negative transfer !</li>
        </ul>
      </li>
      <li>developed <strong>auxiliary loss</strong> that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis</li>
    </ol>
  </li>
  <li>ì´ëŸ¬í•œ CDSRê³¼ SDSRì˜ cooperative learningì€ collaborative dynamics between pacers and runners in a marathonì™€ ìœ ì‚¬í•¨</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Single-Domain Sequential Recommendation (SDSR)
    <ul>
      <li>focuses on <strong>recommending the next item</strong> within a <strong>specific</strong> domain using <strong>only</strong> the <strong>single</strong>-domain sequence</li>
    </ul>
  </li>
  <li>Cross-Domain Sequential Recommendation (CDSR)
    <ul>
      <li><strong>predicts</strong> the <strong>next item</strong> a user will interact with, by leveraging their historical <strong>interaction</strong> sequences across <strong>multiple</strong> domains</li>
    </ul>
  </li>
  <li>ë‘˜ì˜ ì°¨ì´ëŠ” ê²°êµ­ ë‹¤ë¥¸ domainsì˜ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ”ì§€ ì—¬ë¶€</li>
  <li>CDSRì€ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë‹¤ë¥¸ domainsì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì§€ë§Œ í•­ìƒ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê±´ ì•„ë‹˜
    <ul>
      <li>ë§Œì•½ ê·¸ê²ƒ ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë” ì•ˆì¢‹ì•„ì§„ë‹¤ë©´, ê·¸ê±´ <strong>negative transfer</strong>ê°€ ìˆì—ˆê¸° ë•Œë¬¸</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>
    <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” SyNCRec: Asymmetric Cooperative Network for Cross-Domain Sequential Recommendationì„ ì œì•ˆ</p>
  </li>
  <li>
    <ol>
      <li>assess the degree of <strong>negative transfer</strong> of each domain
        <ul>
          <li>by comparing the performance of CDSR and SDSR</li>
        </ul>
      </li>
      <li>adaptively assign this value as <strong>weight to the prediction loss</strong> corresponding to a specific domain
        <ul>
          <li>to reduces its flow in domains with significant negative transfer !</li>
        </ul>
      </li>
      <li>developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis
        <ul>
          <li>to exploit the effective correlation signals inherent in the representation pairs of SDSR and CDSR tasks within a specific domain</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>SDSRì€ negative transferë¥¼ ì¤„ì´ê¸° ìœ„í•œ pacerì˜ ì—­í• ì„ í•¨
    <ul>
      <li>(ë§ˆë¼í†¤ì—ì„œ runnerê°€ ë„ˆë¬´ ë¹ ë¥´ê±°ë‚˜ ëŠë¦¬ê²Œ í•˜ì§€ ì•Šê²Œ í•´ì£¼ëŠ” pacer)</li>
    </ul>
  </li>
  <li>íŠ¹íˆ CDSRì´ SDSRë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì•˜ë˜ (=negative transferê°€ ë°œìƒí•œ) ë„ë©”ì¸ì—ì„œ ì„±ëŠ¥ í–¥ìƒë¨</li>
  <li>ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ domain-specific modelsë¥¼ ë§Œë“¤ í•„ìš”ê°€ ì—†ì„ ê²ƒì„ ê¸°ëŒ€í•¨</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="21-single-domain-sequential-recommendation">2.1. Single-Domain Sequential Recommendation</h3>

<ul>
  <li>SDSR : temporal dynamics in user-item interactionsë¥¼ ë””ìì¸
    <ul>
      <li>GRU-based models : GRU4Rec, STAMP, NARM</li>
      <li>Attention-mechanism : SASRec, BERT4Rec, SINE, LightSANs</li>
      <li>Others : NextItNet(CNN), TransRec(Markov chain), â€¦</li>
    </ul>
  </li>
</ul>

<h3 id="22-cross-domain-sequential-recommendation">2.2 Cross-Domain Sequential Recommendation</h3>

<ul>
  <li>CDSR : information from various other domainsë¥¼ leverage
    <ul>
      <li>Matrix factorization : CMF, CLFM, â€¦</li>
      <li>Multi-task learning : DTCDR, DeepAPF, BiTGCF, CAT-ART</li>
      <li>\(\pi-Net\) :  introduced gating mechanisms designed to transfer information from a single domain to another paired domain</li>
      <li>\(C^2DSR\) : employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations</li>
      <li>\(MIFN\) :  introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains</li>
      <li>\(MAN\) : designed group-prototype attention mechanisms to capture domainspecific and cross-domain relationships</li>
    </ul>
  </li>
  <li>Howeverâ€¦ ê²°êµ­ì—ëŠ” ëª¨ë‘ domain pair ë¼ë¦¬ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§
    <ul>
      <li>3ê°œ ì´ìƒì˜ domainsì˜ ê´€ê³„ë¥¼ íŒŒì•…í•  ë•Œ, domainsì´ ì—„ì²­ ë§ì„ ë•Œì—ëŠ” ì–´ë ¤ì›€</li>
      <li>ê·¸ë˜ì„œ CGRecì—ì„œ CDSRì„ ì œì•ˆí•˜ë©´ì„œ negative transfer ê°œë…ì„ ì œì•ˆ
        <ul>
          <li>high negative transferë¥¼ ê°€ì§€ëŠ” domainì— panaltyë¥¼ ì£¼ëŠ” ë°©ì‹</li>
          <li>í•˜ì§€ë§Œ ì—¬ì „íˆ SDSRë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì€ domainì´ ê½¤ ìˆìŒ</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ ë³¸ ë…¼ë¬¸ì—ì„œì˜ ëª©í‘œëŠ” 3ê°œ ì´ìƒì˜ <strong>ëª¨ë“ </strong> ë„ë©”ì¸ì—ì„œ negative transferë¥¼ <strong>íš¨ìœ¨ì </strong>ìœ¼ë¡œ ì¤„ì´ëŠ” ê²ƒ</li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<ul>
  <li>Domains : \(\mathcal{D}=\{A, B, C, \ldots\}\) where \(\mid \mathcal{D}\mid  \geq 3\)
    <ul>
      <li>\(d \in \mathcal D\) ëŠ” í•˜ë‚˜ì˜ íŠ¹ì • ë„ë©”ì¸ì„ ì˜ë¯¸,</li>
      <li>\(V^d\)ëŠ” set of items specific to the domain \(d\), \(V\)ëŠ” total item set across all domains</li>
    </ul>
  </li>
</ul>

<h3 id="definition-1-single--and-cross-domain-sequential-recommendation">Definition 1. Single- and Cross-Domain Sequential Recommendation</h3>

<ul>
  <li>The single-domain sequences of domain \(d\) : \(X^d=\left[(\mathrm{SOS}), x_1^d, x_2^d, \ldots, x_{\mid X^d\mid -1}^d\right]\)â€‹</li>
  <li>\(x_t^d\) :  interaction occurring at time \(t\)</li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ cross-domain sequenceëŠ” \(X=\left(X^A, X^B, X^C, \ldots\right)\)ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ</li>
  <li>ì˜ˆë¥¼ ë“¤ì–´, \(X=\left[(\mathrm{SOS}), x_1^A, x_2^B, x_3^A, x_4^B, x_5^A, x_6^C, x_7^C\right]\)ì€ \(X^A=\left[(\mathrm{SOS}), x_1^A, x_3^A, x_5^A\right], X^B=\left[(\mathrm{SOS}), x_2^B, x_4^B\right], \text { and } X^C=[(\mathrm{SOS})\left., x_6^C, x_7^C\right]\)ìœ¼ë¡œ split ê°€ëŠ¥</li>
  <li>SDSRì€ í•˜ë‚˜ì˜ domain ì•ˆì—ì„œ recommending, CDSRì€ ì „ì²´ ë„ë©”ì¸ì—ì„œ recommending</li>
</ul>

<h3 id="definition-2-negative-transfer-gap-ntg">Definition 2. Negative Transfer Gap (NTG)</h3>

<ul>
  <li>\(\mathcal{L}_\pi^d\)ëŠ” domain \(d\)ì—ì„œì˜ model \(\pi\)ì˜ lossë¥¼ ì˜ë¯¸ (SDSR ë˜ëŠ” CDSR)</li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ Negative transferëŠ” \(\phi_\pi(d) = \mathcal{L}_\pi^d\left(X^d\right)-\mathcal{L}_\pi^d(X)\)</li>
</ul>

<h3 id="problem-statement">Problem Statement</h3>

<ul>
  <li>historical cross-domain sequences \(X_{1:t}\)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ëª©í‘œëŠ” ë‹¤ìŒ item \(x_{t+1}^d = \underset{x_{t+1}^d \in V^d}{\operatorname{argmax}} P\left(x_{t+1}^d \mid X_{1: t}\right)\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ</li>
  <li>ë§Œì•½ \(\mid \mathcal{D}\mid\)ê°œì˜ single-domain sequences (for SDSR)ê³¼ 1ê°œì˜ sequence (for CDSR)ê°€ ìˆë‹¤ë©´
    <ul>
      <li>multi-tasking learning mannerì˜ ëª¨ë¸ í•˜ë‚˜ëŠ” \(\mid \mathcal{D}\mid +1\)ê°œì˜ next item prediction tasksë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<h2 id="4-model">4. Model</h2>

<p><img src="/assets/img/timeseries/SyNCRec/fig2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="41-shared-embedding-layer">4.1. Shared Embedding Layer</h3>

<ul>
  <li>ì—¬ê¸°ì„œëŠ” <strong>initialized representations</strong> of itemsë¥¼ ì–»ëŠ”ë‹¤.
    <ul>
      <li>for \(\mid \mathcal{D}\mid\) single-domain sequences \(X^d\), and one cross-domain sequence \(X\)</li>
    </ul>
  </li>
  <li>Item embedding matrix \(M^d \in \mathbb R^{\mid V^d\mid \times r}\)â€‹ì´ê³ 
    <ul>
      <li>\(\mid V^d\mid\)ëŠ” domain dì˜ items ê°œìˆ˜, rì€ embedding dimension</li>
    </ul>
  </li>
  <li>ëª¨ë“  domainsì— ëŒ€í•´ concatí•˜ë©´ \(M \in \mathbb R^{\mid V\mid \times r}\)
    <ul>
      <li>\(\mid V\mid\)ëŠ” ëª¨ë“  ë„ë©”ì¸ì—ì„œ items ê°œìˆ˜</li>
    </ul>
  </li>
  <li>ì—¬ê¸°ì„œ ìµœê·¼ Tê°œë§Œì„ ì‚¬ìš© (Tê°œë³´ë‹¤ ì ë‹¤ë©´ ì•ìª½ì— paddingìœ¼ë¡œ ë§ì¶°ì¤Œ)
    <ul>
      <li>ê·¸ëŸ¬ë©´ \(\mathbf{E}^d \in \mathbb{R}^{T \times r} \text { and } \mathbf{E} \in \mathbb{R}^{T \times r}\)ë¥¼ ì–»ìŒ (ê°ê° Fig2(c-1), (c-2))</li>
      <li>\(\mid \mathcal D\mid\)ê°œì˜ \(\mathbf{E}^d\)ë¥¼ aggregationí•œ ê²ƒì´ \(\mathbf{E}^{\text {single }}\) (Fig2(c-1))</li>
      <li>ì°¸ê³ ë¡œ \(\mathbf E, \mathbf E^d\)ì—ëŠ” learnable positional embedding ë”í•´ì ¸ìˆìŒ</li>
      <li>\(t\)-th stepì—ì„œì˜ \(\mathbf E, \mathbf E^d\)ëŠ” ê°ê° \(\mathbf e, \mathbf e^d\)ë¡œ ì •ì˜</li>
    </ul>
  </li>
</ul>

<h3 id="42-asymmetric-cooperative-network-with-mixture-of-sequential-experts-acmoe">4.2. Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMoE)</h3>

<ul>
  <li>Negative Transfer (NTG)ëŠ” <strong>loss of the SDSR</strong>ê³¼ <strong>the loss of CDSR</strong>ì˜ ì°¨ì´ë¡œ ì •ì˜
    <ul>
      <li>NTGê°€ ì‘ìœ¼ë©´ ë‹¤ë¥¸ domainsì˜ ì •ë³´ê°€ ë„ì›€ì´ ì•ˆë˜ëŠ” ê±°ê³  í¬ë©´ ë„ì›€ì´ ë˜ëŠ” ê²ƒ</li>
    </ul>
  </li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ weight for the prediction loss in the domainë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤
    <ul>
      <li>gradient flowë¥¼ ì‘ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œë‹¤</li>
    </ul>
  </li>
  <li>Multi-gate Mixture of Sequential Experts (MoE) architectureë¥¼ ì‚¬ìš©í•˜ì—¬ SDSRê³¼ CDSRë¥¼ ìˆ˜í–‰í•˜ê³ 
    <ul>
      <li><strong>models</strong> relationships between different tasks and <strong>learns</strong> task-specific functionalities</li>
      <li>enabling it to effectively leverage shared representations</li>
    </ul>
  </li>
  <li>SDSRê³¼ CDSRì€ ì„œë¡œ ê°„ì„­í•˜ì§€ ì•Šê³ , expertsë¡œëŠ” Transformerë¥¼ ì‚¬ìš©</li>
</ul>

<h3 id="421-architecture">4.2.1. Architecture</h3>

<ul>
  <li>
    <p><strong>ë¨¼ì € SDSRì„ ë³´ì</strong></p>
  </li>
  <li>
    <p>shared embedding layerë¡œë¶€í„° initialized representations of single- and cross-domain sequences,</p>

    <ul>
      <li>ì¦‰ \(\mathbf E, \mathbf E^d\)ê°€ ì£¼ì–´ì ¸ìˆì„ ë•Œ, ê° expertëŠ” many-to-many sequence learningì„ ìˆ˜í–‰</li>
    </ul>
  </li>
  <li>
    <p>domain \(d\)ì˜ output : \(\begin{aligned}
&amp; \left(\mathbf{Y}^d\right)^{\text {single }}=h^d\left(f^d\left(\mathbf{E}^d\right)\right) \\
&amp; f^d\left(\mathbf{E}^d\right)=\sum_{k=1}^j g^d\left(\mathbf{E}^d\right)_k \mathrm{SG}\left(f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)\right)+\sum_{k=j+1}^K g^d\left(\mathbf{E}^d\right)_k f_{\mathrm{TRM}}^k\left(\mathbf{E}^d\right)
\end{aligned}\)</p>

    <ul>
      <li>\(h^d\) : the tower network for domain \(d\)â€‹ (Fig. 2(c-7))
        <ul>
          <li>feed-forward network with layer normalization</li>
        </ul>
      </li>
      <li>\(f^d\) : the multi-gated mixture of the sequential experts layer</li>
      <li>\(SG\)â€‹ :  the stopgradient operation (Fig. 2(c-4))
        <ul>
          <li>forward passì—ì„œëŠ” identity function</li>
          <li>backward passì—ì„œëŠ” SG ì•ˆì— ìˆëŠ” ê²ƒë“¤ì˜ gradientëŠ” drop</li>
          <li>ìœ„ ì‹ì—ì„œëŠ” \(j+1 \sim K\)ë²ˆì§¸ expertsë§Œ unique sequential pattern of single-domain sequencesë¥¼ í•™ìŠµ</li>
        </ul>
      </li>
      <li>\(f_{\text {TRM }}^k\)â€‹ :  the ğ‘˜-th transformerbased sequential expert (Fig. 2(c-3))</li>
      <li>\(g^d\) :  gating network for domain \(d\) (Fig. 2(c-6))
        <ul>
          <li>\(g^d\left(\mathbf{E}^d\right)=\operatorname{softmax}\left(W_g^d \mathbf{E}^d\right)\) where \(W_g^d \in \mathbb{R}^{K \times d T}\) is trainable FC</li>
        </ul>
      </li>
      <li>The \(t\)-th element of \(\mathrm{Y}^{\text {single }}\)ëŠ” \(\left(y_t^d\right)^{\text {single }}\)</li>
    </ul>
  </li>
  <li>
    <p><strong>ë‹¤ìŒìœ¼ë¡œ CDSRì„ ë³´ì</strong></p>
  </li>
  <li>
    <p>ACMoE module : \(\begin{aligned}
&amp; \mathbf{Y}^{\text {cross }}=h^{\text {cross }}\left(f^{\text {cross }}(\mathbf{E})\right) \\
&amp; f^{\text {cross }}(\mathbf{E})=\sum_{k=1}^j g^{\text {cross }}(\mathbf{E})_k f_{\mathrm{TRM}}^k(\mathbf{E})+\sum_{k=j+1}^K g^{\text {cross }}(\mathbf{E})_k \operatorname{SG}\left(f_{\mathrm{TRM}}^k(\mathbf{E})\right)
\end{aligned}\)</p>

    <ul>
      <li>\(h^{cross}\) :  the tower network (Fig. 2(c-9))</li>
      <li>\(f^{\text {cross }}\) : the multi-gated mixture of sequential experts layer for a cross-domain sequence</li>
      <li>\(SG\)ëŠ” \(j+1\sim K\)-th \(f^k_{TRM}\)ì—ë§Œ ì‚¬ìš©
        <ul>
          <li>ê·¸ëŸ¬ë©´ \(1\sim j\)â€‹ë²ˆì§¸ expertsê°€cross-domain sequencesì—ì„œ the distinct sequential patterns presentë¥¼ í•™ìŠµ</li>
        </ul>
      </li>
      <li>\(g^{\text {cross }}(\mathbf{E})=\operatorname{softmax}\left(W_a^{c r o s s} \mathbf{E}\right)\)â€‹ : gating network for the crossdomain sequence (Fig. 2(c-8))</li>
    </ul>
  </li>
  <li>
    <p>\(\left(y_t^d\right)^{\text {single }} \text { and }\left(y_t\right)^{\text {cross }}\)ëŠ” two representations of different views for the same item</p>
  </li>
</ul>

<h3 id="422-transformer-experts">4.2.2. Transformer Experts</h3>

<ul>
  <li>ê°ê°ì˜ Multi-head Self-Attentionì— \(Z \in \mathbb{R}^{T \times r}\) ê°€ linear transformation
    <ul>
      <li>\(\to\) \(\text { queries } Q_i \in \mathbb{R}^{T \times r / p} \text {, keys } K_i \in \mathbb{R}^{T \times r / p} \text {, } \text { values } V_i \in \mathbb{R}^{T \times r / p}\)ê°€ ë¨</li>
    </ul>
  </li>
  <li>
    <p>\(\begin{aligned}
&amp; \operatorname{Attn}\left(Q_i, K_i, V_i\right)=\operatorname{softmax}\left(\frac{Q_i K_i^{\top}}{\sqrt{r / p}}\right) V_i, Q_i=Z \mathrm{~W}_i^Q, K_i=Z \mathrm{~W}_i^K, V_i=Z \mathrm{~W}_i^V
\end{aligned}\) ê±°ì³ final outputì€ \(\mathbf{H} \in \mathbb{R}^{T \times r}\)</p>
  </li>
  <li>ë§ˆì§€ë§‰ìœ¼ë¡œ \(\operatorname{FFN}(\mathbf{H})=\left[\mathrm{FC}\left(\mathbf{H}_1\right)\left\\mid \mathrm{FC}\left(\mathbf{H}_2\right)\right\\mid , \ldots, \\mid  \mathrm{FC}\left(\mathbf{H}_T\right)\right]\)
    <ul>
      <li>where \(\mathrm{FC}\left(\mathbf{H}_t\right)=\operatorname{GELU}\left(\mathbf{H}_t \mathrm{~W}_1+b_1\right) \mathrm{W}_2+b_2\)</li>
      <li>\(\mathbf{H}_t\) : ğ‘¡-th representation of \(\mathbf{H}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-loss-correction-with-negative-transfer-gap-lc-ntg">4.3. Loss Correction with Negative Transfer Gap (LC-NTG)</h3>

<h3 id="431--single-domain-item-prediction">4.3.1.  Single-Domain Item Prediction</h3>

<ul>
  <li>Fig 2(e-1)</li>
  <li>single domoin sequence \(X_{1: t}^d\)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ ì•„ì´í…œ \(x_{t+1}^d\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ pairwise ranking lossë¥¼ ì‚¬ìš©
    <ul>
      <li>ì¦‰ \(l_t^d=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}^d\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}^d\right)\right), \mathcal{L}_{\text {single }}^d=\sum_{t=1}^T l_t^d\)
        <ul>
          <li>where \(x^{d+}\) : ground-truth item paired with a negative item \(x^{d-}\) sampled froem Unif</li>
          <li>\(P\left(x_{t+1}^d=x^d \mid X_{1: t}^d\right)\) = \(\sigma\left(\left(y_t^d\right)^{\text {single }} \cdot M\left(x^d\right)\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="432-cross-domain-item-prediction">4.3.2. Cross-Domain Item Prediction</h3>

<ul>
  <li>CDSR \(l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t\)
    <ul>
      <li>where \(P\left(x_{t+1}^d=x^d \mid X_{1: t}\right) \text { is obtained by } \sigma\left(\left(y_t\right)^{\text {cross }} \cdot M\left(x^d\right)\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="433--calculating-the-negative-transfer-gap">4.3.3.  Calculating the Negative Transfer Gap</h3>

<ul>
  <li>ì´ì œ NTGë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. \(\phi_\pi(d)=\sum_{t=1}^T\left(l_t^d-l_t\right)\)
    <ul>
      <li>where \(l_t^d\) and \(l_t\) are losses of the SDSR and CDSR tasks in time step \(t\) for the domain \(d\), respectively, calculated with our model \(\pi\)</li>
    </ul>
  </li>
  <li>\(\lambda=\left(\lambda_1, \lambda_2, \ldots, \lambda_{\mid \mathcal{D}\mid }\right)\)ë¥¼ ê° domainì—ì„œì˜ NTGë¼ê³  í•˜ë©´ \(\lambda_d \leftarrow \operatorname{softmax}\left(\alpha * \lambda_d+\beta * \phi_\pi(d) ; \delta\right)\)ë¡œ ê³„ì‚°
    <ul>
      <li>where \(\alpha \text { and } \beta\) are learnable parameters</li>
    </ul>
  </li>
</ul>

<h3 id="434-loss-correction">4.3.4. Loss Correction</h3>

<ul>
  <li>NTGëŠ” weight for the cross-domain item prediction lossë¡œ í™œìš©ë¨
    <ul>
      <li>lossëŠ” \(l_t=\log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right), \mathcal{L}_{\text {cross }}=\sum_{t=1}^T l_t\)</li>
    </ul>
  </li>
  <li>re-aggregate : multiplying the relative NTG for each domain separately
    <ul>
      <li>
\[\mathcal{L}_{\text {cross }}^{l c} = =\sum_{t=1}^T \sum_{d=1}^{\mid \mathcal{D}\mid } \lambda_d \log \sigma\left(P\left(x_{t+1}^d=x^{d+} \mid X_{1: t}\right)-P\left(x_{t+1}^d=x^{d-} \mid X_{1: t}\right)\right)\]
      </li>
    </ul>
  </li>
  <li>ì´ë ‡ê²Œ í•˜ë©´ NTGê°€ ë°œìƒí•˜ëŠ” domainì—ì„œì˜ gradient flowë¥¼ ì¤„ì´ëŠ” ê²ƒ</li>
</ul>

<h2 id="44-single-cross-mutual-information-maximization-sc-mim">4.4. Single-Cross Mutual Information Maximization (SC-MIM)</h2>

<ul>
  <li>SC-MIM: SDSR and CDSR tasks ì‚¬ì´ì˜ ì •ë³´ë¥¼ ì˜ transferí•˜ê¸° ìœ„í•œ ë°©ë²•
    <ul>
      <li>mutual informationìœ¼ë¡œ ë‘ tasksì˜ correlation signalsë¥¼ íŒŒì•…</li>
      <li>mutual information: \(I(X, Y)=D_{K L}(p(X, Y) \\mid  p(X) p(Y))=\mathbb{E}_{p(X, Y)}\left[\log \frac{p(X, Y)}{p(X) p(Y)}\right]\)â€‹</li>
    </ul>
  </li>
  <li>í•˜ì§€ë§Œ ì´ mutual informationì„ high-dimdì—ì„œ êµ¬í•˜ëŠ” ê±´ ì–´ë µê¸° ë•Œë¬¸ì— lower boundë¡œ InfoNCEë¥¼ ì‚¬ìš©
    <ul>
      <li>lower bound : \(I(X, Y) \geq \mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\mathbb{E}_{q(\hat{Y})}\left(\log \sum_{\hat{y} \in \hat{Y}} \exp \rho_\theta(x, \hat{y})\right)\right]+\log \mid \hat{Y}\mid\)
        <ul>
          <li>where \(x, y\)ëŠ” ê°™ì€ inputì˜ ì„œë¡œ ë‹¤ë¥¸ view points</li>
          <li>\(\rho_\theta\) ëŠ” similarity function,</li>
        </ul>
      </li>
      <li>InfoNCEë¥¼ maximizingí•˜ëŠ” ê²ƒì€ standard cross-entropy lossë¥¼ maximizingí•˜ëŠ” ê²ƒê³¼ ê°™ìŒ
        <ul>
          <li>: \($\mathbb{E}_{p(X, Y)}\left[\rho_\theta(x, y)-\log \sum_{\hat{y} \in Y} \exp \rho_\theta(x, \hat{y})\right]\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ì•„ë¬´íŠ¼ ëŒì•„ì™€ì„œ ìš°ë¦¬ëŠ” \($\mathbf{Y}^{\text {single }}$ and $\mathbf{Y}^{\text {cross }}\)ì˜ mutual informationì„ maximizingí•˜ê³  ì‹¶ìŒ
    <ul>
      <li>ê·¸ëŸ¬ë¯€ë¡œ cross-domain representation \(\mathbf{Y}^{\text {ross }}\)ë¥¼ domainë³„ë¡œ splití•´ì„œ \((\mathbf{Y^d})^{\text {ross }}\) êµ¬í•˜ê³ </li>
      <li>ì•„ë˜ ì‹ì²˜ëŸ¼ ê³„ì‚°
        <ul>
          <li>: \(\begin{aligned} &amp; \mathcal{L}_{S C-M I M}^d=\rho\left(\left(\mathbf{Y}^d\right)^{\text {single }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)-\log \sum_{u-} \exp \left(\rho\left(\left(\mathbf{Y}^d\right)^{\text {single- }},\left(\mathbf{Y}^d\right)^{\text {cross }}\right)\right)\end{aligned}\)</li>
          <li>where \(u-\)ëŠ” other users in a training batch,</li>
          <li>\(\left(\mathbf{Y}^d\right)^{\text {single- }}\)ëŠ” subsequence of domain \(ğ‘‘\) of user \(ğ‘¢âˆ’\)â€‹</li>
          <li>\(\rho(\cdot, \cdot)\)ëŠ” \(\rho(U, V)=\sigma\left(U^{\top} \cdot W^H \cdot V\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="45-model-training-and-evaluation">4.5. Model Training and Evaluation</h3>

<ul>
  <li>Total training loss : \(\mathcal{L}=\eta\left(\sum_{d=1}^{\mid \mathcal{D}\mid }\left(\mathcal{L}_{\text {single }}^d\right)+\mathcal{L}_{\text {cross }}^{l c}\right)+(1-\eta) \sum_{d=1}^{\mid \mathcal{D}\mid } \mathcal{L}_{S C-M I M}^d\)
    <ul>
      <li>where \(\eta\) is the harmonic factor</li>
      <li>evaluationí•  ë•Œì—ëŠ” cross-domain representationë§Œ ì‚¬ìš©</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-dataset">5.1. Dataset</h3>

<h3 id="52-experimental-setting">5.2. Experimental Setting</h3>

<ul>
  <li>ë¨¼ì € Amazon datasetê³¼ Telco datasetì— ëŒ€í•œ ì„±ëŠ¥</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table23.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>
    <p>Research Questions:</p>

    <ul>
      <li>
        <p>(RQ1): Does the performance of our model surpass the current stateof-the-art baselines in practical applications that involve more than three domains?</p>
      </li>
      <li>
        <p>(RQ2): Can our model effectively address the challenge of negative transfer across all domains in the CDSR task?</p>
      </li>
      <li>
        <p>(RQ3): What is the impact of various components of our model on its performance in CDSR tasks?</p>
      </li>
      <li>
        <p>(RQ4): How do variations in hyper-parameter settings influence the performance of our model?</p>
      </li>
      <li>
        <p>(RQ5): How does the model perform when deployed online ?</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="53-performance-evaluation-rq1">5.3. Performance Evaluation (RQ1)</h3>

<ul>
  <li>First, The effectiveness of our model can be observed.
    <ul>
      <li>ë‹¤ë¥¸ baseline modelsë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨</li>
    </ul>
  </li>
  <li>Second, Integrating information from all domains simultaneously in a model can improve performance in each domain compared to modeling a pairwise domain-domain relationship.
    <ul>
      <li>ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” CDSR taskì—ì„œ domainë¼ë¦¬ì˜ ì •ë³´ë¥¼ ê²°í•©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="54-discussion-of-the-negative-transfer-rq2">5.4. Discussion of the negative transfer (RQ2)</h3>

<ul>
  <li>ê¸°ì¡´ baseline modelsëŠ” SDSRë³´ë‹¤ CDSRì˜ ì„±ëŠ¥ì´ ë” ì•ˆì¢‹ì•˜ì§€ë§Œ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ëª¨ë¸ì€ ê·¸ë ‡ì§€ ì•Šë‹¤</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table4.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="55-discussion-of-model-variants-rq3">5.5 Discussion of Model Variants (RQ3)</h3>

<ul>
  <li>LC-NTG, SC-MIM, ACMoE ì„¸ ê°€ì§€ components ëª¨ë‘ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ í•„ìš”í•˜ë‹¤</li>
</ul>

<p><img src="/assets/img/timeseries/SyNCRec/table5.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="56-hyperparameter-analysis-rq4">5.6. Hyperparameter Analysis (RQ4)</h3>

<p><img src="/assets/img/timeseries/SyNCRec/fig3.png" alt="ê·¸ë¦¼1" /></p>

<h2 id="6-online-ab-test-rq5">6. Online A/B Test (RQ5)</h2>

<p>pass</p>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>Negative transferë¥¼ ë‹¤ë£¨ëŠ” CDSR frameworkë¥¼ ì œì•ˆ
    <ul>
      <li>Negative transferë¥¼ ì¸¡ì •í•˜ê³  prediction lossì˜ weightë¡œ í™œìš©</li>
    </ul>
  </li>
  <li>SDSR and CDSR tasksì˜ ì •ë³´ë¥¼ êµí™˜ì‹œí‚¤ëŠ” Auxiliary loss ì œì•ˆ</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[SIGIR'24 Best Paper](https://arxiv.org/pdf/2407.11245)]]></summary></entry><entry><title type="html">MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-05-MG-TSD/" rel="alternate" type="text/html" title="MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (ICLR 2024)" /><published>2024-08-05T00:00:00+09:00</published><updated>2024-09-03T15:04:04+09:00</updated><id>http://localhost:4000/timeseries/MG-TSD</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-05-MG-TSD/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>ì–´ë–»ê²Œ Diffusion modelì˜ ì„±ëŠ¥ì„ time series forecastingì— í™œìš©í•  ìˆ˜ ìˆëŠ”ê°€</li>
  <li><strong>M</strong>ulti-<strong>G</strong>ranularity <strong>T</strong>ime <strong>S</strong>eries <strong>D</strong>iffusion <strong>(MG- TSD)</strong>
    <ul>
      <li>leveraging the inherent granularity levels</li>
      <li>intuition: diffusion stepì—ì„œ ì ì°¨ gaussian noiseë¡œ ë§Œë“œëŠ” ê²ƒì„ fine \(\to\) coarseë¡œ ì´í•´</li>
      <li>novel multi-granularity guidance diffusion loss function</li>
      <li>method to effectively utilize coarse-grained data across various granularity levels</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>ìµœê·¼ì—ëŠ” Time series predictive ëª©ì ìœ¼ë¡œ conditional generative modelì„ í™œìš©
    <ul>
      <li>ì²˜ìŒì—ëŠ” Auto-regressive ë°©ì‹ìœ¼ë¡œ í•˜ë‹¤ê°€ CSDIë„ í–ˆì—ˆìŒ</li>
    </ul>
  </li>
  <li>í•˜ì§€ë§Œ ë¬¸ì œëŠ” Diffusionì´ instabilityí•˜ë‹¤ëŠ” ì 
    <ul>
      <li>Imageì—ì„œ diffusionì€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì–´ì„œ ì¥ì ì´ì—ˆëŠ”ë°</li>
      <li>ì‹œê³„ì—´ ì˜ˆì¸¡ ê´€ì ì—ì„œëŠ” ê·¸ê²ƒì´ ì„±ëŠ¥ í•˜ë½ì˜ ì›ì¸ì´ ë  ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/MG-TSD/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>Diffusion stepì—ì„œ ì ì°¨ gaussian noiseë¡œ ë§Œë“œëŠ” ê²ƒì„ fine \(\to\) coarseë¡œ ì´í•´í•œë‹¤ë©´
    <ul>
      <li>Diffusion modelì´ <strong>labelsì„ the source of guidance</strong>ë¡œ í•„ìš”ë¡œ í•˜ëŠ” ë¬¸ì œì—ì„œ</li>
      <li>Time seriesì˜ fine featureê°€ ê·¸ labels as the source of guidance ì—­í• ì„ í•  ìˆ˜ ìˆì„ ê²ƒ</li>
    </ul>
  </li>
  <li>MG-TSDì—ì„œëŠ” coarse-grained dataë¥¼ denoising process í•™ìŠµì˜ guideë¡œ ì¤€ë‹¤.
    <ul>
      <li>\(\to\) intermediate latent statesì—ì„œì˜ constraintsë¡œ ì‘ìš©</li>
      <li>\(\to\) coarser featureëŠ” ë” ë¹ ë¥´ê²Œ ìƒì„±í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ê·¸ë§Œí¼ finer feature recoveryë„ ìš©ì´</li>
      <li>\(\to\) coarse-grained dataì˜ trendì™€ patternì„ ë³´ì¡´í•˜ëŠ” samplingì„ ë§Œë“¬</li>
      <li>\(\to\)â€‹ reduces variability and results in high-quality predictions</li>
    </ul>
  </li>
</ul>

<h2 id="2-background">2. Background</h2>

<ul>
  <li>TimeGrad Model
    <ul>
      <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad Paper</a> <a href="https://lpppj.github.io/timeseries/2024-07-09-Timegrad">TimeGrad Review</a></li>
    </ul>
  </li>
  <li>\(\boldsymbol{X}^{(1)}=\left[\boldsymbol{x}_1^1, \ldots, \boldsymbol{x}_t^1, \ldots, \boldsymbol{x}_T^1\right]\) is the original observed data, where \(t \in[1, T]\) and \(\boldsymbol{x}_t \in \mathbb{R}^D\)
    <ul>
      <li>Mathematical expressions: \(q_{\mathcal{X}}\left(\boldsymbol{x}_{t_0: T}^1 \mid\left\{\boldsymbol{x}_{1: t_0-1}^1\right\}\right)=\prod_{t=t_0}^T q_{\mathcal{X}}\left(\boldsymbol{x}_t^1 \mid\left\{\boldsymbol{x}_{1: t-1}^1\right\}\right)\)</li>
    </ul>
  </li>
</ul>

<h2 id="3-method">3. Method</h2>

<h3 id="31-mg-tsd-model-architecture">3.1. MG-TSD Model Architecture</h3>

<p><img src="/assets/img/timeseries/MG-TSD/fig2.png" alt="ê·¸ë¦¼2" /></p>

<h3 id="multi-granularity-data-generator">Multi-granularity Data Generator</h3>

<p>: for generating multi-granularity data from observations</p>

<ul>
  <li>historical sliding windows with different sizesë¥¼ í†µí•´ fine \(\to\) coaseë¡œ smoothing out</li>
  <li>ì¦‰ \(\boldsymbol{X}^{(g)}=f\left(\boldsymbol{X}^{(1)}, s^g\right)\) with pre-defined sliding window size \(s^g\)</li>
  <li>ì´ ë•Œ non-overlappingí•˜ê²Œ windowë¥¼ slicingí•˜ê³ , \(\boldsymbol{X}^{(g)}\)ëŠ” \(s^g\)ë²ˆ ë³µì œí•´ì„œ \([1, T]\)ë¡œ ë§ì¶¤</li>
</ul>

<h3 id="temporal-process-module">Temporal Process Module</h3>

<p>: designed to capture the temporal dynamics of the multi-granularity time series data</p>

<ul>
  <li>ê°ê°ì˜ granularity level \(g\)ì—ì„œ GRUì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ timestep \(t\)ë¥¼ \(\mathbf{h}_t^g\)ë¡œ encoding</li>
</ul>

<h3 id="guided-diffusion-process-module">Guided Diffusion Process Module</h3>

<p>: designed to generate stable time series predictions at each timestep \(t\)</p>

<ul>
  <li>multi-granularity dataë¥¼ í™œìš©í•˜ì—¬ diffusion learning processì˜ guideë¡œ ì œê³µ</li>
</ul>

<h3 id="32-multi-granularity-guided-diffusion">3.2. Multi-Granularity Guided Diffusion</h3>

<p>: Guided Diffusion Process Moduleì— ëŒ€í•œ details</p>

<h3 id="321-coarse-grained-guidance">3.2.1. Coarse-grained Guidance</h3>

<p>: the derivation of a heuristic guidance loss for the two- granularity case</p>

<ul>
  <li>consider two granularities at a fixed timestep \(t\)
    <ul>
      <li>: \(\text { finest-grained data } \boldsymbol{x}_t^{g_1}\left(g_1=1\right) \text { from } \boldsymbol{X}^{\left(g_1\right)}\) &amp; \(\text { coarse-grained data } \boldsymbol{x}_t^g \text { from } \boldsymbol{X}^{(g)}\)â€‹</li>
    </ul>
  </li>
  <li>ë¨¼ì € coarse-grained targets \(x^g\)ë¥¼ intermediate diffusion step \(N_*^g \in[1, N-1]\)ì— introduce
    <ul>
      <li>ì¦‰ objective functionì´ \(\log p_\theta\left(\boldsymbol{x}^g\right)\)</li>
    </ul>
  </li>
  <li>ê·¸ëŸ¬ë©´ denoising processì—ì„œ recoverëœ coarser featuresëŠ” ì‹¤ì œ coarse-grained sampleì˜ ì •ë³´ë¥¼ ë§ì´ ê°€ì§€ê³  ìˆì„í…Œë‹ˆ
    <ul>
      <li>fine-grained featureë¥¼ recoverí•˜ê¸°ë„ ì‰¬ì›Œì§ˆ ê²ƒ</li>
    </ul>
  </li>
  <li>\(\theta\)-parameterized: \(p_\theta\left(\boldsymbol{x}_{N_*^g}\right)=\int p_\theta\left(\boldsymbol{x}_{N_*^g: N}\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}=\int p\left(\boldsymbol{x}_N\right) \prod_{N_*^g+1}^N p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}\)â€‹
    <ul>
      <li>where \(\boldsymbol{x}_N \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I}), p_\theta\left(\boldsymbol{x}_{n-1} \mid \boldsymbol{x}_n\right)=\mathcal{N}\left(\boldsymbol{x}_{n-1} ; \boldsymbol{\mu}_\theta\left(\boldsymbol{x}_n, n\right), \boldsymbol{\Sigma}_\theta\left(\boldsymbol{x}_n, n\right)\right)\)â€‹</li>
    </ul>
  </li>
  <li>ì´ê±´ \(N_*^g\)ë²ˆì§¸ diffusion stepì—ì„œ \(N\)ë²ˆì§¸ê¹Œì§€ ì´ \(N-N_*^g\) stepsì˜ forward processì´ë¯€ë¡œ
    <ul>
      <li>the guidance objective: \(\log p_\theta\left(\boldsymbol{x}^g\right)=\log \int p_\theta\left(\boldsymbol{x}_{N_*^g}^g, \boldsymbol{x}_{N_*^g+1}^g, \ldots, \boldsymbol{x}_N^g\right) \mathrm{d} \boldsymbol{x}_{\left(N_*^g+1\right): N}^g\)â€‹</li>
    </ul>
  </li>
  <li>sampleì— ëŒ€í•œ loss ëŒ€ì‹  noiseì— ëŒ€í•œ loss ì‚¬ìš©
    <ul>
      <li>loss: \(\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}^g, n}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_n^g, n\right)\right\|^2\right]\)â€‹</li>
      <li>where \(\boldsymbol{x}_n^g=\left(\prod_{i=N_{\boldsymbol{z}}^g}^n \alpha_i^1\right) \boldsymbol{x}^g+\sqrt{ } \mathbf{1}-\prod_{i=N^g}^n \alpha_i^1 \boldsymbol{\epsilon} \text { and } \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})\)</li>
    </ul>
  </li>
</ul>

<h3 id="322-multi-granularity-guidance">3.2.2. Multi-granularity Guidance</h3>

<ul>
  <li>
    <p>Multi-granularity Data Generatorê°€ Gê°œì˜ granularity levelsë§ˆë‹¤ data ìƒì„±: \(\boldsymbol{X}^{(1)}, \boldsymbol{X}^{(2)}, \ldots, \boldsymbol{X}^{(G)}\)</p>
  </li>
  <li>Share ratio: \(r_g:=1-\left(N_*^g-1\right) / N\)
    <ul>
      <li>: the shared percentage of variance schedule between the gth granularity data and the finest-grained data</li>
      <li>ex. finest-grained dataì—ì„œëŠ” \(N_*^1=1 \text { and } r^1=1\)â€‹
        <ul>
          <li>variance schedule for granularity \(g\) is, \(\alpha_n^g\left(N_*^g\right)= \begin{cases}1 &amp; \text { if } n=1, \ldots, N_*^g \\ \alpha_n^1 &amp; \text { if } n=N_*^g+1, \ldots, N\end{cases}\)â€‹</li>
          <li>and \(\left\{\beta_n^g\right\}_{n=1}^N=\left\{1-\alpha_n^g\right\}_{n=1}^N\)â€‹</li>
          <li>accordingly, \(a_n^g\left(N_*^g\right)=\prod_{k=1}^n \alpha_k^g \text {, and } b_n^g\left(N_*^g\right)=1-a_n^g\left(N_*^g\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ì´ ë•Œ \(N^g_*\)ëŠ” : represents the diffusion index for starting sharing the variance schedule across granularity level \(g \in\{1, \ldots, G\}\)</li>
  <li>ì´ë ‡ê²Œ ë˜ë©´ larger coarser granularity levelì¼ìˆ˜ë¡ \(N^g_*\)ê°€ ì»¤ì§„ë‹¤ëŠ” ëœ»
    <ul>
      <li>ì¦‰ coarserí• ìˆ˜ë¡ fineí•œ ì •ë³´ëŠ” ì¤„ì–´ë“¤í…Œë‹ˆ ì´ì „ diffusion stepê³¼ ì°¨ì´ê°€ í¬ì§€ ì•Šì„ ê²ƒ</li>
      <li>ê·¸ëŸ¬ë‹ˆê¹Œ \(N^g_*\)ë¥¼ í¬ê²Œ í•´ì„œ fine-grained featureë¥¼ ìƒì„±í•  stepsë¥¼ ë§ì´ ì¤Œ</li>
    </ul>
  </li>
  <li>Then the guidance loss function \(L^{(g)}(\theta)\) for \(g\)-th granularity \(x^g_{n,t}\) at timestep \(t\) and diffusion step \(n\),
    <ul>
      <li>can be expressed as: \(L^{(g)}(\theta)=\mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, n} \|\left(\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon}, n, \mathbf{h}_{t-1}^g\right) \|_2^2\right.\)</li>
      <li>where \(\mathbf{h}_t^g=\mathrm{RNN}_\theta\left(\boldsymbol{x}_t^g, \mathbf{h}_{t-1}^g\right)\)</li>
    </ul>
  </li>
</ul>

<h3 id="training">Training</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm1.png" alt="ê·¸ë¦¼41" /></p>

<ul>
  <li>ìµœì¢…ì ì¸ training objectivesëŠ” ëª¨ë“  granularitiesì—ì„œì˜ Lossì˜ weighted sum
    <ul>
      <li>: \(L^{\text {final }}=\omega^1 L^{(1)}(\theta)+L^{\text {guidance }}(\theta)=\sum_{q=1}^G \omega^g \mathbb{E}_{\boldsymbol{\epsilon}, \boldsymbol{x}_{0, t}^g, t}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_{n, t}^g, n, \mathbf{h}_{t-1}^g\right)\right\|^2\right]\)</li>
      <li>where \(\boldsymbol{x}_{n, t}^g=\sqrt{a_n^g} \boldsymbol{x}_{0, t}^g+\sqrt{b_n^g} \boldsymbol{\epsilon} \text { and } \sum_{g=1}^G \omega^g=1\)</li>
      <li>ì´ ë•Œ denoising networkì˜ parametersëŠ” shared across all granularities</li>
    </ul>
  </li>
</ul>

<h3 id="inference">Inference</h3>

<p><img src="/assets/img/timeseries/MG-TSD/algorithm2.png" alt="ê·¸ë¦¼42" /></p>

<ul>
  <li>ìš°ë¦¬ì˜ ëª©í‘œëŠ” íŠ¹ì •í•œ prediction stepsì— ëŒ€í•œ  finest-grained dataì— ëŒ€í•œ ì˜ˆì¸¡
    <ul>
      <li>\(t_0-1\) ì‹œì ê¹Œì§€ ì£¼ì–´ì¡Œë‹¤ë©´ ì•„ë˜ algorithm 2ë¥¼ ë”°ë¼ \(t_0\)ì‹œì ì— ëŒ€í•œ ë°ì´í„° ìƒì„±,</li>
      <li>ìš°ë¦¬ê°€ ì›í•˜ëŠ” forecast horizonì´ ë  ë•Œê¹Œì§€ ë°˜ë³µ</li>
      <li>hidden statesì— conditional inputsìœ¼ë¡œ ë¬´ì—‡ì„ ë„£ëŠ”ì§€ì— ë”°ë¼ì„œ ê·¸ì— í•´ë‹¹í•˜ëŠ” granularity levelsë¡œ ìƒ˜í”Œë§</li>
    </ul>
  </li>
</ul>

<h3 id="selection-of-share-ratio">Selection of share ratio</h3>

<ul>
  <li>ìœ„ì—ì„œëŠ” share ratio \(r_g:=1-\left(N_*^g-1\right) / N\)ë¥¼ heuristicí•˜ê²Œ \(N^g_*\)ì— ë”°ë¼ ê²°ì •ë˜ë„ë¡ í–ˆìŒ
    <ul>
      <li>Diffusion step \(N^g_*\)ëŠ” \(q\left(\boldsymbol{x}^g\right) \text { and } p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\)ì˜ ê±°ë¦¬ê°€ ê°€ì¥ ì‘ì„ ë•Œë¡œ ì„¤ì • !</li>
      <li>: \(\to\) \(N_*^g:=\arg \min_n \mathcal{D}\left(q\left(\boldsymbol{x}^g\right), p_\theta\left(\boldsymbol{x}_n^{g_1}\right)\right)\)â€‹
        <ul>
          <li>\(\mathcal{D}\)ëŠ” ë‘ ë¶„í¬ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” metricì´ ë¨ (KL-divergence, â€¦)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/MG-TSD/table12.png" alt="ê·¸ë¦¼112" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig3.png" alt="ê·¸ë¦¼3" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/table3.png" alt="ê·¸ë¦¼13" /></p>

<p><img src="/assets/img/timeseries/MG-TSD/fig4.png" alt="ê·¸ë¦¼4" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Multi-Granularity Time Series Diffusion (MG-TSD)
    <ul>
      <li>leverages the inherent granularity levels within the data, as given targets at intermediate diffusion steps to guide the learning process of diffusion models</li>
      <li>to effectively utilize coarse-grained data across various granularity levels.</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=CZiY6OLktd)]]></summary></entry><entry><title type="html">Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/" rel="alternate" type="text/html" title="Diffusion-TS: Interpretable Diffusion for General Time Series Generation (ICLR 2024)" /><published>2024-08-04T00:00:00+09:00</published><updated>2024-08-04T18:52:22+09:00</updated><id>http://localhost:4000/timeseries/Diffusion-TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-08-04-Diffusion-TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Diffusion-TS: uses an encoder-decoder transformer with disentangled temporal representations</li>
  <li>train the model to directly reconstruct the <strong>sample</strong> instead of the <strong>noise</strong> in each diffusion step</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Synthesizing realistic time series dataëŠ” ë°ì´í„° ê³µìœ ê°€ ê°œì¸ì •ë³´ ì¹¨í•´ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì‚¬ë¡€ì—ì„œì˜ ì†”ë£¨ì…˜</li>
  <li>ì§€ê¸ˆê¹Œì§€ Diffusionì„ í™œìš©í•œ time series generationì€ ëŒ€ë¶€ë¶„ task-agnostic generation
    <ul>
      <li>ì²«ë²ˆì§¸ ë¬¸ì œëŠ” RNN-based Autoregressive ë°©ì‹: limited long-range performance due to error accumulation and slow inference speed</li>
      <li>ë‘ë²ˆì§¸ ë¬¸ì œëŠ” diffusion processì—ì„œ noiseë¥¼ ì¶”ê°€í•  ë•Œ ì‹œê³„ì—´ì˜ combinations of independent components(trend, seasonal, â€¦)ì´ ë§ê°€ì§€ëŠ” ë¬¸ì œ (íŠ¹íˆ ì£¼ê¸°ì„±ì´ ëšœë ·í•œ ê²½ìš° interpretabilityê°€ ë¶€ì¡± <a href="https://openreview.net/pdf?id=rdjeCNUS6TG">Liu et al., (2022)</a>)</li>
    </ul>
  </li>
  <li><strong>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Transformerë¥¼ í™œìš©í•˜ì—¬ trendì™€ seasonalì„ non-autoregressiveí•˜ê²Œ ìƒì„±</strong>
    <ul>
      <li>by imposing different forms of constraints on different representations.</li>
    </ul>
  </li>
  <li>For Reconstruct the <strong>samples</strong> rather than the <strong>noises</strong> in each diffusion step, Fourier-based loss ì‚¬ìš©</li>
</ul>

<h2 id="2-problem-statement">2. Problem Statement</h2>

<ul>
  <li>Nê°œë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì…‹ \(D A=\left\{X_{1: \tau}^i\right\}_{i=1}^N\)â€‹
    <ul>
      <li>where \(X_{1: \tau}=\left(x_1, \ldots, x_\tau\right) \in \mathbb{R}^{\tau \times d}\)</li>
    </ul>
  </li>
  <li>ëª©í‘œëŠ” Gaussian vectors \(Z_i=\left(z_1^i, \ldots, z_t^i\right) \in \mathbb{R}^{\tau \times d \times T}\)ë¥¼ DAì™€ ë¹„ìŠ·í•œ \(\hat{X}_{1: \tau}^i=G\left(Z_i\right)\)ë¡œ ë°”ê¾¸ëŠ” Generator \(G\)ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒ</li>
  <li>Time series modelì€ trendì™€ ì—¬ëŸ¬ ê°œì˜ seasonalityë¡œ êµ¬ì„± : \(x_j=\zeta_j+\sum_{i=1}^m s_{i, j}+e_j\)
    <ul>
      <li>where \(j=0,1, \ldots, \tau-1\)</li>
      <li>\(x_j\) : observed time series</li>
      <li>\(\zeta_j\): trend component</li>
      <li>\(s_{i,j}\): \(i\)-th seasonal component</li>
      <li>\(e_j\): remainder part (contatins the noise and some outliers at time t)</li>
    </ul>
  </li>
</ul>

<h2 id="3-diffusion-ts-interpretable-diffusion-for-time-series">3. Diffusion-TS: Interpretable Diffusion for Time Series</h2>

<ul>
  <li>ì´ëŸ¬í•œ interpretable decomposition architectureì˜ ê·¼ê±°ëŠ” 3ê°€ì§€
    <ul>
      <li>ì²«ì§¸, disentangled patterns in the diffusion modelì€ ì•„ì§ ì—°êµ¬ë˜ì§€ ì•ŠìŒ</li>
      <li>ë‘˜ì§¸, specific designs of architecture and objective ë•ë¶„ì— interpretable</li>
      <li>ì…‹ì§¸, explainable disentangled representations ë•ë¶„ì— complex dynamics íŒŒì•…</li>
    </ul>
  </li>
</ul>

<h3 id="31-diffusion-framework">3.1. Diffusion Framework</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig1.png" alt="ê·¸ë¦¼1" /></p>

<ul>
  <li>Forward process
    <ul>
      <li>\(x_0 \sim q(x)\)ì—ì„œ ì ì  noisy into Gaussian noise \(x_T \sim \mathcal{N}(0, \mathbf{I})\)</li>
      <li>Parameterization: \(q\left(x_t \mid x_{t-1}\right)=\mathcal{N}\left(x_t ; \sqrt{ } 1-\beta_t x_{t-1}, \beta_t \mathbf{I}\right) \text { with } \beta_t \in(0,1)\)</li>
    </ul>
  </li>
  <li>
    <p>Reverse process</p>

    <ul>
      <li>
        <p>ë°˜ëŒ€ë¡œ \(p_\theta\left(x_{t-1} \mid x_t\right)=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)\)</p>
      </li>
      <li>
        <p>MSE: \(\mathcal{L}\left(x_0\right)=\sum_{t=1}^T \underset{q\left(x_t \mid x_0\right)}{\mathbb{E}}\left\|\mu\left(x_t, x_0\right)-\mu_\theta\left(x_t, t\right)\right\|^2\)</p>
        <ul>
          <li>where \(\mu\left(x_t, x_0\right) \text { is the mean of the posterior } q\left(x_{t-1} \mid x_0, x_t\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="32-decomposition-model-architecture">3.2. Decomposition Model Architecture</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig2.png" alt="ê·¸ë¦¼2" /></p>

<ul>
  <li>Noisy sequenceê°€ encoder í†µê³¼í•´ì„œ decoderë¡œ ë“¤ì–´ì˜´ (ì´ˆë¡ìƒ‰)</li>
  <li>DecoderëŠ” multilayer structure, ê° layerì—ëŠ” <strong>Transformer Block</strong>, <strong>FFN</strong>, <strong>Trend and Fourier synthetic layer</strong>ê°€ í¬í•¨ë¨</li>
  <li>ê° layerëŠ” ì‹œê³„ì—´ì˜ ê° componentë¥¼ ìƒì„±í•˜ëŠ” ì—­í• 
    <ul>
      <li>componentì— í•´ë‹¹í•˜ëŠ” inductive biasë¥¼ ê° layerì— ë°˜ì˜í•´ì¤Œìœ¼ë¡œì¨ í•™ìŠµì´ ì‰¬ì›Œì§</li>
      <li>Trend representation captures the intrinsic trend which changes gradually and smoothly</li>
      <li>Seasonality representation illustrates the periodic patterns of the signal</li>
      <li>Error representation characterizes the remaining parts after removing trend and periodicity</li>
    </ul>
  </li>
  <li>\(w_{(\cdot)}^{i, t}\) where \(i \in 1, \ldots, D\)ëŠ” \(i\)ë²ˆì§¸ decoder blockì—ì„œì˜ diffusion step \(t\)ë¥¼ ì˜ë¯¸</li>
</ul>

<h3 id="trend-synthesis">Trend Synthesis</h3>

<ul>
  <li>smooth underlying mean of the data, which aims to model slow-varying behavior</li>
  <li>ê·¸ëŸ¬ë¯€ë¡œ Trend \(V_{t r}^t\)ë¥¼ ìœ„í•´ Polynomial regressor ì‚¬ìš©
    <ul>
      <li>\(V_{t r}^t=\sum_{i=1}^D\left(C \cdot \operatorname{Linear}\left(w_{t r}^{i, t}\right)+\mathcal{X}_{t r}^{i, t}\right)\) where \(C=\left[1, c, \ldots, c^p\right]\)</li>
      <li>\(\mathcal{X}_{t r}^{i, t}\)ëŠ” the mean value of the output of the \(i\)â€‹-th decoder block</li>
      <li>\(C\)ëŠ” slow-varying poly spaceì¸ë°, matrix of powers of vector \(c=[0,1,2, \ldots, \tau-2, \tau-1]^T / \tau\)</li>
      <li>\(p\)ëŠ” small degree (e.g. \(p\)â€‹=3) to model low frequency behavior</li>
    </ul>
  </li>
</ul>

<h3 id="seasonality--error-synthesis">Seasonality &amp; Error Synthesis</h3>

<ul>
  <li>ì´ì œ Trend, Seasonality, Error ëª¨ë‘ ìƒê°í•´ë³´ì.</li>
  <li><strong>ê²°êµ­ ë¬¸ì œëŠ” noisy input \(x_t\)ì—ì„œ seasonal patternsë¥¼ êµ¬ë¶„í•´ë‚´ëŠ” ê²ƒ !</strong></li>
  <li>í‘¸ë¦¬ì— ì‹œë¦¬ì¦ˆì˜ trigonometric representation of seasonal componentsë¥¼ ê¸°ë°˜ìœ¼ë¡œ Fourier basesë¥¼ í™œìš©í•œ Fourier synthetic layersì—ì„œ seasonal component íŒŒì•…</li>
</ul>

<p><img src="/assets/img/timeseries/Diffusion-TS/fomula456.png" alt="ê·¸ë¦¼456" /></p>

<ul>
  <li>\(A_{i, t}^{(k)}, \Phi_{i, t}^{(k)}\) are the phase, amplitude of the \(k\)-th frequency after the DFT \(\mathcal F\) repectively</li>
  <li>\(f_k\)ëŠ” Fourier frequency of the corresponding index \(k\)</li>
  <li>ê²°êµ­ the Fourier synthetic layerëŠ” ì§„í­(amplitude)ì´ í° frequencyë¥¼ ì°¾ê³ , ê·¸ frequencyë“¤ë§Œ IDFT.
    <ul>
      <li>ê·¸ê±¸ seasonalityë¡œ ë³¸ë‹¤. (Pathformerë‘ ê°™ì€ ë°©ì‹)</li>
    </ul>
  </li>
  <li>ìµœì¢…ì ìœ¼ë¡œ original signal: \(\hat{x}_0\left(x_t, t, \theta\right)=V_{t r}^t+\sum_{i=1}^D S_{i, t}+R\)â€‹
    <ul>
      <li>\(R\): output of the last decoder block, which can be regarded as the sum of residual periodicity and other noise.</li>
    </ul>
  </li>
</ul>

<h3 id="33-fourier-based-traning-objective">3.3 Fourier-based Traning Objective</h3>

<ul>
  <li>\(\hat{x}_0\left(x_t, t, \theta\right)\)ë¥¼ directly estimate
    <ul>
      <li>Reverse process: \(x_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \hat{x}_0\left(x_t, t, \theta\right)+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} x_t+\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t z_t\)</li>
      <li>where \(z_t \sim \mathcal{N}(0, \mathbf{I}), \alpha_t=1-\beta_t \text { and } \bar{\alpha}_t=\prod_{s=1}^t \alpha_s\)â€‹</li>
    </ul>
  </li>
  <li>Reweighting strategy: \(\mathcal{L}_{\text {simple }}=\mathbb{E}_{t, x_0}\left[w_t\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2\right], \quad w_t=\frac{\lambda \alpha_t\left(1-\bar{\alpha}_t\right)}{\beta_t^2}\)â€‹
    <ul>
      <li>where \(\lambda\) is constant (i.e. 0.01)</li>
      <li>ì¦‰ small tì—ì„œ down-weighted, ëª¨ë¸ì´ larger diffusion stepì— ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“¬</li>
    </ul>
  </li>
  <li>Fourier-based loss termì´ time serie reconstructionì—ì„œëŠ” ë” ì¢‹ë‹¤ <a href="https://arxiv.org/pdf/2208.05836">Fons et al. (2022)</a>
    <ul>
      <li>: \(\mathcal{L}_\theta=\mathbb{E}_{t, x_0}\left[w_t\left[\lambda_1\left\|x_0-\hat{x}_0\left(x_t, t, \theta\right)\right\|^2+\lambda_2\left\|\mathcal{F} \mathcal{F} \mathcal{T}\left(x_0\right)-\mathcal{F F} \mathcal{T}\left(\hat{x}_0\left(x_t, t, \theta\right)\right)\right\|^2\right]\right]\)</li>
    </ul>
  </li>
</ul>

<h3 id="34-conditional-generation-for-time-series-applications">3.4. Conditional Generation for Time Series Applications</h3>

<ul>
  <li><strong>Conditional extensions of the Diffusion-TS</strong>, in which the modeled \(x_0\) is conditioned on targets \(y\)â€‹</li>
  <li>ëª©í‘œëŠ” pre-trained diffusion modelê³¼ the gradients of a classifierë¥¼ í™œìš©í•˜ì—¬
    <ul>
      <li>Posterior \(p\left(x_{0: T} \mid y\right)=\prod_{t=1}^T p\left(x_{t-1} \mid x_t, y\right)\)ì—ì„œ samplingí•˜ëŠ” ê²ƒ</li>
    </ul>
  </li>
  <li>\(p\left(x_{t-1} \mid x_t, y\right) \propto p\left(x_{t-1} \mid x_t\right) p\left(y \mid x_{t-1}, x_t\right)\)ì´ë¯€ë¡œ bayse theoremì„ í†µí•´ gradient update
    <ul>
      <li>Score function \(\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t, y\right)=\nabla_{x_{t-1}} \log p\left(x_{t-1} \mid x_t\right)+\nabla_{x_{t-1}} \log p\left(y \mid x_{t-1}\right)\)</li>
      <li>\(\log p\left(x_{t-1} \mid x_t\right)\)ì€ diffusion modelì—ì„œ ì •ì˜ë¨.</li>
      <li>\(\log p\left(y \mid x_{t-1}\right)\)ëŠ” classifierì—ì„œ parametrizeë˜ë©°, \(\nabla_{x_{t-1}} \log p\left(y \mid x_{0 \mid t-1}\right)\)ë¡œ ê·¼ì‚¬ë¨</li>
    </ul>
  </li>
  <li>ì¦‰ classifierê°€ ë†’ì€ likelihoodë¥¼ ê°€ì§„ ì˜ì—­ì—ì„œ sampleì´ ìƒì„±ë˜ë„ë¡ í•˜ëŠ” ê²ƒ
    <ul>
      <li>: \(\tilde{x}_0\left(x_t, t, \theta\right)=\hat{x}_0\left(x_t, t, \theta\right)+\eta \nabla_{x_t}\left(\left\|x_a-\hat{x}_a\left(x_t, t, \theta\right)\right\|_2^2+\gamma \log p\left(x_{t-1} \mid x_t\right)\right)\)</li>
      <li>where Conditional part \(x_a\), generative part \(x_b\)</li>
      <li>gradient termì€ reconstruction-based guidance, \(\eta\)ë¡œ ê°•ë„ ì¡°ì ˆ</li>
    </ul>
  </li>
  <li>ê° diffusion stepì—ì„œ ì´ gradient updateë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ quality ë†’ì¸ë‹¤</li>
  <li>Replacing: \(\tilde{x}_a\left(x_t, t, \theta\right):=\sqrt{\bar{\alpha}_t} x_a+\sqrt{ } 1-\bar{\alpha}_t \epsilon\)ì„ í†µí•´, \(\tilde{x}_0\)ë¥¼ ì‚¬ìš©í•œ sample \(x_{t-1}\)ê°€ ìƒì„±ë¨</li>
</ul>

<h2 id="4-empirical-evaluaiton">4. Empirical Evaluaiton</h2>

<h3 id="42-metrics">4.2. Metrics</h3>

<ul>
  <li>Discriminative score (Yoon et al., 2019): measures the similarity using a classification model to distinguish between the original and synthetic data as a supervised task;</li>
  <li>Predictive score (Yoon et al., 2019):  measures the usefulness of the synthesized data by training a post-hoc sequence model to predict next-step temporal vectors using the train-synthesis-and-test-real (TSTR) method;</li>
  <li>Context-Frechet Inception Distance (Context-FID) score Â´ (Paul et al., 2022):  quantifies the quality of the synthetic time series samples by computing the difference between representations of time series that fit into the local context;</li>
  <li>Correlational score (Ni et al., 2020): uses the absolute error between cross correlation matrices by real data and synthetic data to assess the temporal dependency</li>
</ul>

<h3 id="43-interpretability-results">4.3. Interpretability Results</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig3.png" alt="ê·¸ë¦¼3" /></p>

<ul>
  <li>the corrupted samples (shown in (a)) with 50 steps of noise added as input</li>
  <li>outputs the signals (shown in (c)) that try to restore the ground truth (shown in (b))</li>
  <li>with the aid of the decomposition of temporal trend (shown in (d)) and season &amp; error (shown in (e)).</li>
  <li>Result: As would be expected, the trend curve follows the overall shape of the signal, while the season &amp; error oscillates around zero !</li>
</ul>

<h3 id="44-unconditional-time-series-generation">4.4. Unconditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table1.png" alt="ê·¸ë¦¼21" /></p>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig4.png" alt="ê·¸ë¦¼4" /></p>

<h3 id="45-conditional-time-series-generation">4.5. Conditional Time Series Generation</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/fig6.png" alt="ê·¸ë¦¼6" /></p>

<h3 id="46-ablaction-study">4.6. Ablaction Study</h3>

<p><img src="/assets/img/timeseries/Diffusion-TS/table2.png" alt="ê·¸ë¦¼22" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Diffusion-TS, a DDPM-based method for general time series generation
    <ul>
      <li>TS-specific loss design and transformer-based deep decomposition architecture</li>
    </ul>
  </li>
  <li>Unconditionalë¡œ í›ˆë ¨ëœ modelì´ ì‰½ê²Œ conditionalë¡œ í™•ì¥ë  ìˆ˜ ìˆìŒ
    <ul>
      <li>by combining gradients into the sampling !</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/pdf/2403.01742)]]></summary></entry><entry><title type="html">Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting (ICLR 2022)</title><link href="http://localhost:4000/timeseries/2024-07-26-pyraformer/" rel="alternate" type="text/html" title="Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting (ICLR 2022)" /><published>2024-07-26T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/pyraformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-26-pyraformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Pyraformer: a flexible but parsimonious model that can capture a wide range of temporal dependencies, by exploring the multi-resolution representation of the time series
    <ul>
      <li>Pyramidal attention module (PAM)</li>
      <li>the inter-scale tree structure summarizes features at different resolutions</li>
      <li>the intra-scale neighboring connections model the temporal dependencies of different ranges</li>
      <li>the maximum length of the signal traversing path in Pyraformer is a constant with regard to the sequence length L (i.e. \(\mathcal{O}(1)\))</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ ChallengeëŠ” powerful but parsimonious model</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/fig1.png" alt="ê·¸ë¦¼1" /></p>

<p><img src="/assets/img/timeseries/pyraformer/table1.png" alt="ê·¸ë¦¼11" /></p>

<ul>
  <li>Pyraformer: to simultaneously capture temporal dependencies of different ranges in a compact multi-resolution fashion</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-method">3. Method</h2>

<h3 id="31-pyramidal-attention-module-pam">3.1. Pyramidal Attention Module (PAM)</h3>

<ul>
  <li>The inter-scale connections form a C-ary tree, in which each parent has C children.
    <ul>
      <li>the nodes at coarser scales can be regarded as the daily, weekly, and even monthly features of the time series</li>
    </ul>
  </li>
  <li>\(\to\) The pyramidal graph offers a multi-resolution representation of the original time series !
    <ul>
      <li>long-range dependencies íŒŒì•…ì´ ì‰¬ì›Œì§. ê·¸ëƒ¥ ì´ì›ƒ ë…¸ë“œ ì—°ê²°í•˜ê¸°ë§Œ í•˜ë©´ ë˜ë‹ˆê¹Œ (intra-scale)</li>
    </ul>
  </li>
  <li>Original Attention mechanism
    <ul>
      <li>input \(X\), output \(Y\)</li>
      <li>Query \({Q}={X} {W}_Q\), key \({K}={X} {W}_K\), value \({V}={X} {W}_V\)</li>
      <li>where \({W}_Q, {W}_K, {W}_V \in \mathbb{R}^{L \times D_K}\)</li>
      <li>Then, \({y}_i=\sum_{\ell=1}^L \frac{\exp \left({q}_i {k}_{\ell}^T / \sqrt{D_K}\right) {v}_{\ell}}{\sum_{\ell=1}^L \exp \left({q}_i {k}_{\ell}^T / \sqrt{D_K}\right)}\)</li>
      <li>time and space complexity \(\mathcal{O}(L^2)\)</li>
    </ul>
  </li>
  <li>Pyramidal Attention Module (PAM)</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/fomula2.png" alt="ê·¸ë¦¼21" /></p>

<p><img src="/assets/img/timeseries/pyraformer/myfig1.png" alt="ê·¸ë¦¼31" /></p>

<ul>
  <li>Then, \({y}_i=\sum_{\ell \in \mathbb{N}_{\ell}^{(s)}} \frac{\exp \left({q}_i {k}_{\ell}^T / \sqrt{d_K}\right) {v}_{\ell}}{\sum_{\ell \in \mathbb{N}_l^{(s)}} \exp \left({q}_i {k}_{\ell}^T / \sqrt{d_K}\right)}\)</li>
  <li>ëª¨ë“  ì‹œì ë¼ë¦¬ attentionì„ í•˜ì§€ ì•Šê³  conv filterë¡œ nodesë¥¼ ë§Œë“¤ê³  ì´ì›ƒ ë…¸ë“œë¼ë¦¬ attention !</li>
</ul>

<h3 id="32-coarser-saleã„´-construvtion-module-cscm">3.2. Coarser-saleã„´ Construvtion Module (CSCM)</h3>

<p><img src="/assets/img/timeseries/pyraformer/fig3.png" alt="ê·¸ë¦¼3" /></p>

<ul>
  <li>PAMì´ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ pyramidal êµ¬ì¡°ë¥¼ initializeí•˜ëŠ” ì—­í• </li>
</ul>

<h3 id="33-prediction-module">3.3. Prediction Module</h3>

<ul>
  <li>input embedding í•  ë•Œì— ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ê¸¸ì´ë§Œí¼ ë¶™ì—¬ì„œ CSCM, PAMì„ í†µê³¼í•˜ë©´</li>
  <li>ì˜ˆì¸¡ ì‹œì ì— ëŒ€í•œ representationì„ ì–»ì„ ìˆ˜ ìˆìŒ</li>
</ul>

<p><img src="/assets/img/timeseries/pyraformer/myfig3.png" alt="ê·¸ë¦¼33" /></p>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/pyraformer/table2.png" alt="ê·¸ë¦¼12" /></p>

<p><img src="/assets/img/timeseries/pyraformer/table3.png" alt="ê·¸ë¦¼13" /></p>

<p><img src="/assets/img/timeseries/pyraformer/fig4.png" alt="ê·¸ë¦¼14" /></p>

<h2 id="5-conclusion-and-outlook">5. Conclusion and Outlook</h2>

<ul>
  <li>Pyraformer: a novel model based on pyramidal attention
    <ul>
      <li>effectively describe both short and long temporal dependencies with low time and space complexity</li>
      <li>CSCM to construct a C-ary tree, and then design the PAM to pass messages in both the inter-scale and the intra-scale fashion</li>
      <li>Pyraformer can achieve the theoretical \(\mathcal{O}(L)\) complexity and \(\mathcal{O}(1)\) maximum signal traversing path length (L: input sequence length)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2022](https://arxiv.org/pdf/2012.07436)]]></summary></entry><entry><title type="html">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)</title><link href="http://localhost:4000/timeseries/2024-07-26-Informer/" rel="alternate" type="text/html" title="Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI 2021)" /><published>2024-07-26T00:00:00+09:00</published><updated>2024-08-04T11:22:34+09:00</updated><id>http://localhost:4000/timeseries/Informer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-26-Informer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer: quadratic time complexity, high memory usage, and in- herent limitation of the encoder-decoder architecture</li>
  <li><strong>Informer</strong> : efficient transformer-based model for LSTF !
    <ul>
      <li><em>ProbSparse</em> self-attention mechanism
        <ul>
          <li>\(\mathcal{O}(L \log L)\) in time complexity and memory usage</li>
        </ul>
      </li>
      <li>The self-attention distilling
        <ul>
          <li>highlights dominating attention by halving cascading layer input</li>
          <li>and efficiently handles extreme long input sequences</li>
        </ul>
      </li>
      <li>The generative style decoder
        <ul>
          <li>predicts the long time-series sequences at one forward operation, rather than a step-by-step way</li>
          <li>improves the inference speed of long-sequence predictions</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>The major challenge for LSTF is to enhance the <strong>prediction capacity to meet the increasingly long sequence</strong> demand</li>
  <li><em>can we improve Transformer models to be computation, memory, and architecture efficient, as well as maintaining higher prediction capacity?</em></li>
  <li>Vanila Transformerì˜ limitation 3
    <ul>
      <li>The quadratic computation of self-attention \(\mathcal{O}\left(L^2\right)\)</li>
      <li>The memory bottleneck in stacking layers for long inputs \(\mathcal{O}\left(J \cdot L^2\right)\)</li>
      <li>The speed plunge in predicting long outputs</li>
    </ul>
  </li>
</ul>

<h2 id="2-preliminary">2. Preliminary</h2>

<p>pass</p>

<h2 id="3-methodology">3. Methodology</h2>

<p><img src="/assets/img/timeseries/Informer/fig2.png" alt="ê·¸ë¦¼1" /></p>

<h3 id="query-sparsity-measurement">Query Sparsity Measurement</h3>

<ul>
  <li>Based on KL divergence
    <ul>
      <li>: \(K L(q \| p)=\ln \sum_{l=1}^{L_K} e^{\mathbf{q}_i \mathbf{k}_l^{\top} / \sqrt{d}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \mathbf{q}_i \mathbf{k}_j^{\top} / \sqrt{d}-\ln L_K\)â€‹</li>
    </ul>
  </li>
  <li>\(i\)-th queryâ€™s sparsity measurement
    <ul>
      <li>: \(M\left(\mathbf{q}_i, \mathbf{K}\right)=\ln \sum_{j=1}^{L_K} e^{\frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}\)</li>
      <li>ì¦‰ queryë³„ë¡œ keyë“¤ê³¼ì˜ attentionì´ uniform distributionê³¼ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ì¸¡ì •</li>
    </ul>
  </li>
</ul>

<h3 id="probsparse-self-attention"><em>ProbSparse</em> Self-attention</h3>

<ul>
  <li>\(\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Softmax}\left(\frac{\overline{\mathbf{Q}} \mathbf{K}^{\top}}{\sqrt{d}}\right) \mathbf{V}\), where \(\overline{\mathbf{Q}}\) only contains the Top-u queries under the sparsity measurement \(M(\mathbf{q}, \mathbf{K})\)â€‹</li>
  <li>Query ì¤‘ì—ì„œ íŠ¹ì • keyì— ëŒ€í•´ì„œ ë†’ì€ attentionì„ ê°€ì§€ëŠ” queryë„ ìˆì§€ë§Œ (Active) ëª¨ë“  keyì— ëŒ€í•´ì„œ ë¹„ìŠ·í•œ attentionì„ ê°€ì§€ëŠ” queryë„ ìˆìŒ (Lazy)
    <ul>
      <li>êµ³ì´ ëª¨ë“  queryë¥¼ ë‹¤ ë³¼ í•„ìš”ëŠ” ì—†ë‹¤. Active = useful queryì´ê³  Lazy = trivial query</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Informer/myfig3.png" alt="ê·¸ë¦¼2" /></p>

<p><img src="/assets/img/timeseries/Informer/myfig4.png" alt="ê·¸ë¦¼4" /></p>

<ul>
  <li>í•˜ì§€ë§Œ ëª¨ë“  queryë“¤ ì¤‘ queryê°€ activeí•œì§€ ì•Œê¸° ìœ„í•´ì„œëŠ” ë˜ ëª¨ë“  keysì™€ attentionì„ ê³„ì‚°í•´ë´ì•¼ í•  ê²ƒ ê°™ì§€ë§Œ,
    <ul>
      <li>ê·¸ë ‡ì§€ ì•Šê³  keysë¥¼ samplingí•´ì„œ ëª‡ ê°œë§Œ ê°€ì ¸ì™€ì„œ ëª¨ë“  queryë“¤ê³¼ attentionì„ ê³„ì‚°í•´ë„ ëœë‹¤. (ì¦ëª… : lemma1)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Informer/lemma1.png" alt="ê·¸ë¦¼6" /></p>

<ul>
  <li>ê·¸ë ‡ê²Œ ì°¾ì€ useful queryë“¤ë§Œ ê°€ì§€ê³ , ì´ì œëŠ” ëª¨ë“  keysì™€ attentionì„ ê³„ì‚°í•œë‹¤</li>
</ul>

<h3 id="encoder-allowing-for-processing-longer-sequential-inputs-under-the-memory-usage-limitation">Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation</h3>

<p><img src="/assets/img/timeseries/Informer/fig3.png" alt="ê·¸ë¦¼7" /></p>

<ul>
  <li>We use the distilling operation to privilege the superior ones with dominating features and make a focused self-attention fea- ture map in the next layer.</li>
  <li>Distilling procedure : \(\mathbf{X}_{j+1}^t=\operatorname{MaxPool}\left(\operatorname{ELU}\left(\operatorname{Conv1d}\left(\left[\mathbf{X}_j^t\right]_{\mathrm{AB}}\right)\right)\right)\)</li>
</ul>

<h3 id="decoder-generating-long-sequential-outputs-through-one-forward-procedure">Decoder: Generating Long Sequential Outputs Through One Forward Procedure</h3>

<ul>
  <li>
    <p>Transformerì˜ Masked-attentionê³¼ Encoder-Decoder Attention ëŒ€ì‹  <strong>generative inference</strong></p>

    <ul>
      <li>
        <p>Decoderì˜ input : \(\mathbf{X}_{\mathrm{de}}^t=\operatorname{Concat}\left(\mathbf{X}_{\text {token }}^t, \mathbf{X}_{\mathbf{0}}^t\right) \in \mathbb{R}^{\left(L_{\text {token }}+L_y\right) \times d_{\text {model }}}\)</p>
      </li>
      <li>
        <p>Start tokenì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , Target ì§ì „ ì‹œì ì˜ ê°’ ëª‡ê°œë¥¼ start tokenìœ¼ë¡œ ì£¼ê³ , ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê¸¸ì´ì˜ ì˜ˆì¸¡ê°’ì„ í•œ ë²ˆì— decoding</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/Informer/table1.png" alt="ê·¸ë¦¼9" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Informer
    <ul>
      <li><em>ProbSparse</em> self- attention mechanism</li>
      <li>Distilling operation : to handle the challenges of quadratic time complexity and quadratic mem- ory usage in vanilla Transformer</li>
      <li>generative decoder alleviates : alleviates the limitation of tra- ditional encoder-decoder architecture</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2021](https://arxiv.org/pdf/2012.07436)]]></summary></entry><entry><title type="html">Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)</title><link href="http://localhost:4000/timeseries/2024-07-15-timediff/" rel="alternate" type="text/html" title="Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)" /><published>2024-07-15T00:00:00+09:00</published><updated>2024-07-23T18:18:28+09:00</updated><id>http://localhost:4000/timeseries/timediff</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-15-timediff/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>TimeDiff : non-autoregressive diffusion model, w/ two novel conditioning mechanisms
    <ul>
      <li>future mixup : future predictionì˜ ground-truthì˜ ì¼ë¶€ë¥¼ conditioningí•˜ëŠ” ê²ƒì„ í—ˆìš©</li>
      <li>autoregressive initialization : time seriesì˜ basic pattern (short term trends ë“±)ì„ ëª¨ë¸ initializationì— ì‚¬ìš©</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Diffusion model (iterative denoising)ì€ ì´ë¯¸ì§€ ìƒì„±ì—ì„œ ë›°ì–´ë‚œ quality
    <ul>
      <li>í•˜ì§€ë§Œ time series predictionì„ ìœ„í•´ ì–´ë–»ê²Œ ì“¸ì§€ì— ëŒ€í•œ ì—°êµ¬ëŠ” ì•„ì§</li>
      <li>time seriesëŠ” <strong>complex dynamics, nonlinear patterns, long-temporal dependencies</strong></li>
    </ul>
  </li>
  <li>ê¸°ì¡´ diffusion modelë“¤ì€ decoding strategyì— ë”°ë¼ êµ¬ë¶„ë¨
    <ul>
      <li><strong>Autoregressive</strong> : future predictionì´ one by oneìœ¼ë¡œ generated (ex. Timegrad)
        <ul>
          <li>í•˜ì§€ë§Œ error accumulation ë•Œë¬¸ì— long range prediction ì„±ëŠ¥ì´ ë–¨ì–´ì§€ê³ </li>
          <li>í•˜ë‚˜ì”© ì˜ˆì¸¡í•˜ë‹¤ë³´ë‹ˆ inferenceê°€ ëŠë¦¬ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŒ</li>
        </ul>
      </li>
      <li><strong>Non-autoregressive</strong> : CSDI, SSSDì²˜ëŸ¼ denoising networksì— intermediate layersë¥¼ conditioningìœ¼ë¡œ ë„£ê³  the denoising objectiveì— inductive biasë¥¼ introduce
        <ul>
          <li>í•˜ì§€ë§Œ long-range prediction performanceëŠ” Fedformer, NBeatsë³´ë‹¤ ë–¨ì–´ì§</li>
          <li>ì™œëƒí•˜ë©´ conditioning ì „ëµì´ image, textfë¥¼ ìœ„í•œ ê²ƒì´ì§€ time seriesë¥¼ ìœ„í•œ ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸</li>
          <li>inductive biasë¥¼ ìœ„í•´ denoising objectiveë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” lookback windowì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì•Œì•„ë‚´ê¸° ì–´ë µë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” long time series predictionì„ ìœ„í•œ conditional non-autoregressive diffusion modelì¸ TimeDiff ì œì•ˆ
    <ul>
      <li>CSDI, SSSDì™€ ë‹¤ë¥´ê²Œ conditioning moduleì— time seriesë¥¼ ìœ„í•œ additional inductive bias ë„ì…
        <ul>
          <li><strong>future mixup</strong>: randomly reveals parts of the ground-truth future pre- dictions during training</li>
          <li><strong>autoregressive initialization</strong>: better initializes the model with basic components in the time series</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-preliminaries">2. Preliminaries</h2>

<h3 id="21-diffusion-models">2.1. Diffusion Models</h3>

<p>pass</p>

<h3 id="22-conditional-ddpms-for-time-series-prediction">2.2. Conditional DDPMs for Time Series Prediction</h3>

<ul>
  <li>\(\mathbf{x}_{-L+1: 0}^0 \in \mathbb{R}^{d \times L}\)ë¥¼ ë³´ê³  \(\mathbf{x}_{1: H}^0 \in \mathbb{R}^{d \times H}\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ</li>
  <li>\(p_\theta\left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}\right)=p_\theta\left(\mathbf{x}_{1: H}^K\right) \prod_{k=1}^K p_\theta\left(\mathbf{x}_{1: H}^{k-1} \mid \mathbf{x}_{1: H}^k, \mathbf{c}\right)\),
    <ul>
      <li>where \(\mathbf{x}_{1: H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)</li>
    </ul>
  </li>
  <li>ì•„ì§ efficient denoising network \(\mu_{\theta}\)ì™€ conditioning network \(\mathcal F\) in time series diffusion modelsë¥¼ ì–´ë–»ê²Œ ë””ìì¸í•  ê²ƒì¸ì§€ ëª…í™•í•˜ì§€ ì•ŠìŒ</li>
  <li><a href="https://arxiv.org/pdf/2101.12072">TimeGrad (ICML 2021)</a>
    <ul>
      <li>autoregressive manner :
 \(\begin{aligned}
p_\theta &amp; \left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\right) \\
&amp; =\prod_{t=1}^H p_\theta\left(\mathbf{x}_t^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right) \\
&amp; =\prod^H p_\theta\left(\mathbf{x}_t^K\right) \prod^K p_\theta\left(\mathbf{x}_t^{k-1} \mid \mathbf{x}_t^k, \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right)
\end{aligned}\)</li>
      <li>training objectives : \(\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_t^k, k \mid \mathbf{h}_t\right)\right\|^2\right]\)</li>
      <li>autoregressive decoding ë•Œë¬¸ì— error accumulationì´ ë°œìƒí•˜ê³  inferenceê°€ ëŠë¦¬ê³  ë¶€ì •í™•í•¨</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/2107.03502">CSDI(NeurIPS 2021)</a>
    <ul>
      <li>time series \(\mathbf{x}_{-L+1: H}^0\) ì „ì²´ë¥¼ í•œ ë²ˆì— diffusing and denoising</li>
      <li>binary mask \(\mathbf{m} \in\{0,1\}^{d \times(L+H)}\)ë¥¼ ì‚¬ìš©í•˜ì—¬ self-supervised strategy ì œì•ˆ</li>
      <li>training objectives : \(\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_{\text {target }}^k, k \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{\text {observed }}^k\right)\right)\right\|^2\right]\)</li>
    </ul>
  </li>
  <li>í•˜ì§€ë§Œ CSDIì˜ í•œê³„ëŠ”
    <ul>
      <li>Denoising networksê°€ 2ê°œì˜ transformersë¥¼ ì‚¬ìš©í•´ì„œ complexityê°€ ë†’ë‹¤.</li>
      <li>conditioningì— ì‚¬ìš©ë˜ëŠ” maskingì€ visionì˜ inpaintingì´ë‘ ë¹„ìŠ·í•œë°
        <ul>
          <li><a href="https://arxiv.org/abs/2201.09865">(Lugmayr et al., 2022)</a>ì—ì„œëŠ” ì´ ë°©ì‹ì´ maskingê³¼ observedì‚¬ì´ì˜ ë¶€ì¡°í™” ë°œìƒí•œë‹¤ê³  ë°í˜</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/2208.09399">SSSD (TMLR 2022)</a>
    <ul>
      <li>Transfermerë¥¼ structured state space modelë¡œ ëŒ€ì²´</li>
      <li>í•˜ì§€ë§Œ ì—¬ì „íˆ non-autoregressive strategyì´ë¼ì„œ boundary disharmonyê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-model">3. Proposed Model</h2>

<ul>
  <li>Conditional Diffusionì˜ conditioningì€ semantic similarities across modalities íŒŒì•…ì— ì¤‘ì </li>
  <li>í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œì˜ non-stationary time seriesëŠ” complex temporal dependencies íŒŒì•…ì´ ì¤‘ìš”í•¨</li>
</ul>

<h3 id="31-forward-diffusion-process">3.1. Forward Diffusion Process</h3>

<ul>
  <li>Forward process : \(\mathbf{x}_{1: H}^k=\sqrt{\bar{\alpha}_k} \mathbf{x}_{1: H}^0+\sqrt{1-\bar{\alpha}_k} \epsilon\)
    <ul>
      <li>where \(\epsilon\) is sampled from \(\mathcal{N}(0, \mathbf{I})\) with the same size as \(\mathbf{x}_{1: H}^0\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-conditioning-the-backward-denoising-process">3.2. Conditioning the Backward Denoising Process</h3>

<ul>
  <li>Illustration</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/fig1.png" alt="ê·¸ë¦¼1" /></p>

<h4 id="321-future-mixup">3.2.1. FUTURE MIXUP</h4>

<ul>
  <li>ë¨¼ì € <em>future mixup</em>ìœ¼ë¡œ \(\mathbf{z}_{\text {mix }}=\mathbf{m}^k \odot \mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)+\left(1-\mathbf{m}^k\right) \odot \mathbf{x}_{1: H}^0\)ë¥¼ ë§Œë“ ë‹¤.
    <ul>
      <li>past informationâ€™s mapping \(\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)ê³¼ the future ground-truth \(\mathbf{x}_{1: H}^0\)ë¥¼ combine</li>
      <li>trainingì—ì„œ ì ìš©ë˜ëŠ” ê²ƒì´ê³ , inferenceì—ì„œëŠ” \(\mathbf{z}_{\text {mix }}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\)</li>
    </ul>
  </li>
</ul>

<h4 id="322-autoregressive-model">3.2.2. AUTOREGRESSIVE MODEL</h4>

<ul>
  <li>Non-autoregressive modelsëŠ” maskedì™€ observedì˜ ê²½ê³„ì—ì„œ disharmony
    <ul>
      <li>ê·¸ë˜ì„œ linear autoregressive (AR) model \(\mathcal{M}_{a r}\) ì‚¬ìš©. \(\mathbf{z}_{a r}=\sum_{i=-L+1}^0 \mathbf{W}_i \odot \mathbf{X}_i^0+\mathbf{B}\)
        <ul>
          <li>\(\mathbf{X}_i^0 \in \mathbb{R}^{d \times H}\) is a matrix containing \(H\) copies of \(\mathbf{x}_i^0\),</li>
          <li>\(\mathbf{W}_i\) s \(\in \mathbb{R}^{d \times H}, \mathbf{B} \in \mathbb{R}^{d \times H}\) are trainable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>complex nonlinear time seriesëŠ” approximate ëª»í•˜ëŠ” ê±´ ì‚¬ì‹¤ì´ì§€ë§Œ
    <ul>
      <li>simple patterns (short-term trends) ì •ë„ëŠ” ì˜ ì¡ìœ¼ë‹ˆê¹Œ</li>
      <li>ê·¸ë¦¬ê³  one by oneìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ \(\mathbf{z}_{a r}\)ì˜ ëª¨ë“  columnsëŠ” ë™ì‹œì— ê³„ì‚°ë¨</li>
    </ul>
  </li>
</ul>

<h4 id="33-denoising-network">3.3. Denoising Network</h4>

<ul>
  <li>ë¨¼ì € the transformerâ€™s sinusoidal position embeddingìœ¼ë¡œ the diffusion-step embedding \(\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}\)â€‹ ì–»ìŒ
    <ul>
      <li>ì¦‰ \(\begin{aligned}
k_{\text {embedding }}= &amp; {\left[\sin \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \sin \left(10^{\frac{w \times 4}{w-1}} t\right),\right.} \\
&amp; \left.\cos \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \cos \left(10^{\frac{w \times 4}{w-1}} t\right)\right],
\end{aligned}\)â€‹
        <ul>
          <li>where \(w=\frac{d^{\prime}}{2}\)</li>
        </ul>
      </li>
      <li>ê·¸ë¦¬ê³  \(\mathbf{p}^k=\operatorname{SiLU}\left(\mathrm{FC}\left(\operatorname{SiLU}\left(\mathrm{FC}\left(k_{\text {embedding }}\right)\right)\right)\right) \in \mathbb{R}^{d^{\prime} \times 1}\)</li>
    </ul>
  </li>
  <li>ê·¸ ë‹¤ìŒ \(\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}\)ëŠ” diffused input \(\mathbf{x}_{1: H}^k\)ì˜ embedding \(\mathbf{z}_1^k \in \mathbb{R}^{d^{\prime} \times H}\)ì— í•©ì³ì§ (\(2 d^{\prime} \times H\)ì´ ë¨)
    <ul>
      <li>\(\mathbf{z}_1^k\)ëŠ” ì—¬ëŸ¬ ê°œì˜ convolution layersë¡œ ì´ë£¨ì–´ì§„ input projection blockì„ í†µê³¼ì‹œì¼œ ì–»ìŒ</li>
    </ul>
  </li>
  <li>ê·¸ ë‹¤ìŒ multilayer convolution-based <strong>encoder</strong> í†µê³¼í•˜ë©´ \(\mathbf{z}_2^k \in \mathbb{R}^{d^{\prime \prime} \times H}\)ë¡œ representation</li>
  <li>ê·¸ ë‹¤ìŒ \(\mathbf{c}\)ì™€ \(\mathbf{z}_2^k\)ë¥¼ fuseí•´ì„œ \(\left(2 d+d^{\prime \prime}\right) \times H\)ë¡œ ë§Œë“¤ê³ 
    <ul>
      <li>multiple convolution layers <strong>decoder</strong>ì— ë„£ì–´ì„œ \(\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right)\in \mathbb{R^{d \times H}}\)ë¡œ ë§Œë“¬ (\(\mathbf{x}_{1: H}^k\)ì™€ ê°™ì€ size)</li>
    </ul>
  </li>
  <li>ë§ˆì§€ë§‰ìœ¼ë¡œ \(\mu_{\mathbf{x}}\left(\mathbf{x}_\theta\right)=\frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}^k, k \mid \mathbf{c}\right)\)ì„ í†µí•´ denoised output \(\begin{aligned}
\hat{\mathbf{x}}_{1: H}^{k-1}= &amp; \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
+\sigma_k \epsilon
\end{aligned}\)ì–»ìŒ</li>
  <li>í”íˆ ì•„ëŠ” Diffusionì—ì„œëŠ” noise \(\epsilon_\theta\left(\mathbf{x}_{1: H}^k, k\right)\)ë¥¼ ì˜ˆì¸¡í•˜ì§€ë§Œ, time seriesì—ì„œëŠ”  highly irregular noisy componentsë¼ì„œ ë°ì´í„° \(\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k\right)\)ë¥¼ ì˜ˆì¸¡</li>
</ul>

<h3 id="34-training">3.4. Training</h3>

<ul>
  <li>Pesudo code</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/code.png" alt="ê·¸ë¦¼3" /></p>

<ul>
  <li>ê°ê°ì˜ \(\mathbf{x}_{1: H}^0\)ì— ëŒ€í•´ batch of diffusion steps \(k\)â€™së¥¼ samplingí•˜ê³ 
    <ul>
      <li>conditioned variant of lossë¥¼ minimize: \(\min _\theta \mathcal{L}(\theta)=\min _\theta \mathbb{E}_{\mathbf{x}_{1 . H}^0, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), k} \mathcal{L}_k(\theta)\)</li>
    </ul>
  </li>
</ul>

<h3 id="35-inference">3.5. Inference</h3>

<ul>
  <li>Pesudo code</li>
</ul>

<p><img src="/assets/img/timeseries/timediff/code.png" alt="ê·¸ë¦¼4" /></p>

<ul>
  <li>ë¨¼ì € noise vector \(\mathbf{x}_{1 \cdot H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \in \mathbb{R}^{d \times H}\)ë¥¼ ìƒì„±í•˜ê³ </li>
  <li>\(\begin{aligned}
\hat{\mathbf{x}}_{1: H}^{k-1}= &amp; \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
+\sigma_k \epsilon
\end{aligned}\)ì„ ë°˜ë³µ (\(k=1\) ê¹Œì§€)
    <ul>
      <li>when \(k=1\), \(\epsilon=0\)ì´ë¯€ë¡œ \(\hat{\mathbf{x}}_{1: H}^0\)ë¥¼ final predictionìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/timediff/table2.png" alt="ê·¸ë¦¼12" /></p>

<p><img src="/assets/img/timeseries/timediff/table3.png" alt="ê·¸ë¦¼13" /></p>

<p><img src="/assets/img/timeseries/timediff/fig2.png" alt="ê·¸ë¦¼2" /></p>

<h3 id="43-ablation-study">4.3. Ablation study</h3>

<ul>
  <li><strong>The Effectiveness of Future mixup</strong></li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table4.png" alt="ê·¸ë¦¼14" /></p>

<ul>
  <li>
    <p>íŠ¹íˆ ETTh1 ë°ì´í„°ì…‹ì—ì„œ future mixupì„ ì•ˆì¼ì„ ë•Œ ì„±ëŠ¥ì´ ë§ì´ ë–¨ì–´ì§„ë‹¤.</p>
  </li>
  <li>
    <p><strong>The Mixup Strategies in Future mixup</strong></p>
    <ul>
      <li>Hard mixup : The sampled values in \(\mathbf{m}^k\) are binarized by a threshold \(\tau \in (0,1)\)</li>
      <li>Segment mixup : The mask  \(\mathbf{m}^k\), Each masked segment has a length following the geometric distribution with a mean of 3. This is then followed by an unmasked segment with mean length \(3(1 âˆ’ \tau)/\tau\)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table5.png" alt="ê·¸ë¦¼15" /></p>

<ul>
  <li><strong>Predicting \(\mathbf{x}_\theta\) vs Predicting \(\epsilon_\theta\)</strong></li>
</ul>

<p><img src="/assets/img/timeseries/timediff/table6.png" alt="ê·¸ë¦¼16" /></p>

<h3 id="44-integration-into-existing-diffusion-models">4.4. Integration into Existing Diffusion Models</h3>

<p><img src="/assets/img/timeseries/timediff/table7.png" alt="ê·¸ë¦¼17" /></p>

<h3 id="45-inference-efficiency">4.5. Inference Efficiency</h3>

<p><img src="/assets/img/timeseries/timediff/table8.png" alt="ê·¸ë¦¼18" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Timediff : diffusion model for time series prediction,
    <ul>
      <li>1)future mixupê³¼ 2)autoregressive initializationì´ë¼ëŠ” conditioning mechanismsìœ¼ë¡œ</li>
      <li>conditioning networkì— useful inductive biasë¥¼ ì¶”ê°€</li>
      <li>í•œê³„ì ìœ¼ë¡œ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ë§ì„ ë•Œ multivariate dependenciesë¥¼ í•™ìŠµí•˜ê¸° ì–´ë µë‹¤
        <ul>
          <li>graph ì‚¬ìš© ?</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2023](https://arxiv.org/pdf/2306.05043)]]></summary></entry></feed>