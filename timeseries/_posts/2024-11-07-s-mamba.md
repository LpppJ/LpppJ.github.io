---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2403.11144v3)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Is Mamba Effective for Time Series Forecasting? (Arxiv 2024)

## Abstract

- Time series forecasting (TSF)에서 Transformer는 quadratic complexity
- **Simple-Mamba (S-Mamba)** :
  - Tokenize the time points of each variate autonomously via a linear layer
  - Bi-directional Mamba layer is utilized to extract inter-variate correlations
  - Feed-Forward Network is set to learn temporal dependencies
  - Generation of forecast outcomes through a linear mapping layer

## 1. Introduction

- Transformer-based

  - quadratic computational complexity
  - reduce the computational complexity $$\to$$  loss of information $$\to$$ performance degradations

- Linear model

  - solely on linear numerical calculations $$\to$$ do not incorporate in-context information

- SSM

  - convolutional calculation to capture sequence information
  - eliminate hidden states (for parallel computing) $$\to$$ near-linear complexity 
  - But unable to identify and filter content

- Mamba

  - selective mechanism into SSM

- **Simple-Mamba (S-Mamba)** :

  - 1. time points of each variate are tokenized by a linear layer

  - 2. Mamba VC (Inter-variate Correlation) Encoding layer encodes the VC

    - by utilizing a bidirectional Mamba

  - 3. FeedForward Network (FFN) TD (Temporal Dependency) Encoding Layer extracts the TD

  - 4. mapping layer is utilized to output the forecast results

## 2. Related Work

pass

## 3. Preliminaries

### 3.1. Problem Statement

- $$U_{\text {in }}=\left[u_1, u_2, \ldots, u_L\right] \in \mathbb{R}^{L \times V}$$를 보고 $$U_{\text {out }}=\left[u_{L+1}, u_{L+2}, \ldots, u_{L+T}\right] \in \mathbb{R}^{T \times V}$$를 예측
  - 각각의 $$u_n=\left[p_1, p_2, \ldots, p_V\right]$$

### 3.2. State Space Models

- The continuous sequence is discretized by a step size $$\Delta$$, and the discretized SSM model :
  - $$\begin{aligned}
    h_t & =\overline{\boldsymbol{A}} h_{t-1}+\overline{\boldsymbol{B}} x_t, \\
    y_t & =\boldsymbol{C} h_t,
    \end{aligned}$$      where
    - $$\overline{\boldsymbol{A}}=\exp (\Delta \boldsymbol{A}) \text { and } \overline{\boldsymbol{B}}=(\Delta \boldsymbol{A})^{-1}(\exp (\Delta \boldsymbol{A})-I) \cdot \Delta \boldsymbol{B}$$.

### 3.3. Mamba Block

: **Data-dependent selection mechanism into the S4** & **Incorporates hardware-aware parallel algorithms**

![그림1](/assets/img/timeseries/s-mamba/alg1.png)

- Mamba Layer

  - Input : $$X \in \mathbb{R}^{B \times V \times D}$$

  - 1. expands the hidden dimension to $$ED$$ through linear projection $$\to x, z$$를 얻음

  - 2. the projection using convolutional functions and a SiLU $$\to x'$$를 얻음

  - 3. generates the state representation $$y$$

  - 4. $$y$$ is combined with a residual connection from $$z$$ after activation,

    - and the final output $y_t$ at time step $t$ is obtained

  - with state expansion factor $$N$$,

    - a size of convolutional kernel $$k$$,
    - and a block expansion factor $$E$$

  - The final output of the Mamba block is $$Y \in \mathbb{R}^{B \times V \times D}$$.

## 4. Methodology

- 1st layer : the Linear Tokenization Layer (tokenizes the time series with a linear layer)
- 2nd layer :  the Mamba intervariate correlation (VC) Encoding layer (using a bidirectional Mamba block)
- 3rd layer : the FFN Temporal Dependencies (TD) Encoding Layer (learns the temporal sequence information)
  - Feed-Forward Network : generates future series representations
- 4th layer : Projection Layer, is only mapping for forecasting

![그림1](/assets/img/timeseries/s-mamba/alg2.png)

### 4.1. Linear Tokenization Layer	

- $$U=\operatorname{Linear}\left(\operatorname{Batch}\left(U_{\text {in }}\right)\right), \quad$$ where $$U$$ is the output of this layer

### 4.2. Mamba VC Encoding Layer

- 여기서는 유사한 trend를 보이는 변수들을 연결해서 VC를 찾고 싶음
- Transformer는 그냥 모든 변수들끼리 다 연결하니까 정확하긴 한데 변수 개수 따라 complexity 늘어남
- Mamba는 complexity는 near-linear이지만
  - **Selection mechanism이 uni-directional해서 앞쪽의 변수만 볼 수 있음**
  - 그래서 2개의 Mamba를 서로 다른 방향으로 흐르도록 놓음 (Bi-directional Mamba)
  - $$\begin{aligned}&\overrightarrow{\boldsymbol{Y}}=\overrightarrow{\operatorname{Mamba\operatorname {Block}}(\boldsymbol{U}),} \\
    & \overleftarrow{\boldsymbol{Y}}=\overleftarrow{\operatorname{Mamba} \operatorname{Block}}(\boldsymbol{U}) .
    \end{aligned}$$이고 $$\boldsymbol{Y}=\overrightarrow{\boldsymbol{Y}}+\overleftarrow{\boldsymbol{Y}}$$로 Aggregate, with residual : $$\boldsymbol{U}^{\prime}=\boldsymbol{Y}+\boldsymbol{U}$$

### 4.3. FFN TD Encoding Layer

- 1. Normalization layer : enhance convergence and training stability

- 2. FFN layer : encodes observed time series (encodes TD by keeping the sequential relationships)

  - decodes future series representations (adjust the future series representations)

### 4.4. Projection Layer

- FFN TD Encoding layer의 output인 tokenized temporal information이
  - linear mapping을 통해서 reconstructed for predictive outcome

## 5. Experiments

### 5.1. Datasets and Baselines

![그림1](/assets/img/timeseries/s-mamba/table1.png)

- SOTA와 비교 :
  - iTransformer: analyzes the time series information of each **individual variate** and then fuses the information of all variates.
  - PatchTST: segments time series into **subseries patches** as input tokens and uses channel-independent shared embeddings and weights
  - Crossformer: cross-attention mechanism that allows the model to **interact with information between different time steps** 
  - FEDformer: a **frequency-enhanced Transformer** for utilizaing a sparse representation
  - Autoformer: **decomposition architecture** that incorporates an auto-correlation mechanism 
  - RLinear: reversible normalization and channel independence into **pure linear structure**
  - TiDE: Multi-layer Perceptron (MLP) based encoderdecoder model
  - DLinear: **simple one-layer linear** model with decomposition architecture
  - TimesNet: a task-general backbone, **transforms 1D time series into 2D tensors**

### 5.2. Overall Performance

![그림1](/assets/img/timeseries/s-mamba/table2.png)

![그림1](/assets/img/timeseries/s-mamba/table3.png)

![그림1](/assets/img/timeseries/s-mamba/table4.png)

- S-Mamba가 traffic-related, Electricity, and Solar-Energy에서 성능이 좋음
  - 변수들이 periodic한 데이터셋들.
  - 즉 period variates are more likely to contain learnable VC.
  - Mamba VC Fusion Layer가 잘 잡은 것
- ETT, and Exchange datasets에서는 성능이 매우 좋지는 않았음
  - 변수 개수가 적은 데이터셋들 (predominantly of an aperiodic nature)
  - **weak** VCs between these variates 때문에 Mamba VC Encoding layer가 noise를 가져옴
- Weather는 변수도 적고 aperiodic한데 왜 성능이 좋나
  - 변수들의 Trend가 동시에 나타나는 도메인이라서 Mamba VC Encoding layer가 잘 작동
  - Trend가 large sections로 나타나기 때문에 FFN이 이런 거 잘 잡음

![그림1](/assets/img/timeseries/s-mamba/fig4.png)

### 5.3. Model Efficiency

![그림1](/assets/img/timeseries/s-mamba/fig5.png)

### 5.4. Ablation Study

![그림1](/assets/img/timeseries/s-mamba/table5.png)

### 5.5. Can Variate Order Affect the Performance of S-Mamba?

- S-Mamba는 independent channel이라서 variates order 안중요했음
  - 하지만 Mamba VC Encoding Layer는 variates order에 따라 initial bias 발생할 수 있음
- 그래서 Fourier transform해서 variates를 periodic and aperiodic groups으로 나누고 
  - periodic variates =  reliable information / aperiodic variates = potential noise으로 가정
- 그래서 reliable information를 가지고 있는(그럴 것이라고 생각되는) periodic variates를 앞쪽에 배치

![그림1](/assets/img/timeseries/s-mamba/fig6.png)

###  5.6. Can Mamba Outperform Advanced Transformers?

- Transformer의 Encoder layer를 Mamba로 교체
  - Autoformer, Flashformer and Flowformer
  - $$\to$$  Auto-M, FlashM and Flow-M 이라고 부르겠음

![그림1](/assets/img/timeseries/s-mamba/fig7.png)

### 5.7. Can Mamba Help Benefit from Increasing Lookback Length?

- Transformer-based model은:
  - lookback sequence length $$L$$이 늘어나도 성능이 비례해서 좋아지는 건 아님
  - **sequential order를 신경쓰지 않아서 그렇다**
- Mamba는:
  - certain sequential attributes가 잘 유지되는 편
  - Mamba block을 Transformer-based model의 Encoder와 decoder 사이에 배치하면
    - Encoder layer의 output에 (decoder layer에 가기 전에)
    - positional encoding처럼 어떤 정보를 추가해주는 역할을 하는 것
  - 그것을 Reformer, Informer, and Transformer와 비교해서
    -  Refor-M, Infor-M, and Trans-M라고 부르고 비교

![그림1](/assets/img/timeseries/s-mamba/fig8.png)

- S-Mamba와 iTransformer 둘다 $$L$$이 길어지면 어느 정도 성능이 좋아지긴 함
  - 하지만 이건 두 모델 모두 가지고 있는 FFN TD Encoding Layer 때문으로 보임
- S-Mamba가 iTransformer와 비교해서 일관되게 성능이 좋아지는 편인데
  - 이건 Mamba VC Encoding layer와 Transformer의 VC Encoding layer의 차이 !

### 5.8. Is Mamba Generalizable in TSF?

- Transformer는 generalization capabilities가 좋은 편이라서
  - iTransformer의 경우 40%의 변수만을 가지고 나머지 변수들은 masking해도
  - 성능이 큰 폭으로 나빠지지는 않음
- Mamba의 경우에도 40%의 변수만 보고 나머지 변수들은 masking 했을 때
  - iTransformer에 크게 뒤쳐지지 않음

![그림1](/assets/img/timeseries/s-mamba/fig9.png)

## 6. Conclusion

- **Simple-Mamba(S-Mamba)**
  -  inter-variate correlation (VC) encoding은
    - Transformer 대신 **bi-directional** Mamba block으로 하고
    - (더 낮은 overhead로 VC를 파악)
  - Temporal Dependencies (TD)는
    - Feed-Forward Network로 extract
- Mamba는 advanced-Transformer만큼의 **stability**도 있고
  - **generalization** capabilities도 뛰어난 편

