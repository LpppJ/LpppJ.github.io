---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Neyman-Pearson Hypothesis Testing

## Basics of Hypothesis Testing

- 일반적으로 가설 검정을 construction하는 과정은 아래와 같다.
  - Test statistic $$T_n=T_n\left(X_1, \ldots, X_n\right)$$ 선택
  - Rejection region (기각역) 설정
  - 만약 $$T_n \in R$$이면 귀무가설 기각, 그렇지 않으면 기각할 수 없다.
- 가설 검정을 하는 이유는 귀무가설이 참인지 거짓인지 판단하기 위함이 아니다.
  - 정확히는 **귀무가설을 기각할 충분한 evidence**가 있는지를 판단하기 위함이다.

## Neyman-Pearson paradigm

- $H_0: \theta \in \Theta_0 \quad$ versus $$\quad H_1: \theta \in \Theta_1$$

- Pick an $$\alpha \in(0,1)$$​
- Then try to maximize $$\beta(\theta)$$ over $$\Theta_1$$ subject to $$\sup _{\theta \in \Theta_0} \beta(\theta) \leq \alpha$$
  - where Power function : $$\beta(\theta)=P_\theta\left(\left(X_1, \ldots, X_n\right) \in R\right)$$​
- 가설 검정할 때 test statistic이 $$Z_{1-\alpha}$$보다 크면 귀무가설 기각하고... 어쩌고 그게 어디서 나온 것인지를 아래 예시를 통해 알아보자.

### example : One sided test

- Suppose $$X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, \sigma^2\right)$$ with $$\sigma^2$$ known
- $H_0: \theta=\theta_0 \quad$ versus $$\quad H_1: \theta>\theta_0$$ (one sided)
- natural test statistic은 $$T_n\left(X_1, \ldots, X_n\right)=\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta_0}{\sigma / \sqrt{n}}$$이다.
- Power function : $$\beta(\theta)=P_\theta\left(T_n>t\right)=P_\theta\left(\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta}{\sigma / \sqrt{n}}>t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)$$에서
  - Thresholdld $$t$$를 결정해야 한다.
- $$\theta$$는 true parameter라는 점에서 $$\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta}{\sigma / \sqrt{n}} \sim N(0,1)$$이므로 $$\beta(\theta)=P\left(Z>t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)=1-\Phi\left(t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)$$이다.
- 이제 Neyman-Pearson paradigm을 implement한다.
  - $$\sup _{\theta \in \Theta_0}\left\{1-\Phi\left(t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\right\} \leq \alpha$$인데 귀무가설에서는 $$\theta = \theta_0$$이므로
  - $$1-\Phi(t) \leq \alpha$$이고 $$t$$에 대해 정리하면 $$t=\Phi^{-1}(1-\alpha)$$이다.

## Neyman-Pearson Procedure

- 먼저 test function을 정의한다.
  - $$\phi(\boldsymbol{x})= \begin{cases}1, & \text { if } \boldsymbol{x} \in R \\ 0, & \text { if } \boldsymbol{x} \notin R\end{cases}$$ , 즉 $$\phi(\boldsymbol{x})=1$$은 귀무가설 기각을 의미한다.
  - 이 notation에 따르면 $$\text { power }=\int \phi(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x}$$ 이고 $$\text { size }=\int \phi(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}$$이다.
- Neyman–Pearson test statistic은 likelihood ratio이다.
  - $$\Lambda(\boldsymbol{x})=\frac{L\left(\theta_0 \mid \boldsymbol{x}\right)}{L\left(\theta_1 \mid \boldsymbol{x}\right)}=\frac{f_0(\boldsymbol{x})}{f_1(\boldsymbol{x})}$$로 setting하고 $$P_0\left(\Lambda(\boldsymbol{X}) \leq t^*\right)=\alpha$$가 되도록 $$t^*$$를 결정한다.
  - 이렇게 likelihood ratio로 test function을 결정하는 방식이 유의수준이 $$\alpha$$인 모든 test 중에서 가장 power가 높다.
  - 이것을 증명하는 것이 The Neyman–Pearson Lemma이다.

## The Neyman–Pearson Lemma

- 한 마디로 말하자면 귀무가설 하에서 type 1 error를 유의수준 $$\alpha$$와 같게 했을 때, type 2 error의 확률이 최소화된다는 것 (most powerful)

- Consider a test with hypotheses $$H_0: \theta=\theta_0$$ and $$H_1: \theta=\theta_1$$
  - where the pdf (or pmf) is $$f_i(\boldsymbol{x})$$ for $$i=0,1$$.
- Consider the Neyman-Pearson test $$\phi_{\mathrm{NP}}(\boldsymbol{x})=\mathbb{1}\left(\frac{f_0(\boldsymbol{x})}{f_1(\boldsymbol{x})} \leq t^*\right)$$,
  - where $$t^*$$ is chosen such that $$\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}=\alpha$$.
- Consider any arbitrary test $$\phi_A(\boldsymbol{x})$$ such that $$\int \phi_A(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x} \leq \alpha$$,
  - i.e. level $$\alpha$$ test.
- Then the power of $$\phi_A(\boldsymbol{x})$$ is at most the power of the Neyman-Pearson test,
  - that is $$\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x} \geq \int \phi_A(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x}$$

### Proof

- 먼저 $$\int \underbrace{\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right)}_{T_1} \underbrace{\left(f_1(\boldsymbol{x})-\frac{f_0(\boldsymbol{x})}{t^*}\right)}_{T_2} d \boldsymbol{x} \geq 0$$​를 보인다.

  - NP와 arbitrary test가 같은 결정을 내렸다면 $$T_1=0$$이므로 성립한다.
  - NP와 arbitrary test가 다른 결정을 내렸다면 $$T_1$$과 $$T_2$$의 부호가 같으므로 성립한다.

- 이제 다음과 같이 전개가 가능하다.

  $$\begin{aligned}
  \int\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right) f_1(\boldsymbol{x}) d \boldsymbol{x} & \geq \frac{1}{t^*} \int\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right) f_0(\boldsymbol{x}) d \boldsymbol{x} \\
  & =\frac{1}{t^*}(\underbrace{\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}}_{=\alpha}-\underbrace{\int \phi_A(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}}_{\leq \alpha} \geq 0
  \end{aligned}$$​

- 위 결과를 통해 the power of the NP test가 the power of any other test보다 크다는 것을 알 수 있다.