---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/abs/2404.16112)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)

## Abstract

- Sequence modeling에서 RNN, LSTM을 사용했었음
- Transformer가 훌륭한 성능을 보여주었음
  - but $$O(N^2)$$ complexity,inductive bias handling이 어려움
- 본 논문에서는 State Space Model (SSM)를 크게 3가지 카테고리로 분류
  -  Gating architectures
  - Structural architectures
  - Recurrent architectures

## 1. Introduciton

- RNN
  - look at only the last state and current input for predicting the next state
  - gradient calculations being limited to the hidden state and current input
  - exploding or vanishing gradient problem
  - lack sufficient memory for long sequences 
- LSTM
  - complexity with their gating mechanisms
  - exhibit challenges in transfer learning
- Transformer
  - enable each token to interact with every other token in the input sequence
  - but $$O(N^2)$$ complexity

- State Space Model (SSM)
  - Understanding of State Space Models (SSMs) : mathematical fundamentals
  - Categorization and Recent Advances of SSMs : systematic categorization
  - Application of SSMs Across Domains : utility in domains
  - Performance Comparison of SSMs with Transformers : SSM과 Transformer 비교

## 2. Basics of State Space Model

- High-order를 first-order derivatives와 vector quantities로 representation
- Dynamics of  damped mass-spring system : $$m \frac{d^2 y(t)}{d t^2}+c \frac{d y(t)}{d t}+k y(t)=u(t)$$
  - $$u(t)$$ : 질량에 작용하는 외부 힘
  - $$y(t)$$ : 수직 위치
  - $$x(t)$$ : 이 방정식을 1차 미분과 벡터 양으로 표현하기 위해 도입하는 벡터

### 2.1. Spring Mass-Damper system

- State Variables
  - $$x_1$$ : equilibrium으로부터 질량의 위치
  - $$\dot{x_1}$$ : 질량의 속도
- System Dynamics
  - 뉴턴의 제 2법칙으로 표현하면 $$m \ddot{x}_1=-k x_1-c \dot{x}_1$$
  - $$\ddot{x_1}$$는 질량의 가속도, $$-kx_1$$은 위치에 비례하는 스프링의 힘,
  - $$c\dot{x_1}$$은 속도에 비례하는 damping force (운동 에너지 감쇠시키는 힘)
- State-Space Formulation
  - State vector $$x \in \mathbb R^n$$ : 시스템의 내부 상태 변수
  - Input vector $$u\in \mathbb R^m$$ : 시스템에 대한 제어 또는 외부 입력
  - Output vector $$y \in \mathbb R^p$$ : 관심 있는 측정 가능한 양
  - System dynamics : 일차 미분 방정식으로 표현 $$\dot{\mathbf x}=\mathbf A \mathbf x+\mathbf B \mathbf u$$
    - $$\mathbf x=\left[x_1, \dot{x}_1\right]^T$$는 state vector, $$\mathbf u$$는 input,
    - $$\mathbf A\in\mathbb R^{n \times n}$$는 dynamic matrix $$\mathbf A=\left[\begin{array}{cc}
      0 & 1 \\
      -\frac{k}{m} & -\frac{c}{m}
      \end{array}\right]$$
    - $$\mathbf B \in \mathbb R^{n \times m}$$은 input matrix $$\mathbf B=\left[\begin{array}{c}
      0 \\
      \frac{1}{m}
      \end{array}\right] \dot{\mathbf x}$$
    - Output equation : $$\mathbf{y}=\mathbf{C x}+\mathbf{D u}$$
      - $$\mathbf{C} \in \mathbb{R}^{p \times n}$$는 output or sensor matrix
      - $$\mathbf{D} \in \mathbb{R}^{p \times m}$$는 feedthrough matrix

### 2.2. State Space Model

- Definition
  - Discrete-time dynamical system : 
    - $$x(t+1)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t=0,1,2,$$...
    - $$x(t) \in \mathbb{R}^n$$ : t시점에서 state
    - $$u(t) \in\mathbb{R}^p$$ : control variables
    - $$y(t) \in\mathbb{R}^k$$​ : specific outputs of interest
  - Continuous-time model
    - $$\frac{d}{d t} x(t)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t \geq 0$$.

![그림1](/assets/img/mamba/Mamba360/fig3.png)

- Model Formulation
  - Complexity 때문에 Multi-head self-attention 대신 SSM 사용
  - Continuous-time Latent State space는 linear ordinary differential equation으로 표현
    - $$\begin{aligned} \dot{x}(t) & =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
      y(t) & =\boldsymbol{C} x(t)+\boldsymbol{D} u(t) \end{aligned}$$, 
    - evolution parameter $$A \in \mathcal{R}^{N \times N}$$
    - projection parameter $$B \in \mathcal{R}^{N \times 1} \text { and } C \in \mathcal{R}^{N \times 1}$$
- Discrete-time SSM
  - continuous parameters $$A, B, C$$ 를 discrete하게 바꾸기 위해 time-scale parameter $$\Delta$$ 사용
  - 즉 $$\bar{A}=f_A(\Delta, A), \bar{B}=f_B(\Delta, A, B)$$​
  - $$\begin{array}{lll}
    x_k=\overline{\boldsymbol{A}} x_{k-1}+\overline{\boldsymbol{B}} u_k & \overline{\boldsymbol{A}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1}(\boldsymbol{I}+\Delta / 2 \cdot \boldsymbol{A}) & \\
    y_k=\overline{\boldsymbol{C}} x_k & \overline{\boldsymbol{B}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1} \Delta \boldsymbol{B} & \overline{\boldsymbol{C}}=\boldsymbol{C}\end{array}$$​.
  - 원래는 위처럼 생겼음
- Convolutional Kernel Representation
  - 하지만 위 식은 sequential nature 때문에 trainable하지 않음
  - 그래서 아래처럼 continuous convolution을 사용
  - $$\begin{array}{lll}
    x_0=\overline{\boldsymbol{B}} u_0 & x_1=\overline{\boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{B}} u_1 & x_2=\overline{\boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{B}} u_2 \\
    y_0=\overline{\boldsymbol{C} B} u_0 & y_1=\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{C} B} u_1 & y_2=\overline{\boldsymbol{C} \boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{C} B} u_2
    \end{array}$$.
  - vectorize하면 아래와 같음
  - $$\begin{aligned}
    y_k & =\overline{\boldsymbol{C A}}^k \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C A}}^{k-1} \overline{\boldsymbol{B}} u_1+\cdots+\overline{\boldsymbol{C} \boldsymbol{A B}} u_{k-1}+\overline{\boldsymbol{C} \boldsymbol{B}} u_k \\
    y & =\overline{\boldsymbol{K}} * u \\
    \overline{\boldsymbol{K}} \in \mathbb{R}^L: & =\mathcal{K}_L(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}}):=\left(\overline{\boldsymbol{C}}^i \overline{\boldsymbol{B}}\right)_{i \in[L]}=\left(\overline{\boldsymbol{C B}}, \overline{\boldsymbol{C} \boldsymbol{A B}}, \ldots, \overline{\boldsymbol{C}}^{L-1} \overline{\boldsymbol{B}}\right) .
    \end{aligned}$$.

## 3. Recent Advances in State Space Models

- Transformer의 limitations :
  - Computational Complexity
  - Large Memory Requirements : for storing embeddings and intermediate actiavations
  - Fixed Sequence Length : du to positional embeddings
  - Attention Mechanism Scalability : quadratic scaling with input length
  - Lack of Causality in Standard Attention : not inherently capture causality
- SSM의 categorization : 어떻게 long sequence를 다룰 것인가
  - Structured SSMs : based on S4 and variants
  - Recurrent SSMs : based on RNNs and variants
  - Gated SSMs : leveraging gating techniques
  - Miscellaneous SSMs : 기타 다양한 방법들

### 3.1. Structured SSMs

- S4, HiPPO, H3, Liquid-S4 등...

- long-range dependency를 효율적으로 파악하기 위해 다음과 같은 방법 사용 : 

  - polynomial projection operators

  - multi-input multi-output systems

  - and convolutional kernels

### 3.1.1. Structured State Space Sequence (S4)

- Higher-Order Polynomial Project Operator (HiPPO)
  - State and input transition matrices를 효율적으로 memorize
-  Diagonal Plus Low-Rank Parametrization
  - SSM matrix (A)의 rank를 낮게 해서 diagonalizability and stability 보장
- Efficient (convolutional) Kernel Computation
  - FFT와 iFFT 사용해서 complexity를 $$𝑂(𝑁 log(𝑁))$$로 만듬

### 3.1.2. High-Order Polynomial Projection Operators (HiPPO)

- S4에 사용된 행렬의 수학적인 해석을 제공
- 4가지의 변형을 사용하는데,
  - the truncated Fourier basis polynomial (Hippo-FouT)
  - based on Lagurre polynomials(LagT)
  - based on Legendre polynomials(LegT)
  - based on Legendre polynomials with a sliding window(LegS)

### 3.1.3. Hungry Hungry HiPPO (H3)

- SSM에서의 2개의 challenges
  - 첫째, difficulty in recalling earlier tokens
    - 시퀀스 내에서 이전 토큰을 기억하는 데 어려움
  - 둘째, difficult in comparing the tokens across different sequences
    - 서로 다른 시퀀스에서 토큰을 비교하는 데 어려움
- 극복하기 위한 새로운 방법의 3가지 핵심 요소
  - Multiplicative Interactions가 있는 Stacked SSMs
    -  stacking two SSMs with multiplicative interactions between their input and output projections
  - 학습 효율성을 위한 FlashConv
    - FFT를 사용하여  training efficiency 향상
  - Scaling을 위한 State-Passing
    - effectively splits the input into the largest possible chunks that can fit

### 3.1.4. Global Convolution

- 원래는 input만큼 긴 conv kernel을 hidden state matrix에 곱했는데 - 불안정함
- 이 conv kernel을 parametrizing하는 방법을 제안
- 일반적으로 conv kernel은 FFT를 사용하는데, 느릴 수가 있어서 IO-aware algorithm 사용

### 3.1.7. LDStack

- RNN이 다중 입력 다중 출력(MIMO)  Linear Dynamical System(LDS)으로 표현될 수 있음
- 이 때 Parallel scan이 사용됨
- 즉  Single Input Multiple Outputs (SIMO) LDS를 합쳐서  MIMO LDS를 approximate
  - essential characteristics를 유지하면서도 계산은 simple해짐
- LDS를 time-varying state space models로 볼 수 있음

### 3.1.8 S5

- RNN을 다중 입력 다중 출력 선형 동적 시스템(LDS)으로 모델링한 LDStack을 state space models (SSMs)으로 확장
- LDStack과 달리, S5 계층은 여러 입력 및 출력을 동시에 처리

## 3.2. Gated SSMs

- FFT 연산 최적화를 위해 gating units를 사용
- Toepliz NN은 position-encoded Toeplitz matrix로 token mixing
- Mamba는 gated MLP로 SSM의 compoutational inefficiency 극복하고자 함
- (무슨 말 ?) 더 읽어보자

### 3.2.3. Toeplitz Neural Network (TNN)

- Transformer의 **attention-mechanism**과 **positional embedding**을 개선
- position-encoded Toeplitz matrix를 사용하여 token-pair 관계 파악
  - space-time complexity를 $$O(NlogN)$$으로 줄임
  - Relative Position Encoder (RPE)로 상대적 위치를 생성해서 parameters가 input length에 독립적이게 함

### 3.2.4. Mamba

- Transformer의 quadratic computational and memory complexity에 주목
- 특히 SSM은  addressing tasks (selective copying, induction head)에서 비효율적이었음
- Mamba가 이 문제를 푸는 방법은 :
  - novel parametrization approach for SSMs based on input characteristics
  - incorporating a simple selection mechanism
  - efficient hardware-aware algorithm based on selective scan
  - gated technique to reduce the dimensionality of global kernel operations
  - combine gated MLP[93] with the SSM module

## 4. Applications of State Space Models

### 4.1. Language Domain (long sequence)

- 원래는 Transformer 많이 썼는데 $$O(N^2)$$ quadratic complexity $$\to$$ long sequence 불가능
- 그래서  State Space Models (SSMs)이 등장
  - input data를 fixed-size latent state에 표현
  - 하지만 그러다보니 capability to retrieve and copy에서 trade-off

![그림1](/assets/img/mamba/Mamba360/table2.png)

### 4.2. Vision domain

- Vision Mamba나 SiMBA와 같은 Vision-specific Mamba
  - utilize bidirectional and visual state space models
- SiMBA
  - sequence length and channel dimensions이 꼭 perfect square dimensions이 아니어도 됨
  - pyramid version of the transformer architecture (성능 향상)

![그림1](/assets/img/mamba/Mamba360/table3.png)

### 4.7. Time Series Domain

- 옛날에는 ARIMA 쓰다가 Transformer 등장하면서 variants가 많이 나옴
  - Informer, FEDFormer, PatchTST...
  - 하지만 여전히 attention complexity 때문에 long-range dependency 못잡음
- 그래서 SSM 모델인 Timemachine, SiMBA, MambaMix 등장

![그림1](/assets/img/mamba/Mamba360/table11.png)

![그림1](/assets/img/mamba/Mamba360/table14.png)

## 6. Conclusion

- SSM은 3가지 범주로 분류 가능 (structured, gated, and recurrent)
- 아직 Transformer가 더 잘하는 영역이 있긴 하지만 (맥락에서 정보를 검색하는 작업 등)
  - SiMBA는 트랜스포머와 Mamba 아키텍처를 결합해서 Time series에서 SOTA
- SSM을 large network로 안정적으로 확하는 것이 아직 해결되지 않은 문제