<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-04-12T19:25:52+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">(Dish-TS) A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting (AAAI 2023)</title><link href="http://localhost:4000/timeseries/2024-04-12-Dish-TS/" rel="alternate" type="text/html" title="(Dish-TS) A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting (AAAI 2023)" /><published>2024-04-12T00:00:00+09:00</published><updated>2024-04-12T19:25:51+09:00</updated><id>http://localhost:4000/timeseries/Dish-TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-04-12-Dish-TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Distribution shift in TS : series distribution changes over time</li>
  <li>기존 연구들은 the quantification of distribution 정도</li>
  <li>Distribution shift in TS는 2개 카테고리
    <ul>
      <li><strong>intra-space shift</strong> : the distribution within the input-space keeps shifted over time</li>
      <li><strong>inter-space shift</strong> : that the distribution is shifted btw/ input-space and output-space</li>
    </ul>
  </li>
  <li>Dish-TS : neural paradigm for alleviating distribution shift in TSF
    <ul>
      <li>CONET : can be any NN, input sequences into learnable distribution coefficients</li>
      <li>Dual-CONET : separately learn the distribution of input- and output-space</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<p><img src="/assets/img/timeseries/Dish-TS/fig1.png" alt="사진1" /></p>

<ul>
  <li>TS의 non-stationarity(distribution shift over time)는 예측 성능을 방해</li>
  <li><strong>intra-space shift</strong> :  TS distribution changes over time</li>
  <li><strong>inter-space shift</strong> :  Distribution shift btw/ input-space (lookbacks) and output-space (horizons)</li>
  <li>대표적인 alleviate distribution shift solution : RevIN.
    <ul>
      <li>Quantifying true distribution with fixed statistics (e.g., mean and std.)
        <ul>
          <li>But, unreliable (limited in expressiveness for representing the true distribution)</li>
          <li>Different sampling frequencies provide different statistics</li>
        </ul>
      </li>
      <li>Strong assumption : the lookbacks and horizons share the same statistical properties</li>
      <li>But,  always a variation in distribution btw/ input-space and output-space</li>
    </ul>
  </li>
  <li>Dish-TS는 RevIN으로부터 영감을 받은 만큼 전체적인 구조는 유사하다
    <ul>
      <li>two-stage process : normalizing \(\to\) forecasting \(\to\) denormalizing</li>
      <li>CONET : window \(\to\) two learnable coefficients:
        <ul>
          <li>a level coefficient and a scaling coefficient</li>
          <li>to illustrate series overall scale and fluctuation</li>
        </ul>
      </li>
      <li>Dual-CONET
        <ul>
          <li>BACKCONET : coefficients to estimate the distribution of input-space (lookbacks)</li>
          <li>HORICONET : coefficients to infer the distribution of output-space (horizons)</li>
        </ul>
      </li>
      <li>Prior-knowledge : HORICONET 학습할 때 prior 줘서 output-space를 잘 infer(predict) 하도록</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>Models for Time Series Forecasting
    <ul>
      <li>ARMA, BEATS, Transformer, Informer, Autoformer, …</li>
    </ul>
  </li>
  <li>Distribution Shift in Time Series Forecasting
    <ul>
      <li>Adaptive Norm :  puts z-score normalization on series by the computed global statistics</li>
      <li>DAIN (Passalis et al. 2019) :  applies nonlinear NN to adaptively normalize the series</li>
      <li>RevIN (Kim et al. 2022) : instance normalization to reduce series shift</li>
    </ul>
  </li>
  <li>대부분의 연구가 static statistics 사용해서 normalizing한다. (inter-space shift 고려 안함)</li>
</ul>

<h2 id="3-problem-formulations">3. Problem Formulations</h2>

<ul>
  <li>
    <p>Time Series Forecasting</p>

    <ul>
      <li>Formula : \(\left(x_{t: t+H}^{(1)}, \cdots, x_{t: t+H}^{(N)}\right)^T=\mathscr{F}_{\Theta}\left(\left(x_{t-L: t}^{(1)}, \cdots, x_{t-L: t}^{(N)}\right)^T\right)\)​</li>
      <li>\(\mathscr{F}_{\Theta}: \mathbb{R}^{L \times N} \rightarrow \mathbb{R}^{H \times N}\),   \(\Theta\) :  forecasting model parameters</li>
    </ul>
  </li>
  <li>
    <p>Distribution Shift in Time Series</p>

    <ul>
      <li>intra-space shift : \(\mid d\left(\mathcal{X}_{\text {input }}^{(i)}(u), \mathcal{X}_{\text {input }}^{(i)}(v)\right)\mid &gt;\delta\)​</li>
      <li>
        <p>inter-space shift : \(\mid d\left(\mathcal{X}_{\text {input }}^{(i)}(u), \mathcal{X}_{\text {output }}^{(i)}(u)\right)\mid &gt;\delta\)</p>
      </li>
      <li>\(\mathcal{X}_{\text {input }}^{(i)}(u)\)는 \(t=u\) 시점으로부터 과거 방향으로 \(L\) 길이의 lookback window</li>
      <li>\(\mathcal{X}_{\text {output }}^{(i)}(u)\)는 \(t=u\) 시점으로부터 미래 방향으로 \(H\) 길이의 horizon window</li>
    </ul>
  </li>
</ul>

<h2 id="4-dish-ts">4. Dish-TS</h2>

<h3 id="41-overview">4.1. Overview</h3>

<p><img src="/assets/img/timeseries/Dish-TS/fig2.png" alt="사진2" /></p>

<ul>
  <li>CONET :  input series \(\to\)​​ coefficients (for distribution measurement)</li>
  <li>RevIN처럼 two-stage process
    <ul>
      <li>BACKCONET : transformed the lookbacks (before forecasting model)</li>
      <li>HORICONET : transformed the forecasting results
        <ul>
          <li>HORICONET can be trained in a prior knowledgeinduced fashion (4.4에서 설명)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-dual-conet-framework">4.2. Dual-Conet Framework</h3>

<ul>
  <li>기존 연구들은 mean, std로 distribution을 measure \(\to\)​ unreliable (Different frequencies different statistics)</li>
  <li>기본 CONET구조
    <ul>
      <li>\(\varphi, \xi=\operatorname{CONET}(x)\) : any NN (can non-linear mapping)
        <ul>
          <li>\(\varphi \in \mathbb{R}^1\) : level coefficient (overall scale of input series)</li>
          <li>\(\xi \in \mathbb{R}^1\) : scaling coefficient (fluctuation scale)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Mutivariate forecasting을 위한 Dual-CONET
    <ul>
      <li>\(\begin{aligned}
&amp; \varphi_{b, t}^{(i)}, \xi_{b, t}^{(i)}=\operatorname{BACKCONET}\left(x_{t-L: t}^{(i)}\right), i=1, \cdots, N \\
&amp; \varphi_{h, t}^{(i)}, \xi_{h, t}^{(i)}=\operatorname{HORICONET}\left(x_{t-L: t}^{(i)}\right), i=1, \cdots, N
\end{aligned}\)​
        <ul>
          <li>\(\varphi_{b, t}^{(i)}, \xi_{b, t}^{(i)} \in \mathbb{R}^1\) : level, scaling coefficients for lookbacks</li>
          <li>\(\varphi_{h, t}^{(i)}, \xi_{h, t}^{(i)} \in \mathbb{R}^1\) : level, scaling coefficients for horizons</li>
        </ul>
      </li>
      <li>BACKCONET과 HORICONET 둘다 input이 “t 시점에서 L 길이의 historical series”이다 !</li>
    </ul>
  </li>
  <li>Integrating Dual-Conet into Forecasting
    <ul>
      <li>Final transformed forecasting results : \(\hat{x}_{t: t+H}^{(i)}=\xi_{h, t}^{(i)} \mathscr{F}_{\Theta}\left(\frac{1}{\xi_{b, t}^{(i)}}\left(x_{t-L: t}^{(i)}-\varphi_{b, t}^{(i)}\right)\right)+\varphi_{h, t}^{(i)}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-a-simple-and-intuitive-instance-of-conet">4.3. A Simple and Intuitive Instance of Conet</h3>

<ul>
  <li>실제 CONET에서의 연산은 다음과 같다.</li>
  <li>lookback level coefficient : \(\varphi_{b, t}^{(i)}=\sigma\left(\sum_{\tau=1}^{\operatorname{dim}\left(\mathbf{v}_{b, i}^{\ell}\right)} \mathbf{v}_{b, i \tau}^{\ell} x_{\tau-L+t}^{(i)}\right),\)</li>
  <li>horizons level coefficient : \(\varphi_{h, t}^{(i)}=\sigma\left(\sum_{\tau=1}^{\operatorname{dim}\left(\mathbf{v}_{h, i}^{\ell}\right)} \mathbf{v}_{h, i \tau}^{\ell} x_{\tau-L+t}^{(i)}\right)\)​</li>
  <li>lookback scaling coefficient : \(\xi_{b, t}^{(i)}=\sqrt{\mathbb{E}\left(x_t^{(i)}-\varphi_{b, t}^{(i)}\right)^2}\)</li>
  <li>horizons scaling coefficient : \(\xi_{h, t}^{(i)}=\sqrt{\mathbb{E}\left(x_t^{(i)}-\varphi_{h, t}^{(i)}\right)^2}\)</li>
</ul>

<h3 id="44-prior-knowledge-induced-training-strategy">4.4. Prior Knowledge-Induced Training Strategy</h3>

<ul>
  <li>HORICONET은 미래의 정보인 \(\mathcal{X}_{\text {output }}^{(i)}\)의 분포를 infer(predict)해야 하기 때문에 intractable하다.</li>
  <li>그러므로 prior(mean of horizons)을 soft-target으로 줘서 학습의 난이도를 낮춘다.</li>
  <li>final loss는 \(\sum_{k=1}^K \sum_{i=1}^N[\left(\hat{x}_{t_k: t_k+H}^{(i)}-x_{t_k: t_k+H}^{(i)}\right)^2+\underbrace{\left.\alpha\left(\frac{1}{H} \sum_{t=t_k+1}^{t_k+H} x_t^{(i)}-\varphi_{h, t_k}^{(i)}\right)^2\right]}_{\text {Prior Knowledge Guidance }}\)이다.
    <ul>
      <li>MSE term에 prior knowledge를 \(\alpha\)의 weight로 준다.</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiment">5. Experiment</h2>

<p><img src="/assets/img/timeseries/Dish-TS/table1.png" alt="사진3" /></p>

<p><img src="/assets/img/timeseries/Dish-TS/table2.png" alt="사진4" /></p>

<p><img src="/assets/img/timeseries/Dish-TS/table3.png" alt="사진5" /></p>

<p>Dish-TS를 적용하면 Informer, Autoformer, N-BEATS의 성능이 향상되고, 그 정도는 RevIN보다 크다.</p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Systematically summarize the distribution shift in time series forecasting</li>
  <li>as intra-space shift and interspace shift.</li>
  <li>Dish-TS better alleviates the two shift
    <ul>
      <li>prior knowledge-induced training strategy, for effectiveness</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2023](https://arxiv.org/pdf/2302.14829.pdf)]]></summary></entry><entry><title type="html">Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping</title><link href="http://localhost:4000/timeseries/2024-04-04-RTSF/" rel="alternate" type="text/html" title="Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping" /><published>2024-04-04T00:00:00+09:00</published><updated>2024-04-05T09:40:54+09:00</updated><id>http://localhost:4000/timeseries/RTSF</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-04-04-RTSF/"><![CDATA[<ul>
  <li>이전에 review했던 <a href="https://arxiv.org/abs/2205.13504">DLinear</a> paper에서는 ‘Time series의 properties가 무엇인지’, 그리고 ‘왜 Transformer-based model이 완벽할 수 없는지’에 집중했다면, 본 논문은 ‘왜 linear mapping이 단순한데도 성능이 좋은지’에 집중한다.</li>
</ul>

<h2 id="abstract">Abstract</h2>

<ul>
  <li>Linear mapping is critical to prior long-term time series forecasting efforts</li>
  <li>RevIN (reversible normalization) and CI (Channel Independent) play a vital role for performance</li>
  <li>Linear mapping can effectively capture periodic features in TS</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer-based model
    <ul>
      <li>Non-autoregressive methods (to capture long-term temporal correlations)</li>
      <li><a href="https://arxiv.org/pdf/2012.07436.pdf">Informer(AAAI 2021)</a>, <a href="https://arxiv.org/pdf/2106.13008.pdf">Autoformer(NeurIPS 2021)</a>, <a href="https://arxiv.org/pdf/2201.12740.pdf">Fedformer(PMLR 2022)</a>, <a href="https://openreview.net/pdf?id=0EXmFzUn5I">Pyraformer(ICLR 2022)</a>, <a href="https://arxiv.org/pdf/2206.04038.pdf">Scaleformer(ICLR 2023)</a>, <a href="https://openreview.net/pdf?id=vSVLM2j9eie">Crossformer(ICLR 2023)</a></li>
      <li>However, autoregressive에 비해 non-autoregressive의 성능이 좋았던 것이지 transformer가 TS forecasting에 효과적인 것은 아님 <a href="https://arxiv.org/pdf/2205.13504.pdf">DLinear(AAAI 2022)</a></li>
    </ul>
  </li>
  <li>Subsequent approaches (patching)
    <ul>
      <li>Encoder-decoder(ex.transformer) 구조를 버리고 temporal feature extractor 모델링 (attention을 안쓴 건 아님)</li>
      <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/266983d0949aed78a16fa4782237dea7-Paper-Conference.pdf">SCINet(NeurIPS 2022)</a>, <a href="https://arxiv.org/pdf/2211.14730.pdf">PatchTST(ICLR2023)</a>, <a href="https://arxiv.org/pdf/2302.04501.pdf">MTS-Mixers(Arxiv 2023)</a>, <a href="https://openreview.net/pdf?id=ju_Uqw384Oq">Timesnet(ICLR 2023)</a></li>
      <li>But, adjustable hyper-parameters가 너무 많이 필요하다.</li>
    </ul>
  </li>
  <li>본 논문의 Questions
    <ul>
      <li>(1) Are temporal feature extractors effective for long-term time series forecasting?</li>
      <li>(2) What are the <u>underlying mechanisms</u> explaining the effectiveness of <u>linear mapping</u> in time series forecasting?</li>
      <li>(3) What are the limits of linear models and how can we improve them?</li>
    </ul>
  </li>
</ul>

<h2 id="2-problem-definition-and-experimental-setup">2. Problem Definition and Experimental Setup</h2>

<ul>
  <li>\(\mathbf{X}=\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n\right] \in \mathbb{R}^{c \times n}\) 으로 \(\mathbf{Y}=\left[\boldsymbol{x}_{n+1}, \boldsymbol{x}_{n+2}, \ldots, \boldsymbol{x}_{n+m}\right] \in \mathbb{R}^{c \times m}\) 예측(mapping)하는 함수 \(\mathcal{F}: \mathbf{X}^{c \times n} \mapsto \mathbf{Y}^{c \times m}\) 를 학습</li>
  <li><a href="https://arxiv.org/pdf/2211.14730.pdf">PatchTST(ICLR2023)</a>와 동일한 dataset split, Adam optimizer, Nvidia V100 32GB GPU 사용</li>
</ul>

<h2 id="3-are-temporal-feature-extractors-effective-">3. Are Temporal Feature Extractors Effective ?</h2>

<ul>
  <li>TSF 일반적인 Framework는 <strong>RevIN</strong> \(\to\) <strong>temporal feature extractor</strong>(Attention, MLP, Conv, …) \(\to\) <strong>linear projection</strong>
    <ul>
      <li>다른 모델들의 temporal feature extractor를 살펴보면 PatchTST(attention), MTS-Mixers(MLP), TimesNet(conv), SCINet(conv), …</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/RTSF/fig2.png" alt="사진1" /></p>

<ul>
  <li>Fig2 - RLinear : linear projection layer with RevIN</li>
  <li>Fixed random extractor : only initialize the temporal feature extractor randomly and do not update its parameters in the training phase</li>
  <li>Fig2는 RevIN이 예측 성능을 향상시킨다 정도를 보여줄 뿐. simple linear layer가 RevIIN 도움 받으면 PatchTST보다 성능이 좋다.</li>
</ul>

<p><img src="/assets/img/timeseries/RTSF/fig3.png" alt="사진2" /></p>

<ul>
  <li>Fig3는 복잡한 모델들이 결국에는 가장 왼쪽에 있는 simple linear layer의 weights와 비슷한 weights를 학습하게 됨을 보여준다.</li>
</ul>

<h2 id="4-theoretical-and-empirical-study-on-the-linear-mapping">4. Theoretical and Empirical Study on the Linear Mapping</h2>

<h3 id="41-roles-of-linear-mapping-in-forecasting">4.1. Roles of Linear Mapping in Forecasting</h3>

<ul>
  <li>
    <dl>
      <dt><strong>Linear mapping learns periodicity</strong></dt>
      <dd>single linear layer는 periodicity를 학습할 수 있다. (trend는 잘 학습하지 못한다.)</dd>
    </dl>

    <p>아래 가정들과 정리들의 의미를 이해해보자.</p>

    <ul>
      <li>
        <p>single linear layers : \(\mathbf{Y}=\mathbf X \mathbf{W}+\mathbf{b}\)  라 하자.</p>
      </li>
      <li>
        <p><strong>Assumption 1</strong>. A general time series \(x(t)\) can be disentangled into seasonality part \(s(t)\) and trend part \(f(t)\) with tolerable noise, denoted as \(x(t)=s(t)+f(t)+\epsilon\)</p>

        <ul>
          <li>즉 시계열 = seasonality + trend + noise로 분해할 수 있다는 의미이다.</li>
        </ul>
      </li>
      <li><strong>Theorem 1</strong>. Given a seasonal time series satisfying \(x(t)=s(t)=s(t-p)\) where \(p \leq n\) is the period, there always exists an analytical solution for the linear model as
        <ul>
          <li>\(\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n\right] \cdot \mathbf{W}+\mathbf{b}=\left[\boldsymbol{x}_{n+1}, \boldsymbol{x}_{n+2}, \ldots, \boldsymbol{x}_{n+m}\right]\) 이고</li>
          <li>\(\mathbf{W}_{i j}^{(k)}=\left\{\begin{array}{ll} 1, &amp; \text { if } i=n-k p+(j \bmod p) \\ 0, &amp; \text { otherwise } \end{array}, 1 \leq k \in \mathbb{Z} \leq\lfloor n / p\rfloor, b_i=0\right.\) 이다.</li>
          <li>input historical sequence의 길이가 주기보다 길다면 linear mapping은 periodicity를 예측할 수 있다는 의미이다.</li>
          <li>하지만 위 정리는 \(x(t)=s(t)=s(t-p)\) 즉 trend가 없는 경우에 해당한다.</li>
          <li><img src="/assets/img/timeseries/RTSF/fig4.png" alt="사진3" /></li>
          <li>Fig4는 linear model이 seasonality는 잘 학습하지만 trend가 있을 때에는 성능이 좋지 못함을 보여준다.</li>
        </ul>
      </li>
      <li>
        <p><strong>Theorem 2</strong>. Let \(x(t)=s(t)+f(t)\) where \(s(t)\) is a seasonal signal with period \(p\) and \(f(t)\) satisfies \(K\)-Lipschitz continuous. Then there exists a linear model as \(\mathbf{Y}=\mathbf X \mathbf{W}+\mathbf{b}\) with input horizon size \(n=p+\tau, \tau \geq 0\) such that \(\mid x(n+j)-\hat x(n+j) \mid \leq K(p+j), j=1, \ldots, m\).</p>

        <ul>
          <li>linear model의 trend term에 대한 forecasting error의 upper bound를 제시하는 정리이지만, trend에 대해 성능이 좋지 않다는 건 여전하다.</li>
          <li><img src="/assets/img/timeseries/RTSF/proof.png" alt="사진4" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-disentanglement-and-normalization"><strong>4.2. Disentanglement and Normalization</strong></h3>

<ul>
  <li>
    <p><strong>Problems in Disentanglement</strong></p>

    <ul>
      <li>
        <p>시계열에서 trend와 seasonality를 분리할 수 있으면 성능을 높일 수 있을 것</p>

        <ul>
          <li>
            <p>moving average(by an average pooling layer with a sliding window)로 trend를 분리할 수 있다.</p>
          </li>
          <li>
            <p>하지만, sliding window의 크기가 seasonality의 최대 주기보다 커야만 효과적이고</p>

            <p>average pooling layer를 사용할 때 input TS의 양 끝에 padding을 해줘야 하는데, 그러면 원본 시퀀스가 왜곡된다.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Turning trend into seasonality</strong></p>
    <ul>
      <li>Disentanglement의 핵심은 원본 TS에서 movind average를 빼는 것인데, 이건 normalization과 관련이 있다.
        <ul>
          <li>TS의 statistical information(평균, 분산)은 distribution shift로 인해 계속 바뀌기 때문에 RevIN이 사용된다.</li>
          <li>RevIN : Normalization the input \(\to\) Forecasting module \(\to\) Denormalization the output</li>
        </ul>
      </li>
      <li>하지만 input 데이터에 directly normalization하면 오히려 statistical information을 지우는 것과 같다.</li>
      <li><img src="/assets/img/timeseries/RTSF/fig5.png" alt="사진5" /></li>
      <li>RevIN의 경우에는 scaling을 하지만 periodicity는 바꾸지 않는다. 그리고 reversible하다.</li>
      <li><img src="/assets/img/timeseries/RTSF/fig6.png" alt="사진6" /></li>
      <li><img src="/assets/img/timeseries/RTSF/fig7.png" alt="사진7" /></li>
      <li>RevIN은 continuously changing trends를 multiple segments with a fixed and similar trend로 바꾼다.</li>
      <li>그러면 accumulated timesteps in the past로 인한  errors in trend prediction이 완화된다.</li>
    </ul>
  </li>
</ul>

<h2 id="5-experimental-evaluation">5. Experimental Evaluation</h2>

<h3 id="51-comparison-on-real-world-datasets">5.1 Comparison on Real-world Datasets</h3>

<p><img src="/assets/img/timeseries/RTSF/table3.png" alt="사진8" /></p>

<ul>
  <li>learning of periodicity via linear mapping, 그리고 efficiency of reversible normalization 덕분에 well-designed models보다 RLinear의 성능이 좋다.</li>
</ul>

<h3 id="52-when-linear-meets-multiple-periods-among-channels">5.2. When Linear Meets Multiple Periods among Channels</h3>

<p><img src="/assets/img/timeseries/RTSF/table4.png" alt="사진9" /></p>

<ul>
  <li>Multi-channel datasets의 경우에는 Channel Independent(CI) modeling으로 TS의 각 채널을 독립적으로 처리할 때 성능이 좋다.</li>
</ul>

<p><img src="/assets/img/timeseries/RTSF/fig10.png" alt="사진10" /></p>

<ul>
  <li>Channels가 많아지면 channel마다 periodicity가 달라져서 예측이 어려워지는데 input horizon을 늘리면 완화된다.</li>
</ul>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Linear mapping와 foreacsting methods가 학습하는 것은 비슷하다. (input historical observations에서 periodic patterns)</li>
  <li>RevIN과 Channel Independent는 periodicity를 단순하게 만들어 학습을 용이하게 하므로 성능 향상에 필요하다.</li>
  <li>Linear mapping은 MTS에 대해서도 input horizon만 충분하다면 예측 성능이 뛰어나다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2305.10721.pdf)]]></summary></entry><entry><title type="html">(Time-LLM) Time Series Forecasting by Reprogramming Large Language Models</title><link href="http://localhost:4000/timeseries/2024-04-01-TimeLLM/" rel="alternate" type="text/html" title="(Time-LLM) Time Series Forecasting by Reprogramming Large Language Models" /><published>2024-04-01T00:00:00+09:00</published><updated>2024-04-05T09:40:54+09:00</updated><id>http://localhost:4000/timeseries/TimeLLM</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-04-01-TimeLLM/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>CV, NLP는 single large model (또는 pre-trained foundation model)이 거의 모든 tasks에서 성능이 좋음</li>
  <li>반면 TS는 <strong>dat sparsity</strong> 때문에 tasks마다 모델 디자인이 다름</li>
  <li>본 논문에서는 TS와 NLP의 modality gap을 align하기 위해 Time-LLM을 제안</li>
  <li><strong>Prompt-as-Prefix</strong>(PaP, reprogramming the input TS) \(\to\) frozen LLM \(\to\) forecasting</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>LLM을 forecasting model로 발전시키기 위해 필요한 것들
    <ul>
      <li><strong>Generalizability</strong> : Capability for few-shot and zero-shot transfer learning, w/o pre-task retraining</li>
      <li><strong>Data efficiency</strong> : Performance new tasks with only a few examples(limited data)</li>
      <li><strong>Reasoning</strong> : Sophisticated reasoning \(\to\) learned higher-level concepts \(\to\) highly precise forecasting</li>
      <li><strong>Multimodal knowledge</strong> :  Diverse knowledge across modalities \(\to\) synergistic forecasting that fuses different data types</li>
      <li><strong>Easy optimization</strong> : Once on massive computing \(\to\) can be applied to forecasting tasks (without learning from scratch)</li>
    </ul>
  </li>
  <li><strong>Align the modalities of TS &amp; NLP</strong> : Why challenging ?
    <ul>
      <li>첫째로 NLP는 discrete tokens, TS는 본질적으로 continuous</li>
      <li>둘째로 TS reasoning 지식이 LLM pre-training 안에 없다.</li>
    </ul>
  </li>
  <li>So, <strong>Time-LLM</strong>
    <ul>
      <li>Core idea : TS input을 LLM이 활용하기 쉬운 <strong>text prototype</strong>으로 reprogramming하는 framework (backbone model은 그대로)</li>
      <li><strong>Prompt-as-Prefix (PaP)</strong> : 1) enrich the input TS with additional context and 2) providing task instructions in the modality of NLP</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>
<p><img src="/assets/img/timeseries/TimeLLM/fig1.jpeg" alt="사진1" /></p>
<ul>
  <li>TS models는 task-specific \(\to\) ex. ARIMA는 UTS를, LSTM은 seq를, TCN과 Transformer는 longer temporal dependencies를 위해 디자인 \(\to\) versatility and generalizability 부족</li>
  <li>In-modality Adaptation : Pre-training (representation learning) \(\to\)​ fine-tuning (for downstream tasks) ! But <strong>TS data sparsity.</strong>..</li>
  <li>Cross-modality Adaptation (multimodal fine tuning) : Voice2Series(2021)은 TS를 acoustic model에 맞게 editing했고 LLM4TS는 first supervised pre-training on time series, then task-specific fine-tuning</li>
  <li>Time-LLM은 1) input TS를 수정하지도 않고, 2) backbone LLM을 fine tuning하지도 않는다. LLM이 잠재력을 발휘할 수 있도록 TS를 reprogramming</li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>
<ul>
  <li>Goal : <strong>Reprogram an embedding-visible language foundation model for general time series forecasting</strong> without requiring any fine-tuning of the backbone model.</li>
  <li>\(f(\ \mathbf{X} \in \mathbb{R}^{N \times T}\ )= \hat{\mathbf{Y}} \in \mathbb{R}^{N \times H}\), \(f\)는 input TS를 이해하고 예측.
Loss : \(\frac{1}{H} \sum_{h=1}^H\left\|\hat{\mathbf{Y}}_h-\mathbf{Y}_h\right\|_F^2\)​
<img src="/assets/img/timeseries/TimeLLM/fig2.jpeg" alt="사진2" /></li>
  <li>3-main components
    <ul>
      <li>(1) input transformation</li>
      <li>(2) a pre-trained and frozen LLM</li>
      <li>(3) output projection</li>
    </ul>
  </li>
  <li>Step 1) Channel independence : MTS \(\to\)​ N개의 UTS <br />
Step 2) Normalization, Patching, Embedding prior <br />
Step 3) Augment the LLM’s Ts reasoning ability <br />
Step 4) Project to the final forecast \(\hat{\mathbf{Y}}^{i}\)</li>
  <li>Efficiency
    <ul>
      <li>only the parameters of the <strong>lightweight input transformation and output projection</strong> are updated. (backbone LLM is frozen)</li>
      <li>directly optimizing \(\to\) <strong>small set of TS and a few training epochs</strong></li>
      <li>for reduce memory footprint, <strong>off-the-shelf techniques (e.g., quantization)</strong></li>
    </ul>
  </li>
</ul>

<h3 id="31-model-structure">3.1. Model Structure</h3>
<ul>
  <li><strong>Input Embedding</strong>
    <ul>
      <li>step 1) RevIN \(\mathbf X^{(i)}\)</li>
      <li>step 2) Patching with length \(L_p\)
        <ul>
          <li>Total number of input patches : \(P=\left\lfloor\frac{\left(T-L_p\right)}{S}\right\rfloor+2\)</li>
          <li>Underlying motivations :
            <ul>
              <li>Better preserving local semantic information by aggregating local information into each patch</li>
              <li>Serving as tokenization to form a compact sequence of input tokens, reducing computational burdens.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>step 3) Embedding w/ simple linear layer
        <ul>
          <li>\(\mathbf{X}_P^{(i)} \in \mathbb{R}^{P \times L_p} \to \mathbf{\hat X}_P^{(i)} \in \mathbb{R}^{P \times d_m}\)​</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Patch Reprogramming</strong>
<img src="/assets/img/timeseries/TimeLLM/fig3.png" alt="사진3" />
    <ul>
      <li>Goal : to align the modalities of TS and natural language (TS 직접적인 수정 없이)</li>
      <li>How : pre-trained word embedding \(\mathbf E \in \mathbb R^{V \times D}\) in backbone.
        <ul>
          <li>But! no prior knowledge indicating which source token are directly relevant.</li>
          <li>So, simply leveraging small collection of text prototypes by linearly probing \(\mathbf E\), denoted as \(\mathbf E' \in \mathbb R^{V' \times D}, V^{\prime} \ll V\)</li>
          <li>: efficient &amp; allows for the adaptive selection of relevant source information</li>
        </ul>
      </li>
      <li>Multi-head cross-attention
        <ul>
          <li>query matrices \(\mathbf{Q}_k^{(i)}=\hat{\mathbf{X}}_P^{(i)} \mathbf{W}_k^Q\)
key matrices \(\mathbf{K}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^K\)
value matrices \(\mathbf{V}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^V\)
where \(\mathbf{W}_k^Q \in \mathbb{R}^{d_m \times d}\) and \(\mathbf{W}_k^K, \mathbf{W}_k^V \in \mathbb{R}^{D \times d}\)
\(D\) is the hidden dimension of the backbone model, and \(d=\left\lfloor\frac{d_m}{K}\right\rfloor\)</li>
          <li>i-layer : \(\mathbf{Z}_k^{(i)}=\operatorname{ATTENTION}\left(\mathbf{Q}_k^{(i)}, \mathbf{K}_k^{(i)}, \mathbf{V}_k^{(i)}\right)=\operatorname{SOFTmax}\left(\frac{\mathbf{Q}_k^{(i)} \mathbf{K}_k^{(i) \top}}{\sqrt{d_k}}\right) \mathbf{V}_k^{(i)}\)</li>
          <li>By aggregating each \(\mathbf{Z}_k^{(i)} \in \mathbb{R}^{P \times d}\) in every head, \(\mathbf{Z}^{(i)} \in \mathbb{R}^{P \times d_m}\),
then linearly projected \(\to \mathbf{O}^{(i)} \in \mathbb{R}^{P \times D}\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Patch-as-Prefix</strong> (constraints…)
    <ul>
      <li>natural language로 표현된 TS를 예측</li>
      <li>Constraints :
        <ul>
          <li>LLM은 <strong>high-precision numerals</strong> 연산에 sensitivity 떨어지고</li>
          <li>LLM별로 서로 다른 후처리가 필요 ex.  0.61이 [’ 0 ‘, ‘, ‘, 6 ‘, ‘ 1 ‘] 또는 [’ 0 ‘, ‘, ‘, ‘61’]로 표시</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Prompt-as-Prefix</strong> (avoid constraints !)
    <ul>
      <li>(1) Dataset context : input TS의 essential background information을 LLM에 제공
(2) Task instruction : task에 따른 patch embedding 가이드를 LLM에 제공
(3) Input statistics : Enrich the input TS with additional statistics (trends, lags, …)</li>
    </ul>
  </li>
  <li><strong>Output Projection</strong>
    <ul>
      <li>Prefixal part 버리고 output representation 얻어서 flatten하고 linear projection  \(\to \hat{\mathbf{Y}}^{(i)}\)</li>
    </ul>
  </li>
</ul>

<h2 id="4-main-results">4. Main Results</h2>
<h3 id="41-42-longshort-term-forecasting">4.1, 4.2. Long/Short-term Forecasting</h3>
<p><img src="/assets/img/timeseries/TimeLLM/table12.png" alt="사진4" /></p>
<h3 id="43-few-shot-forecasting">4.3. Few-shot Forecasting</h3>
<p><img src="/assets/img/timeseries/TimeLLM/table3.png" alt="사진5" />
<img src="/assets/img/timeseries/TimeLLM/table4.png" alt="사진6" /></p>
<h3 id="44-zero-shot-forecasting">4.4. Zero-shot Forecasting</h3>
<p><img src="/assets/img/timeseries/TimeLLM/table5.png" alt="사진7" /></p>
<h3 id="45-model-analysis">4.5. Model Analysis</h3>
<p><img src="/assets/img/timeseries/TimeLLM/table67.png" alt="사진8" /></p>

<h2 id="5-conclusion-and-future-work">5. Conclusion and Future work</h2>
<ul>
  <li>TS를 test prototype으로 reprogramming해서 frozen LLM 통과</li>
  <li>Prompt-as-Prefix로 guidance를 LLM에 제공</li>
  <li>TS forecasting을 language task로 casting</li>
  <li>결론적으로 Patching + Prompting으로 성능을 더 올린 모델</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/pdf/2310.01728.pdf)]]></summary></entry><entry><title type="html">(Survey paper) Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects (Arxiv 2023)</title><link href="http://localhost:4000/timeseries/2024-03-25-SSL4TS/" rel="alternate" type="text/html" title="(Survey paper) Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects (Arxiv 2023)" /><published>2024-03-25T00:00:00+09:00</published><updated>2024-03-25T19:43:09+09:00</updated><id>http://localhost:4000/timeseries/SSL4TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-25-SSL4TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Self-supervised learning (SSL) : for reducing the dependence of labeled data</li>
  <li>will review SOTA SSL for TS</li>
  <li>will provide taxonomy of SSL for TS : generative-based, contrastive-based, adversarial-based
    <ul>
      <li>intuitions, main frameworks, (dis)advantages</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>To extract useful and informative features, (=hidden patterns and features of the data)
    <ul>
      <li>SSL utilizes pretext tasks to derive supervision signals from unlabeled data (=creating valuable representation for downstream tasks)</li>
    </ul>
  </li>
  <li>Challenges:
    <ul>
      <li>대부분의 pre-text tasks(이미지, 언어, …) designed 모델들은 TS의 unique properties(seasonality, trend, frequency, …)를 고려하기 위한 것이 아니다.</li>
      <li>SSL을 위해 data augmentation할 때 rotation이나 crop과 같은 방식은 TS의 temporal dependency를 학습하기 어렵게 만든다.</li>
      <li>MTS에서 몇몇의 dimension(=channel, variate)에만 유용한 정보가 있는 경우에는 이를 고려해야 한다.</li>
    </ul>
  </li>
  <li>SSL for TS:
    <ul>
      <li>Generative-based : autoregressive-based forecasting / auto-encoder-based reconstruction / diffusion-based generation</li>
      <li>Contrastive-based : sampling contrast / prediction contrast / augmentation contrast / prototype contrast / expert knowledge contrast</li>
      <li>Adversarial-based : generation and imputation and auxiliary representation enhancement
<img src="/assets/img/timeseries/SSL4TS/fig1.png" alt="사진1" />
<img src="/assets/img/timeseries/SSL4TS/fig5.png" alt="사진5" /></li>
    </ul>
  </li>
</ul>

<h2 id="2-related-surveys">2. Related Surveys</h2>

<h3 id="21-definition-of-time-series-data">2.1. Definition of time series data</h3>
<ul>
  <li>Univariate TS : \(X=\left(x_0, x_1, x_2, \ldots, x_t\right)\) where \(x_i\) is  the point at timestamp \(i\)​</li>
  <li>Multivariate TS : \(\mathbf{X}=\left[X_0, X_1, X_2, \ldots, X_p\right]\), where \(p\) is the number of variables</li>
  <li>Multiple multivariate TS : \(\mathcal{X}=\left\{\mathbf{X}_0, \mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n\right\}\), where \(n\) is the number of multivariate TS</li>
</ul>

<h3 id="22-surveys-on-ssl">2.2. Surveys on SSL</h3>
<ul>
  <li>pretext tasks 의 핵심은 pseudo-supervision signals을 만드는 것</li>
  <li>Basic intuition : pull positive samples closer and push negative samples away
    <ul>
      <li>positive and negative samples : multisensory signals / data augmentation(noise injection, …) / local-global consistency / temporal consistency</li>
      <li>pretext task : Context prediction / Instance discrimination / Instance generation</li>
      <li>model architecture</li>
      <li>training loss : contrastive loss functions generally include scoring functions (cosine similarity), energy-based margin functions (pair loss and triplet loss), probabilistic NCE-based functions, and mutual information based functions</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/SSL4TS/table3.png" alt="사진7" /></p>

<h2 id="3-generative-based-models">3. Generative-based Models</h2>
<ul>
  <li>pretext task : to generate the expected data based on a given view of the data
<img src="/assets/img/timeseries/SSL4TS/fig2.png" alt="사진2" /></li>
</ul>

<h3 id="31-autoregressive-based-forecasting">3.1. Autoregressive-based forecasting</h3>
<ul>
  <li>Forecasting : \(\hat{x}_{[t+1: t+K]}=f\left(x_{[1: t]}\right)\)</li>
  <li>Loss : \(\mathcal{L}=\frac{1}{K} \sum_{k=1}^K\left(\hat{x}_{[t+k]}-x_{[t+k]}\right)^2\)​</li>
  <li>RNN-based
    <ul>
      <li>Adv : Long-term dependencies / Adaptable to varying lengths / Global context information extraction</li>
      <li>Dis-adv : Vanishing or exploding gradients / Computational efficiency</li>
      <li>ex : <a href="https://proceedings.neurips.cc/paper/2020/file/97e401a02082021fd24957f852e0e475-Paper.pdf">THOC</a> : Temporal Self-supervision(TSS), which takes the L-layer dilated RNN with skip-connection structure \(\to\)​ different resolutions at the same time</li>
    </ul>
  </li>
  <li>CNN-based
    <ul>
      <li>Adv : Local pattern extraction / Computational efficiency</li>
      <li>Dis-adv : Long-termdependencies / Information loss</li>
    </ul>
  </li>
  <li>GNN-based
    <ul>
      <li>Adv : Adaptability to graph structures / Dynamic relationship capture</li>
      <li>Dis-adv : Computational and storage complexity</li>
      <li>ex : <a href="https://arxiv.org/pdf/2106.06947.pdf">GDN</a> : the correlation among variables</li>
    </ul>
  </li>
</ul>

<h3 id="32-autoencoder-based-reconstruction">3.2. Autoencoder-based reconstruction</h3>
<ul>
  <li>basic autoencoder (BAE)
    <ul>
      <li>Encoder \(x \to z\) , Decoder \(z \to \hat {x}\)  / Loss : \(\mathcal{L}=\|x-\tilde{x}\|_2\)</li>
      <li>E(), D() 같이 train, D() 제거하고 E()를 feature extractor로 사용</li>
      <li>ex. <a href="https://arxiv.org/pdf/1706.08838.pdf">TimeNet</a>, <a href="https://www.nature.com/articles/s41598-019-55320-6">PT-LSTM-SAE</a>, <a href="https://arxiv.org/pdf/1810.10107.pdf">Autowarp</a></li>
    </ul>
  </li>
  <li>Denoising autoencoder (DAE)
    <ul>
      <li>\(x_n=\mathcal{T}(x), \quad Z=E\left(x_n\right), \quad \tilde{x}=D(z)\), \(\quad \mathcal{T}\): add noise  / Loss : \(\mathcal{L}=\|x-\tilde{x}\|_2\)</li>
    </ul>
  </li>
  <li>Mask autoencoder (MAE)
    <ul>
      <li>Intuition : pre-training할 때에 input의 일부를 mask하고 un-mask part 보고 예측</li>
      <li>form : \(\begin{gathered}x_m=\mathcal{M}(x), \quad z=E\left(x_m\right), \quad \tilde{x}=D(z),\ \mathcal{L}=\mathcal{M}\left(\|x-\tilde{x}\|_2\right),\end{gathered}\)</li>
      <li>TS에서는 time-step-wise masking은 interpolation 해버리기 때문에, segment-wise masking or variable-wise masking</li>
      <li>ex. <a href="https://dl.acm.org/doi/pdf/10.1145/3534678.3539329">TARNet</a> : 중요한 역할을 하는 data를 선정하고, 해당 데이터를 masking 하여 reconstruction</li>
    </ul>
  </li>
  <li>Variational autoencoder (VAE)
    <ul>
      <li>Encoder \(x \to P(z\mid x)\), instead of explicit representation \(z\), Decoder는 sampling from \(P(z \mid x)\)​</li>
      <li>\(P(z \mid x)=E(x), \quad z=\mathcal{S}(P(z \mid x)), \quad \tilde{x}=D(z)\) / Loss : \(\mathcal{L}=\|x-\tilde{x}\|_2+\operatorname{KL}(\mathcal{N}(\mu, \delta), \mathcal{N}(0, I))\)</li>
      <li>ex. <a href="https://dl.acm.org/doi/10.1145/3447548.3467075">InterFusion</a>, <a href="https://arxiv.org/pdf/2101.10318.pdf">mTANs</a>, <a href="https://arxiv.org/pdf/2107.11350.pdf">HetVAE</a>(extract seasonal and trend via VAE)</li>
    </ul>
  </li>
</ul>

<h3 id="33-diffusion-based-generation">3.3. Diffusion-based generation</h3>
<ul>
  <li>reverse transition kernel을 NN으로 approximate</li>
  <li>Denoising diffusion probabilistic models <a href="https://arxiv.org/pdf/2006.11239.pdf">DDPM</a>,
    <ul>
      <li>\(p_\theta\left(\boldsymbol{x}_{\boldsymbol{t}-\mathbf{1}} \mid \boldsymbol{x}_{\boldsymbol{t}}\right)=$ $\mathcal{N}\left(\boldsymbol{x}_{t-1} ; \mu_\theta\left(\boldsymbol{x}_t, t\right), \sum_\theta\left(\boldsymbol{x}_t, t\right)\right)\) 일 때,</li>
      <li>Jensen’s inequality에 의해, training loss는 \(\begin{array}{r}\mathbf{K L}\left(q\left(\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_T\right) \| p_\theta\left(\boldsymbol{x}_0, \boldsymbol{x}_1, \ldots, \boldsymbol{x}_T\right)\right) \geq \mathbf{E}\left[-\log p_\theta\left(\boldsymbol{x}_0\right)\right]+\text { const. }\end{array}\)</li>
    </ul>
  </li>
  <li>Score matching diffusion models <a href="https://arxiv.org/pdf/1907.05600.pdf">score matching</a>
    <ul>
      <li>per- turb data with a sequence of Gaussian noise \(\to\) estimating the score function for all the noisy data</li>
    </ul>
  </li>
</ul>

<h2 id="4-contrastive-based-methods">4. Contrastive-based Methods</h2>
<ul>
  <li>positive는 가깝게, negative는 멀게 representation하도록 학습 \(\to\) positive / negative 정하는 룰이 중요함
<img src="/assets/img/timeseries/SSL4TS/fig3.png" alt="사진3" /></li>
  <li><strong>Sampling contrast</strong>
    <ul>
      <li>가정 : 시점이 가까울수록 유사도가 높을 것</li>
      <li>하지만 실제로 꼭 그런 것은 아니다. contextual information으로 pos / neg 정하기가 쉽지 않다</li>
      <li>ex. <a href="https://arxiv.org/pdf/2106.00750.pdf">TNC</a> : augmented Dickey-Fuller (ADF) statistical test 해서 neg samples를 unknown으로 취급, weights 할당, <a href="https://arxiv.org/pdf/2004.11362.pdf">Supervised contrastive learning</a></li>
    </ul>
  </li>
  <li><strong>Prediction contrast</strong>
    <ul>
      <li>maximally preserve the mutual information of the context and the target</li>
      <li>Ex. <a href="https://arxiv.org/pdf/1807.03748.pdf">Contrastive predictive coding (CPC)</a> : Context와 target의 mutual information을 최대한 유지한 채로 prediction</li>
      <li>variants : <a href="https://arxiv.org/pdf/2202.03944.pdf">LNT</a>, <a href="https://arxiv.org/pdf/2106.14112.pdf">TS-TCC</a> <a href="https://arxiv.org/pdf/2208.06616.pdf">CA-TCC</a> (TS data augmentation techniques)</li>
    </ul>
  </li>
  <li><strong>Augmentation contrast</strong>
    <ul>
      <li>different views of an input sample 생성 (같은 sample의 view끼리는 positive) \(\to\) similarity <a href="https://arxiv.org/pdf/2002.05709.pdf">SimCLR</a></li>
      <li>variants : <a href="https://arxiv.org/pdf/2106.10466.pdf">TS2Vec</a> (week augmentation e.g. jitter-and-scale), <a href="https://arxiv.org/pdf/2202.01575.pdf">CoST</a>, <a href="https://arxiv.org/pdf/2206.08496.pdf">TF-C</a> (frequency domain), <a href="https://arxiv.org/pdf/2306.10347.pdf">DCdetector</a>, <a href="https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705122X00075/1-s2.0-S0950705122002726/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHAaCXVzLWVhc3QtMSJIMEYCIQDgmFUeLriQDh4DVodXqV0y990PZCUCjyqFZw5tbaaWOQIhAJJXI3FfJlWxRX7ERxHJD52xqRrpOHF7bO5s8CcvR%2BObKrwFCIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igz9LqUqaUEhwALsmIAqkAVKSauBGYOJAgLt97GY2VGTuMQnRA7AgMmTvZNbQR13nb0eSyB3AddrtMNpzfTApqlafitG63sIURfGc04jk0%2BqiLp73hS3BmM6J9f3LNclZKHKiIQ%2BfUjQppBf9%2BSzb2GYgwcFIWgWQCR5PtN2siu0UfPOBUAUFniH5sww3WHYgzisgA4woX%2F%2B1lpEeSZAfoqOLu%2BD70QDpxoC0KgT%2FXhLrNhBehAy%2FZqTdVfSNmO8PnYW%2BbOc6HJ8zuTY5RATpiy9V7EMrdnYFYXvqu%2F1qW2x%2FtdeEZ0RfWCu524fQvnPpB8zVmySXJH4TXT4LA7QJE%2B1QwAHRqUatsaSoMpckzyqhP4LGwWneO%2Fq9RiEEqDkaOCpaL5F%2F%2B5i1tibhhFGI9ACzd814laQtq%2BIcvidp9986C1DHHJjTFHEXHUgMObbcOMXVA8xcpjCf5yFKcLC42BCHss82InfazC%2BJ8X4if%2BIhr7C%2B4MnowAGueEbmt5yQhoYboaD1tk%2FKKIohIhI2hghsk%2Fj%2BwhkWCn5KyLfoONkWJmJW1CXLDORF38jJLFEREaAQr3LRwExugVvijVdQLlWMyzPoCNRSLI5zkotkTprBsrs6iaJrS7hLn3mxOjCh9mZyJI2yV6%2Brr7jzg5XQ6VHiYlrkcfYuw9OFK5jv5OsoGoi4E46toRGV7X2p9jKlrg7T4m61BP5khSxzLJq61nOGlO%2Bx3zUvUT3dMyvkGMkKtaSk1%2FKXT4RovxkHp%2Bmpc3QUR7j6hzf5z0n9IIsv8FzAtzg58kKHg9OBQmHtCehUyLOErVo7P9Qxj5SIP7umI%2F%2Fe9p%2BK%2BGGZKNKv4LyjUqeB7XdlwPp2x2ysLv4d4o%2BIPtvkyjLqxhNu6sOYmb6VjCX1oSwBjqwAfzu66rFmDL30GLaRAVAW1ntG5mkCnwKEF3lFukhkQUOfwCxm7RM9jOJeG4%2Bmi3EJkTk7YzUpDz3XnUhEC9%2BvejsHkC1qETTJrb0bcqMFFW3EaZrZZYvZ4zodutAKA%2BUzMn5Df82EqEjSmeySV18YkkkKDNTM0usR%2FHaNfviRepwNslRryoSwTWiPWMwiV9tIIbZaIFpcs6nZlQB%2BGqMP1tw1rZAozE5PSe1E%2Btz1kLS&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240325T090224Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYQXRPBKFN%2F20240325%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=2f8baa651eec4fdd10cde55f49d1477a95f8710aa9c8758546ef1df59d330c78&amp;hash=4538839e76c3c485eb651626f2c825dd66a6195b3060bbed9553996335a29682&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0950705122002726&amp;tid=spdf-baf2cd37-88cd-4f95-a9ab-f50f1a42eb98&amp;sid=fc89350759b08043e82bbad-52465f639e79gxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0f11585559020c5807&amp;rr=869db8c7e9f3a7c3&amp;cc=kr">TimeCLR</a></li>
    </ul>
  </li>
  <li><strong>Prototype contrast</strong>
    <ul>
      <li>minimize the distance btw samples and prototype(=virtual sequences, centor, …)
but maximize the distance btw prototype(=virtual sequences, centor, …)</li>
    </ul>
  </li>
  <li><strong>Expert knowledge contrast</strong>
    <ul>
      <li>incorporates expert prior knowledge or information into deep neural networks to guide model training</li>
    </ul>
  </li>
</ul>

<h2 id="5-adversarial-based-methods">5. Adversarial-based Methods</h2>
<ul>
  <li>pre-text tasks를 GAN으로 푼다. (generator \(\mathcal{G}\) and a discriminator \(\mathcal{D}\))</li>
  <li>Learning objective : \(\mathcal{L}=\mathbb{E}_{x \sim \mathcal{P}_{\text {data }}(x)}[\log \mathcal{D}(x)]+\mathbb{E}_{z \sim \mathcal{P}_{\mathbf{z}}(z)}[\log (1-\mathcal{D}(\mathcal{G}(\mathbf{z})))]\)
<img src="/assets/img/timeseries/SSL4TS/fig4.png" alt="사진4" /></li>
  <li><strong>Time series generation and imputation</strong>
    <ul>
      <li>Complete time series generation : <a href="https://papers.nips.cc/paper_files/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf">TimeGAN</a> (autoregressive GAN), <a href="https://arxiv.org/pdf/2202.02691.pdf">TTS-GAN</a> (Transformer treating TS as image)</li>
      <li>time series imputation : pass</li>
    </ul>
  </li>
  <li><strong>Auxiliary representation enhancement</strong>
    <ul>
      <li>pass</li>
    </ul>
  </li>
</ul>

<h2 id="6-applications-and-datasets">6. Applications and Datasets</h2>
<p><img src="/assets/img/timeseries/SSL4TS/table2.png" alt="사진6" /></p>

<h2 id="7-discussion-and-future-directions">7. Discussion and Future Directions</h2>
<h3 id="71-selection-and-combination-of-data-augmentation">7.1. <strong>Selection and combination of data augmentation</strong></h3>
<ul>
  <li>augmentation methods :  jitter, scaling, rotation, permutation, and warping, …</li>
  <li>permutation + rotation + time warping &gt; single method
    <h3 id="72-inductive-bias-for-time-series-ssl">7.2. <strong>Inductive bias for time series SSL</strong></h3>
  </li>
  <li>특히 데이터가 충분하지 않을 수록 합리적인 inductive bias는 필요할 수 있음
    <h3 id="73-ssl-for-irregular-and-sparse-time-series">7.3. <strong>SSL for irregular and sparse time series</strong>=</h3>
  </li>
  <li>Irregular and sparse time series를 interpolation해서 쓰려고 하다보면 undesirable noise가 낄 수도 있으니 그대로 SSL로 활용</li>
</ul>

<h2 id="8-conclusion">8. Conclusion</h2>
<p>Pass</p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2306.10125.pdf)]]></summary></entry><entry><title type="html">(Survey paper) Time Series Forecasting With Deep Learning A Survey (Philos Trans R Soc A. 2020)</title><link href="http://localhost:4000/timeseries/2024-03-19-TSwDLsurvey/" rel="alternate" type="text/html" title="(Survey paper) Time Series Forecasting With Deep Learning A Survey (Philos Trans R Soc A. 2020)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-19T13:15:52+09:00</updated><id>http://localhost:4000/timeseries/TSwDLsurvey</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-TSwDLsurvey/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Time series forecasting
    <ul>
      <li>traditional methods : parametric models informed by domain expertise (ex. autoregressive(AR))</li>
      <li>machine learning : learn temporal dynamics in a puerly data-driven manner</li>
      <li>deep learning : learn complex data representation
        <ul>
          <li>CNN, RNN, Attention-based mechanism</li>
        </ul>
      </li>
      <li>hybrid model : Quantitative TS model + deep learning model</li>
    </ul>
  </li>
  <li>Time series forecasting의 application
    <ul>
      <li>interpretability and counterfactual prediction</li>
    </ul>
  </li>
</ul>

<h2 id="2-deep-learning-architectures-for-tsf">2. Deep Learning Architectures for TSF</h2>
<ul>
  <li>One-step-ahead forecasting : \(\hat{y}_{i, t+1}=f\left(y_{i, t-k: t}, \boldsymbol{x}_{i, t-k: t}, \boldsymbol{s}_i\right)\)
    <ul>
      <li>\(\hat{y}_{i, t+1}\) : model forecast</li>
      <li>\(y_{i, t-k: t}=\left\{y_{i, t-k}, \ldots, y_{i, t}\right\}, \boldsymbol{x}_{i, t-k: t}=\left\{\boldsymbol{x}_{i, t-k}, \ldots, \boldsymbol{x}_{i, t}\right\}\) : observation over look-back window</li>
      <li>\(f(\cdot)\) : the prediction function learntby the model
        <h3 id="2a-basic-building-blocks">2.(a) Basic building blocks</h3>
      </li>
    </ul>
  </li>
  <li>Encoder : \(\boldsymbol{z}_t=g_{\mathrm{enc}}\left(y_{t-k: t}, \boldsymbol{x}_{t-k: t}, \boldsymbol{s}\right)\)</li>
  <li>Decoder : \(f\left(y_{t-k: t}, \boldsymbol{x}_{t-k: t}, \boldsymbol{s}\right)=g_{\mathrm{dec}}\left(\boldsymbol{z}_t\right)\)
    <ul>
      <li>Encoder에서 observations를 latent vector로 representation</li>
      <li>(1) Convolution Neural Networks : \(\begin{aligned} \boldsymbol{h}_t^{l+1} &amp; =A((\boldsymbol{W} * \boldsymbol{h})(l, t)) \\ (\boldsymbol{W} * \boldsymbol{h})(l, t) &amp; =\sum_{\tau=0}^k \boldsymbol{W}(l, \tau) \boldsymbol{h}_{t-\tau}^l \end{aligned}\)
        <ul>
          <li>Convolution과 pooling을 반복하는 구조. TS에서는 과거의 값만 보도록 설계</li>
          <li>\(\boldsymbol{h}_t^l \in \mathbb{R}^{\mathcal{H}_{i n}}\) : intermediate state at layer \(l\) at time \(t\)</li>
          <li>
            <p>\(*\) : convolution operator</p>
          </li>
          <li>\(\boldsymbol{W}(l, \tau) \in$ $\mathbb{R}^{\mathcal{H}_{\text {out }} \times \mathcal{H}_{\text {in }}}\) : fixed filter weight at layer \(l\)</li>
          <li>\(A(.)\) : activation function</li>
        </ul>
      </li>
      <li>Dilated Convolution : \((\boldsymbol{W} * \boldsymbol{h})\left(l, t, d_l\right)=\sum_{\tau=0}^{\left\lfloor k / d_l\right\rfloor} \boldsymbol{W}(l, \tau) \boldsymbol{h}_{t-d_l \tau}^l\)
        <ul>
          <li>\(d_l\) : layer-specific dilation rate</li>
          <li>(WaveNet) \(d_l = 2^l\) at layer \(l\) (fig1.(a))
<img src="/assets/img/timeseries/TSwDLsurvey/fig1.png" alt="사진1" /></li>
        </ul>
      </li>
      <li>(2) Recurrent Neural Networks
        <ul>
          <li>Memory state를 통해 과거 정보를 기억하는 sequential data에 적합한 구조</li>
          <li>
            <p>Gradient vanishing으로 인한 long-range dependency \(\to\) LSTM</p>
          </li>
          <li>
            <p>Memory update funciton : \(\boldsymbol{z}_t=\nu\left(\boldsymbol{z}_{t-1}, y_t, \boldsymbol{x}_t, \boldsymbol{s}\right)\)</p>
          </li>
          <li>Network : \(\begin{aligned} y_{t+1} &amp; =\gamma_y\left(\boldsymbol{W}_y \boldsymbol{z}_t+\boldsymbol{b}_y\right) \\ \boldsymbol{z}_t &amp; =\gamma_z\left(\boldsymbol{W}_{z_1} \boldsymbol{z}_{t-1}+\boldsymbol{W}_{z_2} y_t+\boldsymbol{W}_{z_3} \boldsymbol{x}_t+\boldsymbol{W}_{z_4} \boldsymbol{s}+\boldsymbol{b}_z\right) \end{aligned}\)
            <ul>
              <li>\(W_{.}, \boldsymbol{b}\) : the linear weights and bias</li>
              <li>\(\gamma_y(.), \gamma_z(.)\) : network activation functions</li>
            </ul>
          </li>
          <li>Long Short Term Memory(LSTM)
<img src="/assets/img/timeseries/TSwDLsurvey/fig2.png" alt="사진2" /></li>
        </ul>
      </li>
      <li>(3) Attention mechanisms
        <ul>
          <li>form : \(\boldsymbol{h}_t=\sum_{\tau=0}^k \alpha\left(\boldsymbol{\kappa}_t, \boldsymbol{q}_\tau\right) \boldsymbol{v}_{t-\tau}\)
            <ul>
              <li>key \(\boldsymbol{\kappa}_t\), query \(\boldsymbol{q}_\tau\) and value \(\boldsymbol{v}_{t-\tau}\) are intermediate features produced at different time steps by lower levels of the network</li>
              <li>\(\alpha\left(\boldsymbol{\kappa}_t, \boldsymbol{q}_\tau\right) \in[0,1]\) is the attention weight for \(t-\tau\) generated at time \(t\)</li>
              <li>\(\boldsymbol{h}_t\) is the context vector output of the attention layer
                <h3 id="2b-multi-horizon-forecasting-models">2.(b) Multi-horizon Forecasting Models</h3>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>단순히 다음 한 시점에 대한 예측이 아닌 미래 여러 시점에 대한 예측</li>
  <li>(1) Iterative Methods : Autoregressive forecasting. 각 time step에서의 작은 오차가 누적된다는 단점이 있다.</li>
  <li>(2) Direct Methods : Encoder의 정보를 활용해서 한 번에 target time steps를 예측</li>
</ul>

<h2 id="3-incorporating-domain-knowledge-with-hybrid-models">3. Incorporating Domain Knowledge with Hybrid Models</h2>
<ul>
  <li>Machine learning의 underperformance의 이유는 1) flexibility로 인한 overfitting, 2) pre-processed input에 대한 sensitivity</li>
  <li>Hybrid models
    <ul>
      <li>combine well-studied quantitative time series models together with deep learning</li>
      <li>use domain knowledge \(\to\) hypothesis space를 줄여준다</li>
      <li>(a) Non-probabilistic Hybrid models : forecasting equations를 modify</li>
      <li>(b) Probabilistic Hybrid models : predictive distribution으로 parameters 생성</li>
    </ul>
  </li>
</ul>

<h2 id="4-facilitating-decision-support-using-deep-neural-networks">4. Facilitating Decision Support Using Deep Neural Networks</h2>
<ul>
  <li>연구하는 입장에서는 model의 성능(MSE, Accuracy, …)가 중요하지만, user는 future action에 대한 guide의 지표</li>
  <li>그러므로 Local Interpretable Model-Agnostic Explanations (LIME), Shapley additive explanations (SHAP)과 같은 post-hoc 분석, Attention weights를 통한 inherent interpretability를 이해할 필요가 있다.</li>
  <li>그러면 counterfactual forecast(determining what would have happened if a different set of circumstances had occurred) 가능</li>
</ul>

<h2 id="5-conclusions-and-future-directions">5. Conclusions and Future Directions</h2>
<ul>
  <li>Survey the main architectures used for TS forecasting</li>
  <li>Hybrid DL models : combine statistical and deep learning components</li>
  <li>Limitation : irregular TS나 hierarchical structure에 대한 고민은 하기 이전</li>
</ul>

<h2 id="추가">추가</h2>
<ul>
  <li>2020년에 발표된 survey 논문이지만 최근 Long-term Time Series Forecasting(LTSF)에 활용되는 모델에 대한 내용을 잘 정리한 논문이다.</li>
  <li>본 논문 이후 현재까지 최신 연구들을 이해한 상태로 읽는다면 최신 연구들의 motivation을 이해하는 데에 도움이 되는 논문이다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Philos Trans R Soc A. 2020](https://arxiv.org/pdf/2004.13408.pdf)]]></summary></entry><entry><title type="html">(Survey paper) Transformers in Time Series: A Survey (IJCAI 2023)</title><link href="http://localhost:4000/timeseries/2024-03-19-TFinTSsurvey/" rel="alternate" type="text/html" title="(Survey paper) Transformers in Time Series: A Survey (IJCAI 2023)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-20T17:35:16+09:00</updated><id>http://localhost:4000/timeseries/TFinTSsurvey</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-TFinTSsurvey/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer는 long-range dependencies and interactions를 학습할 수 있다.</li>
  <li>본 논문에서는 Network structure 관점에서 Transformer를 TS forecasting에 사용하기 위해 어떤 adaptaion and modification을 했는지 알아보고</li>
  <li>Application 관점에서 forecasting, anomaly detection, and classification을 포함한 task에 대해 얼마나 잘 작동하는지 알아본다.</li>
  <li>마지막으로 future direction을 제시한다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer : ability for long-range dependencies and interactions in sequential data</li>
  <li>Time series : How to effectively model long-range and short-range temporal dependency and capture seasonality simultaneously ?</li>
  <li>Network modification 관점 : low-level(i.e. module)부터 high-level(i.e. architecture)</li>
  <li>Application 관점 : summarize Transformer for forecasting, anomaly detection, and classification</li>
</ul>

<h2 id="2-preliminaries-of-the-transformer">2. Preliminaries of the Transformer</h2>

<h3 id="21-vanilla-transformer">2.1. Vanilla Transformer</h3>

<ul>
  <li>Encoder : a multi- head self-attention module and a position-wise feed-forward network</li>
  <li>Decoder : cross-attention models between the multi-head self-attention module and the position-wise feed-forward network</li>
</ul>

<h3 id="22-input-encoding-and-positional-encoding">2.2. Input Encoding and Positional Encoding</h3>

<ul>
  <li>No recurrence, instead positional encoding</li>
  <li>Absolute Positional Encoding : \(PE(t)_i= \begin{cases}\sin \left(\omega_i t\right) &amp; i \% 2=0 \\ \cos \left(\omega_i t\right) &amp; i \% 2=1\end{cases}\)
    <ul>
      <li>\(\quad \omega_i\) is the hand-crafted frequency for each dim</li>
    </ul>
  </li>
  <li>Relative Positional Encoding : input의 상대적인 위치에 대해 learnable하지만 train에서 본 적 없는, 더 긴 길이의 input에 대해서 확장이 어려움</li>
</ul>

<h3 id="23-multi-head-attention">2.3. Multi-head Attention</h3>

<ul>
  <li>
    <p>scaled dot-product : \(\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q K}^{\mathbf{T}}}{\sqrt{D_k}}\right) \mathbf{V}\)</p>
  </li>
  <li>
    <p>Multi-head Attention :</p>

\[\begin{aligned}MultiHeadAttn (\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Concat}\left(\right. head _1, \cdots, head \left._H\right) \mathbf{W}^O \\ \text{where } head _i= Attention \left(\mathbf{Q} \mathbf{W}_i^Q, \mathbf{K} \mathbf{W}_i^K, \mathbf{V} \mathbf{W}_i^V\right)\end{aligned}\]
  </li>
</ul>

<h3 id="24-feed-forward-and-residual-network">2.4. Feed-forward and Residual Network</h3>

<ul>
  <li>The feed-forward network(FFN) : \(FFN\left(\mathbf{H}^{\prime}\right)=\operatorname{ReLU}\left(\mathbf{H}^{\prime} \mathbf{W}^1+\mathbf{b}^1\right) \mathbf{W}^2+\mathbf{b}^2\)
    <ul>
      <li>\(\mathbf{H}^{\prime}\) is outputs of previous layer</li>
      <li>\(\mathbf{W}^1 \in \mathcal{R}^{D_m \times D_f}\), \(\mathbf{W}^2 \in \mathcal{R}^{D_f \times D_m}, \mathbf{b}^1 \in \mathcal{R}^{D_f}, \mathbf{b}^2 \in \mathcal{R}^{D_m}\) are trainable parameters</li>
    </ul>
  </li>
  <li>Residual connection module followed by a layer normalization module : \(\begin{aligned} \mathbf{H}^{\prime} &amp; =\operatorname{LayerNorm}(\operatorname{Self} \operatorname{Attn}(\mathbf{X})+\mathbf{X}), \\ \mathbf{H} &amp; =\operatorname{LayerNorm}\left(FFN\left(\mathbf{H}^{\prime}\right)+\mathbf{H}^{\prime}\right)\end{aligned}\)
    <ul>
      <li>SelfAttn(.) : self-attention module</li>
      <li>LayerNorm(.) : the layer normalization operation</li>
    </ul>
  </li>
</ul>

<h2 id="3-taxonomy-of-transformers-in-time-series">3. Taxonomy of Transformers in Time Series</h2>

<p><img src="/assets/img/timeseries/TFinTSsurvey/fig1.png" alt="사진1" /></p>

<h2 id="4-network-modifications-for-time-series">4. Network Modifications for Time Series</h2>

<h3 id="41-positional-encoding">4.1. Positional Encoding</h3>

<ul>
  <li>Vanilla Positional Encoding : fixed, hand-crafted</li>
  <li>Learnable Positional Encoding : more flexible and can adapt to spe- cific tasks</li>
  <li>Timestamp Encoding : calendar timestamps (e.g., second, minute, hour, week, month, and year) and special times- tamps (e.g., holidays and events) \(\to\) additional position encoding</li>
</ul>

<h3 id="42-attention-module">4.2. Attention Module</h3>

<ul>
  <li>Self-attention module : FC layer w weights that are dynamically generated based on the pairwise similarity of input patterns</li>
  <li>Memory complexity \(O(N^2)\)
    <ul>
      <li>explicitly introducing a sparsity bias into the attention mechanism
        <ul>
          <li>e.g. LogTrans [Li <em>et al.</em>, 2019, Pyraformer [Liu <em>et al.</em>, 2022a]</li>
        </ul>
      </li>
      <li>exploring the low-rank property of the self-attention matrix to speed up the computation,
        <ul>
          <li>e.g. Informer [Zhou <em>et al.</em>, 2021], FEDformer [Zhou <em>et al.</em>, 2022].</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table1.png" alt="사진2" /></p>

<h3 id="43-architecture-based-attention-innovation">4.3. Architecture-based Attention Innovation</h3>

<ul>
  <li>Hierarchical architecture : multi-resolution aspect of TS
    <ul>
      <li>Informer [Zhou <em>et al.</em>, 2021]
        <ul>
          <li>max-pooling layers with stride 2 btw attention blocks (down-sample series into its half slice)</li>
        </ul>
      </li>
      <li>Pyraformer [Liu <em>et al.</em>, 2022a]
        <ul>
          <li>C-ary tree-based attention mechanism (finest-origin / coarser-lower resolutions)</li>
          <li>both intra-scale and inter-scale attentions \(\to\) temporal dependencies across different resolutions</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-applications-of-time-series-transformers">5. Applications of Time Series Transformers</h2>

<p><img src="/assets/img/timeseries/TFinTSsurvey/fig2.jpeg" alt="사진9" /></p>

<h3 id="51-transformers-in-forecasting">5.1. Transformers in Forecasting</h3>

<ul>
  <li><strong>Module-level variants</strong> : main architectures는 비슷한데, minor changes
    <ul>
      <li>(1) designing new attention modules
        <ul>
          <li>LogTrans [Li <em>et al.</em>, 2019] : convolution self-attention, sparse bias (Logsparse mask)</li>
          <li>Informer [Zhou <em>et al.</em>, 2021] : selects dominant queries based on queries and key similarities</li>
          <li>AST [Wu <em>et al.</em>, 2020a] : generative adversarial encoder- decoder framework to train a sparse Transformer</li>
          <li>Pyraformer [Liu <em>et al.</em>, 2022a] : hierarchical pyramidal attention module with a binary tree following the path</li>
          <li>Quatformer [Chen <em>et al.</em>, 2022] : learning-to-rotate attention (LRA) based on quaternions that introduce learnable period and phase information</li>
          <li>FEDformer [Zhou <em>et al.</em>, 2022] : attention operation in the frequency domain with Fourier trans- form and wavelet transform</li>
        </ul>
      </li>
      <li>(2) exploring the innovative way to normalize time series data
        <ul>
          <li>Non-stationary Transformer [Liu <em>et al.</em>, 2022b]</li>
        </ul>
      </li>
      <li>(3) utilizing the bias for token inputs
        <ul>
          <li>Autoformer [Wu <em>et al.</em>, 2021] : segmentation-based representation mechanism (auto-correlation mechanism)</li>
          <li>PatchTST [Nie <em>et al.</em>, 2023] : subseries-level patch design which are served as input tokens w/ channel-independency</li>
          <li>Cross- former [Zhang and Yan, 2023] : input is embedded into a 2D vector array and then two-stage attention layer is used to efficiently capture the cross-time and cross-dimension dependency</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Architecture-level variants</strong>
    <ul>
      <li>Triformer [Cirstea <em>et al.</em>, 2022] : triangular,variable-specific patch attention \(\to\) lightweight and linear complex- ity</li>
      <li>Scaleformer [Shabani <em>et al.</em>, 2023] : iteratively refine the forecasted time series at multiple scales with shared weights.</li>
    </ul>
  </li>
  <li><strong>Spatio-temporal Forecasting, Evnet Forecasting</strong>
    <ul>
      <li>Pass</li>
    </ul>
  </li>
</ul>

<h3 id="52-transformers-in-anomaly-detection">5.2. Transformers in Anomaly Detection</h3>

<ul>
  <li>
    <p>Transformer + Generative models</p>

    <ul>
      <li>
        <p>TranAD [Tuli <em>et al.</em>, 2022] : Transformer는 small deviation of anomaly는 놓치므로 reconstruction errors를 amplify하는 adversarial training</p>
      </li>
      <li>
        <p>MT-RVAE [Wang <em>et al.</em>, 2022], TransAnomaly [Zhang <em>et al.</em>, 2021] : Transformer + VAE \(\to\) reduce training costs, 다양한 scale의 정보 통합</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="53-transformers-in-classification">5.3 Transformers in Classification</h3>

<ul>
  <li>GTN [Liu <em>et al.</em>, 2021] : two-tower Transformer(time-step-wise attention and channel-wise attention) \(\to\)  learnable weighted concatenation</li>
  <li>TARNet [Chowdhury <em>et al.</em>, 2022] : utilizes attention score for important timestamps masking and reconstruction</li>
</ul>

<h2 id="6-experimental-evaluation-and-discussion">6. Experimental Evaluation and Discussion</h2>

<ul>
  <li><strong>Robustness Analysis</strong>
    <ul>
      <li>대부분의 attention-based models는 lower the quadratic calculation and memory complexity를 위해 module을 수정했고, 좋은 실험 결과를 위해 짧은 input을 사용했는데, 긴 input을 넣어도 MSE가 커지지 않고 잘 유지되는지 확인했다.</li>
      <li>대부분의 모델들이 긴 input에 대해서는 잘 처리하지 못한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table2.png" alt="사진3" /></p>

<ul>
  <li><strong>Model Size Analysis</strong>
    <ul>
      <li>일반적으로 model size가 커지면 prediction power도 좋아지는데 attention-based models에서 그렇지 않음을 확인했다.</li>
      <li>지금까지의 Transformer 자체가 features를 잘 뽑아내지 못하는 구조일 수 있겠다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table3.png" alt="사진4" /></p>

<ul>
  <li><strong>Seasonal-Trend Decomposition Analysis</strong>
    <ul>
      <li>seasonal-trend decomposition는 Transformer에서 필수적인 부분 : model performance가 50% - 80% boosting</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table4.png" alt="사진5" /></p>

<h2 id="7-future-research-opportunities">7. Future Research Opportunities</h2>

<h3 id="71-inductive-biases-for-time-series-transformers">7.1. Inductive Biases for Time Series Transformers</h3>

<ul>
  <li>Channel-independence와 Cross-channel(dim) dependency은 서로 반대 inductive bias이지만 둘 다 실험 결과가 좋았다.</li>
  <li>즉 cross-channel learning에는 noise도 있고 signal도 있다는 의미</li>
  <li>어떤 inductive bias를 어떻게 induce할지 고려할 필요가 있음</li>
</ul>

<h3 id="72-transformers-and-gnn-for-time-series">7.2. Transformers and GNN for Time Series</h3>

<ul>
  <li>Traffic forecsting처럼 spatial dependency (relationship among dim)이 강한 경우에는 GNN + Transformer의 성능이 좋을 수 있다.</li>
</ul>

<h3 id="73-pre-trained-transformers-for-time-series">7.3. Pre-trained Transformers for Time Series</h3>

<ul>
  <li>Large-scale pre-trained Transformer model의 성능이 좋긴 한데 대부분 classification을 위한 pre-train이라는 점에서, 다른 tasks를 위한 pre-train도 고려할 수 있다.</li>
</ul>

<h3 id="74-transformers-with-architecture-level-variants">7.4. Transformers with Architecture Level Variants</h3>

<ul>
  <li>지금까지는 attention module에 대한 modification이 주로 등장했지만, TS를 위한 architecture-level design도 고려할 수 있다.</li>
</ul>

<h3 id="75-transformers-with-nas-for-time-series">7.5. Transformers with NAS for Time Series</h3>

<ul>
  <li>Neural architecture search(NAS)과 같은 AutoML을 통해, 성능에 영향을 주는 embedding dimension이나 head/layer의 개수 등 효율적인 architecture를 고려할 수 있다.</li>
</ul>

<h2 id="8-conclusion">8. Conclusion</h2>

<ul>
  <li>new taxonomy consisting of network design and application</li>
  <li>strengths and limitations of representative methods by experimental evaluation</li>
  <li>highlight future research directions.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[IJCAI 2023](https://arxiv.org/pdf/2202.07125.pdf)]]></summary></entry><entry><title type="html">(Crossformer) Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting (ICLR 2023)</title><link href="http://localhost:4000/timeseries/2024-03-19-crossformer/" rel="alternate" type="text/html" title="(Crossformer) Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting (ICLR 2023)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-19T21:34:48+09:00</updated><id>http://localhost:4000/timeseries/crossformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-crossformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer-based models
    <ul>
      <li>focus on modeling the temporal dependency (cross-time dependency)</li>
      <li>yet often omit the dependency among different variables (cross- dimension dependency)</li>
    </ul>
  </li>
  <li>Crossformer는
    <ul>
      <li>Dimension-Segment-Wise (DSW) : MTS \(\to\) 2d vector array로 만들고</li>
      <li>Two-Stage Attention (TSA) : 2개의 attention을 거치는데 각각 cross-time and cross-dimension dependency를 학습한다.</li>
      <li>Hierarchical Encoder-Decoder (HED) : 그리고 서로 다른 scales의 정보를 사용해서 coarse, fine한 정보 모두 활용하여 forecasting한다.</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>MTS에서는 cross-time dependency 뿐만 아니라 cross-dimension dependency도 중요한데, Transformer에서 cross-dim dependency를 반영하는 방법은 embedding 뿐이다.</li>
  <li>본 논문에서는 cross-dim dependency를 explicitly하게 사용한다.</li>
  <li>Dimension-Segment-Wise (DSW) : the series(e.g. UTS)는 segments로 나뉘고, 각 segment는 feature vector가 된다. (series는 2d vector array가 된다.)</li>
  <li>Two-Stage Attention (TSA): 2d vector array로부터 cross-time and cross-dimension dependency 학습</li>
  <li>Hierarchical Encoder-Decoder (HED) : 각 layer에서 서로 다른 scale에 대한 dependency를 학습하게 된다.</li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li><strong>Multivariate Time Series Forecasting</strong>
    <ul>
      <li>Statistical models : Vector auto-regressive(VAR), Vector auto-regressive moving average (VARMA)</li>
      <li>Neural models : TCN, DeepAR, LSTnet(CNN+RNN), MTGNN, …</li>
    </ul>
  </li>
  <li><strong>Transformer-based model</strong> : LogTrans, Informer, Autoformer, Pyraformer, FEDformer, Preformer, …</li>
  <li><strong>Vision Transformers</strong> : Transformer를 vision에서 사용할 때 썼던 patching 방식</li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<ul>
  <li>\(\mathbf{x}_{1: T} \in \mathbb{R}^{T \times D}\)를 보고 \(\mathbf{x}_{T+1: T+\tau} \in \mathbb{R}^{\tau \times D}\)​ 예측하는 문제
    <ul>
      <li>\(\tau, T\) is the number of time steps in the future and past, respectively</li>
      <li>\(D&gt;1\) is the number of dimensions</li>
    </ul>
  </li>
</ul>

<h3 id="31-dimension-segment-wise-embedding">3.1. Dimension-Segment-wise Embedding</h3>
<p><img src="/assets/img/timeseries/crossformer/fig1.png" alt="사진1" /></p>
<ul>
  <li>t시점의 모든 dimension의 data point \(\mathbf{x}_t \in \mathbb{R}^D\)를 \(\mathbf{h}_t \in \mathbb{R}^{d_{\text {model }}}\)로 embedding한다.</li>
  <li>
\[\begin{aligned}
\mathbf{x}_{1: T} &amp; =\left\{\mathbf{x}_{i, d}^{(s)} \left\lvert\, 1 \leq i \leq \frac{T}{L_{\text {seg }}}\right., 1 \leq d \leq D\right\} \\
\mathbf{x}_{i, d}^{(s)} &amp; =\left\{x_{t, d} \mid(i-1) \times L_{\text {seg }}&lt;t \leq i \times L_{\text {seg }}\right\} \\ \mathbf{h}_{i, d}&amp;=\mathbf{E} \mathbf{x}_{i, d}^{(s)}+\mathbf{E}_{i, d}^{(p o s)} \end{aligned}\]
    <ul>
      <li>\(\mathbf{x}_{i, d}^{(s)} \in \mathbb{R}^{L_{\text {seg }}}\)  is the \(i\)-th segment in dimension \(d\) with length \(L_{\text {seg }}\)</li>
      <li>\(\mathbf{E} \in \mathbb{R}^{d_{\text {model }} \times L_{\text {seg }}}\) : the learnable projection matrix</li>
      <li>\(\mathbf{E}_{i, d}^{(\text {pos })} \in \mathbb{R}^{d_{\text {model }}}\) : the learnable position embedding for position $(i, d)$.</li>
    </ul>
  </li>
  <li>
\[\mathbf{H}=\left\{\mathbf{h}_{i, d} \mid, 1 \leq i \leq \frac{T}{L_{\text {seg }}}, 1 \leq d \leq D\right\}\]
    <ul>
      <li>where each \(\mathbf{h}_{i, d}\) represents a univariate time series segment.</li>
    </ul>
  </li>
  <li>수식으로 표현하다보니 어려운데 아래 그림과 같고, \(\mathbf{H}\)는 오른쪽처럼 생겼다.
<img src="/assets/img/timeseries/crossformer/myfig1.jpeg" alt="사진2" /></li>
</ul>

<h3 id="32-two-stage-attention-layer">3.2. Two-Stage Attention Layer</h3>
<ul>
  <li>이미지가 아니라 시계열이다보니 height와 width가 서로 바뀌면 의미가 달라지기 때문에 flatten시키면 안되고 바로 \(\mathbf{H}\)에 self-attention을 적용한다.</li>
  <li><strong>Cross-Time Stage</strong>
    <ul>
      <li>\(\mathbf{Z}_{i, \text { : }}\) : the vectors of all dimensions at time step \(i\)</li>
      <li>\(\mathbf{Z}_{:, d}\) the vectors of all time steps in dimension \(d\)
<img src="/assets/img/timeseries/crossformer/myfig2.jpeg" alt="사진3" /></li>
      <li>
\[\begin{aligned} \hat{\mathbf{Z}}_{:, d}^{\text {time }}=\text { LayerNorm }\left(\mathbf{Z}_{:, d}+\operatorname{MSA}^{\text {time }}\left(\mathbf{Z}_{:, d}, \mathbf{Z}_{:, d}, \mathbf{Z}_{:, d}\right)\right) \\ \mathbf{Z}^{\text {time }}=\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {time }}+\operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {time }}\right)\right) \end{aligned}\]
      </li>
      <li>\(\mathbf{Z}^{time}\)이 다음 stage인 Cross-Dimension Stage의 input이 된다.</li>
    </ul>
  </li>
  <li><strong>Cross-Dimension Stage</strong>
    <ul>
      <li>\(\begin{aligned} \mathbf{B}_{i,:} &amp; =\mathrm{MSA}_1^{\operatorname{dim}}\left(\mathbf{R}_{i,:}, \mathbf{Z}_{i,:}^{\text {time }}, \mathbf{Z}_{i,:}^{\text {time }}\right), 1 \leq i \leq L \\ \overline{\mathbf{Z}}_{i,:}^{\text {dim }} &amp; =\mathrm{MSA}_2^{\text {dim }}\left(\mathbf{Z}_{i,:}^{\text {time }}, \mathbf{B}_{i,:}, \mathbf{B}_{i,:}\right), 1 \leq i \leq L \\ \hat{\mathbf{Z}}^{\text {dim }} &amp; =\text { LayerNorm }\left(\mathbf{Z}^{\text {time }}+\overline{\mathbf{Z}}^{\text {dim }}\right) \\ \mathbf{Z}^{\text {dim }} &amp; =\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {dim }}+\operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {dim }}\right)\right) \end{aligned}\)
<img src="/assets/img/timeseries/crossformer/fig2.png" alt="사진4" /></li>
      <li>\(D\)가 클 때에는 router mechanism을 사용하여 fixed number \(c &lt;&lt; D\) vectors에 정보를 모았다가 다시 뿌려준다.</li>
      <li>\(\mathbf{R} \in \mathbb{R}^{L \times c \times d_{\text {model }}}\) : the learnable vector array serving as routers</li>
      <li>\(\mathbf{B} \in \mathbb{R}^{L \times c \times d_{\text {model }}}\) : the aggregated messages from all dimensions</li>
      <li>\(\overline{\mathbf{Z}}^{\text {dim }}\) : output of the router mechanism.</li>
      <li>All time steps \((1 \leq i \leq L)\) share the same \(\mathbf{M S A}_1^{\text {dim }}, \mathbf{M S A}_2^{\text {dim }}\)</li>
      <li>\(\hat{\mathbf{Z}}^{\text {dim }}, \mathbf{Z}^{\text {dim }}\) : output of skip connection and MLP respectively</li>
    </ul>
  </li>
</ul>

<h3 id="33-hierarchical-encoder-decoder">3.3. Hierarchical Encoder-Decoder</h3>
<p><img src="/assets/img/timeseries/crossformer/fig3.png" alt="사진5" /></p>
<ul>
  <li>Upper layer일수록 coarser scale을 사용한 정보를 얻고, 서로 다른 scale로 얻은 정보들로 예측한 값들은 final result에서 더해진다.</li>
  <li><strong>Encoder</strong> : upper layer일수록 coarser scale을 사용한다는 말은 인접한 두 vector(segment)를 merge한다는 것과 같다.</li>
  <li>\(\mathbf{Z}^{e n c, l}=\operatorname{Encoder}\left(\mathbf{Z}^{e n c, l-1}\right)\)의 연산은 아래와 같다.
\(\begin{aligned} &amp; \begin{cases}l=1: &amp; \hat{\mathbf{Z}}^{e n c, l}=\mathbf{H} \\ l&gt;1: &amp; \hat{\mathbf{Z}}_{i, d}^{e n c, l}=\mathbf{M}\left[\mathbf{Z}_{2 i-1, d}^{e n c, l-1} \cdot \mathbf{Z}_{2 i, d}^{e n c, l-1}\right], 1 \leq i \leq \frac{L_{l-1}}{2}, 1 \leq d \leq D\end{cases} \\&amp; \mathbf{Z}^{\text {enc,l}}=\operatorname{TSA}\left(\hat{\mathbf{Z}}^{\text {enc,l}}\right) \end{aligned}\)
    <ul>
      <li>
        <p>\(\mathbf{H}\) denotes the 2D array obtained by DSW embedding</p>
      </li>
      <li>
        <p>\(\mathbf{Z}^{e n c, l}\) denotes the output of the \(l\)-th encoder layer</p>
      </li>
      <li>\(\mathbf{M} \in \mathbb{R}^{d_{\text {model }} \times 2 d_{\text {model }}}\) denotes a learnable matrix for segment merging</li>
      <li>\([\cdot]\) denotes the concatenation operation</li>
      <li>\(L_{l-1}\) denotes the number of segments in each dimension in layer \(l-1\)</li>
      <li>\(\hat{\mathbf{Z}}^{e n c, l}\) denotes the array after segment merging in the \(i\)-th layer</li>
      <li>\(\mathbf{Z}^{\text {enc }, 0}, \mathbf{Z}^{\text {enc }, 1}, \ldots, \mathbf{Z}^{\text {enc }, N},\left(\mathbf{Z}^{\text {enc }, 0}=\mathbf{H}\right)\) is used to represent the \(N+1\) outputs of the encoder</li>
    </ul>
  </li>
  <li><strong>Decoder</strong> : Emcoder에서 얻은 \(N+1\)개의 feature array가 있으면, \(N+1\)개의 layers로 예측한다.
    <ul>
      <li>decoder의 process : \(\mathbf{Z}^{\text {dec, } l}=\operatorname{Decoder}\left(\mathbf{Z}^{\text {dec, },-1}, \mathbf{Z}^{\text {enc, },}\right)\)
\(\begin{aligned} &amp; \left\{\begin{array}{lll} l=0: &amp; \tilde{\mathbf{Z}}^{\text {dec }, l}=\operatorname{TSA}\left(\mathbf{E}^{(d e c)}\right) \\ l&gt;0: &amp; \tilde{\mathbf{Z}}^{\text {dec, }, l}=\operatorname{TSA}\left(\mathbf{Z}^{\text {dec, },-1}\right) \end{array}\right. \\ &amp; \overline{\mathbf{Z}}_{:, d}^{\text {dec, }, l}=\operatorname{MSA}\left(\tilde{\mathbf{Z}}_{:, d}^{\text {dec, }, l}, \mathbf{Z}_{:, d}^{e n c, l}, \mathbf{Z}_{:, d}^{e n c, l}\right), 1 \leq d \leq D \\ &amp; \hat{\mathbf{Z}}^{\text {dec, } l}=\text { LayerNorm }\left(\tilde{\mathbf{Z}}^{\text {dec, }, l}+\overline{\mathbf{Z}}^{\text {dec, } l}\right) \\ &amp; \mathbf{Z}^{\text {dec,l}}=\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {dec, }, l} \operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {dec,l}}\right)\right) \\ &amp; \end{aligned}\)</li>
      <li>\(\mathbf{E}^{(\text {dec })} \in \mathbb{R}^{\frac{\tau}{L_{s e g}} \times D \times d_{\text {model }}}\) denotes the learnable position embedding for decoder</li>
      <li>\(\tilde{\mathbf{Z}}^{\text {dec,l } l}\) is the output of TSA</li>
      <li>The MSA layer takes \(\tilde{\mathbf{Z}}_{:, d}^{d e c,l }\) as query and \(\mathbf{Z}_{:, d}^{e n c, l}\) as the key and value to build the connection between encoder and decoder</li>
      <li>The output of MSA is denoted as \(\overline{\mathbf{Z}}_{:, d}^{\text {dec, },} . \hat{\mathbf{Z}}^{\text {dec,l}, ~} \mathbf{Z}^{\text {dec, }, l}\) denote the output of skip connection and MLP respectively.</li>
      <li>\(\mathbf{Z}^{\text {dec, 0},}, \mathbf{Z}^{e n c, 1}, \ldots, \mathbf{Z}^{\text {dec, } N}\) : is used to represent decoder output</li>
      <li><strong>Linear projection</strong> : 각 layer에서는 linear projection으로 prediction을 만들고, 각 layer의 prediction을 다 더하면 최종 prediction이 된다.
\(\begin{gathered} \text { for } l=0, \ldots, N: \mathbf{x}_{i, d}^{(s), l}=\mathbf{W}^l \mathbf{Z}_{i, d}^{\text {dec,l }} \quad \mathbf{x}_{T+1: T+\tau}^{\text {pred, } l}=\left\{\mathbf{x}_{i, d}^{(s), l} \left\lvert\, 1 \leq i \leq \frac{\tau}{L_{\text {seg }}}\right., 1 \leq d \leq D\right\} \\ \mathbf{x}_{T+1: T+\tau}^{\text {pred }}=\sum_{l=0}^N \mathbf{x}_{T+1: T+\tau}^{\text {pred, }}\end{gathered}\)
        <ul>
          <li>\(\mathbf{W}^l \in \mathbb{R}^{L_{\text {seg }} \times d_{\text {model }}}\) : learnable matrix to project a vector to a ts segment</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<h3 id="41-protocols">4.1. Protocols</h3>
<ul>
  <li>Dataset : 1) ETTh1 (Electricity Transformer Temperature-hourly), 2) ETTm1 (Electricity Transformer Temperature-minutely), 3) WTH (Weather), 4) ECL (Electricity Consuming Load), 5) ILI (Influenza-Like Illness), 6) Traffic</li>
  <li>Baselines : 1) LSTMa (Bah- danau et al., 2015), 2) LSTnet (Lai et al., 2018), 3) MTGNN (Wu et al., 2020), and recent Transformer-based models for MTS forecasting: 4) Transformer (Vaswani et al., 2017), 5) In- former (Zhou et al., 2021), 6) Autoformer (Wu et al., 2021a), 7) Pyraformer (Liu et al., 2021a) and 8) FEDformer (Zhou et al., 2022)
    <h3 id="42-main-results">4.2. Main Results</h3>
    <p><img src="/assets/img/timeseries/crossformer/table1.png" alt="사진6" /></p>
    <h3 id="43-ablation-study">4.3. Ablation Study</h3>
    <p><img src="/assets/img/timeseries/crossformer/table2.png" alt="사진7" /></p>
    <h3 id="44-effect-of-hyper-parameters">4.4. Effect of Hyper-parameters</h3>
    <p><img src="/assets/img/timeseries/crossformer/fig4.png" alt="사진8" /></p>
    <h3 id="45-computational-efficiency-analysis">4.5. Computational Efficiency Analysis</h3>
    <p><img src="/assets/img/timeseries/crossformer/table3.png" alt="사진9" /></p>
  </li>
</ul>

<h3 id="5-conclusion">5. Conclusion</h3>
<ul>
  <li>Crossformer : Transformer-based model utilizing cross-dimension dependency for MTS forecasting</li>
  <li><strong>Dimension-Segment-Wise (DSW)</strong> embedding embeds the input data into a 2D vector array to preserve the information of both time and dimension</li>
  <li><strong>The Two-Stage-Attention (TSA)</strong> layer is devised to capture the cross-time and cross- dimension dependency of the embedded array</li>
  <li>Using DSW embedding and TSA layer, a <strong>Hierarchical Encoder-Decoder (HED)</strong> is devised to utilize the information at different scales</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://openreview.net/forum?id=vSVLM2j9eie)]]></summary></entry><entry><title type="html">(SoftCLT) Soft Contrastive Learning for Time Series (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-03-13-softCLT/" rel="alternate" type="text/html" title="(SoftCLT) Soft Contrastive Learning for Time Series (ICLR 2024)" /><published>2024-03-13T00:00:00+09:00</published><updated>2024-04-02T17:42:19+09:00</updated><id>http://localhost:4000/timeseries/softCLT</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-13-softCLT/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>TS instance만으로 contrasting하거나, 하나의 TS의 adjacent timestamps만으로 contrasting하면 TS의 inherent correlation을 제대로 반영하지 못한다.</li>
  <li><strong>SoftCLT</strong> : instance-wise &amp; temporal Contrastive loss를 사용하여, 0 또는 1이 아닌 0 ~ 1 사이의 값으로 assign
    <ul>
      <li>Instance-wise contrastive loss : Data space에서 두 시계열의 distance</li>
      <li>Temporal contrastive loss : 서로 다른 시점에 대한 loss</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>
<ul>
  <li>Self-supervised learning - Contrastive learning에서는 두 instance의 유사도로 pair를 설정하기보다는, 각 데이터에 대해 2개의 view를 augmentation하고 둘을 positive pair로 설정한다. (다른 데이터에서 augmentation된 view와는 negative)</li>
  <li>TS의 inherent correlations는 유사한 instance 뿐만 아니라, 인접한 timestamps에도 있다.</li>
  <li>설령 같은 data에서 augmentation 되었거나, 같은 timestamp의 value에만 positive(1)로 설정하더라도, 다른 data에서 augmentation 된 다른 timestamp의 value에는 다 똑같의 negative(0)으로 설정하는 것은 optimal하지 않다.</li>
  <li><a href="https://arxiv.org/pdf/1807.03748.pdf">InfoNCE</a>의 loss처럼 positive pair 뿐만 아니라 negative pair에 대해서도 weight를 고려하는 Soft Contrastive Learning for TS를 제안
<img src="/assets/img/timeseries/softclt/table1.png" alt="사진1" /></li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>
<ul>
  <li>Self-supervised learning : 많은 양의 unlabeled data를 활용하는 pretext task를 수행하는 모델을 훈련시키고, 해당 모델을 downstream task의 앞쪽에 가져와서 사용한다. pretext task로는 <code class="language-plaintext highlighter-rouge">next token prediction</code>, <code class="language-plaintext highlighter-rouge">masked token prediction</code>, <code class="language-plaintext highlighter-rouge">jigsaw puzzles</code>, <code class="language-plaintext highlighter-rouge">rotation prediction</code> 등</li>
  <li>Contrastive learning in TS
    <ul>
      <li><a href="https://arxiv.org/pdf/1901.10738.pdf">T-Loss</a> : TS에서 subseries 샘플링, subseries가 속한 TS와는 positive 다른 TS와는 negative</li>
      <li><a href="https://arxiv.org/pdf/2011.13548.pdf">Self-Time</a> : augmented sample로 inter-sample relation 학습, temporal distance로 label을 만들고 classification 해서 intra-temporal relation 학습</li>
      <li><a href="https://arxiv.org/pdf/2106.00750.pdf">TNC</a> : 정규분포 window로 정의한 temporal neighborhood를 positive로 설정</li>
      <li><a href="https://arxiv.org/pdf/2106.14112.pdf">TS-TCC</a> : augmentations가 서로의 미래 시점을 예측하도록 해서 temporal contrastive loss 설정</li>
      <li><a href="https://arxiv.org/pdf/2203.09270.pdf">Mixing-up</a> : 2개의 TS를 섞어서 새로운 TS를 만드는데 mixing weights를 predict</li>
      <li><a href="https://arxiv.org/pdf/2202.01575.pdf">CoST</a> : time, frequency domain의 contrastive losses를 사용하여 representation learning</li>
      <li><a href="https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705122X00075/1-s2.0-S0950705122002726/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCDSwasc4kb8LU0bgnJMwyHRmJ5Xp0qOkMGkgGRC101hgIgQu4zuleiKDecSs%2FYiwU6McTbx88zb7ZGNMt6fPGxxoAqvAUI0v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDF3vdKai06uU77VSpSqQBVBf8gEuvX4RtLZKv9hE3KxXK6cBABJ3MlBBkOHOQG9wKZz7o6d1XmLn2FZSjY4%2FX7pPyQcDRmkq2n%2FJq8%2Fsui0EeAASo4iyS840z2aCXativJcKRglZNYHjwhkBAax3A8xvkoahV70%2BnaX2GHQ5TWPuaUEwQkfUD1Y%2BKJW17Llm6SzL9NFiUMu9oRVuGLNfzeDz8H916pyvPQXFS5ltLq8PmDSlDFSwpmuvjRZvcGJVQCRYJ9QAqN4pjSkBwmRJtZ1zxeYYLfQU88%2FhHpCuqKY%2Fo5PlZqitF%2Fi5tN9wcjv%2BOaUu0e3H8K8qknd2hQhfYZ3mcE319ttggfYVT4PxT6jQv2hH%2BtWO%2BQVZ32moiyr2q2dfvqndSZ%2BcslmaJMEEGPfVbkcqz6OuRXKg9c6wHw%2BzGjJ0qF8lctSoDbLhT5IOZWG%2FmNF%2BMVvU5Wgorfa2swiBav99Hgn3vrf74u8mLsY5T0vx4NEyG%2BNVyPbgKqGOHsQAejW06Vq4ik5UIzPpsQ5HV4XPKK%2Fqymlem1XN6PxFHQeaf3vs0y8kVwp0rvnrfnnQ4LSrKfZc%2FNdpLU%2FXHGx%2BkUvIAHtVRX4a%2BC3sP8xSXKWTctA458XV7b5O7K5sXlyS36%2F3xnTrjC0NcJv80e3dYPRDhg3knQfYgDe2geRGQuU2COYu%2FKksZiGgBXhSAln%2Bud9LWeLVphgzjSipja81DInKiCBNOHOlulkaoOh0WcdIOFQeAQ2q6v4DeoIE1D8tTL5JNgWDLUk8sxQGq9zspYADXCEhc9Ke4hgL%2FuFvRA6Q12rCsxcWroPVYAf2el010OB%2BHSiqaCHw6xiykfcfjw7oO7bINDuWwMASTZbTgTVETF0hx1VpaYUnUbObMOCH4K8GOrEBiMpaUIWZ9pgzWe8VL74b8Keg8P4qJal5QND0KgcUzoVtZv4JMAmxEuey5Xggo3VjcjrCYsQ3sGOrJ9OJ570LbYowhBvMl7GSojd2kqdTrDSXd400eFg4uwE4Vb35B7htjgxzcxpZJeKmMPHzoEJdnMzI61T%2Fkl8%2FoIh3I9dws%2BUKd1pmrot0rGKx7EM68SBELBX2rcQ1SfmvbLKJAKBimMCFZrVhl4BAeMfpNyKSnkQM&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240318T093805Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYRLOY57VC%2F20240318%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=0e72e500b607b8e5538d7a3d24ad28c78f5bc82c4b10c8f43305eb800f767c1b&amp;hash=4ee078d9d743db9b7aed88f3efc12b11dbe7e497747d269063317fe1b35fad70&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0950705122002726&amp;tid=spdf-1cc819cf-9b45-4e50-834c-160b0fdd0a1e&amp;sid=cc5998cf8485c74f0b497fc9e83d21092089gxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=05125c535357540259&amp;rr=86643f6fd90f3079&amp;cc=kr">TimeCLR</a> : phase-shift and amplitude change augmentation based on DTW</li>
    </ul>
  </li>
  <li>Soft contrastive learning
    <ul>
      <li><a href="https://arxiv.org/pdf/2104.14548.pdf">NNCLR</a> : 각 view마다 feature space에서 k-neighbors를 찾아서 추가적인 positive pair를 고려</li>
      <li>non-TS 도메인에서는 soft assignment를 계산할 때 embedding space에서 했지만, TS 도메인에서는 data space에서 계산한다.</li>
    </ul>
  </li>
  <li>Masked modeling in TS
    <ul>
      <li><a href="https://arxiv.org/pdf/2010.02803.pdf">TST</a> : masked modeling paradigm을 TS에 도입</li>
      <li><a href="https://arxiv.org/pdf/2211.14730.pdf">PatchTST</a> :  masked subseries-level patche를 예측해서 local semantic information을 효율적으로 계산</li>
      <li><a href="https://arxiv.org/pdf/2302.00861.pdf">SimMTM</a> : 여러 개의 masked TS로부터 reconstruction</li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>
<ul>
  <li>instance-wise contrastive loss : inter-sample relationship 학습. Distance btw TS on data space</li>
  <li>temporal contrastive loss : intra-temporal relationship 학습. 하나의 TS에서 서로 다른 timestamps의 차이
<img src="/assets/img/timeseries/softclt/fig1.png" alt="사진2" /></li>
</ul>

<h3 id="31-problem-definition">3.1. Problem Definition</h3>
<ul>
  <li>하나의 batch에는 N개의 TS : \(\mathcal{X}=\left\{x_1, \ldots, x_N\right\}\)가 있고
    <ul>
      <li>\(f_\theta = x_i \in \mathbb{R}^{T \times D} \to r_i=\left[r_{i, 1}, \ldots, r_{i, T}\right]^{\top} \in \mathbb{R}^{T \times M}\)를 학습</li>
      <li>\(D\)는 input feature dim, \(M\)은 embedded feature dim</li>
      <li>
        <h3 id="32-soft-instance-wise-contrastive-learning">3.2. Soft Instance-wise Contrastive learning</h3>
      </li>
    </ul>
  </li>
  <li>Vision에서는 pixel-by-pixel distance가 similarity와 관련이 없기 때문에 embedding space에서 similar instance를 학습하지만, TS에서는 data space에서의 거리가 similarity가 된다.</li>
  <li>soft assignment for a pair of data indice \((i, i')\) : \(w_I\left(i, i^{\prime}\right)=2 \alpha \cdot \sigma\left(-\tau_I \cdot D\left(x_i, x_{i^{\prime}}\right)\right)\)
    <ul>
      <li>\(D(\cdot, \cdot)\) : min-max normalized distance metric</li>
      <li>\(\tau_I\) : hyperparameter controlling the sharpness</li>
      <li>\(\alpha\) : the upper bound in the range of [0, 1]. 완전 똑같은 TS의 assignment</li>
      <li>augmented view끼리가 아니라 original TS끼리 계산하는 것</li>
    </ul>
  </li>
  <li>Contrasive loss는 cross-entropy loss로 해석될 수 있으므로(<a href="https://arxiv.org/pdf/2010.08887.pdf">i-Mix</a>), softmax probability of the relative similarity는 \(p_I\left(\left(i, i^{\prime}\right), t\right)=\frac{\exp \left(r_{i, t} \circ r_{i^{\prime}, t}\right)}{\sum_{j=1, j \neq i}^{2 N} \exp \left(r_{i, t} \circ r_{j, t}\right)}\)</li>
  <li>Soft instance-wise contrastive loss for \(x_i\), at \(t\)는 \(\ell_I^{(i, t)}=-\log p_I((i, i+N), t)-\sum_{j=1, j \neq\{i, i+N\}}^{2 N} w_I(i, j \bmod N) \cdot \log p_I((i, j), t)\)
    <ul>
      <li>첫째 term은 positive pair에 대한 loss, 둘째 term은 나머지에 대한 loss인데 \(w_I\left(i, i^{\prime}\right)\)로 weighted.</li>
      <li>\(\forall w_I\left(i, i^{\prime}\right)=0\)이면 hard instance-wise contrastive loss이므로 일반화 버전이다.</li>
      <li>
        <h3 id="33-soft-temporal-contrastive-learning">3.3. Soft Temporal Contrastive learning</h3>
      </li>
    </ul>
  </li>
  <li>soft assignment for a pair of timestamps \((t, t')\) : \(w_T\left(t, t^{\prime}\right)=2 \cdot \sigma\left(-\tau_T \cdot\mid t-t^{\prime}\mid\right)\)
    <ul>
      <li>\(\tau_I\) : hyperparameter controlling the sharpness</li>
    </ul>
  </li>
  <li>: 인접한 timestamp의 values는 비슷할 것이라는 직관</li>
  <li><strong>Hierarchical loss</strong> : <a href="https://arxiv.org/pdf/2106.10466.pdf">TS2Vec</a>의 방식처럼 maxpooling을 해서 loss 계산
<img src="/assets/img/timeseries/softclt/fig2.png" alt="사진3" /></li>
  <li>Softmax probability of the relative similarity는 \(p_T\left(i,\left(t, t^{\prime}\right)\right)=\frac{\exp \left(r_{i, t} \circ r_{i, t^{\prime}}\right)}{\sum_{s=1, s \neq t}^{2 T} \exp \left(r_{i, t} \circ r_{i, s}\right)}\),</li>
  <li>Soft temporal contrastive loss for \(x_i\) at \(t\)는 \(\ell_T^{(i, t)}=-\log p_T(i,(t, t+T))-\sum_{s=1, s \neq\{t, t+T\}}^{2 T} w_T(t, s \bmod T) \cdot \log p_T(i,(t, s))\)
    <ul>
      <li>마찬가지로 \(\forall w_T\left(t, t^{\prime}\right)=0\)이면 hard temporal contrastive loss이므로 일반화 버전이다.</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 SoftCLT의 Final loss : \(\mathcal{L}=\frac{1}{4 N T} \sum_{i=1}^{2 N} \sum_{t=1}^{2 T}\left(\lambda \cdot \ell_I^{(i, t)}+(1-\lambda) \cdot \ell_T^{(i, t)}\right)\)
    <ul>
      <li>\(\lambda\) : hyperparameter controlling the contribution of each loss</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<ul>
  <li>(1) Classification with UTS, MTS (2) Semi-supervised classification (3) Transfer learning in in-domain and cross-domain (4) Anomaly detection
    <h3 id="41-classification">4.1. Classification</h3>
    <p><img src="/assets/img/timeseries/softclt/fig23.png" alt="사진4" /></p>
  </li>
</ul>

<h3 id="42-semi-supervised-classification">4.2. Semi-supervised classification</h3>
<p><img src="/assets/img/timeseries/softclt/table3.png" alt="사진5" /></p>

<h3 id="43-transfer-learning">4.3. Transfer learning</h3>
<p><img src="/assets/img/timeseries/softclt/table4.png" alt="사진6" /></p>

<h3 id="44-anomaly-detection">4.4. Anomaly detection</h3>
<p><img src="/assets/img/timeseries/softclt/table5.png" alt="사진7" /></p>

<h3 id="45-ablation-study">4.5. Ablation study</h3>
<p><img src="/assets/img/timeseries/softclt/table6.png" alt="사진8" /></p>

<ul>
  <li>(a) : soft assignment를 instance-wise와 temporal에 모두 적용했을 때 성능이 가장 좋다.</li>
  <li>(b) : \(W_T\)를 계산하는 방법들에 따른 비교. sigmoid를 사용하는 근거가 된다.</li>
  <li>(c) : \(\alpha=0.5\) 정도로 해서 같은 TS의 similarity of the pairs를 적절히 크게 할 때 성능이 좋다.</li>
  <li>(d) : Distance function에 따른 성능 비교. DTW와 TAM의 성능이 같지만 더 일반적인 DTW 사용했다.
    <h3 id="46-analysis">4.6. Analysis</h3>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Comparison with soft CL methods in computer vision</code> : 앞서 언급했듯이 embedding space에서 similarity를 계산하는 vision domain과 다르게, TS는 data space에서 계산하면 성능이 더 좋다.</li>
  <li><code class="language-plaintext highlighter-rouge">Robustness to seasonality</code> : Seasonality in TS는 extract하기 어렵지도 않고 고려 안해도 성능이 좋아서 직접적으로 고려하지 않았다.</li>
  <li><code class="language-plaintext highlighter-rouge">Instance-wise relationships</code> : layer가 깊어짐에 따라 SoftCLT가 Hard CL보다 TS instance 사이의 관계를 잘 보존한다.</li>
  <li><code class="language-plaintext highlighter-rouge">Temporal relationships</code> : 시간(training epoch)에 따라서도 t-SNE를 비교했을 때, Hard CL은 진한 색(large tarining epoch)을 잘 구분하지 못하는데, large training epoch에는 fine-grained relationship이 학습된다. 즉 Hard CL은 coarse-grained relationship은 잘 학습하지만 SoftCLT는 fine-grained relationship도 잘 학습한다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>SoftCLT : soft assignments based on the instance-wise and temporal relationships on the data space</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/abs/2312.16424)]]></summary></entry><entry><title type="html">(TS2Vec) Towards Universal Representation of Time Series (AAAI 2022)</title><link href="http://localhost:4000/timeseries/2024-03-12-ts2vec/" rel="alternate" type="text/html" title="(TS2Vec) Towards Universal Representation of Time Series (AAAI 2022)" /><published>2024-03-12T00:00:00+09:00</published><updated>2024-03-26T15:10:01+09:00</updated><id>http://localhost:4000/timeseries/ts2vec</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-12-ts2vec/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>본 논문에서 풀고자 하는 문제는 unsupervised TS representation(TS \(\to\) vector)</li>
  <li>TS를 arbitrary semantic level에서 representation하는 방법을 학습하는 framework</li>
  <li>augmented context views를 hierarchical하게 contrastive learning한다.</li>
  <li>(선행연구(<a href="https://arxiv.org/abs/2106.00750">TNC</a>, <a href="https://arxiv.org/abs/1901.10738">T-Loss</a>)를 읽기 전이라면 abstract에서 등장하는 단어들의 의미가 와닿지 않을 수 있다.)</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>이미 instance-level representation을 학습하는 연구들도 있었고(<a href="https://arxiv.org/abs/2106.00750">TNC</a>, <a href="https://arxiv.org/abs/1901.10738">T-Loss</a>),</li>
  <li>contrastive loss를 사용해서 TS의 구조를 학습하는 연구들도 있었다.(<a href="https://arxiv.org/abs/2106.14112">TS-TCC</a>, <a href="https://arxiv.org/abs/1901.10738">T-Loss</a>)</li>
  <li>하지만 지금까지의 방법들의 한계는 아래 3가지로 정리할 수 있다.</li>
  <li>첫째, instance-level representation은 fine-grained representation(e.g. forecasting, anomaly detection)에 적합하지 않다.
    <ul>
      <li>왜냐하면 specific한 timestamp, sub-series를 타겟으로 inference해야 하는데, coarse-grained representation으로는 충분하지 않다.</li>
    </ul>
  </li>
  <li>둘째, 다양한 granularities에서의 multi-scale contextual information을 파악하기 어렵다.
    <ul>
      <li>granularity가 높을수록 더 자세한 정보를 포함 (일별 &lt; 시간별 &lt; 분별 &lt; 초별 …)</li>
      <li>multi-scale information은 다양한 granularities에서 나타나는 정보. Scale에 따라 달라질 수는 있지만, representation의 generalization capability를 향상시킨다는 점에서 TS task에서 필수적인 정보이다.</li>
    </ul>
  </li>
  <li>셋째, CV, NLP에서의 unsupervised representation은 강한 inductive bias가 있는데, TS는 그렇지 않다.
    <ul>
      <li>transformation-invariant : 강아지 사진은 거꾸로 뒤집어도 강아지 사진이지만, TS는 거꾸로 뒤집으면 아예 다른 데이터가 된다.</li>
      <li>cropping-invariant : 사진이나 문장은 일부분을 잘라도 본질적인 정보가 바뀌지 않는 경우가 많아 augmentation 방법으로 쓰이지만, TS는 일부분을 자르면 패턴이나 분포 자체가 달라지게 된다.</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 TS2Vec은 universal contrastive learning framework를 제안
    <ul>
      <li>maxpooling으로 다양한 granularity의 overall representation을 얻고</li>
      <li>instance-wise and temporal dim에서의 hierarchically contrastive learning을 통해</li>
      <li>all-semantic level에서의 representation을 얻는다.</li>
    </ul>
  </li>
</ul>

<h2 id="2-method">2. Method</h2>

<h3 id="21-problem-definition">2.1. Problem Definition</h3>
<ul>
  <li>N개의 시계열 \(\mathcal{X}=\left\{x_1, x_2, \cdots, x_N\right\}\)에 대해서 nonlinear embedding function \(f_\theta : x_i \in \mathbb{R}^{T \times F} \to r_i=\left\{r_{i, 1}, r_{i, 2}, \cdots, r_{i, T}\right\} \in \mathbb{R}^{T \times K}\)를 학습한다.</li>
</ul>

<h3 id="22-model-architecture">2.2. Model Architecture</h3>
<p><img src="/assets/img/timeseries/ts2vec/fig1.png" alt="사진1" /></p>
<ul>
  <li><strong>sub-series</strong> : input TS \(x_i\)에서 2개의 sub-series를 랜덤하게 sampling한다.
    <ul>
      <li>겹치는 부분이 있도록 (겹치는 부분의 contextual representation이 consistent하도록 할거니까)</li>
    </ul>
  </li>
  <li><strong>Encoder</strong> \(f_\theta\)는 3개의 modules로 구성
    <ul>
      <li><strong>projection layer</strong> : t시점의 input을 high-dim latent vector로 mapping하는 FC layer. \(x_{i,t} \to z_{i,t}\)</li>
      <li><strong>timestamp masking module</strong> : latent vectors의 random한 timestamps를 masking(=0)해서 augmented context view를 생성</li>
      <li><strong>dilated CNN module</strong> : 각 timestamp의 contextual representation을 extract. 이 때 1d dilated conv layer를 사용하는데, dilation parameter를 다양하게 해서(\(2^l\) for \(l\)-th block) larger receptive field</li>
    </ul>
  </li>
</ul>

<p class="figcaption"><img src="/assets/img/timeseries/ts2vec/dilatedconv1.png" alt="사진11" />
source : <a href="https://arxiv.org/pdf/1609.03499v2.pdf">WAVENET: A GENERATIVE MODEL FOR RAW AUDIO</a></p>

<h3 id="23-contextual-consistency">2.3. Contextual Consistency</h3>
<ul>
  <li>Contrastive learning을 위한 positive pair를 만드는 전략들을 소개한다.
<img src="/assets/img/timeseries/ts2vec/fig2.jpeg" alt="사진2" /></li>
  <li><strong>Subseries consistency</strong> : 서로 sub-series 관계인 segments를 positive pair로 설정하고 representation을 가깝게 학습</li>
  <li><strong>Temporal consistency</strong> : 인접한 시점의 segments를 positive pair로 설정</li>
  <li><strong>Transformation consistency</strong> : scaling, permutation과 같은 transformation에 invariant한 representation을 학습
<img src="/assets/img/timeseries/ts2vec/fig3.jpeg" alt="사진3" /></li>
  <li>하지만 fig3을 보면 위와 같은 전략들이 시계열 데이터에는 적절하지 않다. sub-series라고 해서, 인접한 시점이라고 해서 패턴이 같은 것은 아니다.</li>
  <li><strong>Contextual consistency</strong> : 본 논문에서 제시하는 방법으로, 동일한 timestamp에 대해 random masking이나 random cropping으로 생성한 contexts를 positive pair로 설정한다.
    <ul>
      <li><strong>Timestamp masking</strong> : 각 timestamp에 대해 latent vector \(z_i = z_{i,t}\)를 \(p=0.5\) bernoulli masking</li>
      <li><strong>Random cropping</strong> : input \(x_i \in \mathbb R^{T \times F}\)에 대해 \(0 &lt; a_1 \le a_2 \le b_1 \le b_2 \le T\)를 만족하는 segments \([a_1, b_1], [a_2, b_2]\)를 random하게 만들고, 각 segment에 대해 overlapping segment인 \([a_2, b_1]\)의 contextual representation이 consistent해지도록 학습한다.</li>
      <li>Masking과 random cropping은 시계열의 magnitude를 바꾸지도 않으면서, 각 timestamp에 대해 복원하도록 학습시키기 때문에 robust한 representation learning 방식이다.</li>
    </ul>
  </li>
</ul>

<h3 id="24-hierarchical-contrasting">2.4. Hierarchical Contrasting</h3>
<ul>
  <li><strong>Hierarchical contraastive loss</strong> : 본 논문에서 제시하는 학습 방식으로, 다양한 scales에서의 representation을 학습하기 위한 loss이다. (scales가 다양하기 때문에 max-pooling을 사용한다.)
    <ul>
      <li>instance-wise &amp; temporal contrastive losses 모두 leverage하는데, 이걸 모든 granularity levels에 대해서 hierarchical하게 적용한다.</li>
    </ul>
  </li>
  <li><strong>Temporal Contrastive Loss</strong>
    <ul>
      <li>\(\ell_{t e m p}^{(i, t)}=-\log \frac{\exp \left(r_{i, t} \cdot r_{i, t}^{\prime}\right)}{\sum_{t^{\prime} \in \Omega}\left(\exp \left(r_{i, t} \cdot r_{i, t^{\prime}}^{\prime}\right)+\mathbb{1}_{\left[t \neq t^{\prime}\right]} \exp \left(r_{i, t} \cdot r_{i, t^{\prime}}\right)\right)}, \quad \Omega \text{ is the set of timestamps}\)이다.</li>
      <li>동일한 input \(x_i\)에 대해서, timestamp가 같으면 positive이고 다르면 negative이다.
<img src="/assets/img/timeseries/ts2vec/myfig1.jpeg" alt="사진5" /></li>
      <li>위 그림에서 빨강색이 분모에 포함되는 timestamps이고, 파랑색이 분자에 해당하는 timestamp이다. 같은 input에 대해 서로 다른 augmentation의 같은 timestamp가 가깝게 representation되도록 학습한다는 의미이다.</li>
    </ul>
  </li>
  <li><strong>Instance-wise Contrastive Loss</strong>
    <ul>
      <li>\(\ell_{i n s t}^{(i, t)}=-\log \frac{\exp \left(r_{i, t} \cdot r_{i, t}^{\prime}\right)}{\sum_{j=1}^B\left(\exp \left(r_{i, t} \cdot r_{j, t}^{\prime}\right)+\mathbb{1}_{[i \neq j]} \exp \left(r_{i, t} \cdot r_{j, t}\right)\right)}, \quad B \text{ is the batch size}\)이다.</li>
      <li>한 시점 t와 모든 instance(input)에 대해 같은 instance이면 positive이고 다른 instance이면 negative이다.
<img src="/assets/img/timeseries/ts2vec/myfig2.jpeg" alt="사진6" /></li>
      <li>역시 빨강색이 분모에 포함되는 instance이고, 파랑색이 분자에 해당하는 instance이다. 한 timestamp에 대해 같은 instance의 augmentation가 가깝게 representation되도록 학습한다는 의미이다.</li>
    </ul>
  </li>
  <li>두 losses는 complementary하다. (예를 들어 다수의 전기 사용량 시계열 데이터라면, instance contrast는 user-specifc 정보를, temporal contrast는 시간에 따른 dynamic trends를 학습한다.)</li>
  <li>The overall loss : \(\mathcal{L}_{\text {dual }}=\frac{1}{N T} \sum_i \sum_t\left(\ell_{\text {temp }}^{(i, t)}+\ell_{\text {inst }}^{(i, t)}\right)\)</li>
</ul>

<h2 id="3-experiments">3. Experiments</h2>
<p><img src="/assets/img/timeseries/ts2vec/table2.jpeg" alt="사진7" />
<img src="/assets/img/timeseries/ts2vec/fig5.png" alt="사진8" /></p>
<ul>
  <li>Informer는 trends는 학습했지만 주기적 패턴을 학습하지 못했고, TCN은 반대로 주기적 패턴은 학습했지만 trends는 학습하지 못했다. (coarse-grained vs fine-grained 둘 다 잘 학습하기 어려움)</li>
</ul>

<h2 id="4-analysis">4. Analysis</h2>
<p><img src="/assets/img/timeseries/ts2vec/table5.png" alt="사진9" /></p>
<ul>
  <li>Ablation study를 통해 components를 justify하였다.
<img src="/assets/img/timeseries/ts2vec/fig7.png" alt="사진10" /></li>
  <li>Heatmap을 통해 급작스러운 spike에 대해서도 적절하게 representation할 수 있음을 확인하였다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>TS2Vec : universial representation learning framework
    <ul>
      <li>hierarchical contrasting을 통해 scale-invariant representation을 학습하였고,</li>
      <li>instance-wise contrasting과 temporal contrasting으로 loss를 설정하였다.</li>
    </ul>
  </li>
  <li>Ablation study를 통해 모델의 components가 모두 필요함을 보여주었다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2022](https://arxiv.org/abs/2106.10466)]]></summary></entry><entry><title type="html">(SimMTM) A Simple Pre-Training Framework for Masked Time-Series Modeling (NeurIPS 2023)</title><link href="http://localhost:4000/timeseries/2024-03-06-SimMTM/" rel="alternate" type="text/html" title="(SimMTM) A Simple Pre-Training Framework for Masked Time-Series Modeling (NeurIPS 2023)" /><published>2024-03-06T00:00:00+09:00</published><updated>2024-03-19T13:15:52+09:00</updated><id>http://localhost:4000/timeseries/SimMTM</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-06-SimMTM/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Labeling 비용을 줄이고 다양한 downstream tasks의 성능을 위해 self-supervised pre-training 방식이 사용된다.
    <ul>
      <li>Contrastive learning : positive and negative pairs를 통해 representation space 최적화</li>
      <li>Masked modeling : unmasked part를 보고 masked content를 reconstruct</li>
    </ul>
  </li>
  <li>하지만 시계열에서는 randomly masking하면 temporal variations(trend, periodicity, peak valley …)가 망가져서 reconstruction task가 너무 어려워진다.</li>
  <li>그래서 본 논문에서 제시하는 SimMTM은 한 개가 아니라 여러 개의 masked series를 assembling해서 reconstruction한다.</li>
</ul>

<h2 id="1-intnroduction">1. Intnroduction</h2>
<ul>
  <li>Self-supervised pre-training(SSL) : 대량의 unlabeled 데이터로 pretext knowledge를 학습하고, 다양한 downstream task에 맞게 개선 (Linear probing / Fine tuning)</li>
  <li>pre-training 방법 중 하나인 Masked modeling을 시계열에 적용
    <ul>
      <li>Masked modeling : 데이터의 일부를 masking하고 unmasked part를 보고 masked part를 reconstruct하는 방식을 학습</li>
    </ul>
  </li>
  <li>이미지나 자연어는 불필요한 정보도 많이 있지만(이미지의 빈 공간, 수식어 등), 시계열에는 temporal variations(trend, periodicity, peak vally…)가 있어서 단순하게 일부를 masking하면 시계열의 본질적인 부분이 변형되거나 망가질 수 있다.</li>
  <li>그래서 multiple masking series로 original data를 reconstruction하면 개별 maksing series에서는 temporal variations가 변형될 수 있지만 각 maksing series는 서로서로 complement하기 때문에 multiple masking series를 봤을 때에는 본질적인 부분이 사라지지 않는다.
<img src="/assets/img/timeseries/SimMTM/fig1.jpeg" alt="사진1" /></li>
  <li>요약하자면 SimMTM은 neighborhood aggregation design for reconstruction이라고 할 수 있고,
    <ul>
      <li>풀어서 설명하자면 SimMTM은 masked part를 reconstruct하기 위해서 series-wise representation의 simailarity가 높은 point-wise representations을 aggregate한다고 할 수 있다.</li>
    </ul>

    <h2 id="2-related-work">2. Related Work</h2>
    <h3 id="21-self-supervised-pre-training">2.1. Self-supervised Pre-training</h3>
    <ul>
      <li>Self-supervised Pre-training(SSL)
        <ul>
          <li>Contrastive leaning : positive pairs는 가깝게, negative pairs는 멀게 representation하도록 학습</li>
          <li>Masked modeling
            <ul>
              <li>TST : learns to predict removed time points based on the remaining time points</li>
              <li>PatchTST : predict masked subseries-level patches to capture the local semantic information</li>
              <li>Ti-MAE : mask modeling as an auxiliary task to boost the forecasting and classification performances</li>
            </ul>
          </li>
          <li>하지만 directly masking time series 방식은 본질적인 temporal variations를 망가지게 할 수 있으니, multiple randomly masked series로 recunstruct한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-simmtm">3. SimMTM</h2>
<ul>
  <li>모델은 크게 2개의 단계로 구성
    <ul>
      <li>multiple time series의 series-wise representation space에서의 similarities를 학습</li>
      <li>학습된 similarities를 바탕으로 point-wise representations를 aggregate</li>
    </ul>
  </li>
</ul>

<h3 id="31-overall-architecture">3.1. Overall Architecture</h3>
<ul>
  <li>모델은 4개의 modules로 구성
    <ul>
      <li>Masking</li>
      <li>Representation learning</li>
      <li>Series-wise similarity learning</li>
      <li>Point-wise aggregation
<img src="/assets/img/timeseries/SimMTM/fig2.png" alt="사진2" /></li>
    </ul>
  </li>
  <li><strong>Masking</strong>
    <ul>
      <li>\(\left\{\mathbf{x}_i\right\}_{i=1}^N\) : a mini-batch of \(N\) time series samples, <br />
where \(\mathbf{x}_i \in \mathbb{R}^{L \times C}\) contains \(L\) time points and \(C\) observed variates</li>
      <li>\(\left\{\overline{\mathbf{x}}_i^j\right\}_{j=1}^M=\operatorname{Mask}_{r}\left(\mathbf{x}_i\right)\) <br />
where \(r \in[0,1]\) denotes the masked portion,
\(M\) is a hyperparameter for the number of masked time series</li>
      <li>
        <p>\(\overline{\mathbf{x}}_i^j \in \mathbb{R}^{L \times C}\) : the \(j\)-th masked time series of \(\mathbf{x}_i\)</p>
      </li>
      <li>All the \((N(M+1))\) input series in a set as \(\mathcal{X}=\bigcup_{i=1}^N\left(\left\{\mathbf{x}_i\right\} \cup\left\{\overline{\mathbf{x}}_i^j\right\}_{j=1}^M\right)\).
        <ul>
          <li>\(N\)은 mini-batch에 있는 시계열 데이터 sample의 개수,</li>
          <li>\(M\)은 multiple masked time series의 개수</li>
          <li>\(1\)은 masking 하지 않은 원본 시계열을 의미한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Representation learning</strong>
    <ul>
      <li>Encoder : Transformer and ResNet (to obtain the point-wise representations \(\mathcal{Z}\))
        <ul>
          <li>\(\mathcal{Z}=\bigcup_{i=1}^N\left(\left\{\mathbf{z}_i\right\} \cup\left\{\overline{\mathbf{z}}_i^j\right\}_{j=1}^M\right)=\operatorname{Enocder}(\mathcal{X})\) <br />
where \(\mathbf{z}_i, \overline{\mathbf{z}}_i^j \in \mathbb{R}^{L \times d_{\text {model }}}\)</li>
          <li>Detail : input 시계열마다 separately하게 통과 : \(\bigcup_{i=1}^N\left(\operatorname{Encoder}\left(\mathbf{x}_i\right) \cup\left\{\text { Encoder }\left(\overline{\mathbf{x}}_i^j\right)\right\}_{j=1}^M\right)\)</li>
        </ul>
      </li>
      <li>Projector : MLP layer along the temporal dim (to obtain the series-wise representations \(\mathcal{S}\))
        <ul>
          <li>\(\mathcal{S}=\bigcup_{i=1}^N\left(\left\{\mathbf{s}_i\right\} \cup\left\{\overline{\mathbf{s}}_i^j\right\}_{j=1}^M\right)=\operatorname{Projector}(\mathcal{Z})\) <br />
where \(\mathbf{s}_i, \overline{\mathbf{s}}_i^j \in \mathbb{R}^{1 \times d_{\text {model }}}\)</li>
        </ul>
      </li>
      <li>Note : \(\mathbf{z}_i, \overline{\mathbf{z}}_i^j \in \mathbb{R}^{L \times d_{\text {model }}}, \mathbf{s}_i, \overline{\mathbf{s}}_i^j \in \mathbb{R}^{1 \times d_{\text {model }}}\)
<img src="/assets/img/timeseries/SimMTM/myfig1.jpeg" alt="사진3" /></li>
    </ul>
  </li>
  <li><strong>Series-wise similarity learning</strong>
    <ul>
      <li>Multiple masked time series를 단순하게 averaging하면 over-smoothing problem이 있기 때문에, similarities among series-wise representation로 weighted aggregation한다.</li>
      <li>
\[\mathbf{R}=\operatorname{Sim}(\mathcal{S}) \in \mathbb{R}^{D \times D}, D=N(M+1), \quad \mathbf{R}_{\mathbf{u}, \mathbf{v}}=\frac{\mathbf{u v}^{\top}}{\|\mathbf{u}\|\|\mathbf{v}\|}, \mathbf{u}, \mathbf{v} \in \mathcal{S}\]
        <ul>
          <li>\(\mathbf{R}=\operatorname{Sim}(\mathcal{S}) \in \mathbb{R}^{D \times D}\)은 \(N(M+1)\)개의 input 각각에 대해 series-wise representation space에서의 similarities가 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Point-wise aggregation</strong>
    <ul>
      <li>The aggregation process는 다음과 같다 : \(\widehat{\mathbf{z}}_i=\sum_{\mathbf{s}^{\prime} \in \mathcal{S} \backslash\left\{\mathbf{s}_i\right\}} \frac{\exp \left(\mathbf{R}_{\mathbf{s}_i, \mathbf{s}^{\prime}} / \tau\right)}{\sum_{\mathbf{s}^{\prime \prime} \in \mathcal{S} \backslash\left\{\mathbf{s}_i\right\}} \exp \left(\mathbf{R}_{\mathbf{s}_i, \mathbf{s}^{\prime \prime}} / \tau\right)} \mathbf{z}^{\prime}\)
        <ul>
          <li>where \(\mathbf{z}^{\prime}=\text { Projector }\left(\mathbf{s}^{\prime}\right)\), \(\tau\) denotes the temperature hyperparameter of softmax normalization for series-wise similarities</li>
          <li>의미적으로는 \(\mathbf{x}_i\)를 reconstruction하기 위해서 \(\mathbf{x}_i\)에 대한 M개의 masked series \(\left\{\overline{\mathbf{x}}_i^j\right\}_{j=1}^M\) 뿐만 아니라, similarities가 높은 다른 series(samples)도 참고하겠다는 것으로, 시계열의 structure를 더 잘 학습하도록 의도했다.</li>
        </ul>
      </li>
      <li>그리고 마지막으로 Decoder를 통과시키면 reconstruction 값을 얻는다 : \(\left\{\widehat{\mathbf{x}}_i\right\}_{i=1}^N=\operatorname{Decoder}\left(\left\{\widehat{\mathbf{z}}_i\right\}_{i=1}^N\right)\)
        <ul>
          <li>\(\operatorname{Decoder}\)는 simple MLP layer (along the channel dim)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="32-self-supervised-pre-training">3.2. Self-supervised Pre-training</h3>
<ul>
  <li>SimMTM의 reconstruction loss는 \(\mathcal{L}_{\text {reconstruction }}=\sum_{i=1}^N\left\|\mathbf{x}_i-\widehat{\mathbf{x}}_i\right\|_2^2\)이다.</li>
  <li>The series-wise representation space에 constraints가 없으면 trivial aggregation이 발생할 수 있기 때문에, 한 series에 대한 multiple masked series끼리는 positive pair, 서로 다른 series에 대해서는 negative pair로 가정하고 (neighborhood assumption) contrastive하게 학습할 수 있도록 loss를 추가해주었다. : \(\mathcal{L}_{\text {constraint }}=-\sum_{\mathbf{s} \in \mathcal{S}}\left(\sum_{\mathbf{s}^{\prime} \in \mathcal{S}^{+}} \log \frac{\exp \left(\mathbf{R}_{\mathbf{s}, \mathbf{s}^{\prime}} / \tau\right)}{\sum_{\mathbf{s}^{\prime \prime} \in \mathcal{S} \backslash\{\mathbf{s}\}} \exp \left(\mathbf{R}_{\mathbf{s}, \mathbf{s}^{\prime \prime}} / \tau\right)}\right)\)</li>
  <li>SimMTM의 overall optimization loss는 다음과 같다 : \(\min _{\Theta} \mathcal{L}_{\text {reconstruction }}+\lambda \mathcal{L}_{\text {constraint }}\)
    <ul>
      <li>\(\mathcal{L}_{\text {constraint }}\)이 trivial aggregation이 발생하는 것에 대한 regularization 역할을 한다.</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<p><img src="/assets/img/timeseries/SimMTM/table1.jpeg" alt="사진4" /></p>
<ul>
  <li>Low-level downstream task인 forecasting, high-level downstream task인 classification을 수행하였다.</li>
  <li>비교한 SOTA 모델들
    <ul>
      <li>contrastive learning methd : TF-C, CoST, TS2Vec, LaST</li>
      <li>masked modeling method : <strong>Ti-MAE</strong>, TST, TF-C
<img src="/assets/img/timeseries/SimMTM/fig3.png" alt="사진5" /></li>
      <li>(x-axis) 왼쪽에 있을수록 MSE가 낮고, (y-axis) 위쪽에 있을수록 Accuracy가 높다.</li>
    </ul>
  </li>
  <li><img src="/assets/img/timeseries/SimMTM/table2.png" alt="사진6" /></li>
  <li><img src="/assets/img/timeseries/SimMTM/table3.png" alt="사진7" /></li>
  <li><img src="/assets/img/timeseries/SimMTM/table4.png" alt="사진8" />
    <ul>
      <li>SimMTM은 학습 데이터와 테스트 데이터가 다른 cross-domain setting에서도 forecasting과 classification 모두 다른 모델보다 뛰어나기 때문에 좋은 baseline 모델이라 할 수 있다.</li>
    </ul>
  </li>
  <li><img src="/assets/img/timeseries/SimMTM/fig4.png" alt="사진9" />
    <ul>
      <li>\(\min _{\Theta} \mathcal{L}_{\text {reconstruction }}+\lambda \mathcal{L}_{\text {constraint }}\) 두 항 모두 loss term에 있을 때에 성능이 더 좋았다.</li>
    </ul>
  </li>
  <li><img src="/assets/img/timeseries/SimMTM/fig5.png" alt="사진10" />
    <ul>
      <li>(left) SimMTM은 학습의 effectiveness가 다른 모델보다 높다. 즉 적은 데이터만으로도 valuable knowledge를 잘 파악한다.</li>
      <li>(right) SimMTM에서 masked ratio가 높을수록 많은 multiple masked series를 만들 때 성능이 높다는 직관과 부합하는 결과이다.</li>
    </ul>
  </li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>SimMTM은 new masked modeling 방법을 제시
    <ul>
      <li>reconstructs the original series from its multiple neighbor masked series</li>
      <li>aggregates the point-wise representations based on the series-wise similarities</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[NeurIPS 2023](https://arxiv.org/abs/2302.00861)]]></summary></entry></feed>