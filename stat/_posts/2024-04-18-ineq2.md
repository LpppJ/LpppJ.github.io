---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Inequalities for Probabilities

## 3. Inequalities for Probabilities

## 3.1. Markov's inequality

- For any $$t \geq 0$$ and a nonnegative random variable $$X$$, $$P(X \geq t) \leq \frac{\mathbb{E}[X]}{t}$$​​
- Markov's inequality는 extreme events의 확률의 upper bound를 제공한다는 점에서 의미가 있다.

### Proof

![그림1](/assets/img/stat/ineq2/fig1.png)

From $$\mathbb{1}(x \geq t) \leq(x / t) \mathbb{1}(x \geq 0)$$​, By taking the expectation on both sides.

- When become equality ?
  - the random variable is **constant** or almost constant일 때에 등식이 성립한다.
  - ex 1. $$\begin{aligned} X  =t \quad(p=1) \\ P(X>t)=1 \\ \mathbb{E}[X] / t  =t/t = 1\end{aligned}$$​​일 때 equality가 성립하고
  - ex 2. $$\begin{aligned} X = \begin{cases}0 \quad (p=1-k) \\ t \quad (p=k)\end{cases} \\ P(X>t)=k \\ E[X] / t = tk/t =k \end{aligned}$$인 경우에도 equality가 성립한다.
  - 즉 확률변수 $$X$$가 constant이거나(ex1), 0또는 constant를 가지는 경우(ex2)이다.

## 3.2. Chebyshev's inequality

- Let $$X$$ be a random variable and have finite mean $$\mu$$ and finite non-zero variance. Then, $$P(\left\vert X-\mu\right\vert \geq t) \leq \frac{\operatorname{Var}(X)}{t^2} .$$

### Proof

$$\begin{aligned}
& \text{By Markov's Inequality, }\\
& P\left((X-\mu)^2 \ge t^2\right) \le \frac{\mathbb{E[(X-\mu)^2]}}{t^2}\\
& \therefore P(\left\vert X-\mu\right\vert \geq t) \leq \frac{\operatorname{Var}(X)}{t^2}
\end{aligned}$$​​​

- $$X$$가 평균에서 $$t$$이상 떨어져있을 확률에 $$t^2$$를 곱해도 $$X$$의 분산보다 작다

- $$P[\left\vert X-\mu\right\vert \geq k \sigma] \le \frac{1}{k^2}$$로 표현한다면
  - $$X$$가 평균에서 ($$k \times$$분산) 만큼 떨어져있을 확률은 $$\frac{1}{k^2}$$​보다 작다.
  - Chebyshev's inequality는 확률변수 $$X$$의 평균으로부터의 extreme events가 발생할 확률에 대한 upper bound를 제공한다.

- When become equality ?
  - 증명에서 Markov's inequality를 사용했기 때문에, Markov's inequality의 등식이 성립했는지를 생각해보면 된다.
  - $$(X-\mu)^2$$가 0 또는 constant인 경우를 생각해야 하므로, 확률변수 $$X$$가 $$\mu$$ 또는 $$\mu-a, \mu+a$$만 될 수 있고 다른 값을 가질 확률이 0인 경우이다. (pdf가 $$\mu$$에 대해 symmetric한 경우)
  - 아래 경우에 왜 등식이 성립할 수 없는지 이해한다면 언제 등식이 성립하는지 알 수 있다.
  - ![그림2](/assets/img/stat/ineq2/fig2.jpeg)
  - $$(X-\mu)^2=\left\{\begin{array}{cc}0 & (p=0.4) \\ a^2 & (p=0.4) \\ b^2 & (p=0.2)\end{array} \quad P\left((X-\mu)^2 \geqslant t^2\right)=\left\{\begin{array}{cc}1 & \left(t^2=0\right) \\ 0.4 & \left(0<t^2 \leqslant a^2\right) \\ 0.2 & \left(a^2<t^2 \leqslant b^2\right) \\ 0 & \left(b^2<t^2\right)\end{array}\right.\right.$$​
  - $$\mathbb{E}[(X-\mu)^2] = 0.4a^2+0.2b^2$$이므로
    - 어떤 $$t^2$$로 나누어도 등식이 성립할 수 없는 이유는 $$a^2$$과 $$b^2$$의 계수가 다르기 때문이다.
    - 그러므로 둘 중 하나의 계수가 0이면 등식이 성립한다.
  - 위 그림에서 $$P(X=\mu-a)=P(X=\mu+a)=0$$이고 $$P(X=\mu-b)=P(X=\mu+b)=0.3$$이 되면
  - $$\begin{aligned}(X-\mu)^2=\left\{\begin{array}{cc}0 & (p=0.4) \\ b^2 & (p=0.6)\end{array} \quad P\left((X-\mu)^2 \geqslant t^2\right)=\left\{\begin{array}{cc}1 & \left(t^2=0\right) \\ 0.6 & \left(0<t^2 \leqslant b^2\right) \\ 0 & \left(b^2<t^2\right)\end{array}\right.\right.\end{aligned}$$이 되고
  - $$\mathbb{E}[(X-\mu)^2] = 0.6b^2$$이므로 $$t^2=b^2$$​에서 등식이 성립한다.

## 3.3. Hoeffding's inequality

- Let $$X_1, \ldots, X_n$$ be independent bounded random variables such that $$X_i$$ falls in the interval $$\left[a_i, b_i\right]$$ with probability one. Then for any $$t>0$$​, we have
  
- $$P\left\{\sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right) \geq t\right\} \leq e^{-2 t^2 / \sum_{i=1}^n\left(b_i-a_i\right)^2}$$,
- and $$P\left\{\sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right) \leq-t\right\} \leq e^{-2 t^2 / \sum_{i=1}^n\left(b_i-a_i\right)^2} $$
- Therefore $$P\left\{\left\vert {\frac{1}{n} \sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right)} \right\vert \geq t\right\} \leq 2 e^{-2 n^2 t^2 / \sum_{i=1}^n\left(b_i-a_i\right)^2}$$

### Proof

- For a given $$\lambda>0$$
- by Markov's inequality,

.$$\begin{aligned}
P\left\{\sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right) \geq t\right\}
& =P\left\{\exp \left(\lambda \sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right)\right) \geq \exp (\lambda t)\right\} \\
& \leq \exp (-\lambda t) \mathbb{E}\left[\exp \left(\lambda \sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right)\right)\right]
\end{aligned}$$

- by independence btw $$X_1, ..., X_n$$, and Hoeffding's Lemma (which is $$\mathbb{E}\left[e^{t(X-\mathbb{E}[X])}\right] \leq e^{t^2(b-a)^2 / 8}$$),
  
.$$\begin{aligned}
\mathbb{E}\left[\exp \left(\lambda \sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right)\right)\right]
& =\prod_{i=1}^n \mathbb{E}\left[\exp \left(\lambda\left(X_i-\mathbb{E}\left[X_i\right]\right)\right)\right] \\
& \leq \prod_{i=1}^n e^{\lambda^2\left(b_i-a_i\right)^2 / 8}=e^{\lambda^2 \sum_{i=1}^n\left(b_i-a_i\right)^2 / 8}
\end{aligned}$$​

- Therefore,

.$$\begin{aligned}
P\left\{\sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right) \geq t\right\}
&\leq e^{-\lambda t+\lambda^2 \sum_{i=1}^n\left(b_i-a_i\right)^2 / 8} \\
& \leq e^{\frac{\sum_{i=1}^n\left(b_i-a_i\right)^2}{8}(\lambda^2-\frac{8t}{\sum_{i=1}^n\left(b_i-a_i\right)^2}\lambda+(\frac{4t}{\sum_{i=1}^n\left(b_i-a_i\right)^2})^2-(\frac{4t}{\sum_{i=1}^n\left(b_i-a_i\right)^2})^2)} \\
& = e^{\frac{\sum_{i=1}^n\left(b_i-a_i\right)^2}{8}(\lambda-\frac{4t}{\sum_{i=1}^n\left(b_i-a_i\right)^2})^2-\frac{2t^2}{\sum_{i=1}^n\left(b_i-a_i\right)^2}} \\
& \leq e^{\frac{-2t^2}{\sum_{i=1}^n\left(b_i-a_i\right)^2}}
\end{aligned}$$

- Proof : Hoeffding's Lemma ($$\mathbb{E}\left[e^{t(X-\mathbb{E}[X])}\right] \leq e^{t^2(b-a)^2 / 8}$$)
  - Step 1 : Hoeffding's Lemma을 위해 다음 등식을 유도한다.
  - For the Rademacher R.V $$X$$ (that take the value $$\{-1, 1\}$$),
  - .$$\begin{aligned}
    \mathbb{E}[\exp (t X)] & =\frac{1}{2}[\exp (t)+\exp (-t)] \\
    & =\frac{1}{2}\left[\sum_{k=0}^{\infty} \frac{t^k}{k !}+\sum_{k=0}^{\infty} \frac{(-t)^k}{k !}\right] \\
    & =\sum_{k=0}^{\infty} \frac{t^{2 k}}{(2 k) !} \leq \sum_{k=0}^{\infty} \frac{t^{2 k}}{2^k k !} \\
    & =\exp \left(t^2 / 2\right)
    \end{aligned}$$
  - Step 2 : Symmetrization
  - For the R.V $$X$$ with zero mean, support $$[a,b]$$, and $$X^{\prime}$$ which is independent copy of $$X$$​,
  - .$$\begin{aligned}
    \mathbb{E}_X[\exp (t X)]& = \mathbb{E}_X[\exp(t(X-\mathbb{E}[X^{\prime}]))]\\ & \le \mathbb{E}_{X, X^{\prime}}\left[\exp \left(t\left(X-X^{\prime}\right)\right]\right. \quad \text{by Jensen's inequality}\\ & =\mathbb{E}_{X, X^{\prime}}\left[\mathbb{E}_\epsilon\left[\exp \left(t \epsilon\left(X-X^{\prime}\right) \mid X, X^{\prime}\right]\right]\right. \\
    & \leq \mathbb{E}_{X, X^{\prime}}\left[\exp \left(t^2\left(X-X^{\prime}\right)^2 / 2\right]\right. \quad \text{by step 1}\\ & \le \exp(t^2(b-a)^2/2)
    \end{aligned}$$​
  - 추가적인 theorem으로 $$\exp(t^2(b-a)^2/8)$$까지 bound를 낮출 수 있다. (skip the proof)

### Comparison btw Chebyshev's inequality vs. Hoeffding's inequality

- $$X$$가 Rademacher R.V (-1 또는 1이 되는데 각각 1/2확률)일 때, $$\mathbb{E}[\bar X]=0, Var(\bar X)=\frac{1}{n}$$이므로
- Chebyshev's inequality는 $$P(\left\vert{ X-\mu }\right\vert \geq t) \leq \frac{\operatorname{Var}(X)}{t^2}$$​이고, 우측 항을 $$\delta$$로 만들면 아래와 같다.
  - $$\begin{aligned}
    & P(|\bar{X}-\mathbb{E}[\bar{X}]| \geq t) \leq \frac{1}{n t^2} \\
    & \Longleftrightarrow P\left(|\bar{X}-\mathbb{E}[\bar{X}]| \geq \sqrt{\frac{1}{n \delta}}\right) \leq \delta \end{aligned}$$​
- Hoeffding's inequality는 $$P\left\{\left\vert{ \frac{1}{n} \sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right)}\right\vert \geq t\right\} \leq 2 e^{-2 n^2 t^2 / \sum_{i=1}^n\left(b_i-a_i\right)^2}$$이고, 우측 항을 $$\delta$$로 만들면 아래와 같다.
  - $$\begin{aligned}
    & P(|\bar{X}-\mathbb{E}[\bar{X}]| \geq t) \leq 2 e^{-n t^2 / 2} \\
    & \Longleftrightarrow P\left(|\bar{X}-\mathbb{E}[\bar{X}]| \geq \sqrt{\frac{2}{n} \log \left(\frac{2}{\delta}\right)}\right) \leq \delta .
    \end{aligned}$$​
- $$\delta \to 0$$일 때 Chebyshev's의 bound는 polynomially 커지고, Hoeffding's의 bound는 logarithmically 커진다.
  - $$\delta=0.001$$일 때, 즉 99.9%의 확률에 대한 bounds는
    Chebyshev's는 $$\sqrt{\frac{1}{n \times 0.001}} \approx \frac{31}{\sqrt{n}}$$ 
    Hoeffding's는 $$\sqrt{\frac{2}{n} \log \left(\frac{2}{0.001}\right)} \approx \frac{2.57}{\sqrt{n}}$$이다.
- 결론적으로, Hoeffding's inequality가 tail (extreme) probability에 대해 훨씬 tight한 bound를 제공한다.

### Example : Classification error

- Classifier $$h$$의 error $$R(h)=P(Y \neq h(X))$$는 $$\widehat{R}_n(h)=\frac{1}{n} \sum_{i=1}^n \mathbb{1}\left(Y_i \neq h\left(X_i\right)\right)$$로 추정한다.
- By Hoeffding's inequality,
  - $$P\left(\left\vert{ \widehat{R}_n(h)-R(h)} \right\vert \geq t\right) \leq 2 e^{-2 n t^2} \quad \text { for all } t \geq 0$$이다.
  - 즉 $$\left\vert{ \widehat{R}_n(h)-R(h)} \right\vert \leq \sqrt{\frac{1}{2 n} \log \left(\frac{2}{\delta}\right)}$$ with probability at least $$1-\delta$$

-  Hoeffding's inequality는 확률변수의 범위에 대한 가정을 제외하면 어떠한 정보(가정)도 사용하지 않는다는 점에서 유용하다.

## 3.4. Bernstein's inequality

-  **R.V $$X$$의 분산이 작으면** Hoeffding's inequality보다 sharper inequality인 Bernstein's inequality가 된다.

- Let $$X_1, \ldots, X_n$$ be independent and suppose that $$\left\vert {X_i} \right\vert \leq c$$ and $$\mathbb{E}\left[X_i\right]=\mu$$.
- Then for any $$t \geq 0,\ P\left(\left\vert{\frac{1}{n} \sum_{i=1}^n X_i-\mu}\right\vert \geq t\right) \leq 2 \exp \left\{-\frac{n t^2}{2 \sigma^2+2 c t / 3}\right\}$$
  - where $$\sigma^2=\frac{1}{n} \sum_{i=1}^n \operatorname{Var}\left(X_i\right)$$​

- Skip the proof
- 확률변수의 분산이 작으면, $$2 \sigma^2+\frac{2 c t}{3}<2 c^2$$이면 Hoeffding's inequality보다 tight하다.
  - Hoeffding's inequality $$P\left\{\left\vert{\frac{1}{n} \sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right)}\right\vert \geq t\right\} \leq 2 e^{-2 n^2 t^2 / \sum_{i=1}^n\left(b_i-a_i\right)^2}$$는 $$\left\vert{X_i}\right\vert \leq c$$에서 $$P\left\{\left\vert{\frac{1}{n} \sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right)}\right\vert \geq t\right\} \leq 2 e^{-2 n^2 t^2 / \sum_{i=1}^n\left(2c\right)^2}=2 e^{-2 n^2 t^2 / n4c^2}=2 e^{-n t^2 / 2c^2}$$이다.
  - $$2 e^{-n t^2 / 2c^2}$$보다 $$2 \exp \left\{-\frac{n t^2}{2 \sigma^2+2 c t / 3}\right\}$$가 더 **작은** 경우를 의미한다.
    - Error의 upper bound가 더 작은 것이 더 tight한 것이니까

## 3.5. McDiarmid's inequality

- Suppose that $$\left(X_1, \ldots, X_n\right)$$ are independent random variables, and let $$g: \mathcal{X}^n \mapsto \mathbb{R}$$ satisfy **the bounded difference property** with constants $$c_1, \ldots, c_n$$​.
  - **the bounded difference property**의 의미는
  - : $$\sup _{x_1, \ldots, x_n, x_i^{\prime} \in \mathcal{X}}\left\vert{g\left(x_1, \ldots, x_n\right)-g\left(x_1, \ldots, x_{i-1}, x_i^{\prime}, x_{i+1}, \ldots, x_n\right)}\right\vert \leq c_i, \quad \text { for } 1 \leq i \leq n$$​ 
  - 즉 $$x_i$$를 $$x^{\prime}_i$$로 replace했을 때 발생하는 g의 difference가 $$c_i$$보다 작다, bounded 되어있다.
- Then $$P\left(\left\vert{g\left(X_1, \ldots, X_n\right)-\mathbb{E}\left[g\left(X_1, \ldots, X_n\right)\right]}\right\vert \geq t\right) \leq 2 \exp \left(-\frac{2 t^2}{\sum_{i=1}^n c_i^2}\right)$$​

### Proof

- 쉽게 말해서
  - 왼쪽에 있는 $$g\left(X_1, \ldots, X_n\right)$$는 $$X_1, ..., X_n$$​이 아무것도 given이 아닌 상태인데,
    - $$X_1$$이 given이 되었을 때 생기는 g의 difference $$Z_1$$은 $$\to$$ $$c_1$$보다 작으니 interval $$[0,c_1]$$에 bounded,
    - $$X_2$$이 given이 되었을 때 생기는 g의 difference $$Z_2$$는 $$\to$$ $$c_2$$보다 작으니 interval $$[0,c_2]$$에 bounded,
  - 오른쪽에 있는 $$\mathbb{E}[g(X_1, ..., X_n)]\ \text{는 }\mathbb{E}\left[\mathbb{E}[g(X_1,...,.X_n)\mid X_1, .., X_n]\right]$$이라는 점에서
    - $$X_1, ..., X_n$$이 모두 given인 때로 볼 수 있으므로
    - 왼쪽에 있는 $$g\left(X_1, \ldots, X_n\right)$$과의 difference $$\sum Z_i$$는 $$\to$$  $$\displaystyle\sum_{i=1}^n c_i$$보다 작으니 interval $$[0,\displaystyle\sum_{i=1}^n c_i]$$에 bounded
  - 이제 Hoeffding's inequality : $$P\left\{\sum_{i=1}^n\left(X_i-\mathbb{E}\left[X_i\right]\right) \geq t\right\} \leq e^{-2 t^2 / \sum_{i=1}^n\left(b_i-a_i\right)^2}$$에 의해
    - $$P\left(\left\vert{g\left(X_1, \ldots, X_n\right)-\mathbb{E}\left[g\left(X_1, \ldots, X_n\right)\right]}\right\vert \geq t\right) \leq 2 \exp \left(-\frac{2 t^2}{\sum_{i=1}^n c_i^2}\right)$$가 된다.

## 3.6. DKW inequality

- There exists a finite positive constant $$C$$ such that $$P\left(\sup _{x \in \mathbb{R}}\left\vert{F_n(x)-F(x)}\right\vert \geq t\right) \leq C e^{-2 n t^2} \quad \text { for all } t \geq 0$$
- The best possible constant $$C$$ is known as 2

Pass