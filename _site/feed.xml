<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-04-14T14:26:44+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">Total variation distance and Convergence in distribution</title><link href="http://localhost:4000/stat/2024-04-14-dTV/" rel="alternate" type="text/html" title="Total variation distance and Convergence in distribution" /><published>2024-04-14T00:00:00+09:00</published><updated>2024-04-14T14:25:28+09:00</updated><id>http://localhost:4000/stat/dTV</id><content type="html" xml:base="http://localhost:4000/stat/2024-04-14-dTV/"><![CDATA[<ul>
  <li>통계학에서 Convergence in distribution은 중요한 성질이다. 우리는 Sample size n이 커질수록 확률변수 또는 estimator가 어떤 분포에 가까워지는지 궁금하기 때문이다.</li>
  <li>그러므로 Convergence in distribution을 imply하는 성질도 중요하다. Convergence in distribution을 보일 수 있는 방법들 중 하나가 되기 때문이다.</li>
  <li>Total variation distance는 두 분포 사이의 distance metric 중 하나이다. Total variation distance가 작으면 두 분포가 가깝다는 의미이다.</li>
  <li>본 게시글에서 Total variation distance에 대해서 알아보고 <strong>Total variation distance가 0으로 수렴</strong>하면 한 분포가 다른 분포로 근사한다, 즉 Convergence in distribution한다는 것을 보인다.</li>
</ul>

<h2 id="total-variation-distanced_tv">Total variation distance(\(d_{TV}\))</h2>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures">Wikipedia</a>에 따르면 Total variation distance의 정의는 아래와 같다.
    <ul>
      <li>Consider a measurable space \((\Omega, \mathcal{F})\) and probability measures \(P\) and \(Q\) defined on \((\Omega, \mathcal{F})\). The total variation distance between \(P\) and \(Q\) is defined as: \(\delta(P, Q)=\sup _{A \in \mathcal{F}}\mid P(A)-Q(A)\mid\)​</li>
    </ul>
  </li>
  <li>\(d_{TV}\)​는 half of the L1 distance btw/ the probability functions이다.
    <ul>
      <li><img src="/assets/img/stat/dTV/fig1.png" alt="그림1" /></li>
      <li>Geometric 의미는 두 분포의 차이에 해당하는 회색 영역의 넓이를 2로 나눈 값이다.</li>
      <li>discrete : \(\delta(P, Q)=\frac{1}{2} \sum_x\mid P(x)-Q(x)\mid\)</li>
      <li>continuous : \(\delta(P, Q)=\frac{1}{2} \int\mid p(x)-q(x)\mid  \mathrm{d} x\)​</li>
      <li>\(\frac{1}{2}\)를 곱해주는 이유는 \(0 \le d_{TV} \le 1\)으로 만들어주기 위함인데, 아래 사진처럼 \(d_{TV}\)가 매우 큰 경우에 두 분포의 차이의 넓이가 2에 가까워지기 때문에 \(\frac{1}{2}\)를 곱한다.</li>
      <li><img src="/assets/img/stat/dTV/fig2.png" alt="그림2" /></li>
    </ul>
  </li>
</ul>

<h2 id="convergence-in-total-variation-distance">Convergence in Total variation distance</h2>

<ul>
  <li>Convergence in Total variation distance는 Convergence in distribution보다 강력한 성질이다.</li>
  <li>
    <p><strong>즉 두 분포의 Total variation distance가 0으로 수렴하면, 두 분포는 Convergence in distribution이다.</strong></p>

    <p>: \(\begin{aligned}\text{When }\delta(P, Q)&amp;=\sup _{A \in \mathcal{F}}\mid P(A)-Q(A)\mid  \\ \lim _{n \rightarrow \infty} \delta\left(P_n, Q\right)&amp;=0 \\ \lim _{n \rightarrow \infty}\mid P_n(A)-Q(A)\mid &amp;=0 \quad \forall A \in \mathcal{F} \\ \lim _{n \rightarrow \infty}\mid F_n(x)-F(x)\mid &amp;=0 \end{aligned}\)</p>
  </li>
  <li>
    <p>하지만 역은 성립하지 않는다. Convergence in distribution이라고 해서 Total variation distance가 0으로 수렴하는 것은 아니다.</p>

    <ul>
      <li>Counterexample : \(f_n \sim \frac{2}{\pi} \cos ^2(n x) \mathbf{1}_{[0, \pi]}(x) d x\)</li>
    </ul>

    <p>: \(\begin{aligned}F_n(x)&amp;=\int_0^x f_n(t)dt \\ &amp;=\int_0^x\frac{2}{\pi} \cos ^2(nt) \mathbf{1}_{[0, \pi]}(x)dt \\ &amp; = \frac{2}{\pi}\mathbf{1}_{[0, \pi]}(x) \int_0^x \cos ^2(nt)dt \\ &amp;= \frac{2}{\pi}\mathbf{1}_{[0, \pi]}(x) \frac{sin(2nx)+2nx}{4n} \\  \lim_{n \to \infty} F_n(x) &amp;=\frac{2}{\pi}\mathbf{1}_{[0, \pi]}(x) \frac{x}{2} \\ &amp;=\frac{x}{\pi},\quad 0 \le x \le \pi \end{aligned}\)</p>

    <ul>
      <li>즉 \(\lim_{n \to \infty} F_n = F_{\infty}\)는 \(Unif(0, \pi)\)의 CDF가 되므로 Convergence in distribution이다.</li>
      <li>하지만 \(\lim_{n \to \infty}d_{TV}(F_n, F_{\infty}) \nrightarrow 0\)이다.</li>
    </ul>

    <p>: \(\begin{aligned} d_{TV}\left(F_n, F_{\infty}\right) &amp; =\frac{1}{2} \int_0^\pi\mid f_n(x)-f_{\infty}(x)\mid  d x \\ &amp; =\frac{1}{2} \int_0^\pi\mid \frac{2}{\pi} \cos ^2(n x)-\frac{1}{\pi}\mid  d x \\ &amp; =\frac{1}{\pi} \int_0^\pi\mid \cos ^2(n x)-\frac{1}{2}\mid  d x \\ &amp; &gt;0 \end{aligned}\)</p>

    <ul>
      <li><img src="/assets/img/stat/dTV/fig3.png" alt="그림3" /></li>
      <li>\(cos^2(nx)\)는 함수값 0과 1를 주기적으로 가지는 함수이고, \(n\)이 커져도 주기가 짧아질 뿐 함수값 0과 1을 주기적으로 가진다는 것은 변하지 않기 때문에, \(\mid \cos ^2(n x)-\frac{1}{2}\mid\)는 strictly positive이다.</li>
      <li>그러므로 Total variation distance가 0으로 수렴하지 않는다.</li>
      <li>만약 \(f_n, f_{\infty}\)​가 continuous random variable의 density이고 unimodal한 경우에는 Convergence in distribution과 Total variation distance이 0으로 수렴하는 것은 if and only if(필요충분)이다. (skip the proof)</li>
      <li><a href="https://hal.science/hal-00821911/document">Reference : Ivan Nourdin, Guillaume Poly, Convergence in law implies convergence in total variation for polynomials in independent Gaussian, Gamma or Beta random variables</a></li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[통계학에서 Convergence in distribution은 중요한 성질이다. 우리는 Sample size n이 커질수록 확률변수 또는 estimator가 어떤 분포에 가까워지는지 궁금하기 때문이다. 그러므로 Convergence in distribution을 imply하는 성질도 중요하다. Convergence in distribution을 보일 수 있는 방법들 중 하나가 되기 때문이다. Total variation distance는 두 분포 사이의 distance metric 중 하나이다. Total variation distance가 작으면 두 분포가 가깝다는 의미이다. 본 게시글에서 Total variation distance에 대해서 알아보고 Total variation distance가 0으로 수렴하면 한 분포가 다른 분포로 근사한다, 즉 Convergence in distribution한다는 것을 보인다.]]></summary></entry><entry><title type="html">MGF(Moment Generating Function) and Characteristic Function에 대해서</title><link href="http://localhost:4000/stat/2024-04-12-MGF/" rel="alternate" type="text/html" title="MGF(Moment Generating Function) and Characteristic Function에 대해서" /><published>2024-04-12T00:00:00+09:00</published><updated>2024-04-13T15:39:49+09:00</updated><id>http://localhost:4000/stat/MGF</id><content type="html" xml:base="http://localhost:4000/stat/2024-04-12-MGF/"><![CDATA[<h2 id="moments">Moments</h2>

<ul>
  <li>Definition
    <ul>
      <li>For each integer \(n\), the \(n\)th moment of \(X\) is \(\mu_n=\mathbb{E}\left[X^n\right]\).</li>
      <li>The \(n\)th central moment of \(X\) is \(\mu_n^{\prime}=\mathbb{E}\left[(X-\mu)^n\right]\) where \(\mu=\mu_1^{\prime}=\mathbb{E}[X]\)</li>
    </ul>
  </li>
  <li><u>확률변수의 moments는 tail probability와 밀접한 연관이 있다.</u>
    <ul>
      <li>
        <p>Let \(X\) is non-negative random variable</p>

        <p>: \(\begin{aligned}\mathbb{E}[X]&amp;=\mathbb{E}\left[\int_0^{\infty} \mathbb{1}(t&lt;X) d t\right]\\&amp;=\int_0^{\infty} \mathbb{E}[\mathbb{1}(t&lt;X)] d t\\&amp;=\int_0^{\infty} P(t&lt;X) d t\\&amp;=\int_0^{\infty}\left[1-F_X(x)\right] d x \end{aligned}\)</p>
      </li>
      <li>
        <p>위 결과를 확장하면 아래와 같다.</p>

        <p>: \(\begin{aligned} \mathbb{E}\left[\mid X\mid ^p\right] &amp; =\int_0^{\infty}\left[1-F_{\mid X\mid ^p}(y)\right] d y \\ P\left(\mid X\mid ^p&gt;y\right) &amp; =P\left(\mid X\mid &gt;y^{\frac{1}{p}}\right) \\ &amp; =1-F_{\mid X\mid }\left(y^{\frac{1}{p}}\right) \\ \text{Let } y&amp;=x^p, \text{then } \frac{d y}{d x}=p x^{p-1} \\ \mathbb{E}\left[\mid X\mid ^p\right] &amp; =\int_0^{\infty}\left[1-F_{\mid X\mid }(x)\right] p x^{p-1} d x \\ &amp; =\int_0^{\infty} p x^{p-1} P(\mid X\mid &gt;x) d x \end{aligned}\)</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="moment-generating-function">Moment Generating Function</h2>

<ul>
  <li>
    <p>Definition : \(M_X(t)=\mathbb{E}\left[e^{t X}\right]\)</p>
  </li>
  <li>
    <p>MGF가 왜 중요할까 ?</p>

    <ul>
      <li><u>첫째, 확률변수 X의 MGF는 X의 모든 moments를 생성한다.</u></li>
      <li><u>둘째, 확률변수 X의 MGF는 X의 분포를 uniquely 결정한다.</u></li>
      <li><u>셋째, MGF의 수렴은 분포수렴(convergence in distribution)을 의미한다.</u></li>
      <li><u>넷째, MGF로 probability tail bound를 구한다. (Hoeffding's inequality)</u></li>
    </ul>
  </li>
</ul>

<h3 id="첫째-확률변수-x의-mgf는-x의-모든-moments를-생성한다">첫째, 확률변수 X의 MGF는 X의 모든 moments를 생성한다.</h3>

<ul>
  <li>
    <p>Step1) MGF가 존재한다고 가정했다면 \(\mathbb{E}\left[e^{\mid t X\mid }\right] \leq \mathbb{E}\left[e^{t X}\right]+\mathbb{E}\left[e^{-t X}\right]&lt;\infty \quad \text { for all }\mid t\mid &lt;\epsilon\)​</p>
  </li>
  <li>
    <p>c.f. <strong>(Dominated Convergence Theorem)</strong>. If \(\left\{f_n: \mathbb{R} \mapsto \mathbb{R}\right\}\) is a sequence of measurable functions which converge pointwise almost everywhere to \(f\), and if there exists an integrable function \(g\) (that is \(\left.\int_{-\infty}^{\infty}\mid g(x)\mid  d x&lt;\infty\right)\) such that \(\mid f_n(x)\mid  \leq\mid g(x)\mid\) for all \(n\) and for all \(x\), then \(f\) is integrable and</p>

    <ul>
      <li>\(\int_{-\infty}^{\infty} f(x) d x=\int_{-\infty}^{\infty} \lim _{n \rightarrow \infty} f_n(x) d x=\lim _{n \rightarrow \infty} \int_{-\infty}^{\infty} f_n(x) d x\)​​</li>
      <li>즉 \(\mid f_n(x)\mid\)의 bound 역할을 하는 \(\mid g(x)\mid\)가 \(\int_{-\infty}^{\infty}\mid g(x)\mid  d x&lt;\infty\)이면 \(\int_{-\infty}^{\infty} \lim _{n \rightarrow \infty} f_n(x) d x\)의 \(\int\)와 \(\lim\)의 위치를 바꿀 수 있다.</li>
    </ul>
  </li>
  <li>
    <p>Step2) DCT에 의해 X의 MGF는 아래와 같다.</p>

    <ul>
      <li>MGF</li>
    </ul>

    <p>: \(\begin{aligned}\int_{-\infty}^{\infty} e^{t x} f_X(x) d x &amp; =\lim _{n \rightarrow \infty} \int_{-\infty}^{\infty} \sum_{j=0}^n t^j \frac{x^j}{j !} f_X(x) d x \\ &amp; =\lim _{n \rightarrow \infty} \sum_{j=0}^n \frac{t^j}{j !} \underbrace{\int_{-\infty}^{\infty} x^j f_X(x) d x}_{=\mu_j} \\ &amp; =\sum_{j=0}^{\infty} \frac{t^j \mu_j}{j !} \end{aligned}\)</p>

    <ul>
      <li>moment</li>
    </ul>

    <p>: \(\begin{aligned} M_X^{(m)}(t) &amp; =\frac{d^m}{d t^m}\left(\sum_{k=0}^{\infty} \frac{t^k \mu_k}{k !}\right) \\ &amp; =\sum_{k=0}^{\infty} \frac{\mu_k}{k !} \frac{d^m\left(t^k\right)}{d t^m} \\ &amp; =\sum_{k=m}^{\infty} \mu_k \frac{t^{k-m}}{(k-m) !} \\ &amp; =\sum_{j=0}^{\infty} \mu_{m+j} \frac{t^j}{j !} \quad(j \equiv k-m) \end{aligned}\)</p>
    <ul>
      <li>Thus, \(M_X^{(m)}(0)=\mathbb{E}\left[X^m\right] \text { by noting } 0^0=1,0^j=0 \text { for } j \geq 1\)</li>
    </ul>
  </li>
</ul>

<h3 id="둘째-확률변수-x의-mgf는-x의-분포를-uniquely-결정한다">둘째, 확률변수 X의 MGF는 X의 분포를 uniquely 결정한다.</h3>

<ul>
  <li>
    <p>확률변수 X와 Y의 분포가 같다면 \(\to\) MGF가 같다는 것을 보이는 것은 쉽다.</p>
  </li>
  <li>
    <p>반대로 두 확률변수 X와 Y의 MGF가 같으면 같은 분포를 따름은 아래와 같이 증명한다.</p>
  </li>
  <li>
    <p>Suppose that \(X\) and \(Y\) have the same MGF for all \(t\) : \(\sum_{x=0}^n e^{t x} f_X(x)=\sum_{y=0}^n e^{t y} f_Y(y)\)
  Let \(s=e^t\) and \(c_i=f_X(i)-f_Y(i)\) for \(i=0,1, \ldots, n\)</p>
  </li>
</ul>

<p>: \(\begin{aligned} \sum_{x=0}^n e^{t x} f_X(x)-\sum_{y=0}^n e^{t y} f_Y(y)=0 \\ \Rightarrow \sum_{x=0}^n s^x f_X(x)-\sum_{y=0}^n s^y f_Y(y)=0 \\ \Rightarrow \sum_{x=0}^n s^x f_X(x)-\sum_{x=0}^n s^x f_Y(x)=0 \\ \Rightarrow \sum_{x=0}^n s^x\left[f_X(x)-f_Y(x)\right]=0 \\ \Rightarrow \sum_{x=0}^n s^x c_x=0 \quad \forall s&gt;0 \end{aligned}\)</p>

<ul>
  <li>
    <p>The above is simply a polynomial in \(\mathrm{s}\) with coefficients \(c_0, c_1, \ldots, c_n\). The only way it can be zero for all values of \(\mathrm{s}\) is if \(c_0=c_1=\cdots=c_n=0\)</p>
  </li>
  <li>
    <p>Therefore, \(0=c_i=f_X(i)-f_Y(i)\) for \(i=0,1, \ldots, n\), which means \(f_X(i)=f_Y(i)\) for \(i=0,1, \ldots, n\).</p>
  </li>
</ul>

<h3 id="셋째-mgf의-수렴은-분포수렴convergence-in-distribution을-의미한다">셋째, MGF의 수렴은 분포수렴(convergence in distribution)을 의미한다.</h3>

<ul>
  <li>즉 \(\lim _{n \rightarrow \infty} M_{X_n}(t)=M_X(t)\)이면 \(\lim _{n \rightarrow \infty} F_{X_n}(x)=F_X(x)\)이다.</li>
  <li><strong>(Portmanteau lemma)</strong> For random vector \(Y_n\) and \(Y\) with \(Y_n \sim P_n\) and \(Y \sim P\), the following are equivalent :
    <ul>
      <li>\(Y_n \xrightarrow{d} Y\);</li>
      <li>\(P\left(Y_n \leq t\right) \rightarrow P(Y \leq t)\) for all continuous points \(t\) of \(t \mapsto P(Y \leq t)\);</li>
      <li>\(\mathbb{E}\left[g\left(Y_n\right)\right] \rightarrow \mathbb{E}[g(Y)]\) for all bounded, continuous \(g: \mathbb{R}^d \rightarrow \mathbb{R}\)​;</li>
      <li>(Skip the proof)</li>
      <li>그러므로 아래 (Lévy’s continuity theorem)가 성립한다.</li>
    </ul>
  </li>
  <li>
    <p><strong>(Lévy’s continuity theorem)</strong> Let \(X_n\) and \(X\) be random vectors in \(\mathbb{R}^d\). Then \(X_n \xrightarrow{d} X\) if and only if \(\lim _{n \rightarrow \infty} \mathbb{E}\left[e^{i t^{\top} X_n}\right]=\mathbb{E}\left[e^{i t^{\top} X}\right]\) for every \(t \in \mathbb{R}^d\)</p>
  </li>
  <li>위의 high-dimension에서의 증명은 (Cramér–Wold device)를 통해 1-dimension에서도 성립된다.</li>
  <li><strong>(Cramér-Wold device)</strong> If \(t^{\top} X_n \xrightarrow{d} t^{\top} X\) for all \(t \in \mathbb{R}^d\), then \(X_n \xrightarrow{d} X\)</li>
</ul>

<h3 id="넷째-mgf로-probability-tail-bound를-구한다-hoeffdings-inequality">넷째, MGF로 probability tail bound를 구한다. (Hoeffding’s inequality)</h3>
<ul>
  <li>pass</li>
</ul>

<h2 id="characteristic-function">Characteristic Function</h2>

<ul>
  <li>Definitnion : \(\phi_X(t)=\mathbb{E}[\exp (i t X)]=\mathbb{E}[\cos (t X)+i \sin (t X)], \quad t \in \mathbb{R},\)
    <ul>
      <li>where \(i=\sqrt{-1}\) is the imaginary unit (note Euler’s formula: \(e^{i x}=\cos (x)+i \sin (x)\)​)</li>
    </ul>
  </li>
  <li>
    <p>characteristic function은 언제 사용할까</p>

    <ul>
      <li>MGF가 존재하지 않는 분포(ex. Cauchy)에 대한 특성을 파악할 때</li>
      <li>Lévy’s continuity theorem 증명할 때</li>
    </ul>
  </li>
  <li>
    <p>Element properties:</p>

    <ul>
      <li>
        <p>If \(Y=a X+b, \phi_Y(t)=e^{i b t} \phi_X(a t)\).</p>
      </li>
      <li>
        <p>If \(X\) and \(Y\) are independent, then \(\phi_{X+Y}(t)=\phi_X(t) \phi_Y(t)\)​</p>
      </li>
    </ul>
  </li>
  <li>
    <p>아래 4가지 properties를 하나씩 증명한다.</p>

    <ul>
      <li>
        <p>첫째, \(\phi_X(0)=1\) and \(\mid \phi_X(t)\mid \leq 1\) for all \(t \in \mathbb{R}\)​</p>
      </li>
      <li>둘째, \(\phi_X(t)\) is uniformly continuous on  \(\mathbb{R}\)​​
        <ul>
          <li>즉 \(h \to 0\)에 따라 \(\psi(h) \rightarrow 0\) s.t.  \(\mid \phi_X(t+h)-\phi_X(t)\mid \leq \psi(h)\)</li>
        </ul>
      </li>
      <li>
        <p>셋째, The characteristic function of a symmetric random variable, that is  \(X \stackrel{d}{=}-X\), is real-valued</p>
      </li>
      <li>넷째, MGF처럼 두 확률변수의 characteristic function이 같으면 같은 분포를 따른다.</li>
    </ul>
  </li>
</ul>

<h3 id="첫째-phi_x01-and-mid-phi_xtmid-leq-1-for-all-t-in-mathbbr">첫째, \(\phi_X(0)=1\) and \(\mid \phi_X(t)\mid \leq 1\) for all \(t \in \mathbb{R}\)</h3>

<ul>
  <li>\(\mid e^{i t x}\mid =\sqrt{\cos ^2(t x)+\sin ^2(t x)}=1\) for all \(t\) and \(x\). Therefore \(\mid \phi_X(t)\mid =\mid \mathbb{E}\left[e^{i t X}\right]\mid  \leq \mathbb{E}\left[\mid e^{i t X}\mid \right]=1\)</li>
</ul>

<h3 id="둘째-phi_xt-is-uniformly-continuous-on--mathbbr">둘째, \(\phi_X(t)\) is uniformly continuous on  \(\mathbb{R}\)</h3>

<ul>
  <li>\(\mid \underbrace{\mathbb{E}\left[e^{i(t+h) X}\right]}_{=\phi_X(t+h)}-\underbrace{\mathbb{E}\left[e^{i t X}\right]}_{=\phi_X(t)}\mid =\mid \mathbb{E}\left[e^{i t X}\left(e^{i h X}-1\right)\right]\mid  \leq \mathbb{E}\left[\mid e^{i h X}-1\mid \right]\) 이고</li>
  <li>Let \(g_X(h)=\mid e^{i h X}-1\mid , \text{ Then } g_X(h) \rightarrow 0 \text{ as } h \to 0\) 이므로</li>
</ul>

<p>: \(\begin{aligned} g_X(h) &amp; =\mid e^{i h X}-1\mid  \\ &amp; =\mid \{\cos (h X)-1\}+i \sin (h X)\mid  \\ &amp; =\sqrt{\{\cos (h X)-1\}^2+\sin ^2(h X)} \\ &amp; =\sqrt{2-2 \cos (h X)} \leq 2 \end{aligned}\)</p>

<ul>
  <li>\(g_X(h)\)가 uniformly bounded by 2이므로 DCT에 의해 \(\mathbb E[g_X(h)] \rightarrow 0 \text{ as } h \to 0\)</li>
</ul>

<h3 id="셋째-x-stackreld-x-characteristic-function-of-x는-real-valued">셋째, \(X \stackrel{d}{=}-X\), characteristic function of \(X\)는 real-valued</h3>

<ul>
  <li>
    <p>: \(\begin{aligned} \mathbb{E}\left[e^{i t X}\right] &amp; =\mathbb{E}[\cos (t X)]+i \mathbb{E}[\sin (t X)] \stackrel{(\mathrm{i})}{=} \mathbb{E}[\cos (t X)]+i \mathbb{E}[\sin (-t X)] \\ &amp; \stackrel{(\mathrm{ii})}{=} \mathbb{E}[\cos (t X)]-i \mathbb{E}[\sin (t X)] \end{aligned}\).</p>
  </li>
  <li>
    <p>(i)는 \(X \stackrel{d}{=}-X\) 때문이고, (ii)는 sin의 성질 \(\sin (-x)=-\sin (x)\) 때문이다.</p>
  </li>
</ul>

<h3 id="넷째-mgf처럼-두-확률변수의-characteristic-function이-같으면-같은-분포를-따른다">넷째, MGF처럼 두 확률변수의 characteristic function이 같으면 같은 분포를 따른다.</h3>

<ul>
  <li>두 확률변수가 characteristic function이 같으면 같은 분포를 따름만 증명한다.</li>
  <li>본 증명에는 다양한 기법이 사용되는데 그 중 몇 가지를 소개한다. 아래와 같다.
    <ul>
      <li>Convolution : \(X\)와 \(Y\)가 독립이면 \(Z=X+Y\)의 density function은 \(f_Z(z)=\int_{-\infty}^{\infty} f_X(x) f_Y(z-x) d x\)​</li>
      <li>Characteristic function of \(Z \sim N(0, \sigma^2)\) : \(\mathbb{E}\left[e^{i t Z}\right]=e^{-\frac{\sigma^2 t^2}{2}} \text { for all } t \in \mathbb{R}\)​</li>
      <li>Fubini’s theorem : \(\int_{X \times Y}\mid f(x, y)\mid  d(x, y)&lt;\infty\) 이면 \(\int\)의 순서를 바꿀 수 있다.</li>
    </ul>
  </li>
  <li>\(X+Z\)와 \(Y+Z\)가 같은 분포를 따른다는 것을 증명하고 Slutsky’s theorem으로  \(X\)와 \(Y\)가 같은 분포를 따름을 증명한다.</li>
  <li>\(\begin{aligned}
f_{X+Z}(t) &amp; =\int_{-\infty}^{\infty} f_Z(w) f_X(t-w) d w \\
&amp; =\int_{-\infty}^{\infty} f_Z(-w) f_X(t-w) d w \quad \text { since } f_Z(-w)=f_Z(w) \\
&amp; =\int_{-\infty}^{\infty} f_Z(w) f_X(t+w) d w \quad \text { by the change of variables }(-w \rightarrow w) \\
&amp; =\frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\infty} e^{-\frac{w^2}{2 \sigma^2}} f_X(t+w) d w \\
&amp; =\frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\infty} \mathbb{E}_Z\left[e^{i w Z \sigma^{-2}}\right] f_X(t+w) d w \\ &amp; \text {by Characteristic function of } Z \sim N(0, \sigma^2) \text { and } Z \sigma^{-2} \sim N\left(0, \sigma^{-2}\right) \\
&amp; =\frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[\int_{-\infty}^{\infty} e^{i w Z \sigma^{-2}} f_X(t+w) d w\right] \quad \text { by Fubini's theorem } \\
&amp; =\frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[e^{-i t Z \sigma^{-2}} \int_{-\infty}^{\infty} e^{i(t+w) Z \sigma^{-2}} f_X(t+w) d w\right] \\
&amp; =\frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[e^{-i t Z \sigma^{-2}} \int_{-\infty}^{\infty} e^{i w Z \sigma^{-2}} f_X(w) d w\right] \\ &amp;\text { by the change of variables }(t+w \rightarrow w) \\ &amp; =\frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[e^{-i t Z \sigma^{-2}} \phi_X\left(Z \sigma^{-2}\right)\right] = \frac{1}{\sqrt{2 \pi} \sigma} \mathbb{E}_Z\left[e^{-i t Z \sigma^{-2}} \phi_Y\left(Z \sigma^{-2}\right)\right] \\ &amp;( \because \phi_X = \phi_Y ) \\ &amp;=f_{Y+Z}(t)
\end{aligned}\)​</li>
  <li>By Slutsky’s theorem, \(X\)와 \(Y\)는 같은 분포를 따른다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[Moments]]></summary></entry><entry><title type="html">(Dish-TS) A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting (AAAI 2023)</title><link href="http://localhost:4000/timeseries/2024-04-12-Dish-TS/" rel="alternate" type="text/html" title="(Dish-TS) A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting (AAAI 2023)" /><published>2024-04-12T00:00:00+09:00</published><updated>2024-04-12T19:26:03+09:00</updated><id>http://localhost:4000/timeseries/Dish-TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-04-12-Dish-TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Distribution shift in TS : series distribution changes over time</li>
  <li>기존 연구들은 the quantification of distribution 정도</li>
  <li>Distribution shift in TS는 2개 카테고리
    <ul>
      <li><strong>intra-space shift</strong> : the distribution within the input-space keeps shifted over time</li>
      <li><strong>inter-space shift</strong> : that the distribution is shifted btw/ input-space and output-space</li>
    </ul>
  </li>
  <li>Dish-TS : neural paradigm for alleviating distribution shift in TSF
    <ul>
      <li>CONET : can be any NN, input sequences into learnable distribution coefficients</li>
      <li>Dual-CONET : separately learn the distribution of input- and output-space</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<p><img src="/assets/img/timeseries/Dish-TS/fig1.png" alt="사진1" /></p>

<ul>
  <li>TS의 non-stationarity(distribution shift over time)는 예측 성능을 방해</li>
  <li><strong>intra-space shift</strong> :  TS distribution changes over time</li>
  <li><strong>inter-space shift</strong> :  Distribution shift btw/ input-space (lookbacks) and output-space (horizons)</li>
  <li>대표적인 alleviate distribution shift solution : RevIN.
    <ul>
      <li>Quantifying true distribution with fixed statistics (e.g., mean and std.)
        <ul>
          <li>But, unreliable (limited in expressiveness for representing the true distribution)</li>
          <li>Different sampling frequencies provide different statistics</li>
        </ul>
      </li>
      <li>Strong assumption : the lookbacks and horizons share the same statistical properties</li>
      <li>But,  always a variation in distribution btw/ input-space and output-space</li>
    </ul>
  </li>
  <li>Dish-TS는 RevIN으로부터 영감을 받은 만큼 전체적인 구조는 유사하다
    <ul>
      <li>two-stage process : normalizing \(\to\) forecasting \(\to\) denormalizing</li>
      <li>CONET : window \(\to\) two learnable coefficients:
        <ul>
          <li>a level coefficient and a scaling coefficient</li>
          <li>to illustrate series overall scale and fluctuation</li>
        </ul>
      </li>
      <li>Dual-CONET
        <ul>
          <li>BACKCONET : coefficients to estimate the distribution of input-space (lookbacks)</li>
          <li>HORICONET : coefficients to infer the distribution of output-space (horizons)</li>
        </ul>
      </li>
      <li>Prior-knowledge : HORICONET 학습할 때 prior 줘서 output-space를 잘 infer(predict) 하도록</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>Models for Time Series Forecasting
    <ul>
      <li>ARMA, BEATS, Transformer, Informer, Autoformer, …</li>
    </ul>
  </li>
  <li>Distribution Shift in Time Series Forecasting
    <ul>
      <li>Adaptive Norm :  puts z-score normalization on series by the computed global statistics</li>
      <li>DAIN (Passalis et al. 2019) :  applies nonlinear NN to adaptively normalize the series</li>
      <li>RevIN (Kim et al. 2022) : instance normalization to reduce series shift</li>
    </ul>
  </li>
  <li>대부분의 연구가 static statistics 사용해서 normalizing한다. (inter-space shift 고려 안함)</li>
</ul>

<h2 id="3-problem-formulations">3. Problem Formulations</h2>

<ul>
  <li>
    <p>Time Series Forecasting</p>

    <ul>
      <li>Formula : \(\left(x_{t: t+H}^{(1)}, \cdots, x_{t: t+H}^{(N)}\right)^T=\mathscr{F}_{\Theta}\left(\left(x_{t-L: t}^{(1)}, \cdots, x_{t-L: t}^{(N)}\right)^T\right)\)​</li>
      <li>\(\mathscr{F}_{\Theta}: \mathbb{R}^{L \times N} \rightarrow \mathbb{R}^{H \times N}\),   \(\Theta\) :  forecasting model parameters</li>
    </ul>
  </li>
  <li>
    <p>Distribution Shift in Time Series</p>

    <ul>
      <li>intra-space shift : \(\mid d\left(\mathcal{X}_{\text {input }}^{(i)}(u), \mathcal{X}_{\text {input }}^{(i)}(v)\right)\mid &gt;\delta\)​</li>
      <li>
        <p>inter-space shift : \(\mid d\left(\mathcal{X}_{\text {input }}^{(i)}(u), \mathcal{X}_{\text {output }}^{(i)}(u)\right)\mid &gt;\delta\)</p>
      </li>
      <li>\(\mathcal{X}_{\text {input }}^{(i)}(u)\)는 \(t=u\) 시점으로부터 과거 방향으로 \(L\) 길이의 lookback window</li>
      <li>\(\mathcal{X}_{\text {output }}^{(i)}(u)\)는 \(t=u\) 시점으로부터 미래 방향으로 \(H\) 길이의 horizon window</li>
    </ul>
  </li>
</ul>

<h2 id="4-dish-ts">4. Dish-TS</h2>

<h3 id="41-overview">4.1. Overview</h3>

<p><img src="/assets/img/timeseries/Dish-TS/fig2.png" alt="사진2" /></p>

<ul>
  <li>CONET :  input series \(\to\)​​ coefficients (for distribution measurement)</li>
  <li>RevIN처럼 two-stage process
    <ul>
      <li>BACKCONET : transformed the lookbacks (before forecasting model)</li>
      <li>HORICONET : transformed the forecasting results
        <ul>
          <li>HORICONET can be trained in a prior knowledgeinduced fashion (4.4에서 설명)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-dual-conet-framework">4.2. Dual-Conet Framework</h3>

<ul>
  <li>기존 연구들은 mean, std로 distribution을 measure \(\to\)​ unreliable (Different frequencies different statistics)</li>
  <li>기본 CONET구조
    <ul>
      <li>\(\varphi, \xi=\operatorname{CONET}(x)\) : any NN (can non-linear mapping)
        <ul>
          <li>\(\varphi \in \mathbb{R}^1\) : level coefficient (overall scale of input series)</li>
          <li>\(\xi \in \mathbb{R}^1\) : scaling coefficient (fluctuation scale)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Mutivariate forecasting을 위한 Dual-CONET
    <ul>
      <li>\(\begin{aligned}
&amp; \varphi_{b, t}^{(i)}, \xi_{b, t}^{(i)}=\operatorname{BACKCONET}\left(x_{t-L: t}^{(i)}\right), i=1, \cdots, N \\
&amp; \varphi_{h, t}^{(i)}, \xi_{h, t}^{(i)}=\operatorname{HORICONET}\left(x_{t-L: t}^{(i)}\right), i=1, \cdots, N
\end{aligned}\)​
        <ul>
          <li>\(\varphi_{b, t}^{(i)}, \xi_{b, t}^{(i)} \in \mathbb{R}^1\) : level, scaling coefficients for lookbacks</li>
          <li>\(\varphi_{h, t}^{(i)}, \xi_{h, t}^{(i)} \in \mathbb{R}^1\) : level, scaling coefficients for horizons</li>
        </ul>
      </li>
      <li>BACKCONET과 HORICONET 둘다 input이 “t 시점에서 L 길이의 historical series”이다 !</li>
    </ul>
  </li>
  <li>Integrating Dual-Conet into Forecasting
    <ul>
      <li>Final transformed forecasting results : \(\hat{x}_{t: t+H}^{(i)}=\xi_{h, t}^{(i)} \mathscr{F}_{\Theta}\left(\frac{1}{\xi_{b, t}^{(i)}}\left(x_{t-L: t}^{(i)}-\varphi_{b, t}^{(i)}\right)\right)+\varphi_{h, t}^{(i)}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-a-simple-and-intuitive-instance-of-conet">4.3. A Simple and Intuitive Instance of Conet</h3>

<ul>
  <li>실제 CONET에서의 연산은 다음과 같다.</li>
  <li>lookback level coefficient : \(\varphi_{b, t}^{(i)}=\sigma\left(\sum_{\tau=1}^{\operatorname{dim}\left(\mathbf{v}_{b, i}^{\ell}\right)} \mathbf{v}_{b, i \tau}^{\ell} x_{\tau-L+t}^{(i)}\right),\)</li>
  <li>horizons level coefficient : \(\varphi_{h, t}^{(i)}=\sigma\left(\sum_{\tau=1}^{\operatorname{dim}\left(\mathbf{v}_{h, i}^{\ell}\right)} \mathbf{v}_{h, i \tau}^{\ell} x_{\tau-L+t}^{(i)}\right)\)​</li>
  <li>lookback scaling coefficient : \(\xi_{b, t}^{(i)}=\sqrt{\mathbb{E}\left(x_t^{(i)}-\varphi_{b, t}^{(i)}\right)^2}\)</li>
  <li>horizons scaling coefficient : \(\xi_{h, t}^{(i)}=\sqrt{\mathbb{E}\left(x_t^{(i)}-\varphi_{h, t}^{(i)}\right)^2}\)</li>
</ul>

<h3 id="44-prior-knowledge-induced-training-strategy">4.4. Prior Knowledge-Induced Training Strategy</h3>

<ul>
  <li>HORICONET은 미래의 정보인 \(\mathcal{X}_{\text {output }}^{(i)}\)의 분포를 infer(predict)해야 하기 때문에 intractable하다.</li>
  <li>그러므로 prior(mean of horizons)을 soft-target으로 줘서 학습의 난이도를 낮춘다.</li>
  <li>final loss는 \(\sum_{k=1}^K \sum_{i=1}^N[\left(\hat{x}_{t_k: t_k+H}^{(i)}-x_{t_k: t_k+H}^{(i)}\right)^2+\underbrace{\left.\alpha\left(\frac{1}{H} \sum_{t=t_k+1}^{t_k+H} x_t^{(i)}-\varphi_{h, t_k}^{(i)}\right)^2\right]}_{\text {Prior Knowledge Guidance }}\)이다.
    <ul>
      <li>MSE term에 prior knowledge를 \(\alpha\)의 weight로 준다.</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiment">5. Experiment</h2>

<p><img src="/assets/img/timeseries/Dish-TS/table1.png" alt="사진3" /></p>

<p><img src="/assets/img/timeseries/Dish-TS/table2.png" alt="사진4" /></p>

<p><img src="/assets/img/timeseries/Dish-TS/table3.png" alt="사진5" /></p>

<p>Dish-TS를 적용하면 Informer, Autoformer, N-BEATS의 성능이 향상되고, 그 정도는 RevIN보다 크다.</p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Systematically summarize the distribution shift in time series forecasting</li>
  <li>as intra-space shift and interspace shift.</li>
  <li>Dish-TS better alleviates the two shift
    <ul>
      <li>prior knowledge-induced training strategy, for effectiveness</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2023](https://arxiv.org/pdf/2302.14829.pdf)]]></summary></entry><entry><title type="html">Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping</title><link href="http://localhost:4000/timeseries/2024-04-04-RTSF/" rel="alternate" type="text/html" title="Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping" /><published>2024-04-04T00:00:00+09:00</published><updated>2024-04-05T09:40:54+09:00</updated><id>http://localhost:4000/timeseries/RTSF</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-04-04-RTSF/"><![CDATA[<ul>
  <li>이전에 review했던 <a href="https://arxiv.org/abs/2205.13504">DLinear</a> paper에서는 ‘Time series의 properties가 무엇인지’, 그리고 ‘왜 Transformer-based model이 완벽할 수 없는지’에 집중했다면, 본 논문은 ‘왜 linear mapping이 단순한데도 성능이 좋은지’에 집중한다.</li>
</ul>

<h2 id="abstract">Abstract</h2>

<ul>
  <li>Linear mapping is critical to prior long-term time series forecasting efforts</li>
  <li>RevIN (reversible normalization) and CI (Channel Independent) play a vital role for performance</li>
  <li>Linear mapping can effectively capture periodic features in TS</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer-based model
    <ul>
      <li>Non-autoregressive methods (to capture long-term temporal correlations)</li>
      <li><a href="https://arxiv.org/pdf/2012.07436.pdf">Informer(AAAI 2021)</a>, <a href="https://arxiv.org/pdf/2106.13008.pdf">Autoformer(NeurIPS 2021)</a>, <a href="https://arxiv.org/pdf/2201.12740.pdf">Fedformer(PMLR 2022)</a>, <a href="https://openreview.net/pdf?id=0EXmFzUn5I">Pyraformer(ICLR 2022)</a>, <a href="https://arxiv.org/pdf/2206.04038.pdf">Scaleformer(ICLR 2023)</a>, <a href="https://openreview.net/pdf?id=vSVLM2j9eie">Crossformer(ICLR 2023)</a></li>
      <li>However, autoregressive에 비해 non-autoregressive의 성능이 좋았던 것이지 transformer가 TS forecasting에 효과적인 것은 아님 <a href="https://arxiv.org/pdf/2205.13504.pdf">DLinear(AAAI 2022)</a></li>
    </ul>
  </li>
  <li>Subsequent approaches (patching)
    <ul>
      <li>Encoder-decoder(ex.transformer) 구조를 버리고 temporal feature extractor 모델링 (attention을 안쓴 건 아님)</li>
      <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/266983d0949aed78a16fa4782237dea7-Paper-Conference.pdf">SCINet(NeurIPS 2022)</a>, <a href="https://arxiv.org/pdf/2211.14730.pdf">PatchTST(ICLR2023)</a>, <a href="https://arxiv.org/pdf/2302.04501.pdf">MTS-Mixers(Arxiv 2023)</a>, <a href="https://openreview.net/pdf?id=ju_Uqw384Oq">Timesnet(ICLR 2023)</a></li>
      <li>But, adjustable hyper-parameters가 너무 많이 필요하다.</li>
    </ul>
  </li>
  <li>본 논문의 Questions
    <ul>
      <li>(1) Are temporal feature extractors effective for long-term time series forecasting?</li>
      <li>(2) What are the <u>underlying mechanisms</u> explaining the effectiveness of <u>linear mapping</u> in time series forecasting?</li>
      <li>(3) What are the limits of linear models and how can we improve them?</li>
    </ul>
  </li>
</ul>

<h2 id="2-problem-definition-and-experimental-setup">2. Problem Definition and Experimental Setup</h2>

<ul>
  <li>\(\mathbf{X}=\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n\right] \in \mathbb{R}^{c \times n}\) 으로 \(\mathbf{Y}=\left[\boldsymbol{x}_{n+1}, \boldsymbol{x}_{n+2}, \ldots, \boldsymbol{x}_{n+m}\right] \in \mathbb{R}^{c \times m}\) 예측(mapping)하는 함수 \(\mathcal{F}: \mathbf{X}^{c \times n} \mapsto \mathbf{Y}^{c \times m}\) 를 학습</li>
  <li><a href="https://arxiv.org/pdf/2211.14730.pdf">PatchTST(ICLR2023)</a>와 동일한 dataset split, Adam optimizer, Nvidia V100 32GB GPU 사용</li>
</ul>

<h2 id="3-are-temporal-feature-extractors-effective-">3. Are Temporal Feature Extractors Effective ?</h2>

<ul>
  <li>TSF 일반적인 Framework는 <strong>RevIN</strong> \(\to\) <strong>temporal feature extractor</strong>(Attention, MLP, Conv, …) \(\to\) <strong>linear projection</strong>
    <ul>
      <li>다른 모델들의 temporal feature extractor를 살펴보면 PatchTST(attention), MTS-Mixers(MLP), TimesNet(conv), SCINet(conv), …</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/RTSF/fig2.png" alt="사진1" /></p>

<ul>
  <li>Fig2 - RLinear : linear projection layer with RevIN</li>
  <li>Fixed random extractor : only initialize the temporal feature extractor randomly and do not update its parameters in the training phase</li>
  <li>Fig2는 RevIN이 예측 성능을 향상시킨다 정도를 보여줄 뿐. simple linear layer가 RevIIN 도움 받으면 PatchTST보다 성능이 좋다.</li>
</ul>

<p><img src="/assets/img/timeseries/RTSF/fig3.png" alt="사진2" /></p>

<ul>
  <li>Fig3는 복잡한 모델들이 결국에는 가장 왼쪽에 있는 simple linear layer의 weights와 비슷한 weights를 학습하게 됨을 보여준다.</li>
</ul>

<h2 id="4-theoretical-and-empirical-study-on-the-linear-mapping">4. Theoretical and Empirical Study on the Linear Mapping</h2>

<h3 id="41-roles-of-linear-mapping-in-forecasting">4.1. Roles of Linear Mapping in Forecasting</h3>

<ul>
  <li>
    <dl>
      <dt><strong>Linear mapping learns periodicity</strong></dt>
      <dd>single linear layer는 periodicity를 학습할 수 있다. (trend는 잘 학습하지 못한다.)</dd>
    </dl>

    <p>아래 가정들과 정리들의 의미를 이해해보자.</p>

    <ul>
      <li>
        <p>single linear layers : \(\mathbf{Y}=\mathbf X \mathbf{W}+\mathbf{b}\)  라 하자.</p>
      </li>
      <li>
        <p><strong>Assumption 1</strong>. A general time series \(x(t)\) can be disentangled into seasonality part \(s(t)\) and trend part \(f(t)\) with tolerable noise, denoted as \(x(t)=s(t)+f(t)+\epsilon\)</p>

        <ul>
          <li>즉 시계열 = seasonality + trend + noise로 분해할 수 있다는 의미이다.</li>
        </ul>
      </li>
      <li><strong>Theorem 1</strong>. Given a seasonal time series satisfying \(x(t)=s(t)=s(t-p)\) where \(p \leq n\) is the period, there always exists an analytical solution for the linear model as
        <ul>
          <li>\(\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n\right] \cdot \mathbf{W}+\mathbf{b}=\left[\boldsymbol{x}_{n+1}, \boldsymbol{x}_{n+2}, \ldots, \boldsymbol{x}_{n+m}\right]\) 이고</li>
          <li>\(\mathbf{W}_{i j}^{(k)}=\left\{\begin{array}{ll} 1, &amp; \text { if } i=n-k p+(j \bmod p) \\ 0, &amp; \text { otherwise } \end{array}, 1 \leq k \in \mathbb{Z} \leq\lfloor n / p\rfloor, b_i=0\right.\) 이다.</li>
          <li>input historical sequence의 길이가 주기보다 길다면 linear mapping은 periodicity를 예측할 수 있다는 의미이다.</li>
          <li>하지만 위 정리는 \(x(t)=s(t)=s(t-p)\) 즉 trend가 없는 경우에 해당한다.</li>
          <li><img src="/assets/img/timeseries/RTSF/fig4.png" alt="사진3" /></li>
          <li>Fig4는 linear model이 seasonality는 잘 학습하지만 trend가 있을 때에는 성능이 좋지 못함을 보여준다.</li>
        </ul>
      </li>
      <li>
        <p><strong>Theorem 2</strong>. Let \(x(t)=s(t)+f(t)\) where \(s(t)\) is a seasonal signal with period \(p\) and \(f(t)\) satisfies \(K\)-Lipschitz continuous. Then there exists a linear model as \(\mathbf{Y}=\mathbf X \mathbf{W}+\mathbf{b}\) with input horizon size \(n=p+\tau, \tau \geq 0\) such that \(\mid x(n+j)-\hat x(n+j) \mid \leq K(p+j), j=1, \ldots, m\).</p>

        <ul>
          <li>linear model의 trend term에 대한 forecasting error의 upper bound를 제시하는 정리이지만, trend에 대해 성능이 좋지 않다는 건 여전하다.</li>
          <li><img src="/assets/img/timeseries/RTSF/proof.png" alt="사진4" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-disentanglement-and-normalization"><strong>4.2. Disentanglement and Normalization</strong></h3>

<ul>
  <li>
    <p><strong>Problems in Disentanglement</strong></p>

    <ul>
      <li>
        <p>시계열에서 trend와 seasonality를 분리할 수 있으면 성능을 높일 수 있을 것</p>

        <ul>
          <li>
            <p>moving average(by an average pooling layer with a sliding window)로 trend를 분리할 수 있다.</p>
          </li>
          <li>
            <p>하지만, sliding window의 크기가 seasonality의 최대 주기보다 커야만 효과적이고</p>

            <p>average pooling layer를 사용할 때 input TS의 양 끝에 padding을 해줘야 하는데, 그러면 원본 시퀀스가 왜곡된다.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Turning trend into seasonality</strong></p>
    <ul>
      <li>Disentanglement의 핵심은 원본 TS에서 movind average를 빼는 것인데, 이건 normalization과 관련이 있다.
        <ul>
          <li>TS의 statistical information(평균, 분산)은 distribution shift로 인해 계속 바뀌기 때문에 RevIN이 사용된다.</li>
          <li>RevIN : Normalization the input \(\to\) Forecasting module \(\to\) Denormalization the output</li>
        </ul>
      </li>
      <li>하지만 input 데이터에 directly normalization하면 오히려 statistical information을 지우는 것과 같다.</li>
      <li><img src="/assets/img/timeseries/RTSF/fig5.png" alt="사진5" /></li>
      <li>RevIN의 경우에는 scaling을 하지만 periodicity는 바꾸지 않는다. 그리고 reversible하다.</li>
      <li><img src="/assets/img/timeseries/RTSF/fig6.png" alt="사진6" /></li>
      <li><img src="/assets/img/timeseries/RTSF/fig7.png" alt="사진7" /></li>
      <li>RevIN은 continuously changing trends를 multiple segments with a fixed and similar trend로 바꾼다.</li>
      <li>그러면 accumulated timesteps in the past로 인한  errors in trend prediction이 완화된다.</li>
    </ul>
  </li>
</ul>

<h2 id="5-experimental-evaluation">5. Experimental Evaluation</h2>

<h3 id="51-comparison-on-real-world-datasets">5.1 Comparison on Real-world Datasets</h3>

<p><img src="/assets/img/timeseries/RTSF/table3.png" alt="사진8" /></p>

<ul>
  <li>learning of periodicity via linear mapping, 그리고 efficiency of reversible normalization 덕분에 well-designed models보다 RLinear의 성능이 좋다.</li>
</ul>

<h3 id="52-when-linear-meets-multiple-periods-among-channels">5.2. When Linear Meets Multiple Periods among Channels</h3>

<p><img src="/assets/img/timeseries/RTSF/table4.png" alt="사진9" /></p>

<ul>
  <li>Multi-channel datasets의 경우에는 Channel Independent(CI) modeling으로 TS의 각 채널을 독립적으로 처리할 때 성능이 좋다.</li>
</ul>

<p><img src="/assets/img/timeseries/RTSF/fig10.png" alt="사진10" /></p>

<ul>
  <li>Channels가 많아지면 channel마다 periodicity가 달라져서 예측이 어려워지는데 input horizon을 늘리면 완화된다.</li>
</ul>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Linear mapping와 foreacsting methods가 학습하는 것은 비슷하다. (input historical observations에서 periodic patterns)</li>
  <li>RevIN과 Channel Independent는 periodicity를 단순하게 만들어 학습을 용이하게 하므로 성능 향상에 필요하다.</li>
  <li>Linear mapping은 MTS에 대해서도 input horizon만 충분하다면 예측 성능이 뛰어나다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2305.10721.pdf)]]></summary></entry><entry><title type="html">(Time-LLM) Time Series Forecasting by Reprogramming Large Language Models</title><link href="http://localhost:4000/timeseries/2024-04-01-TimeLLM/" rel="alternate" type="text/html" title="(Time-LLM) Time Series Forecasting by Reprogramming Large Language Models" /><published>2024-04-01T00:00:00+09:00</published><updated>2024-04-05T09:40:54+09:00</updated><id>http://localhost:4000/timeseries/TimeLLM</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-04-01-TimeLLM/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>CV, NLP는 single large model (또는 pre-trained foundation model)이 거의 모든 tasks에서 성능이 좋음</li>
  <li>반면 TS는 <strong>dat sparsity</strong> 때문에 tasks마다 모델 디자인이 다름</li>
  <li>본 논문에서는 TS와 NLP의 modality gap을 align하기 위해 Time-LLM을 제안</li>
  <li><strong>Prompt-as-Prefix</strong>(PaP, reprogramming the input TS) \(\to\) frozen LLM \(\to\) forecasting</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>LLM을 forecasting model로 발전시키기 위해 필요한 것들
    <ul>
      <li><strong>Generalizability</strong> : Capability for few-shot and zero-shot transfer learning, w/o pre-task retraining</li>
      <li><strong>Data efficiency</strong> : Performance new tasks with only a few examples(limited data)</li>
      <li><strong>Reasoning</strong> : Sophisticated reasoning \(\to\) learned higher-level concepts \(\to\) highly precise forecasting</li>
      <li><strong>Multimodal knowledge</strong> :  Diverse knowledge across modalities \(\to\) synergistic forecasting that fuses different data types</li>
      <li><strong>Easy optimization</strong> : Once on massive computing \(\to\) can be applied to forecasting tasks (without learning from scratch)</li>
    </ul>
  </li>
  <li><strong>Align the modalities of TS &amp; NLP</strong> : Why challenging ?
    <ul>
      <li>첫째로 NLP는 discrete tokens, TS는 본질적으로 continuous</li>
      <li>둘째로 TS reasoning 지식이 LLM pre-training 안에 없다.</li>
    </ul>
  </li>
  <li>So, <strong>Time-LLM</strong>
    <ul>
      <li>Core idea : TS input을 LLM이 활용하기 쉬운 <strong>text prototype</strong>으로 reprogramming하는 framework (backbone model은 그대로)</li>
      <li><strong>Prompt-as-Prefix (PaP)</strong> : 1) enrich the input TS with additional context and 2) providing task instructions in the modality of NLP</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>
<p><img src="/assets/img/timeseries/TimeLLM/fig1.jpeg" alt="사진1" /></p>
<ul>
  <li>TS models는 task-specific \(\to\) ex. ARIMA는 UTS를, LSTM은 seq를, TCN과 Transformer는 longer temporal dependencies를 위해 디자인 \(\to\) versatility and generalizability 부족</li>
  <li>In-modality Adaptation : Pre-training (representation learning) \(\to\)​ fine-tuning (for downstream tasks) ! But <strong>TS data sparsity.</strong>..</li>
  <li>Cross-modality Adaptation (multimodal fine tuning) : Voice2Series(2021)은 TS를 acoustic model에 맞게 editing했고 LLM4TS는 first supervised pre-training on time series, then task-specific fine-tuning</li>
  <li>Time-LLM은 1) input TS를 수정하지도 않고, 2) backbone LLM을 fine tuning하지도 않는다. LLM이 잠재력을 발휘할 수 있도록 TS를 reprogramming</li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>
<ul>
  <li>Goal : <strong>Reprogram an embedding-visible language foundation model for general time series forecasting</strong> without requiring any fine-tuning of the backbone model.</li>
  <li>\(f(\ \mathbf{X} \in \mathbb{R}^{N \times T}\ )= \hat{\mathbf{Y}} \in \mathbb{R}^{N \times H}\), \(f\)는 input TS를 이해하고 예측.
Loss : \(\frac{1}{H} \sum_{h=1}^H\left\|\hat{\mathbf{Y}}_h-\mathbf{Y}_h\right\|_F^2\)​
<img src="/assets/img/timeseries/TimeLLM/fig2.jpeg" alt="사진2" /></li>
  <li>3-main components
    <ul>
      <li>(1) input transformation</li>
      <li>(2) a pre-trained and frozen LLM</li>
      <li>(3) output projection</li>
    </ul>
  </li>
  <li>Step 1) Channel independence : MTS \(\to\)​ N개의 UTS <br />
Step 2) Normalization, Patching, Embedding prior <br />
Step 3) Augment the LLM’s Ts reasoning ability <br />
Step 4) Project to the final forecast \(\hat{\mathbf{Y}}^{i}\)</li>
  <li>Efficiency
    <ul>
      <li>only the parameters of the <strong>lightweight input transformation and output projection</strong> are updated. (backbone LLM is frozen)</li>
      <li>directly optimizing \(\to\) <strong>small set of TS and a few training epochs</strong></li>
      <li>for reduce memory footprint, <strong>off-the-shelf techniques (e.g., quantization)</strong></li>
    </ul>
  </li>
</ul>

<h3 id="31-model-structure">3.1. Model Structure</h3>
<ul>
  <li><strong>Input Embedding</strong>
    <ul>
      <li>step 1) RevIN \(\mathbf X^{(i)}\)</li>
      <li>step 2) Patching with length \(L_p\)
        <ul>
          <li>Total number of input patches : \(P=\left\lfloor\frac{\left(T-L_p\right)}{S}\right\rfloor+2\)</li>
          <li>Underlying motivations :
            <ul>
              <li>Better preserving local semantic information by aggregating local information into each patch</li>
              <li>Serving as tokenization to form a compact sequence of input tokens, reducing computational burdens.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>step 3) Embedding w/ simple linear layer
        <ul>
          <li>\(\mathbf{X}_P^{(i)} \in \mathbb{R}^{P \times L_p} \to \mathbf{\hat X}_P^{(i)} \in \mathbb{R}^{P \times d_m}\)​</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Patch Reprogramming</strong>
<img src="/assets/img/timeseries/TimeLLM/fig3.png" alt="사진3" />
    <ul>
      <li>Goal : to align the modalities of TS and natural language (TS 직접적인 수정 없이)</li>
      <li>How : pre-trained word embedding \(\mathbf E \in \mathbb R^{V \times D}\) in backbone.
        <ul>
          <li>But! no prior knowledge indicating which source token are directly relevant.</li>
          <li>So, simply leveraging small collection of text prototypes by linearly probing \(\mathbf E\), denoted as \(\mathbf E' \in \mathbb R^{V' \times D}, V^{\prime} \ll V\)</li>
          <li>: efficient &amp; allows for the adaptive selection of relevant source information</li>
        </ul>
      </li>
      <li>Multi-head cross-attention
        <ul>
          <li>query matrices \(\mathbf{Q}_k^{(i)}=\hat{\mathbf{X}}_P^{(i)} \mathbf{W}_k^Q\)
key matrices \(\mathbf{K}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^K\)
value matrices \(\mathbf{V}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^V\)
where \(\mathbf{W}_k^Q \in \mathbb{R}^{d_m \times d}\) and \(\mathbf{W}_k^K, \mathbf{W}_k^V \in \mathbb{R}^{D \times d}\)
\(D\) is the hidden dimension of the backbone model, and \(d=\left\lfloor\frac{d_m}{K}\right\rfloor\)</li>
          <li>i-layer : \(\mathbf{Z}_k^{(i)}=\operatorname{ATTENTION}\left(\mathbf{Q}_k^{(i)}, \mathbf{K}_k^{(i)}, \mathbf{V}_k^{(i)}\right)=\operatorname{SOFTmax}\left(\frac{\mathbf{Q}_k^{(i)} \mathbf{K}_k^{(i) \top}}{\sqrt{d_k}}\right) \mathbf{V}_k^{(i)}\)</li>
          <li>By aggregating each \(\mathbf{Z}_k^{(i)} \in \mathbb{R}^{P \times d}\) in every head, \(\mathbf{Z}^{(i)} \in \mathbb{R}^{P \times d_m}\),
then linearly projected \(\to \mathbf{O}^{(i)} \in \mathbb{R}^{P \times D}\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Patch-as-Prefix</strong> (constraints…)
    <ul>
      <li>natural language로 표현된 TS를 예측</li>
      <li>Constraints :
        <ul>
          <li>LLM은 <strong>high-precision numerals</strong> 연산에 sensitivity 떨어지고</li>
          <li>LLM별로 서로 다른 후처리가 필요 ex.  0.61이 [’ 0 ‘, ‘, ‘, 6 ‘, ‘ 1 ‘] 또는 [’ 0 ‘, ‘, ‘, ‘61’]로 표시</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Prompt-as-Prefix</strong> (avoid constraints !)
    <ul>
      <li>(1) Dataset context : input TS의 essential background information을 LLM에 제공
(2) Task instruction : task에 따른 patch embedding 가이드를 LLM에 제공
(3) Input statistics : Enrich the input TS with additional statistics (trends, lags, …)</li>
    </ul>
  </li>
  <li><strong>Output Projection</strong>
    <ul>
      <li>Prefixal part 버리고 output representation 얻어서 flatten하고 linear projection  \(\to \hat{\mathbf{Y}}^{(i)}\)</li>
    </ul>
  </li>
</ul>

<h2 id="4-main-results">4. Main Results</h2>
<h3 id="41-42-longshort-term-forecasting">4.1, 4.2. Long/Short-term Forecasting</h3>
<p><img src="/assets/img/timeseries/TimeLLM/table12.png" alt="사진4" /></p>
<h3 id="43-few-shot-forecasting">4.3. Few-shot Forecasting</h3>
<p><img src="/assets/img/timeseries/TimeLLM/table3.png" alt="사진5" />
<img src="/assets/img/timeseries/TimeLLM/table4.png" alt="사진6" /></p>
<h3 id="44-zero-shot-forecasting">4.4. Zero-shot Forecasting</h3>
<p><img src="/assets/img/timeseries/TimeLLM/table5.png" alt="사진7" /></p>
<h3 id="45-model-analysis">4.5. Model Analysis</h3>
<p><img src="/assets/img/timeseries/TimeLLM/table67.png" alt="사진8" /></p>

<h2 id="5-conclusion-and-future-work">5. Conclusion and Future work</h2>
<ul>
  <li>TS를 test prototype으로 reprogramming해서 frozen LLM 통과</li>
  <li>Prompt-as-Prefix로 guidance를 LLM에 제공</li>
  <li>TS forecasting을 language task로 casting</li>
  <li>결론적으로 Patching + Prompting으로 성능을 더 올린 모델</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/pdf/2310.01728.pdf)]]></summary></entry><entry><title type="html">(Survey paper) Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects (Arxiv 2023)</title><link href="http://localhost:4000/timeseries/2024-03-25-SSL4TS/" rel="alternate" type="text/html" title="(Survey paper) Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects (Arxiv 2023)" /><published>2024-03-25T00:00:00+09:00</published><updated>2024-03-25T19:43:09+09:00</updated><id>http://localhost:4000/timeseries/SSL4TS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-25-SSL4TS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Self-supervised learning (SSL) : for reducing the dependence of labeled data</li>
  <li>will review SOTA SSL for TS</li>
  <li>will provide taxonomy of SSL for TS : generative-based, contrastive-based, adversarial-based
    <ul>
      <li>intuitions, main frameworks, (dis)advantages</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>To extract useful and informative features, (=hidden patterns and features of the data)
    <ul>
      <li>SSL utilizes pretext tasks to derive supervision signals from unlabeled data (=creating valuable representation for downstream tasks)</li>
    </ul>
  </li>
  <li>Challenges:
    <ul>
      <li>대부분의 pre-text tasks(이미지, 언어, …) designed 모델들은 TS의 unique properties(seasonality, trend, frequency, …)를 고려하기 위한 것이 아니다.</li>
      <li>SSL을 위해 data augmentation할 때 rotation이나 crop과 같은 방식은 TS의 temporal dependency를 학습하기 어렵게 만든다.</li>
      <li>MTS에서 몇몇의 dimension(=channel, variate)에만 유용한 정보가 있는 경우에는 이를 고려해야 한다.</li>
    </ul>
  </li>
  <li>SSL for TS:
    <ul>
      <li>Generative-based : autoregressive-based forecasting / auto-encoder-based reconstruction / diffusion-based generation</li>
      <li>Contrastive-based : sampling contrast / prediction contrast / augmentation contrast / prototype contrast / expert knowledge contrast</li>
      <li>Adversarial-based : generation and imputation and auxiliary representation enhancement
<img src="/assets/img/timeseries/SSL4TS/fig1.png" alt="사진1" />
<img src="/assets/img/timeseries/SSL4TS/fig5.png" alt="사진5" /></li>
    </ul>
  </li>
</ul>

<h2 id="2-related-surveys">2. Related Surveys</h2>

<h3 id="21-definition-of-time-series-data">2.1. Definition of time series data</h3>
<ul>
  <li>Univariate TS : \(X=\left(x_0, x_1, x_2, \ldots, x_t\right)\) where \(x_i\) is  the point at timestamp \(i\)​</li>
  <li>Multivariate TS : \(\mathbf{X}=\left[X_0, X_1, X_2, \ldots, X_p\right]\), where \(p\) is the number of variables</li>
  <li>Multiple multivariate TS : \(\mathcal{X}=\left\{\mathbf{X}_0, \mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n\right\}\), where \(n\) is the number of multivariate TS</li>
</ul>

<h3 id="22-surveys-on-ssl">2.2. Surveys on SSL</h3>
<ul>
  <li>pretext tasks 의 핵심은 pseudo-supervision signals을 만드는 것</li>
  <li>Basic intuition : pull positive samples closer and push negative samples away
    <ul>
      <li>positive and negative samples : multisensory signals / data augmentation(noise injection, …) / local-global consistency / temporal consistency</li>
      <li>pretext task : Context prediction / Instance discrimination / Instance generation</li>
      <li>model architecture</li>
      <li>training loss : contrastive loss functions generally include scoring functions (cosine similarity), energy-based margin functions (pair loss and triplet loss), probabilistic NCE-based functions, and mutual information based functions</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/SSL4TS/table3.png" alt="사진7" /></p>

<h2 id="3-generative-based-models">3. Generative-based Models</h2>
<ul>
  <li>pretext task : to generate the expected data based on a given view of the data
<img src="/assets/img/timeseries/SSL4TS/fig2.png" alt="사진2" /></li>
</ul>

<h3 id="31-autoregressive-based-forecasting">3.1. Autoregressive-based forecasting</h3>
<ul>
  <li>Forecasting : \(\hat{x}_{[t+1: t+K]}=f\left(x_{[1: t]}\right)\)</li>
  <li>Loss : \(\mathcal{L}=\frac{1}{K} \sum_{k=1}^K\left(\hat{x}_{[t+k]}-x_{[t+k]}\right)^2\)​</li>
  <li>RNN-based
    <ul>
      <li>Adv : Long-term dependencies / Adaptable to varying lengths / Global context information extraction</li>
      <li>Dis-adv : Vanishing or exploding gradients / Computational efficiency</li>
      <li>ex : <a href="https://proceedings.neurips.cc/paper/2020/file/97e401a02082021fd24957f852e0e475-Paper.pdf">THOC</a> : Temporal Self-supervision(TSS), which takes the L-layer dilated RNN with skip-connection structure \(\to\)​ different resolutions at the same time</li>
    </ul>
  </li>
  <li>CNN-based
    <ul>
      <li>Adv : Local pattern extraction / Computational efficiency</li>
      <li>Dis-adv : Long-termdependencies / Information loss</li>
    </ul>
  </li>
  <li>GNN-based
    <ul>
      <li>Adv : Adaptability to graph structures / Dynamic relationship capture</li>
      <li>Dis-adv : Computational and storage complexity</li>
      <li>ex : <a href="https://arxiv.org/pdf/2106.06947.pdf">GDN</a> : the correlation among variables</li>
    </ul>
  </li>
</ul>

<h3 id="32-autoencoder-based-reconstruction">3.2. Autoencoder-based reconstruction</h3>
<ul>
  <li>basic autoencoder (BAE)
    <ul>
      <li>Encoder \(x \to z\) , Decoder \(z \to \hat {x}\)  / Loss : \(\mathcal{L}=\|x-\tilde{x}\|_2\)</li>
      <li>E(), D() 같이 train, D() 제거하고 E()를 feature extractor로 사용</li>
      <li>ex. <a href="https://arxiv.org/pdf/1706.08838.pdf">TimeNet</a>, <a href="https://www.nature.com/articles/s41598-019-55320-6">PT-LSTM-SAE</a>, <a href="https://arxiv.org/pdf/1810.10107.pdf">Autowarp</a></li>
    </ul>
  </li>
  <li>Denoising autoencoder (DAE)
    <ul>
      <li>\(x_n=\mathcal{T}(x), \quad Z=E\left(x_n\right), \quad \tilde{x}=D(z)\), \(\quad \mathcal{T}\): add noise  / Loss : \(\mathcal{L}=\|x-\tilde{x}\|_2\)</li>
    </ul>
  </li>
  <li>Mask autoencoder (MAE)
    <ul>
      <li>Intuition : pre-training할 때에 input의 일부를 mask하고 un-mask part 보고 예측</li>
      <li>form : \(\begin{gathered}x_m=\mathcal{M}(x), \quad z=E\left(x_m\right), \quad \tilde{x}=D(z),\ \mathcal{L}=\mathcal{M}\left(\|x-\tilde{x}\|_2\right),\end{gathered}\)</li>
      <li>TS에서는 time-step-wise masking은 interpolation 해버리기 때문에, segment-wise masking or variable-wise masking</li>
      <li>ex. <a href="https://dl.acm.org/doi/pdf/10.1145/3534678.3539329">TARNet</a> : 중요한 역할을 하는 data를 선정하고, 해당 데이터를 masking 하여 reconstruction</li>
    </ul>
  </li>
  <li>Variational autoencoder (VAE)
    <ul>
      <li>Encoder \(x \to P(z\mid x)\), instead of explicit representation \(z\), Decoder는 sampling from \(P(z \mid x)\)​</li>
      <li>\(P(z \mid x)=E(x), \quad z=\mathcal{S}(P(z \mid x)), \quad \tilde{x}=D(z)\) / Loss : \(\mathcal{L}=\|x-\tilde{x}\|_2+\operatorname{KL}(\mathcal{N}(\mu, \delta), \mathcal{N}(0, I))\)</li>
      <li>ex. <a href="https://dl.acm.org/doi/10.1145/3447548.3467075">InterFusion</a>, <a href="https://arxiv.org/pdf/2101.10318.pdf">mTANs</a>, <a href="https://arxiv.org/pdf/2107.11350.pdf">HetVAE</a>(extract seasonal and trend via VAE)</li>
    </ul>
  </li>
</ul>

<h3 id="33-diffusion-based-generation">3.3. Diffusion-based generation</h3>
<ul>
  <li>reverse transition kernel을 NN으로 approximate</li>
  <li>Denoising diffusion probabilistic models <a href="https://arxiv.org/pdf/2006.11239.pdf">DDPM</a>,
    <ul>
      <li>\(p_\theta\left(\boldsymbol{x}_{\boldsymbol{t}-\mathbf{1}} \mid \boldsymbol{x}_{\boldsymbol{t}}\right)=$ $\mathcal{N}\left(\boldsymbol{x}_{t-1} ; \mu_\theta\left(\boldsymbol{x}_t, t\right), \sum_\theta\left(\boldsymbol{x}_t, t\right)\right)\) 일 때,</li>
      <li>Jensen’s inequality에 의해, training loss는 \(\begin{array}{r}\mathbf{K L}\left(q\left(\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_T\right) \| p_\theta\left(\boldsymbol{x}_0, \boldsymbol{x}_1, \ldots, \boldsymbol{x}_T\right)\right) \geq \mathbf{E}\left[-\log p_\theta\left(\boldsymbol{x}_0\right)\right]+\text { const. }\end{array}\)</li>
    </ul>
  </li>
  <li>Score matching diffusion models <a href="https://arxiv.org/pdf/1907.05600.pdf">score matching</a>
    <ul>
      <li>per- turb data with a sequence of Gaussian noise \(\to\) estimating the score function for all the noisy data</li>
    </ul>
  </li>
</ul>

<h2 id="4-contrastive-based-methods">4. Contrastive-based Methods</h2>
<ul>
  <li>positive는 가깝게, negative는 멀게 representation하도록 학습 \(\to\) positive / negative 정하는 룰이 중요함
<img src="/assets/img/timeseries/SSL4TS/fig3.png" alt="사진3" /></li>
  <li><strong>Sampling contrast</strong>
    <ul>
      <li>가정 : 시점이 가까울수록 유사도가 높을 것</li>
      <li>하지만 실제로 꼭 그런 것은 아니다. contextual information으로 pos / neg 정하기가 쉽지 않다</li>
      <li>ex. <a href="https://arxiv.org/pdf/2106.00750.pdf">TNC</a> : augmented Dickey-Fuller (ADF) statistical test 해서 neg samples를 unknown으로 취급, weights 할당, <a href="https://arxiv.org/pdf/2004.11362.pdf">Supervised contrastive learning</a></li>
    </ul>
  </li>
  <li><strong>Prediction contrast</strong>
    <ul>
      <li>maximally preserve the mutual information of the context and the target</li>
      <li>Ex. <a href="https://arxiv.org/pdf/1807.03748.pdf">Contrastive predictive coding (CPC)</a> : Context와 target의 mutual information을 최대한 유지한 채로 prediction</li>
      <li>variants : <a href="https://arxiv.org/pdf/2202.03944.pdf">LNT</a>, <a href="https://arxiv.org/pdf/2106.14112.pdf">TS-TCC</a> <a href="https://arxiv.org/pdf/2208.06616.pdf">CA-TCC</a> (TS data augmentation techniques)</li>
    </ul>
  </li>
  <li><strong>Augmentation contrast</strong>
    <ul>
      <li>different views of an input sample 생성 (같은 sample의 view끼리는 positive) \(\to\) similarity <a href="https://arxiv.org/pdf/2002.05709.pdf">SimCLR</a></li>
      <li>variants : <a href="https://arxiv.org/pdf/2106.10466.pdf">TS2Vec</a> (week augmentation e.g. jitter-and-scale), <a href="https://arxiv.org/pdf/2202.01575.pdf">CoST</a>, <a href="https://arxiv.org/pdf/2206.08496.pdf">TF-C</a> (frequency domain), <a href="https://arxiv.org/pdf/2306.10347.pdf">DCdetector</a>, <a href="https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705122X00075/1-s2.0-S0950705122002726/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHAaCXVzLWVhc3QtMSJIMEYCIQDgmFUeLriQDh4DVodXqV0y990PZCUCjyqFZw5tbaaWOQIhAJJXI3FfJlWxRX7ERxHJD52xqRrpOHF7bO5s8CcvR%2BObKrwFCIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igz9LqUqaUEhwALsmIAqkAVKSauBGYOJAgLt97GY2VGTuMQnRA7AgMmTvZNbQR13nb0eSyB3AddrtMNpzfTApqlafitG63sIURfGc04jk0%2BqiLp73hS3BmM6J9f3LNclZKHKiIQ%2BfUjQppBf9%2BSzb2GYgwcFIWgWQCR5PtN2siu0UfPOBUAUFniH5sww3WHYgzisgA4woX%2F%2B1lpEeSZAfoqOLu%2BD70QDpxoC0KgT%2FXhLrNhBehAy%2FZqTdVfSNmO8PnYW%2BbOc6HJ8zuTY5RATpiy9V7EMrdnYFYXvqu%2F1qW2x%2FtdeEZ0RfWCu524fQvnPpB8zVmySXJH4TXT4LA7QJE%2B1QwAHRqUatsaSoMpckzyqhP4LGwWneO%2Fq9RiEEqDkaOCpaL5F%2F%2B5i1tibhhFGI9ACzd814laQtq%2BIcvidp9986C1DHHJjTFHEXHUgMObbcOMXVA8xcpjCf5yFKcLC42BCHss82InfazC%2BJ8X4if%2BIhr7C%2B4MnowAGueEbmt5yQhoYboaD1tk%2FKKIohIhI2hghsk%2Fj%2BwhkWCn5KyLfoONkWJmJW1CXLDORF38jJLFEREaAQr3LRwExugVvijVdQLlWMyzPoCNRSLI5zkotkTprBsrs6iaJrS7hLn3mxOjCh9mZyJI2yV6%2Brr7jzg5XQ6VHiYlrkcfYuw9OFK5jv5OsoGoi4E46toRGV7X2p9jKlrg7T4m61BP5khSxzLJq61nOGlO%2Bx3zUvUT3dMyvkGMkKtaSk1%2FKXT4RovxkHp%2Bmpc3QUR7j6hzf5z0n9IIsv8FzAtzg58kKHg9OBQmHtCehUyLOErVo7P9Qxj5SIP7umI%2F%2Fe9p%2BK%2BGGZKNKv4LyjUqeB7XdlwPp2x2ysLv4d4o%2BIPtvkyjLqxhNu6sOYmb6VjCX1oSwBjqwAfzu66rFmDL30GLaRAVAW1ntG5mkCnwKEF3lFukhkQUOfwCxm7RM9jOJeG4%2Bmi3EJkTk7YzUpDz3XnUhEC9%2BvejsHkC1qETTJrb0bcqMFFW3EaZrZZYvZ4zodutAKA%2BUzMn5Df82EqEjSmeySV18YkkkKDNTM0usR%2FHaNfviRepwNslRryoSwTWiPWMwiV9tIIbZaIFpcs6nZlQB%2BGqMP1tw1rZAozE5PSe1E%2Btz1kLS&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240325T090224Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYQXRPBKFN%2F20240325%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=2f8baa651eec4fdd10cde55f49d1477a95f8710aa9c8758546ef1df59d330c78&amp;hash=4538839e76c3c485eb651626f2c825dd66a6195b3060bbed9553996335a29682&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0950705122002726&amp;tid=spdf-baf2cd37-88cd-4f95-a9ab-f50f1a42eb98&amp;sid=fc89350759b08043e82bbad-52465f639e79gxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0f11585559020c5807&amp;rr=869db8c7e9f3a7c3&amp;cc=kr">TimeCLR</a></li>
    </ul>
  </li>
  <li><strong>Prototype contrast</strong>
    <ul>
      <li>minimize the distance btw samples and prototype(=virtual sequences, centor, …)
but maximize the distance btw prototype(=virtual sequences, centor, …)</li>
    </ul>
  </li>
  <li><strong>Expert knowledge contrast</strong>
    <ul>
      <li>incorporates expert prior knowledge or information into deep neural networks to guide model training</li>
    </ul>
  </li>
</ul>

<h2 id="5-adversarial-based-methods">5. Adversarial-based Methods</h2>
<ul>
  <li>pre-text tasks를 GAN으로 푼다. (generator \(\mathcal{G}\) and a discriminator \(\mathcal{D}\))</li>
  <li>Learning objective : \(\mathcal{L}=\mathbb{E}_{x \sim \mathcal{P}_{\text {data }}(x)}[\log \mathcal{D}(x)]+\mathbb{E}_{z \sim \mathcal{P}_{\mathbf{z}}(z)}[\log (1-\mathcal{D}(\mathcal{G}(\mathbf{z})))]\)
<img src="/assets/img/timeseries/SSL4TS/fig4.png" alt="사진4" /></li>
  <li><strong>Time series generation and imputation</strong>
    <ul>
      <li>Complete time series generation : <a href="https://papers.nips.cc/paper_files/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf">TimeGAN</a> (autoregressive GAN), <a href="https://arxiv.org/pdf/2202.02691.pdf">TTS-GAN</a> (Transformer treating TS as image)</li>
      <li>time series imputation : pass</li>
    </ul>
  </li>
  <li><strong>Auxiliary representation enhancement</strong>
    <ul>
      <li>pass</li>
    </ul>
  </li>
</ul>

<h2 id="6-applications-and-datasets">6. Applications and Datasets</h2>
<p><img src="/assets/img/timeseries/SSL4TS/table2.png" alt="사진6" /></p>

<h2 id="7-discussion-and-future-directions">7. Discussion and Future Directions</h2>
<h3 id="71-selection-and-combination-of-data-augmentation">7.1. <strong>Selection and combination of data augmentation</strong></h3>
<ul>
  <li>augmentation methods :  jitter, scaling, rotation, permutation, and warping, …</li>
  <li>permutation + rotation + time warping &gt; single method
    <h3 id="72-inductive-bias-for-time-series-ssl">7.2. <strong>Inductive bias for time series SSL</strong></h3>
  </li>
  <li>특히 데이터가 충분하지 않을 수록 합리적인 inductive bias는 필요할 수 있음
    <h3 id="73-ssl-for-irregular-and-sparse-time-series">7.3. <strong>SSL for irregular and sparse time series</strong>=</h3>
  </li>
  <li>Irregular and sparse time series를 interpolation해서 쓰려고 하다보면 undesirable noise가 낄 수도 있으니 그대로 SSL로 활용</li>
</ul>

<h2 id="8-conclusion">8. Conclusion</h2>
<p>Pass</p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2306.10125.pdf)]]></summary></entry><entry><title type="html">(Survey paper) Time Series Forecasting With Deep Learning A Survey (Philos Trans R Soc A. 2020)</title><link href="http://localhost:4000/timeseries/2024-03-19-TSwDLsurvey/" rel="alternate" type="text/html" title="(Survey paper) Time Series Forecasting With Deep Learning A Survey (Philos Trans R Soc A. 2020)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-19T13:15:52+09:00</updated><id>http://localhost:4000/timeseries/TSwDLsurvey</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-TSwDLsurvey/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Time series forecasting
    <ul>
      <li>traditional methods : parametric models informed by domain expertise (ex. autoregressive(AR))</li>
      <li>machine learning : learn temporal dynamics in a puerly data-driven manner</li>
      <li>deep learning : learn complex data representation
        <ul>
          <li>CNN, RNN, Attention-based mechanism</li>
        </ul>
      </li>
      <li>hybrid model : Quantitative TS model + deep learning model</li>
    </ul>
  </li>
  <li>Time series forecasting의 application
    <ul>
      <li>interpretability and counterfactual prediction</li>
    </ul>
  </li>
</ul>

<h2 id="2-deep-learning-architectures-for-tsf">2. Deep Learning Architectures for TSF</h2>
<ul>
  <li>One-step-ahead forecasting : \(\hat{y}_{i, t+1}=f\left(y_{i, t-k: t}, \boldsymbol{x}_{i, t-k: t}, \boldsymbol{s}_i\right)\)
    <ul>
      <li>\(\hat{y}_{i, t+1}\) : model forecast</li>
      <li>\(y_{i, t-k: t}=\left\{y_{i, t-k}, \ldots, y_{i, t}\right\}, \boldsymbol{x}_{i, t-k: t}=\left\{\boldsymbol{x}_{i, t-k}, \ldots, \boldsymbol{x}_{i, t}\right\}\) : observation over look-back window</li>
      <li>\(f(\cdot)\) : the prediction function learntby the model
        <h3 id="2a-basic-building-blocks">2.(a) Basic building blocks</h3>
      </li>
    </ul>
  </li>
  <li>Encoder : \(\boldsymbol{z}_t=g_{\mathrm{enc}}\left(y_{t-k: t}, \boldsymbol{x}_{t-k: t}, \boldsymbol{s}\right)\)</li>
  <li>Decoder : \(f\left(y_{t-k: t}, \boldsymbol{x}_{t-k: t}, \boldsymbol{s}\right)=g_{\mathrm{dec}}\left(\boldsymbol{z}_t\right)\)
    <ul>
      <li>Encoder에서 observations를 latent vector로 representation</li>
      <li>(1) Convolution Neural Networks : \(\begin{aligned} \boldsymbol{h}_t^{l+1} &amp; =A((\boldsymbol{W} * \boldsymbol{h})(l, t)) \\ (\boldsymbol{W} * \boldsymbol{h})(l, t) &amp; =\sum_{\tau=0}^k \boldsymbol{W}(l, \tau) \boldsymbol{h}_{t-\tau}^l \end{aligned}\)
        <ul>
          <li>Convolution과 pooling을 반복하는 구조. TS에서는 과거의 값만 보도록 설계</li>
          <li>\(\boldsymbol{h}_t^l \in \mathbb{R}^{\mathcal{H}_{i n}}\) : intermediate state at layer \(l\) at time \(t\)</li>
          <li>
            <p>\(*\) : convolution operator</p>
          </li>
          <li>\(\boldsymbol{W}(l, \tau) \in$ $\mathbb{R}^{\mathcal{H}_{\text {out }} \times \mathcal{H}_{\text {in }}}\) : fixed filter weight at layer \(l\)</li>
          <li>\(A(.)\) : activation function</li>
        </ul>
      </li>
      <li>Dilated Convolution : \((\boldsymbol{W} * \boldsymbol{h})\left(l, t, d_l\right)=\sum_{\tau=0}^{\left\lfloor k / d_l\right\rfloor} \boldsymbol{W}(l, \tau) \boldsymbol{h}_{t-d_l \tau}^l\)
        <ul>
          <li>\(d_l\) : layer-specific dilation rate</li>
          <li>(WaveNet) \(d_l = 2^l\) at layer \(l\) (fig1.(a))
<img src="/assets/img/timeseries/TSwDLsurvey/fig1.png" alt="사진1" /></li>
        </ul>
      </li>
      <li>(2) Recurrent Neural Networks
        <ul>
          <li>Memory state를 통해 과거 정보를 기억하는 sequential data에 적합한 구조</li>
          <li>
            <p>Gradient vanishing으로 인한 long-range dependency \(\to\) LSTM</p>
          </li>
          <li>
            <p>Memory update funciton : \(\boldsymbol{z}_t=\nu\left(\boldsymbol{z}_{t-1}, y_t, \boldsymbol{x}_t, \boldsymbol{s}\right)\)</p>
          </li>
          <li>Network : \(\begin{aligned} y_{t+1} &amp; =\gamma_y\left(\boldsymbol{W}_y \boldsymbol{z}_t+\boldsymbol{b}_y\right) \\ \boldsymbol{z}_t &amp; =\gamma_z\left(\boldsymbol{W}_{z_1} \boldsymbol{z}_{t-1}+\boldsymbol{W}_{z_2} y_t+\boldsymbol{W}_{z_3} \boldsymbol{x}_t+\boldsymbol{W}_{z_4} \boldsymbol{s}+\boldsymbol{b}_z\right) \end{aligned}\)
            <ul>
              <li>\(W_{.}, \boldsymbol{b}\) : the linear weights and bias</li>
              <li>\(\gamma_y(.), \gamma_z(.)\) : network activation functions</li>
            </ul>
          </li>
          <li>Long Short Term Memory(LSTM)
<img src="/assets/img/timeseries/TSwDLsurvey/fig2.png" alt="사진2" /></li>
        </ul>
      </li>
      <li>(3) Attention mechanisms
        <ul>
          <li>form : \(\boldsymbol{h}_t=\sum_{\tau=0}^k \alpha\left(\boldsymbol{\kappa}_t, \boldsymbol{q}_\tau\right) \boldsymbol{v}_{t-\tau}\)
            <ul>
              <li>key \(\boldsymbol{\kappa}_t\), query \(\boldsymbol{q}_\tau\) and value \(\boldsymbol{v}_{t-\tau}\) are intermediate features produced at different time steps by lower levels of the network</li>
              <li>\(\alpha\left(\boldsymbol{\kappa}_t, \boldsymbol{q}_\tau\right) \in[0,1]\) is the attention weight for \(t-\tau\) generated at time \(t\)</li>
              <li>\(\boldsymbol{h}_t\) is the context vector output of the attention layer
                <h3 id="2b-multi-horizon-forecasting-models">2.(b) Multi-horizon Forecasting Models</h3>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>단순히 다음 한 시점에 대한 예측이 아닌 미래 여러 시점에 대한 예측</li>
  <li>(1) Iterative Methods : Autoregressive forecasting. 각 time step에서의 작은 오차가 누적된다는 단점이 있다.</li>
  <li>(2) Direct Methods : Encoder의 정보를 활용해서 한 번에 target time steps를 예측</li>
</ul>

<h2 id="3-incorporating-domain-knowledge-with-hybrid-models">3. Incorporating Domain Knowledge with Hybrid Models</h2>
<ul>
  <li>Machine learning의 underperformance의 이유는 1) flexibility로 인한 overfitting, 2) pre-processed input에 대한 sensitivity</li>
  <li>Hybrid models
    <ul>
      <li>combine well-studied quantitative time series models together with deep learning</li>
      <li>use domain knowledge \(\to\) hypothesis space를 줄여준다</li>
      <li>(a) Non-probabilistic Hybrid models : forecasting equations를 modify</li>
      <li>(b) Probabilistic Hybrid models : predictive distribution으로 parameters 생성</li>
    </ul>
  </li>
</ul>

<h2 id="4-facilitating-decision-support-using-deep-neural-networks">4. Facilitating Decision Support Using Deep Neural Networks</h2>
<ul>
  <li>연구하는 입장에서는 model의 성능(MSE, Accuracy, …)가 중요하지만, user는 future action에 대한 guide의 지표</li>
  <li>그러므로 Local Interpretable Model-Agnostic Explanations (LIME), Shapley additive explanations (SHAP)과 같은 post-hoc 분석, Attention weights를 통한 inherent interpretability를 이해할 필요가 있다.</li>
  <li>그러면 counterfactual forecast(determining what would have happened if a different set of circumstances had occurred) 가능</li>
</ul>

<h2 id="5-conclusions-and-future-directions">5. Conclusions and Future Directions</h2>
<ul>
  <li>Survey the main architectures used for TS forecasting</li>
  <li>Hybrid DL models : combine statistical and deep learning components</li>
  <li>Limitation : irregular TS나 hierarchical structure에 대한 고민은 하기 이전</li>
</ul>

<h2 id="추가">추가</h2>
<ul>
  <li>2020년에 발표된 survey 논문이지만 최근 Long-term Time Series Forecasting(LTSF)에 활용되는 모델에 대한 내용을 잘 정리한 논문이다.</li>
  <li>본 논문 이후 현재까지 최신 연구들을 이해한 상태로 읽는다면 최신 연구들의 motivation을 이해하는 데에 도움이 되는 논문이다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Philos Trans R Soc A. 2020](https://arxiv.org/pdf/2004.13408.pdf)]]></summary></entry><entry><title type="html">(Survey paper) Transformers in Time Series: A Survey (IJCAI 2023)</title><link href="http://localhost:4000/timeseries/2024-03-19-TFinTSsurvey/" rel="alternate" type="text/html" title="(Survey paper) Transformers in Time Series: A Survey (IJCAI 2023)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-20T17:35:16+09:00</updated><id>http://localhost:4000/timeseries/TFinTSsurvey</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-TFinTSsurvey/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer는 long-range dependencies and interactions를 학습할 수 있다.</li>
  <li>본 논문에서는 Network structure 관점에서 Transformer를 TS forecasting에 사용하기 위해 어떤 adaptaion and modification을 했는지 알아보고</li>
  <li>Application 관점에서 forecasting, anomaly detection, and classification을 포함한 task에 대해 얼마나 잘 작동하는지 알아본다.</li>
  <li>마지막으로 future direction을 제시한다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer : ability for long-range dependencies and interactions in sequential data</li>
  <li>Time series : How to effectively model long-range and short-range temporal dependency and capture seasonality simultaneously ?</li>
  <li>Network modification 관점 : low-level(i.e. module)부터 high-level(i.e. architecture)</li>
  <li>Application 관점 : summarize Transformer for forecasting, anomaly detection, and classification</li>
</ul>

<h2 id="2-preliminaries-of-the-transformer">2. Preliminaries of the Transformer</h2>

<h3 id="21-vanilla-transformer">2.1. Vanilla Transformer</h3>

<ul>
  <li>Encoder : a multi- head self-attention module and a position-wise feed-forward network</li>
  <li>Decoder : cross-attention models between the multi-head self-attention module and the position-wise feed-forward network</li>
</ul>

<h3 id="22-input-encoding-and-positional-encoding">2.2. Input Encoding and Positional Encoding</h3>

<ul>
  <li>No recurrence, instead positional encoding</li>
  <li>Absolute Positional Encoding : \(PE(t)_i= \begin{cases}\sin \left(\omega_i t\right) &amp; i \% 2=0 \\ \cos \left(\omega_i t\right) &amp; i \% 2=1\end{cases}\)
    <ul>
      <li>\(\quad \omega_i\) is the hand-crafted frequency for each dim</li>
    </ul>
  </li>
  <li>Relative Positional Encoding : input의 상대적인 위치에 대해 learnable하지만 train에서 본 적 없는, 더 긴 길이의 input에 대해서 확장이 어려움</li>
</ul>

<h3 id="23-multi-head-attention">2.3. Multi-head Attention</h3>

<ul>
  <li>
    <p>scaled dot-product : \(\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q K}^{\mathbf{T}}}{\sqrt{D_k}}\right) \mathbf{V}\)</p>
  </li>
  <li>
    <p>Multi-head Attention :</p>

\[\begin{aligned}MultiHeadAttn (\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Concat}\left(\right. head _1, \cdots, head \left._H\right) \mathbf{W}^O \\ \text{where } head _i= Attention \left(\mathbf{Q} \mathbf{W}_i^Q, \mathbf{K} \mathbf{W}_i^K, \mathbf{V} \mathbf{W}_i^V\right)\end{aligned}\]
  </li>
</ul>

<h3 id="24-feed-forward-and-residual-network">2.4. Feed-forward and Residual Network</h3>

<ul>
  <li>The feed-forward network(FFN) : \(FFN\left(\mathbf{H}^{\prime}\right)=\operatorname{ReLU}\left(\mathbf{H}^{\prime} \mathbf{W}^1+\mathbf{b}^1\right) \mathbf{W}^2+\mathbf{b}^2\)
    <ul>
      <li>\(\mathbf{H}^{\prime}\) is outputs of previous layer</li>
      <li>\(\mathbf{W}^1 \in \mathcal{R}^{D_m \times D_f}\), \(\mathbf{W}^2 \in \mathcal{R}^{D_f \times D_m}, \mathbf{b}^1 \in \mathcal{R}^{D_f}, \mathbf{b}^2 \in \mathcal{R}^{D_m}\) are trainable parameters</li>
    </ul>
  </li>
  <li>Residual connection module followed by a layer normalization module : \(\begin{aligned} \mathbf{H}^{\prime} &amp; =\operatorname{LayerNorm}(\operatorname{Self} \operatorname{Attn}(\mathbf{X})+\mathbf{X}), \\ \mathbf{H} &amp; =\operatorname{LayerNorm}\left(FFN\left(\mathbf{H}^{\prime}\right)+\mathbf{H}^{\prime}\right)\end{aligned}\)
    <ul>
      <li>SelfAttn(.) : self-attention module</li>
      <li>LayerNorm(.) : the layer normalization operation</li>
    </ul>
  </li>
</ul>

<h2 id="3-taxonomy-of-transformers-in-time-series">3. Taxonomy of Transformers in Time Series</h2>

<p><img src="/assets/img/timeseries/TFinTSsurvey/fig1.png" alt="사진1" /></p>

<h2 id="4-network-modifications-for-time-series">4. Network Modifications for Time Series</h2>

<h3 id="41-positional-encoding">4.1. Positional Encoding</h3>

<ul>
  <li>Vanilla Positional Encoding : fixed, hand-crafted</li>
  <li>Learnable Positional Encoding : more flexible and can adapt to spe- cific tasks</li>
  <li>Timestamp Encoding : calendar timestamps (e.g., second, minute, hour, week, month, and year) and special times- tamps (e.g., holidays and events) \(\to\) additional position encoding</li>
</ul>

<h3 id="42-attention-module">4.2. Attention Module</h3>

<ul>
  <li>Self-attention module : FC layer w weights that are dynamically generated based on the pairwise similarity of input patterns</li>
  <li>Memory complexity \(O(N^2)\)
    <ul>
      <li>explicitly introducing a sparsity bias into the attention mechanism
        <ul>
          <li>e.g. LogTrans [Li <em>et al.</em>, 2019, Pyraformer [Liu <em>et al.</em>, 2022a]</li>
        </ul>
      </li>
      <li>exploring the low-rank property of the self-attention matrix to speed up the computation,
        <ul>
          <li>e.g. Informer [Zhou <em>et al.</em>, 2021], FEDformer [Zhou <em>et al.</em>, 2022].</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table1.png" alt="사진2" /></p>

<h3 id="43-architecture-based-attention-innovation">4.3. Architecture-based Attention Innovation</h3>

<ul>
  <li>Hierarchical architecture : multi-resolution aspect of TS
    <ul>
      <li>Informer [Zhou <em>et al.</em>, 2021]
        <ul>
          <li>max-pooling layers with stride 2 btw attention blocks (down-sample series into its half slice)</li>
        </ul>
      </li>
      <li>Pyraformer [Liu <em>et al.</em>, 2022a]
        <ul>
          <li>C-ary tree-based attention mechanism (finest-origin / coarser-lower resolutions)</li>
          <li>both intra-scale and inter-scale attentions \(\to\) temporal dependencies across different resolutions</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-applications-of-time-series-transformers">5. Applications of Time Series Transformers</h2>

<p><img src="/assets/img/timeseries/TFinTSsurvey/fig2.jpeg" alt="사진9" /></p>

<h3 id="51-transformers-in-forecasting">5.1. Transformers in Forecasting</h3>

<ul>
  <li><strong>Module-level variants</strong> : main architectures는 비슷한데, minor changes
    <ul>
      <li>(1) designing new attention modules
        <ul>
          <li>LogTrans [Li <em>et al.</em>, 2019] : convolution self-attention, sparse bias (Logsparse mask)</li>
          <li>Informer [Zhou <em>et al.</em>, 2021] : selects dominant queries based on queries and key similarities</li>
          <li>AST [Wu <em>et al.</em>, 2020a] : generative adversarial encoder- decoder framework to train a sparse Transformer</li>
          <li>Pyraformer [Liu <em>et al.</em>, 2022a] : hierarchical pyramidal attention module with a binary tree following the path</li>
          <li>Quatformer [Chen <em>et al.</em>, 2022] : learning-to-rotate attention (LRA) based on quaternions that introduce learnable period and phase information</li>
          <li>FEDformer [Zhou <em>et al.</em>, 2022] : attention operation in the frequency domain with Fourier trans- form and wavelet transform</li>
        </ul>
      </li>
      <li>(2) exploring the innovative way to normalize time series data
        <ul>
          <li>Non-stationary Transformer [Liu <em>et al.</em>, 2022b]</li>
        </ul>
      </li>
      <li>(3) utilizing the bias for token inputs
        <ul>
          <li>Autoformer [Wu <em>et al.</em>, 2021] : segmentation-based representation mechanism (auto-correlation mechanism)</li>
          <li>PatchTST [Nie <em>et al.</em>, 2023] : subseries-level patch design which are served as input tokens w/ channel-independency</li>
          <li>Cross- former [Zhang and Yan, 2023] : input is embedded into a 2D vector array and then two-stage attention layer is used to efficiently capture the cross-time and cross-dimension dependency</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Architecture-level variants</strong>
    <ul>
      <li>Triformer [Cirstea <em>et al.</em>, 2022] : triangular,variable-specific patch attention \(\to\) lightweight and linear complex- ity</li>
      <li>Scaleformer [Shabani <em>et al.</em>, 2023] : iteratively refine the forecasted time series at multiple scales with shared weights.</li>
    </ul>
  </li>
  <li><strong>Spatio-temporal Forecasting, Evnet Forecasting</strong>
    <ul>
      <li>Pass</li>
    </ul>
  </li>
</ul>

<h3 id="52-transformers-in-anomaly-detection">5.2. Transformers in Anomaly Detection</h3>

<ul>
  <li>
    <p>Transformer + Generative models</p>

    <ul>
      <li>
        <p>TranAD [Tuli <em>et al.</em>, 2022] : Transformer는 small deviation of anomaly는 놓치므로 reconstruction errors를 amplify하는 adversarial training</p>
      </li>
      <li>
        <p>MT-RVAE [Wang <em>et al.</em>, 2022], TransAnomaly [Zhang <em>et al.</em>, 2021] : Transformer + VAE \(\to\) reduce training costs, 다양한 scale의 정보 통합</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="53-transformers-in-classification">5.3 Transformers in Classification</h3>

<ul>
  <li>GTN [Liu <em>et al.</em>, 2021] : two-tower Transformer(time-step-wise attention and channel-wise attention) \(\to\)  learnable weighted concatenation</li>
  <li>TARNet [Chowdhury <em>et al.</em>, 2022] : utilizes attention score for important timestamps masking and reconstruction</li>
</ul>

<h2 id="6-experimental-evaluation-and-discussion">6. Experimental Evaluation and Discussion</h2>

<ul>
  <li><strong>Robustness Analysis</strong>
    <ul>
      <li>대부분의 attention-based models는 lower the quadratic calculation and memory complexity를 위해 module을 수정했고, 좋은 실험 결과를 위해 짧은 input을 사용했는데, 긴 input을 넣어도 MSE가 커지지 않고 잘 유지되는지 확인했다.</li>
      <li>대부분의 모델들이 긴 input에 대해서는 잘 처리하지 못한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table2.png" alt="사진3" /></p>

<ul>
  <li><strong>Model Size Analysis</strong>
    <ul>
      <li>일반적으로 model size가 커지면 prediction power도 좋아지는데 attention-based models에서 그렇지 않음을 확인했다.</li>
      <li>지금까지의 Transformer 자체가 features를 잘 뽑아내지 못하는 구조일 수 있겠다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table3.png" alt="사진4" /></p>

<ul>
  <li><strong>Seasonal-Trend Decomposition Analysis</strong>
    <ul>
      <li>seasonal-trend decomposition는 Transformer에서 필수적인 부분 : model performance가 50% - 80% boosting</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table4.png" alt="사진5" /></p>

<h2 id="7-future-research-opportunities">7. Future Research Opportunities</h2>

<h3 id="71-inductive-biases-for-time-series-transformers">7.1. Inductive Biases for Time Series Transformers</h3>

<ul>
  <li>Channel-independence와 Cross-channel(dim) dependency은 서로 반대 inductive bias이지만 둘 다 실험 결과가 좋았다.</li>
  <li>즉 cross-channel learning에는 noise도 있고 signal도 있다는 의미</li>
  <li>어떤 inductive bias를 어떻게 induce할지 고려할 필요가 있음</li>
</ul>

<h3 id="72-transformers-and-gnn-for-time-series">7.2. Transformers and GNN for Time Series</h3>

<ul>
  <li>Traffic forecsting처럼 spatial dependency (relationship among dim)이 강한 경우에는 GNN + Transformer의 성능이 좋을 수 있다.</li>
</ul>

<h3 id="73-pre-trained-transformers-for-time-series">7.3. Pre-trained Transformers for Time Series</h3>

<ul>
  <li>Large-scale pre-trained Transformer model의 성능이 좋긴 한데 대부분 classification을 위한 pre-train이라는 점에서, 다른 tasks를 위한 pre-train도 고려할 수 있다.</li>
</ul>

<h3 id="74-transformers-with-architecture-level-variants">7.4. Transformers with Architecture Level Variants</h3>

<ul>
  <li>지금까지는 attention module에 대한 modification이 주로 등장했지만, TS를 위한 architecture-level design도 고려할 수 있다.</li>
</ul>

<h3 id="75-transformers-with-nas-for-time-series">7.5. Transformers with NAS for Time Series</h3>

<ul>
  <li>Neural architecture search(NAS)과 같은 AutoML을 통해, 성능에 영향을 주는 embedding dimension이나 head/layer의 개수 등 효율적인 architecture를 고려할 수 있다.</li>
</ul>

<h2 id="8-conclusion">8. Conclusion</h2>

<ul>
  <li>new taxonomy consisting of network design and application</li>
  <li>strengths and limitations of representative methods by experimental evaluation</li>
  <li>highlight future research directions.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[IJCAI 2023](https://arxiv.org/pdf/2202.07125.pdf)]]></summary></entry><entry><title type="html">(Crossformer) Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting (ICLR 2023)</title><link href="http://localhost:4000/timeseries/2024-03-19-crossformer/" rel="alternate" type="text/html" title="(Crossformer) Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting (ICLR 2023)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-19T21:34:48+09:00</updated><id>http://localhost:4000/timeseries/crossformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-crossformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer-based models
    <ul>
      <li>focus on modeling the temporal dependency (cross-time dependency)</li>
      <li>yet often omit the dependency among different variables (cross- dimension dependency)</li>
    </ul>
  </li>
  <li>Crossformer는
    <ul>
      <li>Dimension-Segment-Wise (DSW) : MTS \(\to\) 2d vector array로 만들고</li>
      <li>Two-Stage Attention (TSA) : 2개의 attention을 거치는데 각각 cross-time and cross-dimension dependency를 학습한다.</li>
      <li>Hierarchical Encoder-Decoder (HED) : 그리고 서로 다른 scales의 정보를 사용해서 coarse, fine한 정보 모두 활용하여 forecasting한다.</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>MTS에서는 cross-time dependency 뿐만 아니라 cross-dimension dependency도 중요한데, Transformer에서 cross-dim dependency를 반영하는 방법은 embedding 뿐이다.</li>
  <li>본 논문에서는 cross-dim dependency를 explicitly하게 사용한다.</li>
  <li>Dimension-Segment-Wise (DSW) : the series(e.g. UTS)는 segments로 나뉘고, 각 segment는 feature vector가 된다. (series는 2d vector array가 된다.)</li>
  <li>Two-Stage Attention (TSA): 2d vector array로부터 cross-time and cross-dimension dependency 학습</li>
  <li>Hierarchical Encoder-Decoder (HED) : 각 layer에서 서로 다른 scale에 대한 dependency를 학습하게 된다.</li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li><strong>Multivariate Time Series Forecasting</strong>
    <ul>
      <li>Statistical models : Vector auto-regressive(VAR), Vector auto-regressive moving average (VARMA)</li>
      <li>Neural models : TCN, DeepAR, LSTnet(CNN+RNN), MTGNN, …</li>
    </ul>
  </li>
  <li><strong>Transformer-based model</strong> : LogTrans, Informer, Autoformer, Pyraformer, FEDformer, Preformer, …</li>
  <li><strong>Vision Transformers</strong> : Transformer를 vision에서 사용할 때 썼던 patching 방식</li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<ul>
  <li>\(\mathbf{x}_{1: T} \in \mathbb{R}^{T \times D}\)를 보고 \(\mathbf{x}_{T+1: T+\tau} \in \mathbb{R}^{\tau \times D}\)​ 예측하는 문제
    <ul>
      <li>\(\tau, T\) is the number of time steps in the future and past, respectively</li>
      <li>\(D&gt;1\) is the number of dimensions</li>
    </ul>
  </li>
</ul>

<h3 id="31-dimension-segment-wise-embedding">3.1. Dimension-Segment-wise Embedding</h3>
<p><img src="/assets/img/timeseries/crossformer/fig1.png" alt="사진1" /></p>
<ul>
  <li>t시점의 모든 dimension의 data point \(\mathbf{x}_t \in \mathbb{R}^D\)를 \(\mathbf{h}_t \in \mathbb{R}^{d_{\text {model }}}\)로 embedding한다.</li>
  <li>
\[\begin{aligned}
\mathbf{x}_{1: T} &amp; =\left\{\mathbf{x}_{i, d}^{(s)} \left\lvert\, 1 \leq i \leq \frac{T}{L_{\text {seg }}}\right., 1 \leq d \leq D\right\} \\
\mathbf{x}_{i, d}^{(s)} &amp; =\left\{x_{t, d} \mid(i-1) \times L_{\text {seg }}&lt;t \leq i \times L_{\text {seg }}\right\} \\ \mathbf{h}_{i, d}&amp;=\mathbf{E} \mathbf{x}_{i, d}^{(s)}+\mathbf{E}_{i, d}^{(p o s)} \end{aligned}\]
    <ul>
      <li>\(\mathbf{x}_{i, d}^{(s)} \in \mathbb{R}^{L_{\text {seg }}}\)  is the \(i\)-th segment in dimension \(d\) with length \(L_{\text {seg }}\)</li>
      <li>\(\mathbf{E} \in \mathbb{R}^{d_{\text {model }} \times L_{\text {seg }}}\) : the learnable projection matrix</li>
      <li>\(\mathbf{E}_{i, d}^{(\text {pos })} \in \mathbb{R}^{d_{\text {model }}}\) : the learnable position embedding for position $(i, d)$.</li>
    </ul>
  </li>
  <li>
\[\mathbf{H}=\left\{\mathbf{h}_{i, d} \mid, 1 \leq i \leq \frac{T}{L_{\text {seg }}}, 1 \leq d \leq D\right\}\]
    <ul>
      <li>where each \(\mathbf{h}_{i, d}\) represents a univariate time series segment.</li>
    </ul>
  </li>
  <li>수식으로 표현하다보니 어려운데 아래 그림과 같고, \(\mathbf{H}\)는 오른쪽처럼 생겼다.
<img src="/assets/img/timeseries/crossformer/myfig1.jpeg" alt="사진2" /></li>
</ul>

<h3 id="32-two-stage-attention-layer">3.2. Two-Stage Attention Layer</h3>
<ul>
  <li>이미지가 아니라 시계열이다보니 height와 width가 서로 바뀌면 의미가 달라지기 때문에 flatten시키면 안되고 바로 \(\mathbf{H}\)에 self-attention을 적용한다.</li>
  <li><strong>Cross-Time Stage</strong>
    <ul>
      <li>\(\mathbf{Z}_{i, \text { : }}\) : the vectors of all dimensions at time step \(i\)</li>
      <li>\(\mathbf{Z}_{:, d}\) the vectors of all time steps in dimension \(d\)
<img src="/assets/img/timeseries/crossformer/myfig2.jpeg" alt="사진3" /></li>
      <li>
\[\begin{aligned} \hat{\mathbf{Z}}_{:, d}^{\text {time }}=\text { LayerNorm }\left(\mathbf{Z}_{:, d}+\operatorname{MSA}^{\text {time }}\left(\mathbf{Z}_{:, d}, \mathbf{Z}_{:, d}, \mathbf{Z}_{:, d}\right)\right) \\ \mathbf{Z}^{\text {time }}=\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {time }}+\operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {time }}\right)\right) \end{aligned}\]
      </li>
      <li>\(\mathbf{Z}^{time}\)이 다음 stage인 Cross-Dimension Stage의 input이 된다.</li>
    </ul>
  </li>
  <li><strong>Cross-Dimension Stage</strong>
    <ul>
      <li>\(\begin{aligned} \mathbf{B}_{i,:} &amp; =\mathrm{MSA}_1^{\operatorname{dim}}\left(\mathbf{R}_{i,:}, \mathbf{Z}_{i,:}^{\text {time }}, \mathbf{Z}_{i,:}^{\text {time }}\right), 1 \leq i \leq L \\ \overline{\mathbf{Z}}_{i,:}^{\text {dim }} &amp; =\mathrm{MSA}_2^{\text {dim }}\left(\mathbf{Z}_{i,:}^{\text {time }}, \mathbf{B}_{i,:}, \mathbf{B}_{i,:}\right), 1 \leq i \leq L \\ \hat{\mathbf{Z}}^{\text {dim }} &amp; =\text { LayerNorm }\left(\mathbf{Z}^{\text {time }}+\overline{\mathbf{Z}}^{\text {dim }}\right) \\ \mathbf{Z}^{\text {dim }} &amp; =\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {dim }}+\operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {dim }}\right)\right) \end{aligned}\)
<img src="/assets/img/timeseries/crossformer/fig2.png" alt="사진4" /></li>
      <li>\(D\)가 클 때에는 router mechanism을 사용하여 fixed number \(c &lt;&lt; D\) vectors에 정보를 모았다가 다시 뿌려준다.</li>
      <li>\(\mathbf{R} \in \mathbb{R}^{L \times c \times d_{\text {model }}}\) : the learnable vector array serving as routers</li>
      <li>\(\mathbf{B} \in \mathbb{R}^{L \times c \times d_{\text {model }}}\) : the aggregated messages from all dimensions</li>
      <li>\(\overline{\mathbf{Z}}^{\text {dim }}\) : output of the router mechanism.</li>
      <li>All time steps \((1 \leq i \leq L)\) share the same \(\mathbf{M S A}_1^{\text {dim }}, \mathbf{M S A}_2^{\text {dim }}\)</li>
      <li>\(\hat{\mathbf{Z}}^{\text {dim }}, \mathbf{Z}^{\text {dim }}\) : output of skip connection and MLP respectively</li>
    </ul>
  </li>
</ul>

<h3 id="33-hierarchical-encoder-decoder">3.3. Hierarchical Encoder-Decoder</h3>
<p><img src="/assets/img/timeseries/crossformer/fig3.png" alt="사진5" /></p>
<ul>
  <li>Upper layer일수록 coarser scale을 사용한 정보를 얻고, 서로 다른 scale로 얻은 정보들로 예측한 값들은 final result에서 더해진다.</li>
  <li><strong>Encoder</strong> : upper layer일수록 coarser scale을 사용한다는 말은 인접한 두 vector(segment)를 merge한다는 것과 같다.</li>
  <li>\(\mathbf{Z}^{e n c, l}=\operatorname{Encoder}\left(\mathbf{Z}^{e n c, l-1}\right)\)의 연산은 아래와 같다.
\(\begin{aligned} &amp; \begin{cases}l=1: &amp; \hat{\mathbf{Z}}^{e n c, l}=\mathbf{H} \\ l&gt;1: &amp; \hat{\mathbf{Z}}_{i, d}^{e n c, l}=\mathbf{M}\left[\mathbf{Z}_{2 i-1, d}^{e n c, l-1} \cdot \mathbf{Z}_{2 i, d}^{e n c, l-1}\right], 1 \leq i \leq \frac{L_{l-1}}{2}, 1 \leq d \leq D\end{cases} \\&amp; \mathbf{Z}^{\text {enc,l}}=\operatorname{TSA}\left(\hat{\mathbf{Z}}^{\text {enc,l}}\right) \end{aligned}\)
    <ul>
      <li>
        <p>\(\mathbf{H}\) denotes the 2D array obtained by DSW embedding</p>
      </li>
      <li>
        <p>\(\mathbf{Z}^{e n c, l}\) denotes the output of the \(l\)-th encoder layer</p>
      </li>
      <li>\(\mathbf{M} \in \mathbb{R}^{d_{\text {model }} \times 2 d_{\text {model }}}\) denotes a learnable matrix for segment merging</li>
      <li>\([\cdot]\) denotes the concatenation operation</li>
      <li>\(L_{l-1}\) denotes the number of segments in each dimension in layer \(l-1\)</li>
      <li>\(\hat{\mathbf{Z}}^{e n c, l}\) denotes the array after segment merging in the \(i\)-th layer</li>
      <li>\(\mathbf{Z}^{\text {enc }, 0}, \mathbf{Z}^{\text {enc }, 1}, \ldots, \mathbf{Z}^{\text {enc }, N},\left(\mathbf{Z}^{\text {enc }, 0}=\mathbf{H}\right)\) is used to represent the \(N+1\) outputs of the encoder</li>
    </ul>
  </li>
  <li><strong>Decoder</strong> : Emcoder에서 얻은 \(N+1\)개의 feature array가 있으면, \(N+1\)개의 layers로 예측한다.
    <ul>
      <li>decoder의 process : \(\mathbf{Z}^{\text {dec, } l}=\operatorname{Decoder}\left(\mathbf{Z}^{\text {dec, },-1}, \mathbf{Z}^{\text {enc, },}\right)\)
\(\begin{aligned} &amp; \left\{\begin{array}{lll} l=0: &amp; \tilde{\mathbf{Z}}^{\text {dec }, l}=\operatorname{TSA}\left(\mathbf{E}^{(d e c)}\right) \\ l&gt;0: &amp; \tilde{\mathbf{Z}}^{\text {dec, }, l}=\operatorname{TSA}\left(\mathbf{Z}^{\text {dec, },-1}\right) \end{array}\right. \\ &amp; \overline{\mathbf{Z}}_{:, d}^{\text {dec, }, l}=\operatorname{MSA}\left(\tilde{\mathbf{Z}}_{:, d}^{\text {dec, }, l}, \mathbf{Z}_{:, d}^{e n c, l}, \mathbf{Z}_{:, d}^{e n c, l}\right), 1 \leq d \leq D \\ &amp; \hat{\mathbf{Z}}^{\text {dec, } l}=\text { LayerNorm }\left(\tilde{\mathbf{Z}}^{\text {dec, }, l}+\overline{\mathbf{Z}}^{\text {dec, } l}\right) \\ &amp; \mathbf{Z}^{\text {dec,l}}=\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {dec, }, l} \operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {dec,l}}\right)\right) \\ &amp; \end{aligned}\)</li>
      <li>\(\mathbf{E}^{(\text {dec })} \in \mathbb{R}^{\frac{\tau}{L_{s e g}} \times D \times d_{\text {model }}}\) denotes the learnable position embedding for decoder</li>
      <li>\(\tilde{\mathbf{Z}}^{\text {dec,l } l}\) is the output of TSA</li>
      <li>The MSA layer takes \(\tilde{\mathbf{Z}}_{:, d}^{d e c,l }\) as query and \(\mathbf{Z}_{:, d}^{e n c, l}\) as the key and value to build the connection between encoder and decoder</li>
      <li>The output of MSA is denoted as \(\overline{\mathbf{Z}}_{:, d}^{\text {dec, },} . \hat{\mathbf{Z}}^{\text {dec,l}, ~} \mathbf{Z}^{\text {dec, }, l}\) denote the output of skip connection and MLP respectively.</li>
      <li>\(\mathbf{Z}^{\text {dec, 0},}, \mathbf{Z}^{e n c, 1}, \ldots, \mathbf{Z}^{\text {dec, } N}\) : is used to represent decoder output</li>
      <li><strong>Linear projection</strong> : 각 layer에서는 linear projection으로 prediction을 만들고, 각 layer의 prediction을 다 더하면 최종 prediction이 된다.
\(\begin{gathered} \text { for } l=0, \ldots, N: \mathbf{x}_{i, d}^{(s), l}=\mathbf{W}^l \mathbf{Z}_{i, d}^{\text {dec,l }} \quad \mathbf{x}_{T+1: T+\tau}^{\text {pred, } l}=\left\{\mathbf{x}_{i, d}^{(s), l} \left\lvert\, 1 \leq i \leq \frac{\tau}{L_{\text {seg }}}\right., 1 \leq d \leq D\right\} \\ \mathbf{x}_{T+1: T+\tau}^{\text {pred }}=\sum_{l=0}^N \mathbf{x}_{T+1: T+\tau}^{\text {pred, }}\end{gathered}\)
        <ul>
          <li>\(\mathbf{W}^l \in \mathbb{R}^{L_{\text {seg }} \times d_{\text {model }}}\) : learnable matrix to project a vector to a ts segment</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<h3 id="41-protocols">4.1. Protocols</h3>
<ul>
  <li>Dataset : 1) ETTh1 (Electricity Transformer Temperature-hourly), 2) ETTm1 (Electricity Transformer Temperature-minutely), 3) WTH (Weather), 4) ECL (Electricity Consuming Load), 5) ILI (Influenza-Like Illness), 6) Traffic</li>
  <li>Baselines : 1) LSTMa (Bah- danau et al., 2015), 2) LSTnet (Lai et al., 2018), 3) MTGNN (Wu et al., 2020), and recent Transformer-based models for MTS forecasting: 4) Transformer (Vaswani et al., 2017), 5) In- former (Zhou et al., 2021), 6) Autoformer (Wu et al., 2021a), 7) Pyraformer (Liu et al., 2021a) and 8) FEDformer (Zhou et al., 2022)
    <h3 id="42-main-results">4.2. Main Results</h3>
    <p><img src="/assets/img/timeseries/crossformer/table1.png" alt="사진6" /></p>
    <h3 id="43-ablation-study">4.3. Ablation Study</h3>
    <p><img src="/assets/img/timeseries/crossformer/table2.png" alt="사진7" /></p>
    <h3 id="44-effect-of-hyper-parameters">4.4. Effect of Hyper-parameters</h3>
    <p><img src="/assets/img/timeseries/crossformer/fig4.png" alt="사진8" /></p>
    <h3 id="45-computational-efficiency-analysis">4.5. Computational Efficiency Analysis</h3>
    <p><img src="/assets/img/timeseries/crossformer/table3.png" alt="사진9" /></p>
  </li>
</ul>

<h3 id="5-conclusion">5. Conclusion</h3>
<ul>
  <li>Crossformer : Transformer-based model utilizing cross-dimension dependency for MTS forecasting</li>
  <li><strong>Dimension-Segment-Wise (DSW)</strong> embedding embeds the input data into a 2D vector array to preserve the information of both time and dimension</li>
  <li><strong>The Two-Stage-Attention (TSA)</strong> layer is devised to capture the cross-time and cross- dimension dependency of the embedded array</li>
  <li>Using DSW embedding and TSA layer, a <strong>Hierarchical Encoder-Decoder (HED)</strong> is devised to utilize the information at different scales</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://openreview.net/forum?id=vSVLM2j9eie)]]></summary></entry><entry><title type="html">(SoftCLT) Soft Contrastive Learning for Time Series (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-03-13-softCLT/" rel="alternate" type="text/html" title="(SoftCLT) Soft Contrastive Learning for Time Series (ICLR 2024)" /><published>2024-03-13T00:00:00+09:00</published><updated>2024-04-02T17:42:19+09:00</updated><id>http://localhost:4000/timeseries/softCLT</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-13-softCLT/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>TS instance만으로 contrasting하거나, 하나의 TS의 adjacent timestamps만으로 contrasting하면 TS의 inherent correlation을 제대로 반영하지 못한다.</li>
  <li><strong>SoftCLT</strong> : instance-wise &amp; temporal Contrastive loss를 사용하여, 0 또는 1이 아닌 0 ~ 1 사이의 값으로 assign
    <ul>
      <li>Instance-wise contrastive loss : Data space에서 두 시계열의 distance</li>
      <li>Temporal contrastive loss : 서로 다른 시점에 대한 loss</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>
<ul>
  <li>Self-supervised learning - Contrastive learning에서는 두 instance의 유사도로 pair를 설정하기보다는, 각 데이터에 대해 2개의 view를 augmentation하고 둘을 positive pair로 설정한다. (다른 데이터에서 augmentation된 view와는 negative)</li>
  <li>TS의 inherent correlations는 유사한 instance 뿐만 아니라, 인접한 timestamps에도 있다.</li>
  <li>설령 같은 data에서 augmentation 되었거나, 같은 timestamp의 value에만 positive(1)로 설정하더라도, 다른 data에서 augmentation 된 다른 timestamp의 value에는 다 똑같의 negative(0)으로 설정하는 것은 optimal하지 않다.</li>
  <li><a href="https://arxiv.org/pdf/1807.03748.pdf">InfoNCE</a>의 loss처럼 positive pair 뿐만 아니라 negative pair에 대해서도 weight를 고려하는 Soft Contrastive Learning for TS를 제안
<img src="/assets/img/timeseries/softclt/table1.png" alt="사진1" /></li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>
<ul>
  <li>Self-supervised learning : 많은 양의 unlabeled data를 활용하는 pretext task를 수행하는 모델을 훈련시키고, 해당 모델을 downstream task의 앞쪽에 가져와서 사용한다. pretext task로는 <code class="language-plaintext highlighter-rouge">next token prediction</code>, <code class="language-plaintext highlighter-rouge">masked token prediction</code>, <code class="language-plaintext highlighter-rouge">jigsaw puzzles</code>, <code class="language-plaintext highlighter-rouge">rotation prediction</code> 등</li>
  <li>Contrastive learning in TS
    <ul>
      <li><a href="https://arxiv.org/pdf/1901.10738.pdf">T-Loss</a> : TS에서 subseries 샘플링, subseries가 속한 TS와는 positive 다른 TS와는 negative</li>
      <li><a href="https://arxiv.org/pdf/2011.13548.pdf">Self-Time</a> : augmented sample로 inter-sample relation 학습, temporal distance로 label을 만들고 classification 해서 intra-temporal relation 학습</li>
      <li><a href="https://arxiv.org/pdf/2106.00750.pdf">TNC</a> : 정규분포 window로 정의한 temporal neighborhood를 positive로 설정</li>
      <li><a href="https://arxiv.org/pdf/2106.14112.pdf">TS-TCC</a> : augmentations가 서로의 미래 시점을 예측하도록 해서 temporal contrastive loss 설정</li>
      <li><a href="https://arxiv.org/pdf/2203.09270.pdf">Mixing-up</a> : 2개의 TS를 섞어서 새로운 TS를 만드는데 mixing weights를 predict</li>
      <li><a href="https://arxiv.org/pdf/2202.01575.pdf">CoST</a> : time, frequency domain의 contrastive losses를 사용하여 representation learning</li>
      <li><a href="https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705122X00075/1-s2.0-S0950705122002726/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCDSwasc4kb8LU0bgnJMwyHRmJ5Xp0qOkMGkgGRC101hgIgQu4zuleiKDecSs%2FYiwU6McTbx88zb7ZGNMt6fPGxxoAqvAUI0v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDF3vdKai06uU77VSpSqQBVBf8gEuvX4RtLZKv9hE3KxXK6cBABJ3MlBBkOHOQG9wKZz7o6d1XmLn2FZSjY4%2FX7pPyQcDRmkq2n%2FJq8%2Fsui0EeAASo4iyS840z2aCXativJcKRglZNYHjwhkBAax3A8xvkoahV70%2BnaX2GHQ5TWPuaUEwQkfUD1Y%2BKJW17Llm6SzL9NFiUMu9oRVuGLNfzeDz8H916pyvPQXFS5ltLq8PmDSlDFSwpmuvjRZvcGJVQCRYJ9QAqN4pjSkBwmRJtZ1zxeYYLfQU88%2FhHpCuqKY%2Fo5PlZqitF%2Fi5tN9wcjv%2BOaUu0e3H8K8qknd2hQhfYZ3mcE319ttggfYVT4PxT6jQv2hH%2BtWO%2BQVZ32moiyr2q2dfvqndSZ%2BcslmaJMEEGPfVbkcqz6OuRXKg9c6wHw%2BzGjJ0qF8lctSoDbLhT5IOZWG%2FmNF%2BMVvU5Wgorfa2swiBav99Hgn3vrf74u8mLsY5T0vx4NEyG%2BNVyPbgKqGOHsQAejW06Vq4ik5UIzPpsQ5HV4XPKK%2Fqymlem1XN6PxFHQeaf3vs0y8kVwp0rvnrfnnQ4LSrKfZc%2FNdpLU%2FXHGx%2BkUvIAHtVRX4a%2BC3sP8xSXKWTctA458XV7b5O7K5sXlyS36%2F3xnTrjC0NcJv80e3dYPRDhg3knQfYgDe2geRGQuU2COYu%2FKksZiGgBXhSAln%2Bud9LWeLVphgzjSipja81DInKiCBNOHOlulkaoOh0WcdIOFQeAQ2q6v4DeoIE1D8tTL5JNgWDLUk8sxQGq9zspYADXCEhc9Ke4hgL%2FuFvRA6Q12rCsxcWroPVYAf2el010OB%2BHSiqaCHw6xiykfcfjw7oO7bINDuWwMASTZbTgTVETF0hx1VpaYUnUbObMOCH4K8GOrEBiMpaUIWZ9pgzWe8VL74b8Keg8P4qJal5QND0KgcUzoVtZv4JMAmxEuey5Xggo3VjcjrCYsQ3sGOrJ9OJ570LbYowhBvMl7GSojd2kqdTrDSXd400eFg4uwE4Vb35B7htjgxzcxpZJeKmMPHzoEJdnMzI61T%2Fkl8%2FoIh3I9dws%2BUKd1pmrot0rGKx7EM68SBELBX2rcQ1SfmvbLKJAKBimMCFZrVhl4BAeMfpNyKSnkQM&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240318T093805Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYRLOY57VC%2F20240318%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=0e72e500b607b8e5538d7a3d24ad28c78f5bc82c4b10c8f43305eb800f767c1b&amp;hash=4ee078d9d743db9b7aed88f3efc12b11dbe7e497747d269063317fe1b35fad70&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0950705122002726&amp;tid=spdf-1cc819cf-9b45-4e50-834c-160b0fdd0a1e&amp;sid=cc5998cf8485c74f0b497fc9e83d21092089gxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=05125c535357540259&amp;rr=86643f6fd90f3079&amp;cc=kr">TimeCLR</a> : phase-shift and amplitude change augmentation based on DTW</li>
    </ul>
  </li>
  <li>Soft contrastive learning
    <ul>
      <li><a href="https://arxiv.org/pdf/2104.14548.pdf">NNCLR</a> : 각 view마다 feature space에서 k-neighbors를 찾아서 추가적인 positive pair를 고려</li>
      <li>non-TS 도메인에서는 soft assignment를 계산할 때 embedding space에서 했지만, TS 도메인에서는 data space에서 계산한다.</li>
    </ul>
  </li>
  <li>Masked modeling in TS
    <ul>
      <li><a href="https://arxiv.org/pdf/2010.02803.pdf">TST</a> : masked modeling paradigm을 TS에 도입</li>
      <li><a href="https://arxiv.org/pdf/2211.14730.pdf">PatchTST</a> :  masked subseries-level patche를 예측해서 local semantic information을 효율적으로 계산</li>
      <li><a href="https://arxiv.org/pdf/2302.00861.pdf">SimMTM</a> : 여러 개의 masked TS로부터 reconstruction</li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>
<ul>
  <li>instance-wise contrastive loss : inter-sample relationship 학습. Distance btw TS on data space</li>
  <li>temporal contrastive loss : intra-temporal relationship 학습. 하나의 TS에서 서로 다른 timestamps의 차이
<img src="/assets/img/timeseries/softclt/fig1.png" alt="사진2" /></li>
</ul>

<h3 id="31-problem-definition">3.1. Problem Definition</h3>
<ul>
  <li>하나의 batch에는 N개의 TS : \(\mathcal{X}=\left\{x_1, \ldots, x_N\right\}\)가 있고
    <ul>
      <li>\(f_\theta = x_i \in \mathbb{R}^{T \times D} \to r_i=\left[r_{i, 1}, \ldots, r_{i, T}\right]^{\top} \in \mathbb{R}^{T \times M}\)를 학습</li>
      <li>\(D\)는 input feature dim, \(M\)은 embedded feature dim</li>
      <li>
        <h3 id="32-soft-instance-wise-contrastive-learning">3.2. Soft Instance-wise Contrastive learning</h3>
      </li>
    </ul>
  </li>
  <li>Vision에서는 pixel-by-pixel distance가 similarity와 관련이 없기 때문에 embedding space에서 similar instance를 학습하지만, TS에서는 data space에서의 거리가 similarity가 된다.</li>
  <li>soft assignment for a pair of data indice \((i, i')\) : \(w_I\left(i, i^{\prime}\right)=2 \alpha \cdot \sigma\left(-\tau_I \cdot D\left(x_i, x_{i^{\prime}}\right)\right)\)
    <ul>
      <li>\(D(\cdot, \cdot)\) : min-max normalized distance metric</li>
      <li>\(\tau_I\) : hyperparameter controlling the sharpness</li>
      <li>\(\alpha\) : the upper bound in the range of [0, 1]. 완전 똑같은 TS의 assignment</li>
      <li>augmented view끼리가 아니라 original TS끼리 계산하는 것</li>
    </ul>
  </li>
  <li>Contrasive loss는 cross-entropy loss로 해석될 수 있으므로(<a href="https://arxiv.org/pdf/2010.08887.pdf">i-Mix</a>), softmax probability of the relative similarity는 \(p_I\left(\left(i, i^{\prime}\right), t\right)=\frac{\exp \left(r_{i, t} \circ r_{i^{\prime}, t}\right)}{\sum_{j=1, j \neq i}^{2 N} \exp \left(r_{i, t} \circ r_{j, t}\right)}\)</li>
  <li>Soft instance-wise contrastive loss for \(x_i\), at \(t\)는 \(\ell_I^{(i, t)}=-\log p_I((i, i+N), t)-\sum_{j=1, j \neq\{i, i+N\}}^{2 N} w_I(i, j \bmod N) \cdot \log p_I((i, j), t)\)
    <ul>
      <li>첫째 term은 positive pair에 대한 loss, 둘째 term은 나머지에 대한 loss인데 \(w_I\left(i, i^{\prime}\right)\)로 weighted.</li>
      <li>\(\forall w_I\left(i, i^{\prime}\right)=0\)이면 hard instance-wise contrastive loss이므로 일반화 버전이다.</li>
      <li>
        <h3 id="33-soft-temporal-contrastive-learning">3.3. Soft Temporal Contrastive learning</h3>
      </li>
    </ul>
  </li>
  <li>soft assignment for a pair of timestamps \((t, t')\) : \(w_T\left(t, t^{\prime}\right)=2 \cdot \sigma\left(-\tau_T \cdot\mid t-t^{\prime}\mid\right)\)
    <ul>
      <li>\(\tau_I\) : hyperparameter controlling the sharpness</li>
    </ul>
  </li>
  <li>: 인접한 timestamp의 values는 비슷할 것이라는 직관</li>
  <li><strong>Hierarchical loss</strong> : <a href="https://arxiv.org/pdf/2106.10466.pdf">TS2Vec</a>의 방식처럼 maxpooling을 해서 loss 계산
<img src="/assets/img/timeseries/softclt/fig2.png" alt="사진3" /></li>
  <li>Softmax probability of the relative similarity는 \(p_T\left(i,\left(t, t^{\prime}\right)\right)=\frac{\exp \left(r_{i, t} \circ r_{i, t^{\prime}}\right)}{\sum_{s=1, s \neq t}^{2 T} \exp \left(r_{i, t} \circ r_{i, s}\right)}\),</li>
  <li>Soft temporal contrastive loss for \(x_i\) at \(t\)는 \(\ell_T^{(i, t)}=-\log p_T(i,(t, t+T))-\sum_{s=1, s \neq\{t, t+T\}}^{2 T} w_T(t, s \bmod T) \cdot \log p_T(i,(t, s))\)
    <ul>
      <li>마찬가지로 \(\forall w_T\left(t, t^{\prime}\right)=0\)이면 hard temporal contrastive loss이므로 일반화 버전이다.</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 SoftCLT의 Final loss : \(\mathcal{L}=\frac{1}{4 N T} \sum_{i=1}^{2 N} \sum_{t=1}^{2 T}\left(\lambda \cdot \ell_I^{(i, t)}+(1-\lambda) \cdot \ell_T^{(i, t)}\right)\)
    <ul>
      <li>\(\lambda\) : hyperparameter controlling the contribution of each loss</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<ul>
  <li>(1) Classification with UTS, MTS (2) Semi-supervised classification (3) Transfer learning in in-domain and cross-domain (4) Anomaly detection
    <h3 id="41-classification">4.1. Classification</h3>
    <p><img src="/assets/img/timeseries/softclt/fig23.png" alt="사진4" /></p>
  </li>
</ul>

<h3 id="42-semi-supervised-classification">4.2. Semi-supervised classification</h3>
<p><img src="/assets/img/timeseries/softclt/table3.png" alt="사진5" /></p>

<h3 id="43-transfer-learning">4.3. Transfer learning</h3>
<p><img src="/assets/img/timeseries/softclt/table4.png" alt="사진6" /></p>

<h3 id="44-anomaly-detection">4.4. Anomaly detection</h3>
<p><img src="/assets/img/timeseries/softclt/table5.png" alt="사진7" /></p>

<h3 id="45-ablation-study">4.5. Ablation study</h3>
<p><img src="/assets/img/timeseries/softclt/table6.png" alt="사진8" /></p>

<ul>
  <li>(a) : soft assignment를 instance-wise와 temporal에 모두 적용했을 때 성능이 가장 좋다.</li>
  <li>(b) : \(W_T\)를 계산하는 방법들에 따른 비교. sigmoid를 사용하는 근거가 된다.</li>
  <li>(c) : \(\alpha=0.5\) 정도로 해서 같은 TS의 similarity of the pairs를 적절히 크게 할 때 성능이 좋다.</li>
  <li>(d) : Distance function에 따른 성능 비교. DTW와 TAM의 성능이 같지만 더 일반적인 DTW 사용했다.
    <h3 id="46-analysis">4.6. Analysis</h3>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Comparison with soft CL methods in computer vision</code> : 앞서 언급했듯이 embedding space에서 similarity를 계산하는 vision domain과 다르게, TS는 data space에서 계산하면 성능이 더 좋다.</li>
  <li><code class="language-plaintext highlighter-rouge">Robustness to seasonality</code> : Seasonality in TS는 extract하기 어렵지도 않고 고려 안해도 성능이 좋아서 직접적으로 고려하지 않았다.</li>
  <li><code class="language-plaintext highlighter-rouge">Instance-wise relationships</code> : layer가 깊어짐에 따라 SoftCLT가 Hard CL보다 TS instance 사이의 관계를 잘 보존한다.</li>
  <li><code class="language-plaintext highlighter-rouge">Temporal relationships</code> : 시간(training epoch)에 따라서도 t-SNE를 비교했을 때, Hard CL은 진한 색(large tarining epoch)을 잘 구분하지 못하는데, large training epoch에는 fine-grained relationship이 학습된다. 즉 Hard CL은 coarse-grained relationship은 잘 학습하지만 SoftCLT는 fine-grained relationship도 잘 학습한다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>SoftCLT : soft assignments based on the instance-wise and temporal relationships on the data space</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/abs/2312.16424)]]></summary></entry></feed>