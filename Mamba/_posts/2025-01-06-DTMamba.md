---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2405.07022)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# DTMamba : Dual Twin Mamba for Time Series Forecasting (Arxiv 2024)

## Introduction

- RNN-based methods
  - vanishing gradients
  - cannot be parallelized effectively,
  - leading tolower computational efficiency
- TCN-based methods
  - limited modeling capabilities for long-term dependencies.
- Transformers
  - quadratic complexity
- Simple linear models
  - short in terms of performance.

- Mamba
  - inear timeseries modeling approach using State Space Models
    - RNN’s sequential processing capability
    - CNN’s global information processing capability
  - selection mechanism within the SSM framework
    - focus on essentialinformation while filtering out irrelevant details
  - incorporates a summary of all preceding information

- Dual Twin Mamba(DTMamba)
  - RevIN - two TMamba blocks(Residual networks) - projection layer - reverse RevIN 

## 2. Related Work

- Traditional methods
  - poor modeling performance
- RNN-based models
  - vanishing gradients, limited parallelization capabilities,
- Transformer-based methods
  - self-attention : suitable for modelinglong-term time series data
  - quadratic complexity
- Linear-based methods
  - solely rely on pastobserved temporal patterns
- TCN-based methods
  - larger receptivefied
  - but limited modeling capabilities forlong-term dependencies

## 3. Proposed Methods

### 3.1 Problem Statement

-  $$X=\left\{x_1, \ldots, x_T\right\} \in \mathbb{R}^{T \times N}$$를 보고 $$\hat{X}=\left\{\hat{x}_{T+1}, \ldots, \hat{x}_{T+S}\right\} \in \mathbb{R}^{S \times N}$$ 예측

### 3.2 Normalization

-  $$X \in \mathbb{R}^{T \times N}$$  into $$X^0=\left\{x_1^0, \ldots, x_T^0\right\} \in \mathbb{R}^{T \times N}$$, via $$X^0=\operatorname{RevIN}(X)$$.

### 3.3 Channel Independence & Reversed Channel Independence 

- model overfitting 때문에 함
- $$\text{Batch}\left(X^0\right)$$:  (Batch_Size, Lookback length, Dimension) $$= (B, T, N)$$ 를 reshape
  - Batch $$\left(X^{\boldsymbol{I}}\right)= \text{ChannelIndepence}\left(\operatorname{Batch}\left(X^0\right)\right) : (B \times N, 1, T)$$
- 다시 되돌릴 때에는 $\operatorname{Batch}\left(X^P\right):(B \times N, 1, S)$를 reshape
  - Batch $$(\hat{\boldsymbol{X}})=\text{ ChannelIndepence}^{-1}\left(\operatorname{Batch}\left(X^P\right)\right) : (B, S, N)$$

### 3.4 Twin Mamba

![그림1](/assets/img/Mamba/DTMamba/fig1.png)

![그림1](/assets/img/Mamba/DTMamba/fig2.png)

- Embedding layer - residual
- Dropout layer - twin Mambas

### 3.4.1. Embedding Layers

- linear layer as the Embedding layer $$\to$$ global feature representation
- $$X^E = \text{Embedding}(X^I):(B \times N, 1, ni)$$.
- Embedding layer는 TMamba Block 안에 있는데
  - TMamba Block이 두 개니까 각각의 Embedding layer를 Embedding 1,2라 함

### 3.4.2. Residual

- proposed by ResNet
- to prevent overfitting, stable training

### 3.4.3. Dropout

- To prevent overfitting
- $$X^E$$ into $$X^D:(B \times N, 1, n i)$$ (dimension은 그대로)

### 3.4.4. Mamba

- S4 learns how to map an input $$x(t)$$ to an output $$y(t)$$ through an intermediate state $$h(t)$$
- TMamba Block에서는 2개의 Mamba 사용됨 (multi-level feature learning)
  - low-level temporal features & high-level temporal patterns

### 3.5. Projection Layer

![그림1](/assets/img/Mamba/DTMamba/algorithm1.png)

- 지금까지 2개의 TMamba Block와 residuals $$R^1$$, $$R^2$$ 더해서 hidden information을 얻음
- Output의 shape에 맞게 FC layer 태운 뒤
  - 앞서 언급한 것처럼 $$\text{ ChannelIndepence}^{-1}$$와 $$\text{RevIN}^{-1}$$을 수행

## 4. Experiment

### 4.1. Settings

pass

### 4.2. Long-term Forecasting

![그림1](/assets/img/Mamba/DTMamba/table1.png)

![그림1](/assets/img/Mamba/DTMamba/table2-1.png)
![그림1](/assets/img/Mamba/DTMamba/table2-2.png)

![그림1](/assets/img/Mamba/DTMamba/table3.png)

### 4.3. Hyperparameter Sensitivity Analysis and Ablation Study

![그림1](/assets/img/Mamba/DTMamba/table4.png)

![그림1](/assets/img/Mamba/DTMamba/table5.png)



