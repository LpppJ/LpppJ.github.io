---
layout: post
related_posts:
  _
title: 
description: >
  [ICLR 2025](https://arxiv.org/pdf/2405.16440)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting (ICLR 2025)

## Abstract

- Transformers : quadratic complexity and permutation invariant bias
- Mamba의 한계 4가지를 개선한 **MambaTS** 제시
  - variable scan : 모든 변수의 과거 정보를 함께 arrange
  - causal convolution (in Mamba block) 필요없음 $$\to$$ Temporal Mamba Block(TMB)
  - dropout mechanism (for selective parameters of TMB)
  - variable permutation training : variable scan order sensitivity 문제 해결
- 추가적으로 variable-aware scan
  - 훈련 과정에서 변수 관계를 동적으로 발견하고,
  - 추론 시 모든 노드를 방문하는 최단 경로 문제를 해결하여 최적의 변수 스캔 순서를 디코딩

## 1. Introduction

- Transformer의 문제는
  - quadratic complexity
  - look-back window가 커진다고 성능이 향상되는 것이 아님
  - permutation invariant bias의 effectiveness 의문
- Mamba : SSM + selection mechanism + hardware-aware design
  - Transformer based methods(PatchTST 및 iTransformer)에서
  - Transformer 블록을 Mamba 블록으로 교체했더니
  - 학습 속도가 1.3배 더 빠르고, GPU 메모리 사용량이 각각 5.3배 및 7.0배 감소
  - **하지만 Mamba가 Transformer보다 성능적 우위를 보이지는 못함**

![그림1](/assets/img/timeseries/MambaTS/fig1.png)

- 그러므로 Abstract에서 소개한 MambaTS를 제시

## 2. Related Work

### Long-term time series forecasting

- variable-mixing : dependencies across time and variables
  - RNN, TCN, Transformer, MLP...
  - 현재는 patching으로 quadratic complexity 개선하고자 함
- Variable-independent : the assumption of variable independence 
  - 문제를 과도하게 단순화해서 부적절할 수도 있긴 함

### State Space Models

- Mamba : SSM + selection mechanism + hardware-aware design
  - Mamba의 scan order sensitivity를 다루기 위해 Vison 도메인에서는 
  - **bidirectional scanning**(Vision Mamba), **multi-directional scanning**(VMamba, Mamba-nd), **automatic direction scanning**(Local Mamba)
  - 하지만 temporal problem에는 활발한 연구 X
- 본 논문에서 제시하는 MambaTS는
- VAST(Variable-Aware Scan along Time)으로 표현력 강화

## 3. Preliminaries

pass

## 4. Model Architecture

![그림1](/assets/img/timeseries/MambaTS/fig2.png)

- $$\left(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_L \right)$$, where $$\mathbf{x}_i \in \mathbb{R}^K$$를 보고 $$\left(\mathbf{x}_{L+1}, \cdots, \mathbf{x}_{L+2}, \cdots, \mathbf{x}_{L+T}\right)$$ 예측하고자 함

### 4.1. Overall Architecture

- **Patching and Tokenization**
  - PatchTST처럼 각 변수를 $$M=L / s$$개의 patches로 나누어 각 patch를 $$D$$차원으로 mapping
- **Variable Scan along Time**
  - $$K$$ 개의 변수를 임베딩함으로써 $$K \times M$$ 개의 tokens 얻음
  - Variable Scan Along Time(VST) : 첫 시점에서 모든 변수의 토큰 $$\to$$ 두 번째 시점에서 모든 변수의 토큰 $$\to$$ ... 마지막 시점에서 모든 변수의 토근 순서로 정렬
  - 그림으로 표현하면 아래와 같음

![그림1](/assets/img/timeseries/MambaTS/fig2-1.png)

- **Encoder**
  - $$N$$개의 Temporal Mamba Block(TMB)로 구성됨
  - 각 TMB는 2개의 branches
    - 하나는 sequence modeling에 집중, 하나는 비선형성에 집중
    - 식으로 표현하면 $$h_t=\operatorname{SSM}\left(\operatorname{Dropout}\left(\operatorname{Linear}\left(x_t\right)\right)\right)+\sigma\left(\right. Linear \left.\left(x_t\right)\right)$$이고
    - 그림으로 표현하면 아래와 같음
    - 왼쪽 Mamba block에서 Conv 없애고 dropout 추가함

![그림1](/assets/img/timeseries/MambaTS/fig2-2.png)

- **Prediction Head**
  - Encoder가 global dependencies를 잡으니까 channel-independent decoding
- **Instance Normalization**
  - RevIN 사용
- **Loss Function**
  - MSE 사용 : $$\mathcal{L}=\mathbb{E}_{\mathbf{x}} \frac{1}{M} \sum_{i=1}^M\left\|\hat{\mathbf{x}}_{L+1: L+T}^{(i)}-\mathbf{x}_{L+1: L+T}^{(i)}\right\|_2^2$$

### 4.2.  Variable Permutation Training

- channel orders의 영향을 줄이고 local context interactions을 늘리기 위해
- variable permutation training (VPT) strategy를 사용
  - $$K \times M$$개의 tokens가 있으면 each time step에서 $$K$$개를 섞고 디코딩 후에 복원

## 4.3. Variable-Aware Scan along Time

- 최적의 **변수 스캔 순서**를 결정하려면 변수간 관계를 알아야 하고
- 이를 위해 Variable-Aware Scan along Time (VAST) 제안

- **Training**
  - 변수 $$K$$개, directed graph adjacency matrix $$P \in \mathbb{R}^{K \times K}$$를 만듬
    - $$p_{i, j}$$ : cost from node $$i$$ to node $$j$$
    - 다양한 variable scan order 탐색해서 Loss 계산 할 것
  - 이제 node index sequence $$V=\left\{v_1, v_2, \ldots, v_K\right\}$$가 나옴
    - $$v_k$$는 shuffled sequnce의 새로운 index
  - $$K-1$$ 개의 transition tuples $$\left\{\left(v_1, v_2\right),\left(v_2, v_3\right), \ldots,\left(v_{K-1}, v_K\right)\right\}$$이 도출됨
  - 각 sample마다 network의 $$t$$-th iteration동안 training loss $$l^{(t)}$$ 계산되고
    - **directed graph adjacency matrix** $$P \in \mathbb{R}^{K \times K}$$를 update (with exponential moving average)
    - $$p_{v_k, v_{k+1}}^{(t)}=\beta p_{v_k, v_{k+1}}^{(t-1)}+(1-\beta) l^{(t)}$$.
    - $$\beta$$는 rate of the moving average (hyperparameter)
  - 샘플 배치 간의 영향을 제거하기 위해 위 식을 batch 버전으로 확장
    - 즉 $$\bar{l}^{(t)}=l^{(t)}-\mu\left(l^{(t)}\right)$$를 사용하여 centralization ($$\mu$$는 mean function)
    - $$p_{v_k, v_{k+1}}^{(t)}=\beta p_{v_k, v_{k+1}}^{(t-1)}+(1-\beta) \bar{l}^{(t)}$$이 됨
- **Inference**
  - Training동안 directed graph adjacency matrix $$P$$ 는 optimal variable scan order 결정하는 데에 사용
  - 즉 Asymmetric Traveling Salesman Problem, ATSP 문제가 되고
    -  heuristic-based simulated annealing algorithm 사용해서 경로 디코딩

## 5. Experiments

![그림1](/assets/img/timeseries/MambaTS/table2.png)

### 5.1. Main results

![그림1](/assets/img/timeseries/MambaTS/table3.png)

### 5.2.  Ablation studies and analyses

- Component Ablation

![그림1](/assets/img/timeseries/MambaTS/table4.png)

- Dropout Ablation

![그림1](/assets/img/timeseries/MambaTS/fig3.png)

- VAST Ablation

![그림1](/assets/img/timeseries/MambaTS/table5.png)

### 5.3 Model Analysis

- Increasing Lookback Window

![그림1](/assets/img/timeseries/MambaTS/fig5.png)

- Efficiency Analys

![그림1](/assets/img/timeseries/MambaTS/table6.png)

## 6. Conclusion

- Variable Scan along Time (VST)
  - to organize the historical information of all variables,
  - forming a global retrospective sequence
- Temporal Mamba Block (TMB)
  - causal convolution in Mamba 제거
  - dropout regularization 추가
- Variable Permutation Training (VPT)
  - local context interaction 능력 향상
- Variable-Aware Scan along Time (VAST)
  - 훈련 중 변수 간 관계를 동적으로 발견
  - ATSP solver로 the optimal variable scann order 결정



