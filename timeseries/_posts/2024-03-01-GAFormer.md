---
layout: post
related_posts:
  _
title: 
description: >
  [ICLR 2024](https://openreview.net/forum?id=c56TWtYp0W)
sitemap: true
hide_last_modified: false
---

# (GAFormer) Enhancing Timeseries Transformers Through Group-Aware Embeddings

## Abstract
- Multivariate TS의 복잡한 inter-channel relationship과 dynamic shifts로 인해 Robust and generalizable representation을 학습하기 어렵다.
- 본 논문에서 제시하는 GAFormer는 set of group tokens를 학습하고 instance-specific group embedding layer를 만든다.

## 1. Introduction
- Multivariate TS의 temporal dynamics(temporal structure) of each channel, 그리고 relationship across channels(channel-wise structure)는 TS의 representation을 만드는 요소
- TS는 **no predetermined ordering**, 그리고 **instance-specific relationships across channels and time**으로 인해 position embedding을 그대로 사용하기에 적절하지 않다.
- 본 논문에서 제시하는 GAFormer는 channel structure와 temporal structure를 통합하여 token에 'group embedding'한다.

## 2. Method
- Instance-specific group embeddings : grouping across different tokens, either channel-wise (spatially) or time-wise (temporally)
### 2.1. Group Embeddings
- Sequence of tokens : $$X=\left[\mathbf{x}_1, \ldots, \mathbf{x}_N\right] \in \mathbb{R}^{N \times D}$$
  - $$N$$ : the total # of tokens in a seq
  - $$D$$ : token dim
- Linear weight matrix : $$W \in \mathbb{R}^{D \times K}$$ project each token down to a space of $$K$$ dim
  - `operation` $$\operatorname{Encoder}(X) W \in \mathbb{R}^{N \times K}$$
- 그 다음 softmax : sparsify the coefficients that assign group tokens to input tokens (group awareness)
  - `operation` $$\mathbb{S}(\operatorname{Encoder}(X) W)$$
    - where $$\mathbb{S}$$ represents the softmax (along $$(D)$$ dim)
- 각 tokens를 K차원으로 줄였기 때문에,  $$\mathbf{G} \in \mathbb{R}^{N \times D}$$을 곱해줄 수 있다.
  - `operation` $$\operatorname{GE}(X)=\mathbb{S}(\operatorname{Encoder}(X) W) \cdot G$$
- 이제 $$ X $$에 더해준다
  - `operation` $$X \leftarrow X+\operatorname{GE}(X)$$

### 2.2. GAFormer: A Group-Aware SpatioTemporal Transformer
- 