---
layout: post
related_posts:
  _
title: 
description: >
  [ICLR 2024](https://openreview.net/forum?id=c56TWtYp0W)
sitemap: true
hide_last_modified: false
---

# (GAFormer) Enhancing Timeseries Transformers Through Group-Aware Embeddings

## Abstract
- Multivariate TS의 복잡한 inter-channel relationship과 dynamic shifts로 인해 Robust and generalizable representation을 학습하기 어렵다.
- 본 논문에서 제시하는 GAFormer는 set of group tokens를 학습하고 instance-specific group embedding layer를 만든다.

## 1. Introduction
- Multivariate TS의 temporal dynamics(temporal structure) of each channel, 그리고 relationship across channels(channel-wise structure)는 TS의 representation을 만드는 요소
- TS는 **no predetermined ordering**, 그리고 **instance-specific relationships across channels and time**으로 인해 position embedding을 그대로 사용하기에 적절하지 않다.
- 본 논문에서 제시하는 GAFormer는 channel structure와 temporal structure를 통합하여 token에 'group embedding'한다.

## 2. Method
- Instance-specific group embeddings : grouping across different tokens, either channel-wise (spatially) or time-wise (temporally)
### 2.1. Group Embeddings
- Sequence of tokens : $$X=\left[\mathbf{x}_1, \ldots, \mathbf{x}_N\right] \in \mathbb{R}^{N \times D}$$ (시계열의 채널, 길이와 다름)
  - $$N$$ : the total # of tokens in a seq
  - $$D$$ : token dim
- Linear weight matrix : $$W \in \mathbb{R}^{D \times K}$$ project each token down to a space of $$K$$ dim
  - `operation` $$\operatorname{Encoder}(X) W \in \mathbb{R}^{N \times K}$$
- 그 다음 softmax : sparsify the coefficients that assign group tokens to input tokens (group awareness)
  - `operation` $$\mathbb{S}(\operatorname{Encoder}(X) W)$$
    - where $$\mathbb{S}$$ represents the softmax (along $$(D)$$ dim)
- 각 tokens를 K차원으로 줄였기 때문에,  $$\mathbf{G} \in \mathbb{R}^{N \times D}$$을 곱해줄 수 있다.
  - `operation` $$\operatorname{GE}(X)=\mathbb{S}(\operatorname{Encoder}(X) W) \cdot G$$
- 이제 $$ X $$에 더해준다
  - `operation` $$X \leftarrow X+\operatorname{GE}(X)$$

### 2.2. GAFormer: A Group-Aware SpatioTemporal Transformer
- **Tokenization Layer** : 시계열 $$X \in \mathbb{R}^{C \times T}$$를 P개의 패치로 잘라서 tensor $$X \in \mathbb{R}^{C \times P \times L}$$를 만든다. 그리고 `Token`이라는 encoder를 통과시키면 $$Z=\operatorname{Token}(X) \in \mathbb{R}^{C \times P \times D}$$가 된다. 이 때 channel-wise separation이 유지되고 각 channel에서의 temporal semantics는 알아낼 수 있다.
- **Spatial (Channel-Wise) Group Awareness** : `Trans-S`(Transformer encoder)와 `SGE`(spatial group embedding)으로 channel-wise group embedding을 학습한다. \
  ![사진2](/assets/img/timeseries/GAFormer/fig2.jpeg)
  ($$Z^{\prime}=\operatorname{Trans}-\mathrm{S}\left(Z_S+\operatorname{SGE}\left(Z_S\right)\right)$$) Group embedding을 하지 않고 fixed positional embedding을 하면 두 시계열에 같은 사건이 발생했는데도 발생 시점이 다르다는 이유로 두 시계열의 structure를 다르게 학습하는데, group embedding을 하면 사건이 발생한 인접 시점들에 대해 embedding을 하기 때문에 같은 structure로 학습할 수 있다.
  ![사진1](/assets/img/timeseries/GAFormer/fig1.jpeg)
- **Temporal Group Awareness** : 먼저 dimension reduction layer $$H$$를 통과시켜 $$D$$차원 token을 $$D'$$차원 token으로 압축할 수 있다. 그리고 Spatial Group Awareness layer와 유사하게 학습한다. \
  ($$Z^{\text {final}}=\operatorname{Trans}-\mathrm{T}\left(Z_T+\operatorname{TGE}\left(Z_T\right)\right)$$) 이렇게 학습된 $$Z^{\text {final}}$$은 linear classifier로 들어간다.

## 3. Results
### 3.1. An Intuitive example : Noisy many-body systems
- 본 논문에서는 multivariate TS를 생성하기 위해 상호 작용하는 입자들로 구성된 many-body system의 궤적을 기반으로 한 실험 결과를 제시한다.
- 시스템의 총 에너지를 분류하여, 고에너지 시스템인지 저에너지 시스템인지를 결정하는 것인데, 실험의 복잡성을 증가시키기 위해 상호 작용하지 않는 무관한 body들이 시스템을 오염시키는 상황을 가정했다.
- 3가지의 embedding 방식을 실험했다.
  - 1) learnable positional embedding
  - 2) parameter-free sin-cos positional embedding
  - 3) Group embedding
- 3가지의 setting을 실험했다.
  - 1) Stable : the relative position of object(channels) never shifts
  - 2) Shuffle : the observed objects could be in any position and are randomized similarly
  - 3) Biased : the observed objects have different position that are randomly sampled from non-overapping set
- 실험 결과 Group embedding은 channel mismatch와 distribution shifts가 있을 때 다른 embedding보다 robust했다.
  ![사진3](/assets/img/timeseries/GAFormer/fig3.jpeg)
### 3.2. Time series Classification tasks
- table1, table2 : TGE(temporal dimension embedding)에 Group embedding을 했을 때 classification 성능이 향상된다.
  ![사진3](/assets/img/timeseries/GAFormer/table1.jpeg)
  ![사진4](/assets/img/timeseries/GAFormer/table2.jpeg)
### 3.3. Classification and Regression tasks on Neural Recordings
  ![사진5](/assets/img/timeseries/GAFormer/table3.jpeg)
### 3.4. Ablation Studies
- table4 :  temporal group embedding 또는 spatial group embedding 둘 중 하나만 적용하거나 둘 다 적용한 결과 둘 다 성능 향상에 필요하다.
  ![사진6](/assets/img/timeseries/GAFormer/table4.jpeg)

## 4. Related Work
## 5. Discussion
- 본 논문에서는 TS의 group-level structure를 다루는 새로운 프레임워크를 제안하였다.
- Group embedding은 group token across channel and temporal dimension을 학습해서 representation space로 보낸다.