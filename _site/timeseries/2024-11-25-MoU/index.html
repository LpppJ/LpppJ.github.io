<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v9.1.6 <https://hydejack.com/>
-->







<head>
  






  
    
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024) | LpppJ</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)" />
<meta name="author" content="GW Jeong" />
<meta property="og:locale" content="en" />
<meta name="description" content="Arxiv 2024" />
<meta property="og:description" content="Arxiv 2024" />
<link rel="canonical" href="http://localhost:4000/timeseries/2024-11-25-MoU/" />
<meta property="og:url" content="http://localhost:4000/timeseries/2024-11-25-MoU/" />
<meta property="og:site_name" content="LpppJ" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-11-25T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"GW Jeong"},"dateModified":"2024-11-25T17:10:30+09:00","datePublished":"2024-11-25T00:00:00+09:00","description":"Arxiv 2024","headline":"Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/timeseries/2024-11-25-MoU/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/me/logo.jpg"},"name":"GW Jeong"},"url":"http://localhost:4000/timeseries/2024-11-25-MoU/"}</script>
<!-- End Jekyll SEO tag -->


  

  



  <meta name="theme-color" content="rgb(230, 217, 195)">


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="LpppJ">
<meta name="apple-mobile-web-app-status-bar-style" content="default">

<meta name="application-name" content="LpppJ">

<meta name="generator" content="Hydejack v9.1.6" />


<link rel="alternate" href="http://localhost:4000/timeseries/2024-11-25-MoU/" hreflang="en">

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="LpppJ" />


<link rel="shortcut icon"    href="/assets/icons/favicon.ico">
<link rel="apple-touch-icon" href="/assets/icons/icon-192x192.png">

<link rel="manifest" href="/assets/site.webmanifest">

<link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com">



<link rel="preload" href="/assets/img/swipe.svg" as="image" id="_hrefSwipeSVG">






<script>!function(r,c){"use strict";function a(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}r.loadJS=function(e,t){var n=c.createElement("script"),e=(n.src=e,t&&a(n,"load",t,{once:!0}),c.scripts[0]);return e.parentNode.insertBefore(n,e),n},r._loaded=!1,r.loadJSDeferred=function(e,t){var n=c.createElement("script");function o(){r._loaded=!0,t&&a(n,"load",t,{once:!0});var e=c.scripts[0];e.parentNode.insertBefore(n,e)}return n.src=e,r._loaded?o():a(r,"load",o,{once:!0}),n},r.setRel=r.setRelStylesheet=function(e){a(c.getElementById(e),"load",function(){this.rel="stylesheet"},{once:!0})}}(window,document);
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
!function(w) {
  w._baseURL = '/';
  w._publicPath = '/assets/js/';
  w._noPushState = false;
  w._noDrawer = false;
  w._noNavbar = false;
  w._noToc = false;
  w._noSearch = false;
  w._search = {
    DATA_URL: '/assets/sitedata.json?no-cache',
    STORAGE_KEY: 'mini-search/',
    INDEX_KEY: 'index--2025-01-08T16:43:54+09:00',
  };
  w._clapButton = true;
}(window);</script>


<script async src="/assets/bower_components/MathJax/es5/tex-mml-chtml.js" id="_MathJax"></script>


<!--[if gt IE 8]><!---->

  




<link rel="stylesheet" href="/assets/css/hydejack-9.1.6.css" id="_stylePreload">
<link rel="stylesheet" href="/assets/icomoon/style.css" id="_iconsPreload">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap" id="_fontsPreload">



  <style id="_pageStyle">

html{--accent-color: rgb(94, 97, 94);--accent-color-faded: rgba(94, 97, 94, 0.5);--accent-color-highlight: rgba(94, 97, 94, 0.1);--accent-color-darkened: #4b4e4b;--theme-color: rgb(230, 217, 195)}
</style>


<!--<![endif]-->




</head>

<body class="no-break-layout">
  


<hy-push-state
  id="_pushState"
  replace-selector="#_main"
  link-selector="a[href]:not([href^='/assets/']):not(.external):not(.no-push-state)"
  script-selector="script"
  duration="500"
  hashchange
>
  
  
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <div class="nav-btn-bar">
      <a id="_menu" class="nav-btn no-hover" href="#_drawer--opened">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <div class="nav-span"></div>
    </div>
  </div>
</div>
<hr class="sr-only" hidden />

  <main
  id="_main"
  class="content layout-post"
  role="main"
>
  <nav id="breadcrumbs" class="screen-only"><ul>
  
  
    <li><a href="/">home</a></li>
    
      <li>
        
          <span>/</span>
          
          
          <a href="/timeseries/">timeseries</a>
        
      </li>
    
      <li>
        
          <span>/</span>
          <span>2024-11-25-MoU</span>
        
      </li>
    
  
</ul></nav>
  










<article id="post-timeseries-MoU" class="page post mb6" role="article">
  <header>
    <h1 class="post-title flip-project-title">
      
        Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)
      
    </h1>

    <div class="post-date">
      
      <span class="ellipsis mr1">
        <time datetime="2024-11-25T00:00:00+09:00">25 Nov 2024</time> in <span>Timeseries</span> 
      </span>
      
    </div>

    
    

    



  
    <p class="note-sm" >
      <a href="https://arxiv.org/pdf/2408.15997">Arxiv 2024</a>

    </p>
  


  </header>

  
    <h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer는 높은 계산 비용(quadratic computational cost)이 문제,</li>
  <li>Mamba는 longterm forecasting에서 성능이 effective하지 않음 (potential information loss 때문)</li>
  <li>본 논문에서 제시하는 <strong>Mixture of Universals (MoU)</strong>의 components:
    <ul>
      <li><strong>Mixture of Feature Extractors (MoF)</strong>: adaptive patch representations
        <ul>
          <li>(for <strong>short</strong>-term dependency)</li>
        </ul>
      </li>
      <li><strong>Mixture of Architectures (MoA)</strong>:  Mamba, FeedForward, Convolution, and Self-Attention 연결한 것
        <ul>
          <li>(for  <strong>long</strong>-term dependency )</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>PatchTST</strong>:  patch embedding에서 uniform linear transformations 사용 \(\to\) varying semantic contexts 손실
    <ul>
      <li>Vision에서 Dynamic Convolution, Conditional Convolution (for informative representations)등장했지만</li>
      <li>Time series에서는 poor performances</li>
    </ul>
  </li>
  <li><strong>Transformer</strong>: 장기 의존성은 잘 처리하지만 계산 비용이 큼</li>
  <li><strong>Mamba</strong>: 효율적이지만 정보 손실로 인해 장기 예측에서 성능이 떨어질 수 있음</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig2.png" alt="그림1" /></p>

<ul>
  <li><strong>MoF(Mixture of Feature Extractors)</strong>:
    <ul>
      <li>multiple sub-extractors로 구성되어 있고</li>
      <li>sparse activation을 사용해서 input patch에 적합한 sub-extractor만 활성화</li>
      <li>learning of diverse contexts and <strong>minimal parameter increase</strong> !</li>
    </ul>
  </li>
  <li><strong>MoA(Mixture of Architectures)</strong> Mamba에서 시작해 국소적인 관점에서 전역적인 Self-Attention 계층으로 확장하며 장기 의존성을 효율적으로 캡처하는 계층적 구조.
    <ul>
      <li>hierarchical structure를 가진 encoder</li>
      <li><strong>Mamba layer</strong> that selects and learns key dependencies using a Selective State-Space Model (SSM).</li>
      <li><strong>FeedForward transition layer</strong> and a <strong>Convolution-layer</strong> that broadens the receptive field to capture longer dependencies.</li>
      <li><strong>Self-Attention layer</strong> integrates information globally to fully capture long-term dependencies</li>
    </ul>
  </li>
</ul>

<h2 id="2-approach">2. Approach</h2>

<h3 id="21-problem-setting-and-model-structure">2.1 Problem Setting and Model Structure</h3>

<ul>
  <li>\(\mathbf{X}_{\text {input }}=\left[\mathbf{X}^1, \mathbf{X}^2, \ldots, \mathbf{X}^M\right] \in \mathbb{R}^{M \times L}\) where \(\mathbf{X}^i=\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_L\right] \in \mathbb{R}^L\)</li>
  <li>\(\hat{\mathbf{X}}_{\text {output }}=\left[\hat{\mathbf{X}}^1, \hat{\mathbf{X}}^2, \ldots, \hat{\mathbf{X}}^M\right] \in \mathbb{R}^{M \times T}\).</li>
  <li>
    <p>Goal : learning a function \(\mathcal{F}: \mathbf{X} \rightarrow \hat{\mathbf{X}}\)</p>
  </li>
  <li>Overall process는 아래와 같음.
    <ul>
      <li>처음에는 raw time series \(\mathbf{X} \in \mathbb{R}^L\)에서 시작 (variate independence setting이라서 channel =1)</li>
      <li>\(N\)개의 patch tokens를 만듬 :
        <ul>
          <li>\(\mathbf{X}_p=\operatorname{Patch}(\mathbf{X}) \in \mathbb{R}^{N \times P}\) with fixed size \(P\), stride \(S\)</li>
        </ul>
      </li>
      <li><strong>MoF</strong> module에 넣어서 adaptive representations를 얻음 :
        <ul>
          <li>\(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\).</li>
          <li>MoF는 parameters를 조절하면서 계산 비용 최적화 (<strong>2.2</strong>에서 자세히 설명)</li>
        </ul>
      </li>
      <li>이제 <strong>MoA</strong>에 넣어서 long-term dependencies (among tokens) 잡음
        <ul>
          <li>\(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoA}\left(\mathbf{X}_{\mathrm{rep}}\right)\in \mathbb{R}^{N \times D}\).</li>
          <li>MoA는 long-term encoder based on the Mixture of Architectures (<strong>2.3</strong>에서 자세히 설명)</li>
        </ul>
      </li>
      <li>마지막으로 linear projector에 넣어서 final prediction 얻음
        <ul>
          <li>\(\hat{\mathbf{X}}=\mathbf{P}\left(\operatorname{Flatten}\left(\mathbf{X}_{\mathrm{rep}^{\prime}}\right)\right)\in \mathbb{R}^{M \times T}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-mixture-of-feature-extractors">2.2 Mixture of Feature Extractors</h3>

<ul>
  <li>MoF의 목적은 <strong>patch의 representative embedding</strong>을 만드는 것</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig3.png" alt="그림1" /></p>

<ul>
  <li>Sub-extractors \(\left\{F_1, F_2, \ldots, F_c\right\}\)가 있고 각각은 independent linear mapping</li>
  <li>MoF를 통과하면 \(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\)를 얻음
    <ul>
      <li>where \(R_i(\cdot)\)는  input-relevant router (sub-extractor를 sparse하게 활성화)
        <ul>
          <li>즉 \(R\left(\mathbf{X}_p\right)_i=\operatorname{Softmax}\left(\operatorname{Top}_{\mathrm{k}}\left(H\left(\mathbf{X}_p\right)_i, k\right)\right)\)
            <ul>
              <li>\(\operatorname{Softmax}(\cdot)\)는  \(\operatorname{Topk}(\cdot, k)\)에 의해 선택된 상위 \(k\)개의 점수를 정규화</li>
              <li>\(H\left(X_p\right)=\left[H\left(\mathbf{X}_p\right)_1, H\left(\mathbf{X}_p\right)_2, \ldots H\left(\mathbf{X}_p\right)_c\right]\) 는 sub-extractors의 score vector</li>
              <li>여기서 \(H\left(\mathbf{X}_p\right)_i=\left(\mathbf{X}_p \cdot W_g\right)_i+\text { SN } \cdot \text { Softplus }\left(\left(\mathbf{X}_p \cdot W_{\text {noise }}\right)_i\right)\)
                <ul>
                  <li>where \(W_g\)는 linear layer이고 두번째 항은 load balancin을 위한 tunable noise를 넣는 것</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이렇게 하면 patch token를 \(c\)개의 서로 다른 patterns들의 조합으로 분할할 수 있음
    <ul>
      <li>each pattern은 최적의 sub-extractors에 의해 뽑힌 것이니</li>
      <li>most representative embedding (for patches with divergent context)라고 할 수 있음</li>
    </ul>
  </li>
</ul>

<h3 id="23-mixture-of-architectures">2.3. Mixture of Architectures</h3>

<ul>
  <li>MoA의 목적은 <strong>comprehensive long-term dependencies</strong>를 모델링하는 것</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig4.png" alt="그림1" /></p>

<ul>
  <li><strong>Mamba, FeedForward, Convolution</strong>, and <strong>Self-Attention layer</strong>로 구성되어
    <ul>
      <li>각각이 다른 관점에서 long-term dependencie를 학습</li>
      <li><strong>gradually expanding perspective</strong>를 통해 effectiveness and efficiency 둘 다 챙김</li>
    </ul>
  </li>
  <li><strong>Mamba-layer in Time Series</strong> : <strong>relevant data를 선택하고 time-variant dependencies를 학습하는 곳</strong>
    <ul>
      <li>input은 MoF의 output \(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\)인데 \(x\)로 denote</li>
      <li>\(\begin{gathered}
\boldsymbol{x}^{\prime}=\sigma(\text { Conv1D }(\text { Linear }(\boldsymbol{x}))) \\
\boldsymbol{z}=\sigma(\text { Linear }(\boldsymbol{x}))
\end{gathered}\), where \(\sigma\)는 \(SiLU\) activation</li>
      <li>다음으로 \(\begin{gathered}
\boldsymbol{y}^{\prime}=\operatorname{Linear}\left(\operatorname{SelectiveSSM}\left(\boldsymbol{x}^{\prime}\right) \otimes \boldsymbol{z}\right) \\
\boldsymbol{y}=\operatorname{LayerNorm}\left(\boldsymbol{y}^{\prime}+\boldsymbol{x}\right)
\end{gathered}\), where \(\begin{gathered}
\text { SelectiveSSM }\left(\boldsymbol{x}_t^{\prime}\right)=\boldsymbol{y}_t \\
\boldsymbol{y}_t=C \boldsymbol{h}_t, \quad \boldsymbol{h}_t=\bar{A} \boldsymbol{h}_{t-1}+\bar{B} \boldsymbol{x}_t^{\prime}
\end{gathered}\)
        <ul>
          <li>\(h_t\)는 latent state, \(y_t\)는 output representation</li>
          <li>The discrete matrices는 \(\begin{gathered}
B_t=S_B\left(\boldsymbol{x}_t^{\prime}\right), \quad C_t=S_C\left(\boldsymbol{x}_t^{\prime}\right) \\
\Delta_t=\operatorname{softplus}\left(S_{\Delta}\left(\boldsymbol{x}_t^{\prime}\right)\right)
\end{gathered}\)
            <ul>
              <li>\(S\)들은 linear layers이고, \(\begin{gathered}
f_A\left(\Delta_t, A\right)=\exp \left(\Delta_t A\right) \\
f_B\left(\Delta_t, A, B_t\right)=\left(\Delta_t A\right)^{-1}\left(\exp \left(\Delta_t A\right)-I\right) \cdot \Delta B_t \\
\bar{A}_t=f_A\left(\Delta_t, A\right), \quad \bar{B}_t=f_B\left(\Delta_t, A, B_t\right)
\end{gathered}\)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>좀 복잡한데 위에 있는 fig4 보는 것이 낫겠다.</li>
    </ul>
  </li>
  <li><strong>FeedForward-layer</strong> : <strong>non-linearity를 강화하는 곳</strong>
    <ul>
      <li>\(\boldsymbol{x}_{\mathrm{ffn}}=\text { FeedForward }\left(\boldsymbol{y}_t ; w_1, \sigma, w_2\right)\), where
        <ul>
          <li>\(w_1\) and \(w_2\) are parameters, \(\sigma\) is activation function</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Convolution-layer</strong> : <strong>MoA의 receptive field를 넓히는 곳</strong>
    <ul>
      <li>partial long-term dependencies를 담고 있는 tokens끼리의 정보 교환을 촉진</li>
      <li>\(\boldsymbol{x}_{\mathrm{conv}}= \operatorname{Conv}\left(\boldsymbol{x}_{\mathrm{ffn}} ; \mathbf{k}, s, p, c_{\mathrm{out}}\right)\),
        <ul>
          <li>where \(\mathbf{k}\) is the kernel size, \(s\) is the stride, \(p\) is the padding,</li>
          <li>and \(c_{\text {out }}\) is the number of output channels</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Self-Attention-layer</strong> : <strong>global perspective에서 포괄적인 long-term dependencies를 파악하는 곳</strong>
    <ul>
      <li>: \(\begin{aligned} &amp; x_{\mathrm{att}}=\operatorname{FeedForward}(\operatorname{Attention}(Q, K, V)) \\ &amp; \operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V \\ &amp; Q=x_{\text {conv }} W_Q, K=x_{\text {conv }} W_K, V=x_{\text {conv }} W_V\end{aligned}\)</li>
    </ul>
  </li>
  <li><strong>Partial-to-global Design for Time Series</strong>
    <ul>
      <li><strong>gradually expanding perspective</strong>라는 말의 의미는
        <ul>
          <li>Mamba layer : Selective SSM을 사용하여 시간적으로 변화하는 의존성을 처리</li>
          <li>FeedForward layer : 이러한 부분 의존성을 더 복잡한 표현으로 전환</li>
          <li>Convolution layer : 수용 영역을 확장하여 보다 넓은 시간적 관계를 학습</li>
          <li>Self-Attention layer : 로컬화된 정보를 통합하여 장기 의존성에 대한 포괄적인 이해</li>
        </ul>
      </li>
      <li>를 거치면서 선택적으로 일부 의존성에 초점을 맞춘 후, 이를 점차 확장하여 전체적(global) 관점으로 발전시킨다는 뜻</li>
    </ul>
  </li>
</ul>

<h3 id="24-computational-complexity-and-model-parameter">2.4. Computational Complexity and Model Parameter</h3>

<ul>
  <li>Tokens \(T\)개가 주어졌을 때 top-\(k\) experts를 선택한다고 하면
    <ul>
      <li>\(C_{\mathrm{MOU}}=\underbrace{k T \times d^2}_{\text {MoF }}+\underbrace{T \times d^2}_{\text {Mamba }}+\underbrace{T \times d^2}_{\text {FFN }}+\underbrace{k T d^2}_{\text {Conv }}+\underbrace{T^2 \times d+T \times d^2}_{\text {Transformer }}\),
        <ul>
          <li>where \(\mathbf{k}\) is the kernel size in the convolutional layer,</li>
          <li>\(d\) is the dimension of the vector representations</li>
        </ul>
      </li>
      <li>Transformer 블록을 제외하면 선형적인 복잡도</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig1.png" alt="그림1" /></p>

<h2 id="3-experiments">3. Experiments</h2>

<h3 id="31-dataset">3.1. Dataset</h3>

<ul>
  <li>7 commonly used datasets</li>
  <li>Pass</li>
</ul>

<h3 id="32-baselines-and-setup">3.2. Baselines and Setup</h3>

<ul>
  <li>Mamba-based Models (S-Mamba)</li>
  <li>Linear-based Models (D-Linear)</li>
  <li>Convolution-based Models (ModernTCN)</li>
  <li>Transformer-based Models (PatchTST)</li>
</ul>

<h3 id="33-main-results">3.3. Main Results</h3>

<p><img src="/assets/img/timeseries/MoU/table1.png" alt="그림1" /></p>

<h3 id="34-ablation-study">3.4. Ablation Study</h3>

<p><img src="/assets/img/timeseries/MoU/table2.png" alt="그림1" /></p>

<ul>
  <li>3개의 adaptive feature extractors를 비교했을 때 MoF(in MoU)가 가장 좋았으며
    <ul>
      <li>Dyconv가 parameters 수를 크게 증가시키기 때문에 time series patch와 같은 작은 데이터셋에는 적합하지 않음</li>
      <li>SE-M의  calibration strategy는 representation을 normalized gating vector에 곱하는 방식이라서 다양한 컨텍스트 정보를 처리하는 데에는 한계</li>
    </ul>
  </li>
  <li>특히 MoF가 uniform transformation method (Linear)보다 좋다는 점이 주목할만함</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/table3.png" alt="그림1" /></p>

<ul>
  <li>
    <p>AA, MM, MFA, AAA, MMA, AMM, MAM, AMA, AFM, AFCM</p>

    <ul>
      <li>여기서 A, M, F, C는 각각 Self-Attention, Mamba, FeedForward, Convolution</li>
      <li>글자의 순서는 레이어의 배치 순서</li>
    </ul>
  </li>
  <li>
    <p>M-A 순서(MAM, AMA, MMA) &gt; M-A 순서를 가지지 않은 모델(AMM)</p>

    <ul>
      <li>
        <p>Mamba를 Self-Attention 이전에 배치하는 것이 장기 의존성을 캡처하는 데 더 효과적</p>
      </li>
      <li>
        <p>A-M 순서보다 M-A 순서가 장기 의존성 학습에서 더 중요한 역할</p>
      </li>
    </ul>
  </li>
  <li>
    <p>F-C 순서 &gt; F</p>

    <ul>
      <li>Convolution 레이어가 Mamba 레이어의 수용 영역을 확장하여</li>
      <li>Mamba 레이어의 부분적 관점과 Self-Attention 레이어의 글로벌 관점을 연결하는 중간 관점을 제공한다고 해석됨</li>
    </ul>
  </li>
</ul>

<h3 id="35-model-analysis">3.5. Model Analysis</h3>

<ul>
  <li>Does MoF actually learn contexts within patches?</li>
  <li>What is learned by the layers of MoA?</li>
</ul>

<p><img src="/assets/img/timeseries/MoU/fig56.png" alt="그림1" /></p>

<h2 id="5-conclusion">5 Conclusion</h2>

<ul>
  <li>Mixture of Universals (MoU)
    <ul>
      <li>Mixture of Feature Extractors (MoF)
        <ul>
          <li>an adaptive method specifically designed to enhance time series patch representations for capturing short-term dependencies</li>
        </ul>
      </li>
      <li>Mixture of Architectures (MoA)
        <ul>
          <li>hierarchically integrates multiple architectures in a structured sequence to model long-term dependencies from a hybrid perspective</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

  
</article>



  <hr class="dingbat related mb6" />






  
     


  <aside class="about related mt4 mb4" role="complementary">
    
    

<div class="author mt4">
  

  
    


<img
  
    src="https://via.placeholder.com/128x128"
    srcset="/assets/img/me/me.JPG 1x,/assets/img/me/me.JPG 2x"
    
  
  alt="GW Jeong"
  class="avatar"
  
  width="120"
  height="120"
  loading="lazy"
/>

  

  
  
  <h2  class="page-title hr-bottom">
    About
  </h2>

  <p>Bachelor’s degree in Applied Statistics. Yonsei Univ. (2018~2024) <br />
DataScienceLab 8th 학회장 (2022~2023) <br />
Master’s degree in Statitstics. Yonsei Univ. (2024~)</p>


  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>

  </aside>


  

  
  

  
    


  

  
  

  
    

  


  
<footer class="content" role="contentinfo">
  <hr/>
  
    <p><small class="copyright">© Geonwoo Jeong.
</small></p>
  
  
    <p><small>Powered by <a class="external" href="https://hydejack.com/">Hydejack</a> v<span id="_version">9.1.6</span></small></p>
  <hr class="sr-only"/>
</footer>


</main>

  <hy-drawer
  id="_drawer"
  class=""
  side="left"
  threshold="10"
  noscroll
  
>
  <header id="_sidebar" class="sidebar" role="banner">
    




<div class="sidebar-bg sidebar-overlay" style="background-color:rgb(230, 217, 195);background-image:url(/assets/img/me/sidebar.jpg)"></div>

    <div class="sidebar-sticky">
  <div class="sidebar-about">
    
      <a class="no-hover" href="/" tabindex="-1">
        <img src="/assets/img/me/logo.jpg" class="avatar" alt="LpppJ" width="120" height="120" loading="lazy" />
      </a>
    
    <a class="sidebar-title" href="/"><h2 class="h1">LpppJ</h2></a>
    
    
      <p class="">
        DataScience and AI

      </p>
    
  </div>

  <nav class="sidebar-nav heading" role="navigation">
    <span class="sr-only">Navigation:</span>
<ul>
  
    
      
      <li>
        <a
          id="_drawer--opened"
          href="/timeseries/"
          class="sidebar-nav-item "
          
        >
          TimeSeries
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/pytorch/"
          class="sidebar-nav-item "
          
        >
          Pytorch
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/stat/"
          class="sidebar-nav-item "
          
        >
          Statistics
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/project/"
          class="sidebar-nav-item "
          
        >
          Project
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/presentation/"
          class="sidebar-nav-item "
          
        >
          Presentation
        </a>
      </li>
    
  
</ul>

  </nav>

  
  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>
  </header>
</hy-drawer>
<hr class="sr-only" hidden />

</hy-push-state>


  <!--[if gt IE 10]><!---->
  <script nomodule>!function(){var t,n=document.createElement("script");!("noModule"in n)&&"onbeforeload"in n&&(t=!1,document.addEventListener("beforeload",function(e){if(e.target===n)t=!0;else if(!e.target.hasAttribute("nomodule")||!t)return;e.preventDefault()},!0),n.type="module",n.src=".",document.head.appendChild(n),n.remove())}();
</script>
  <script src="/assets/js/hydejack-9.1.6.js" type="module"></script>
  <script src="/assets/js/LEGACY-hydejack-9.1.6.js" nomodule defer></script>
  

  

<!--<![endif]-->
  <!-- <script>
  document.querySelector('hy-push-state').setAttribute('prefetch', '');

  document.querySelectorAll('.sidebar a[href^="/"]').forEach(function (el) { 
    el.addEventListener('click', function (e) {
      if (el.pathname === window.location.pathname) {
        e.preventDefault();
        e.stopPropagation();
        document.querySelector('hy-drawer').close();
      }
    });
  });
</script> -->

<!--
Code for integrating CloudFlare's email protection with Hydejack's single page app loading.
-->
<script>
  document.getElementById('_pushState').addEventListener('hy-push-state-after', function (e) {
    function e(e){
      (console.error?console.error:console.log).call(console,e)
    }

    function t(e){
      return l.innerHTML='<a href="'+e.replace(/"/g,"&quot;")+'"></a>',l.childNodes[0].getAttribute("href")
    }

    function r(e,t){
      var r=e.substr(t,2);return parseInt(r,16)
    }

    function n(e,n){
      for(var o="",c=r(e,n),a=n+2;a<e.length;a+=2){
        var l=r(e,a)^c;
        o+=String.fromCharCode(l)
      }
      return t(o)
    }

    var o="/cdn-cgi/l/email-protection#",
        c=".__cf_email__",
        a="data-cfemail",
        l=document.createElement("div");

    !function(){
      for(var t=document.getElementsByTagName("a"),r=0;r<t.length;r++)
        try{
          var c=t[r],a=c.href.indexOf(o);
          a>-1&&(c.href="mailto:"+n(c.href,a+o.length))
        }catch(t){
          e(t)
        }
    }(),
    function(){
      for(var t=document.querySelectorAll(c),r=0;r<t.length;r++)
        try{
          var o=t[r],l=n(o.getAttribute(a),0),i=document.createTextNode(l);
          o.parentNode.replaceChild(i,o)
        }catch(t){
          e(t)
        }
    }()
  });
</script>





<div hidden>
  
  <h2 class="sr-only">Templates (for web app):</h2>

  <template id="_animation-template">
  <div class="animation-main fixed-top">
    <nav id="breadcrumbs" class="screen-only"><ul>
  
  
</ul></nav>
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

  <template id="_loading-template">
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

  <template id="_error-template">
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

  <template id="_permalink-template">
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="content-hash"></span>
  </a>
</template>

</div>


</body>
</html>
