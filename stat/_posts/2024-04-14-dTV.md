---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Total variation distance and Convergence in distribution

- 통계학에서 Convergence in distribution은 중요한 성질이다. 우리는 Sample size n이 커질수록 확률변수 또는 estimator가 어떤 분포에 가까워지는지 궁금하기 때문이다.
- 그러므로 Convergence in distribution을 imply하는 성질도 중요하다. Convergence in distribution을 보일 수 있는 방법들 중 하나가 되기 때문이다.
- Total variation distance는 두 분포 사이의 distance metric 중 하나이다. Total variation distance가 작으면 두 분포가 가깝다는 의미이다.
- 본 게시글에서 Total variation distance에 대해서 알아보고 **Total variation distance가 0으로 수렴**하면 한 분포가 다른 분포로 근사한다, 즉 Convergence in distribution한다는 것을 보인다.

## Total variation distance($$d_{TV}$$)

- [Wikipedia](https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures)에 따르면 Total variation distance의 정의는 아래와 같다.
  - Consider a measurable space $$(\Omega, \mathcal{F})$$ and probability measures $$P$$ and $$Q$$ defined on $$(\Omega, \mathcal{F})$$. The total variation distance between $$P$$ and $$Q$$ is defined as: $$\delta(P, Q)=\sup _{A \in \mathcal{F}}\mid P(A)-Q(A)\mid $$​
- $$d_{TV}$$​는 half of the L1 distance btw/ the probability functions이다.
  - ![그림1](/assets/img/stat/dTV/fig1.png)
  - Geometric 의미는 두 분포의 차이에 해당하는 회색 영역의 넓이를 2로 나눈 값이다.
  - discrete : $$\delta(P, Q)=\frac{1}{2} \sum_x\mid P(x)-Q(x)\mid $$
  - continuous : $$\delta(P, Q)=\frac{1}{2} \int\mid p(x)-q(x)\mid  \mathrm{d} x$$​
  - $$\frac{1}{2}$$를 곱해주는 이유는 $$0 \le d_{TV} \le 1$$으로 만들어주기 위함인데, 아래 사진처럼 $$d_{TV}$$가 매우 큰 경우에 두 분포의 차이의 넓이가 2에 가까워지기 때문에 $$\frac{1}{2}$$를 곱한다.
  - ![그림2](/assets/img/stat/dTV/fig2.png)

## Convergence in Total variation distance

- Convergence in Total variation distance는 Convergence in distribution보다 강력한 성질이다.
- **즉 두 분포의 Total variation distance가 0으로 수렴하면, 두 분포는 Convergence in distribution이다.**
  
  : $$\begin{aligned}\text{When }\delta(P, Q)&=\sup _{A \in \mathcal{F}}\mid P(A)-Q(A)\mid  \\ \lim _{n \rightarrow \infty} \delta\left(P_n, Q\right)&=0 \\ \lim _{n \rightarrow \infty}\mid P_n(A)-Q(A)\mid &=0 \quad \forall A \in \mathcal{F} \\ \lim _{n \rightarrow \infty}\mid F_n(x)-F(x)\mid &=0 \end{aligned}$$

- 하지만 역은 성립하지 않는다. Convergence in distribution이라고 해서 Total variation distance가 0으로 수렴하는 것은 아니다.

  - Counterexample : $$f_n \sim \frac{2}{\pi} \cos ^2(n x) \mathbf{1}_{[0, \pi]}(x) d x$$

  : $$\begin{aligned}F_n(x)&=\int_0^x f_n(t)dt \\ &=\int_0^x\frac{2}{\pi} \cos ^2(nt) \mathbf{1}_{[0, \pi]}(x)dt \\ & = \frac{2}{\pi}\mathbf{1}_{[0, \pi]}(x) \int_0^x \cos ^2(nt)dt \\ &= \frac{2}{\pi}\mathbf{1}_{[0, \pi]}(x) \frac{sin(2nx)+2nx}{4n} \\  \lim_{n \to \infty} F_n(x) &=\frac{2}{\pi}\mathbf{1}_{[0, \pi]}(x) \frac{x}{2} \\ &=\frac{x}{\pi},\quad 0 \le x \le \pi \end{aligned}$$

  - 즉 $$\lim_{n \to \infty} F_n = F_{\infty}$$는 $$Unif(0, \pi)$$의 CDF가 되므로 Convergence in distribution이다.
  - 하지만 $$\lim_{n \to \infty}d_{TV}(F_n, F_{\infty}) \nrightarrow 0$$이다.
  
  : $$\begin{aligned} d_{TV}\left(F_n, F_{\infty}\right) & =\frac{1}{2} \int_0^\pi\mid f_n(x)-f_{\infty}(x)\mid  d x \\ & =\frac{1}{2} \int_0^\pi\mid \frac{2}{\pi} \cos ^2(n x)-\frac{1}{\pi}\mid  d x \\ & =\frac{1}{\pi} \int_0^\pi\mid \cos ^2(n x)-\frac{1}{2}\mid  d x \\ & >0 \end{aligned}$$

  - ![그림3](/assets/img/stat/dTV/fig3.png)
  - $$cos^2(nx)$$는 함수값 0과 1를 주기적으로 가지는 함수이고, $$n$$이 커져도 주기가 짧아질 뿐 함수값 0과 1을 주기적으로 가진다는 것은 변하지 않기 때문에, $$\mid \cos ^2(n x)-\frac{1}{2}\mid $$는 strictly positive이다.
  - 그러므로 Total variation distance가 0으로 수렴하지 않는다.
  - 만약 $$f_n, f_{\infty}$$​가 continuous random variable의 density이고 unimodal한 경우에는 Convergence in distribution과 Total variation distance이 0으로 수렴하는 것은 if and only if(필요충분)이다. (skip the proof)
  - [Reference : Ivan Nourdin, Guillaume Poly, Convergence in law implies convergence in total variation for polynomials in independent Gaussian, Gamma or Beta random variables](https://hal.science/hal-00821911/document)

