<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-02-22T18:56:43+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">FITS: Modeling Time Series with 10k Parameters</title><link href="http://localhost:4000/timeseries/2024-02-22-FITS/" rel="alternate" type="text/html" title="FITS: Modeling Time Series with 10k Parameters" /><published>2024-02-22T00:00:00+09:00</published><updated>2024-02-22T17:05:18+09:00</updated><id>http://localhost:4000/timeseries/FITS</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-22-FITS/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>text</li>
</ul>

<h2 id="1-title">1. Title</h2>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://arxiv.org/abs/2307.03756)]]></summary></entry><entry><title type="html">\(2\). Geometric diatance (QGIS)</title><link href="http://localhost:4000/project/2024-02-21-prop2/" rel="alternate" type="text/html" title="\(2\). Geometric diatance (QGIS)" /><published>2024-02-21T00:00:00+09:00</published><updated>2024-02-22T18:56:43+09:00</updated><id>http://localhost:4000/project/prop2</id><content type="html" xml:base="http://localhost:4000/project/2024-02-21-prop2/"><![CDATA[<h2 id="1-shapefile-다운로드">1. Shapefile 다운로드</h2>
<ul>
  <li>지도에 두 주소 또는 좌표를 입력했을 때 출력되는 geometric distance를 구하려면 패키지가 아닌 지리 API를 활용해야 한다. 여기서는 <a href="https://qgis.org/en/site/forusers/download.html">QGIS</a>를 사용한다.</li>
  <li><code class="language-plaintext highlighter-rouge">서울시 도로망 layer</code>, <code class="language-plaintext highlighter-rouge">(지하철역, 버스정류장, 학교) layer</code>, 그리고 <code class="language-plaintext highlighter-rouge">아파트 layer</code>를 깔고 <strong>네트워크 분석</strong>을 진행할 것이다.</li>
  <li><code class="language-plaintext highlighter-rouge">서울시 도로망 layer</code>는 <a href="https://www.openstreetmap.org">OpenStreetMap</a>의 지도 정보를 추출해야 하는데, 3가지의 extractor가 있다.
    <ul>
      <li>1) <a href="https://download.geofabrik.de">Geofabrik</a>에서 sub region 지정
        <ul>
          <li>가장 하위 sub region이 South Korea이다. 즉 서울만 선택하는 것이 불가능하다.</li>
        </ul>
      </li>
      <li>2) <a href="https://extract.bbbike.org">BBBike</a>에서 region search (특정 도시 직접 search 가능)
        <ul>
          <li>특정 도시를 직접 search 가능하다. shp파일이 필요하므로 Shapefile을 선택하고 extract한다.</li>
        </ul>
      </li>
      <li>3) <a href="https://www.vworld.kr/v4po_main.do">국가공간정보포털(브이월드)</a>
        <ul>
          <li><code class="language-plaintext highlighter-rouge">공간정보 다운로드</code> \(\to\) <code class="language-plaintext highlighter-rouge">오픈마켓</code> \(\to\) <code class="language-plaintext highlighter-rouge">(도로명주소)도로구간</code>에서 지역별 도로 shp파일을 수집할 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>2) BBBike 또는 3) 국가공간정보포털을 선택하면 되는데, 2)를 선택했다.
    <ul>
      <li><img src="/assets/img/project/prop/prop2_1.png" alt="사진1" /></li>
      <li>왼쪽(국가공간정보포털)은 서울시에 포함되는 정확한 Node와 Link를 제공하지만, Node와 Link가 있어도 지나갈 수 없는 길 많다. 그래서 실제로 갈 수 있는 경로임에도 경로를 찾지 못하는 경우가 많다.</li>
      <li>오른쪽(BBBike)는 Node와 Link가 있어도 지나갈 수 없는 길이 훨씬 적지만, 실제와 다른 Link가 존재한다. (ex. 중앙에서 우측 하단으로 길게 그려진 직선)</li>
    </ul>
  </li>
</ul>

<h2 id="2-qgis-layer-추가">2. QGIS Layer 추가</h2>
<ul>
  <li>이제 QGIS를 실행한다. 기본 설정으로 <code class="language-plaintext highlighter-rouge">보기</code> \(\to\) <code class="language-plaintext highlighter-rouge">패널</code>에서 탐색기, 레이어, 공간 처리 툴박스를 선택한다.</li>
  <li><code class="language-plaintext highlighter-rouge">Z_KAIS_TL_SPRD_MANAGE_11000.shp</code>를 레이어로 추가한다. (<code class="language-plaintext highlighter-rouge">레이어</code> \(\to\) <code class="language-plaintext highlighter-rouge">레이어 추가</code> \(\to\) <code class="language-plaintext highlighter-rouge">벡터 레이어 추가</code> \(\to\) <code class="language-plaintext highlighter-rouge">*.shp 선택</code>하면 되는데 그냥 드래그해도 된다.)</li>
  <li>그리고 그 위에 미리 수집해놓은 지하철역 좌표를 레이어로 추가한다. 경도와 위도가 있는 csv파일이어야 한다. (<code class="language-plaintext highlighter-rouge">레이어</code> \(\to\) <code class="language-plaintext highlighter-rouge">레이어 추가</code> \(\to\) <code class="language-plaintext highlighter-rouge">구분자로 분리된 텍스트 레이어 추가</code> \(\to\) <code class="language-plaintext highlighter-rouge">지하철역.csv 파일 선택</code>)</li>
  <li><img src="/assets/img/project/prop/prop2_2.png" alt="사진2" /></li>
  <li>추가적으로 수집한 데이터(버스정류장, 학교 등)가 있다면 같은 방식으로 추가하면 되겠다.</li>
</ul>

<h2 id="3-최단경로-찾기">3. 최단경로 찾기</h2>
<ul>
  <li>우측 <code class="language-plaintext highlighter-rouge">공간 처리 툴박스</code> \(\to\) <code class="language-plaintext highlighter-rouge">네트워크 분석</code> \(\to\) <code class="language-plaintext highlighter-rouge">최단경로</code>를 선택하면 된다. <code class="language-plaintext highlighter-rouge">포인트에서 포인트</code>는 <code class="language-plaintext highlighter-rouge">ex.집에서 학교까지</code>이고, <code class="language-plaintext highlighter-rouge">포인트에서 레이어</code>는 <code class="language-plaintext highlighter-rouge">ex.집에서 모든 지하철역들까지</code>이다.</li>
  <li>네트워크 레이어, 시작점(출발지) 레이어, 그리고 종단 레이어를 지하철역으로 선택한다.</li>
  <li><img src="/assets/img/project/prop/prop2_3.png" alt="사진3" /></li>
  <li>연세대학교 정문(point)에서 모든 지하철역(layer)로 가는 최단거리를 표시했다. <code class="language-plaintext highlighter-rouge">레이어</code> \(\to\) <code class="language-plaintext highlighter-rouge">속성 테이블 열기</code>에서 cost가 낮은 순서대로 정렬하면 가까운 지하철역을 정렬할 수 있다.</li>
</ul>

<h2 id="4-최단경로-excel에-저장하기">4. 최단경로 excel에 저장하기</h2>
<ul>
  <li>좌측 하단 <code class="language-plaintext highlighter-rouge">레이어</code> 패널에서 <code class="language-plaintext highlighter-rouge">최단 경로</code>를 우클릭 후, export를 누르면 최단경로의 속성 테이블을 원하는 형태(ex. *.xlsx)로 추출할 수 있다.</li>
  <li><img src="/assets/img/project/prop/prop2_4.png" alt="사진4" /></li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="project" /><summary type="html"><![CDATA[1. Shapefile 다운로드 지도에 두 주소 또는 좌표를 입력했을 때 출력되는 geometric distance를 구하려면 패키지가 아닌 지리 API를 활용해야 한다. 여기서는 QGIS를 사용한다. 서울시 도로망 layer, (지하철역, 버스정류장, 학교) layer, 그리고 아파트 layer를 깔고 네트워크 분석을 진행할 것이다. 서울시 도로망 layer는 OpenStreetMap의 지도 정보를 추출해야 하는데, 3가지의 extractor가 있다. 1) Geofabrik에서 sub region 지정 가장 하위 sub region이 South Korea이다. 즉 서울만 선택하는 것이 불가능하다. 2) BBBike에서 region search (특정 도시 직접 search 가능) 특정 도시를 직접 search 가능하다. shp파일이 필요하므로 Shapefile을 선택하고 extract한다. 3) 국가공간정보포털(브이월드) 공간정보 다운로드 \(\to\) 오픈마켓 \(\to\) (도로명주소)도로구간에서 지역별 도로 shp파일을 수집할 수 있다. 2) BBBike 또는 3) 국가공간정보포털을 선택하면 되는데, 2)를 선택했다. 왼쪽(국가공간정보포털)은 서울시에 포함되는 정확한 Node와 Link를 제공하지만, Node와 Link가 있어도 지나갈 수 없는 길 많다. 그래서 실제로 갈 수 있는 경로임에도 경로를 찾지 못하는 경우가 많다. 오른쪽(BBBike)는 Node와 Link가 있어도 지나갈 수 없는 길이 훨씬 적지만, 실제와 다른 Link가 존재한다. (ex. 중앙에서 우측 하단으로 길게 그려진 직선)]]></summary></entry><entry><title type="html">\(1\). 데이터 수집 및 전처리</title><link href="http://localhost:4000/project/2024-02-20-prop1/" rel="alternate" type="text/html" title="\(1\). 데이터 수집 및 전처리" /><published>2024-02-20T00:00:00+09:00</published><updated>2024-02-22T18:54:30+09:00</updated><id>http://localhost:4000/project/prop1</id><content type="html" xml:base="http://localhost:4000/project/2024-02-20-prop1/"><![CDATA[<h2 id="11-데이터-수집">1.1. 데이터 수집</h2>
<p><img src="/assets/img/project/prop/prop1_1.png" alt="사진1" /></p>
<ul>
  <li>아파트 매매 데이터는 <a href="https://rt.molit.go.kr/pt/xls/xls.do?mobileAt=">국토교통부 실거래가 공개시스템</a>에서 다운로드하였다.</li>
</ul>

<p><img src="/assets/img/project/prop/prop1_2.png" alt="사진2" /></p>
<ul>
  <li>아파트의 위치, 면적, 층, 매매가격 정도를 알 수가 있다. 부동산 분석을 위해서는 주변 정보도 수집할 필요가 있는데, 1.3.에서 포스팅한다.</li>
</ul>

<h2 id="12-좌표-변환--geocode">1.2. 좌표 변환 : Geocode</h2>
<ul>
  <li>Google Geocode를 활용하여 주소를 경도, 위도 좌표계로 변환하였다. 구글 spreadsheets에서 파일을 열고, 확장 프로그램에서 Geocode by Awesome Table을 다운받아 확장프로그램 \(\to\) Start Geocoding을 실행하였다.
<img src="/assets/img/project/prop/prop1_3.png" alt="사진3" /></li>
  <li>그러면 아래 사진처럼 Latitude와 Longitude 열이 생기면서 주소에 해당하는 좌표가 자동으로 입력된다. 다만 속도가 빠르지 않다는 점, Geocode premium을 사용하지 않으면 사용량에 제한이 있다.
<img src="/assets/img/project/prop/prop1_4.png" alt="사진4" /></li>
  <li>Geocode 중복 사용량을 없애기 위해서 같은 아파트의 여러 매매에 대해 매번 좌표를 계산하지 않도록, 아파트에 대해 unique하게 좌표를 계산하고 다시 merge하여 사용량을 1.2%로 줄였다.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">2013년 ~ 2023년 거래된 아파트의 수 : </span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">add20u</span><span class="p">))</span>
<span class="mi">2013</span><span class="n">년</span> <span class="o">~</span> <span class="mi">2023</span><span class="n">년</span> <span class="n">거래된</span> <span class="n">아파트의</span> <span class="n">수</span> <span class="p">:</span>  <span class="mi">9166</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">2013년 ~ 2023년 거래건수: </span><span class="sh">"</span><span class="p">,</span> <span class="nf">all_len</span><span class="p">(</span><span class="n">apt20</span><span class="p">))</span>
<span class="mi">2013</span><span class="n">년</span> <span class="o">~</span> <span class="mi">2023</span><span class="n">년</span> <span class="n">거래건수</span><span class="p">:</span>  <span class="mi">822029</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="13-주변-정보-수집">1.3. 주변 정보 수집</h2>
<ul>
  <li>부동산 분석을 위해서는 다양한 요인이 고려되어야겠지만, 그 중 하나가 주변 정보이다. 아파트의 주변에 지하철역이 있는지, 초/중/고등학교가 있는지 등등이다. 일단은 지하철역, 버스정류장, 학교의 위치를 수집하였다.</li>
  <li>수도권 지하철역 위치 : <a href="https://gaussian37.github.io/python-etc-수도권-지하철/">JINSOL KIM님 Blog</a></li>
  <li>서울시 버스정류장 위치 : <a href="https://topis.seoul.go.kr">서울시 교통정보 시스템</a></li>
  <li>서울시 초/중/고등학교 :  <a href="https://open.neis.go.kr/portal/mainPage.do">나이스 교육정보 개방 포털</a></li>
</ul>

<h2 id="14-euclidean-distance--haversine">1.4. Euclidean distance : haversine</h2>
<ul>
  <li>(* 1.4.는 trial and error 기록을 위한 포스팅일 뿐, 결과적으로 사용하지 않은 방법이니 넘어가도 된다.)</li>
  <li>이제 haversine 패키지를 사용해서 아파트별로 근처에 지하철역, 버스정류장, 그리고 학교가 몇 개인지 계산해주었다. harversine 사용법은 매우 쉽다. 예를 들어 서울역과 고속터미널역의 거리는 6.35km 정도 된다.
<img src="/assets/img/project/prop/prop1_5.png" alt="사진5" /></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">haversine</span> <span class="kn">import</span> <span class="n">haversine</span>
<span class="c1"># 서울역
</span><span class="n">seoul_station</span> <span class="o">=</span> <span class="p">(</span><span class="mf">37.5547278494914</span><span class="p">,</span> <span class="mf">126.969526329341</span><span class="p">)</span>
<span class="c1"># 고속터미널역
</span><span class="n">terminal_station</span> <span class="o">=</span> <span class="p">(</span><span class="mf">37.5049267445237</span><span class="p">,</span> <span class="mf">127.004949918697</span><span class="p">)</span>

<span class="nf">haversine</span><span class="p">(</span><span class="n">seoul_station</span><span class="p">,</span> <span class="n">terminal_station</span><span class="p">)</span>
<span class="c1"># 6.357909897291526
</span></code></pre></div></div>

<ul>
  <li>이제 아래와 같이 학교, 지하철역, 버스정류장의 좌표 데이터를 준비하고 harversine을 통해 아파트별로 가까운 지하철역이 몇 개인지를 센다.
<img src="/assets/img/project/prop/prop1_6.png" alt="사진6" /></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">NearStation</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">station</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="c1"># For all apts
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">(Station) (</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="s">)-th apt is completed !</span><span class="sh">"</span><span class="p">)</span>
        <span class="c1"># For all stations
</span>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">station</span><span class="p">)):</span>
            <span class="n">apt_lat</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">Latitude</span><span class="sh">"</span><span class="p">];</span> <span class="n">apt_long</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">Longitude</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">bus_lat</span> <span class="o">=</span> <span class="n">station</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="sh">"</span><span class="s">Latitude</span><span class="sh">"</span><span class="p">];</span> <span class="n">bus_long</span> <span class="o">=</span> <span class="n">station</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="sh">"</span><span class="s">Longitude</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="nf">haversine</span><span class="p">((</span><span class="n">apt_lat</span><span class="p">,</span> <span class="n">apt_long</span><span class="p">),</span> <span class="p">(</span><span class="n">bus_lat</span><span class="p">,</span> <span class="n">bus_long</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">dist</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
                <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">NearStation</span><span class="sh">"</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<ul>
  <li>하지만 우리가 실제로 아파트가 지하철역에서 얼마나 떨어져있는지를 생각할 때에는 <strong>euclidean distance</strong>가 아니라 실제로 지나가는 최단거리와 소요시간을 생각한다. 그러므로 최단거리에 해당하는 <strong>geometric distance</strong>을 사용할 필요가 있다. Geometric distance는 QGIS를 사용하므로 다음 게시글에서 다룬다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="project" /><summary type="html"><![CDATA[1.1. 데이터 수집 아파트 매매 데이터는 국토교통부 실거래가 공개시스템에서 다운로드하였다. 아파트의 위치, 면적, 층, 매매가격 정도를 알 수가 있다. 부동산 분석을 위해서는 주변 정보도 수집할 필요가 있는데, 1.3.에서 포스팅한다.]]></summary></entry><entry><title type="html">(PatchTST) A Time Series Is Worth 64 Words: Long-term Forecasting With Transformers</title><link href="http://localhost:4000/timeseries/2024-02-18-PatchTST/" rel="alternate" type="text/html" title="(PatchTST) A Time Series Is Worth 64 Words: Long-term Forecasting With Transformers" /><published>2024-02-18T00:00:00+09:00</published><updated>2024-02-19T01:09:16+09:00</updated><id>http://localhost:4000/timeseries/PatchTST</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-18-PatchTST/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>논문에서 요약을 잘 해놔서 굳이 번역하지 않고 그대로 가져왔다.</li>
  <li>2개의 Key components
    <ul>
      <li>Segmentation of time series into <strong>subseries-level patches</strong> which are served as input tokens to Transformer</li>
      <li><strong>Channel-independence</strong> where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series.</li>
    </ul>
  </li>
  <li>Patching design의 장점 3가지
    <ul>
      <li><strong>local semantic information</strong> is retained in the embedding</li>
      <li><strong>computation and memory usage</strong> of the attention maps are quadratically reduced</li>
      <li>the model can attend <strong>longer history</strong></li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Patching : 단일 time-step을 token으로 만들면 (<code class="language-plaintext highlighter-rouge">point-wise input token</code>) 시계열의 포괄적인 의미 정보를 파악할 수 없기 때문에, time-steps를 합쳐서 subseries-level patchs를 만들어 locality를 강화하고 포괄적인 의미 정보를 파악한다.</li>
  <li>Channel-independence : 각 token이 오직 하나의 채널(feature)의 정보만 담는 것이다. (반대로 <code class="language-plaintext highlighter-rouge">channel-mixing</code>은 token이 모든 채널(features)를 embedding space에 projection해서 정보를 섞는 방식이다.)</li>
  <li>PatchTST의 장점 3
    <ul>
      <li>Reduction on time and space complexity</li>
      <li>Longer look-back window</li>
      <li>Capability of representation learning</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related work</h2>
<ul>
  <li>Patching의 milestone은 ViT(2021)</li>
  <li>LogTrans(2019)
    <ul>
      <li>key, query는 point-wise 내적 안하지만 여전히 value는 single time step에 기반한다.</li>
    </ul>
  </li>
  <li>Autoformer(2021)
    <ul>
      <li>patch level connection을 얻기 위해 auto-correlation을 사용하지만 handcrafted design이라서 패치 내 의미 정보를 모두 파악하기 어렵다.</li>
    </ul>
  </li>
  <li>Triformer(2022)
    <ul>
      <li>patch attention을 제안하긴 하지만 patch를 input으로 사용하지 않는다는 점에서 의미 정보를 파악하기 어렵다.</li>
    </ul>
  </li>
  <li>Unlabelled data로 인해 self-supervised learning이 많이 떴는데, transformer를 통해 time series에 적용하기 위한 representation을 학습하는 시도는 아직 완전하지 않다.</li>
</ul>

<h2 id="3-proposed-method">3. Proposed Method</h2>

<h3 id="31-model-structure">3.1. Model Structure</h3>
<ul>
  <li>\((\boldsymbol{x_{1}}, ..., \boldsymbol x_L)\) 를 보고 \((\boldsymbol x_{L+1}, ..., \boldsymbol x_{L+T})\)를 예측하는 문제이고, PatchTST는 transformer의 encoder를 핵심으로 한다.
<img src="/assets/img/timeseries/PatchTST/fig1.jpeg" alt="사진1" /></li>
  <li><strong>Forward Process</strong> : 시계열에 있는 M개의 변수가 있고 길이가 L이라고 할 때, \(i\)번째 series는 \(\boldsymbol{x}_{1:L}^{(i)}=(x_{1}^{(i)}, ... , x_{L}^{(i)})\)이다.</li>
  <li>M개의 \(\boldsymbol{x}^{(i)} \in \mathbb R^{1 \times L}\) 가 각각 transformer backbone으로 들어가고 (channel-independence) 각각의 transformer는 \(\boldsymbol{\hat x}^{(i)} =(\hat x_{L+1}^{(i)}, ..., \hat x_{L+T}^{(i)})\in \mathbb R^{1 \times L}\)를 output으로 한다.</li>
  <li><strong>Patching</strong> : 아래 그림처럼 univariate time series \(\boldsymbol{x}^{(i)}\)를 \(\boldsymbol{x}_p^{(i)} \in \mathbb R^{P \times N}\)으로 patching한다.
<img src="/assets/img/timeseries/PatchTST/myfig1.jpeg" alt="사진2" /></li>
  <li>Input token의 개수가 \(L\)에서 \(N=\left\lfloor\frac{(L-P)}{S}\right\rfloor+2\)로 줄어들기 때문에, 사용할 수 있는 memory와 complexity가 확보되면서 더 긴 historical sequence를 볼 수 있어 성능이 향상된다.</li>
  <li><strong>Transformer Encoder</strong> :
    <ul>
      <li>1) Mapping to the transformer latent space : \(\boldsymbol{x}_d^{(i)}= \mathbf W_p\boldsymbol{x}_p^{(i)}+ \mathbf W_{pos}\)
        <ul>
          <li>where trainable linear projection \(\mathbf W_p \in \mathbb R^{D \times P}\), learnable addictive position encoding \(\mathbf W_{pos} \in \mathbb R^{D \times N}\))</li>
        </ul>
      </li>
      <li>
        <dl>
          <dt>2) Multi-head attention (with Batchnorm and Residual connection)</dt>
          <dd><code class="language-plaintext highlighter-rouge">Query</code> \(Q_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^Q\), <code class="language-plaintext highlighter-rouge">Key</code> \(K_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^K\) and <code class="language-plaintext highlighter-rouge">Value</code> \(V_h^{(i)}=\left(\boldsymbol{x}_d^{(i)}\right)^T \mathbf{W}_h^V\)</dd>
        </dl>
        <ul>
          <li>where \(\mathbf{W}_h^Q, \mathbf{W}_h^K \in \mathbb{R}^{D \times d_k}\) and \(\mathbf W_h^V \in \mathbb R^{D \times D}\)</li>
        </ul>
      </li>
      <li>3) Getting attention : \(\mathbf O_h^{(i)} \in \mathbb R^{D \times N}\)
        <ul>
          <li>where \(\left(\mathbf{O}_h^{(i)}\right)^T=\operatorname{Attention}\left(Q_h^{(i)}, K_h^{(i)}, V_h^{(i)}\right)=\operatorname{Softmax}\left(\frac{Q_h^{(i)} K_h^{(i)^T}}{\sqrt{d_k}}\right) V_h^{(i)}\)</li>
        </ul>
      </li>
      <li>4) Flatten and Linear head : \(\boldsymbol{\hat x}^{(i)}=(\boldsymbol{\hat x}_{L+1}^{(i)}, ..., \boldsymbol{\hat x}_{L+T}^{(i)}) \in \mathbb R^{1 \times T}\)</li>
    </ul>
  </li>
  <li><strong>Loss function</strong> : MSE. \(\mathcal{L}=\mathbb{E}_{\boldsymbol{x}} \frac{1}{M} \sum_{i=1}^M\left\|\hat{\boldsymbol{x}}_{L+1: L+T}^{(i)}-\boldsymbol{x}_{L+1: L+T}^{(i)}\right\|_2^2\)</li>
  <li><strong>Instance Normalization</strong> : Pathcing 전에 각 univariate time series에 <code class="language-plaintext highlighter-rouge">mean=0</code>, <code class="language-plaintext highlighter-rouge">std=1</code>하고, output prediction 전에 다시 더해준다.</li>
</ul>

<h3 id="32-representation-learning">3.2. Representation Learning</h3>
<ul>
  <li>Self-supervised representation learning 방법 중에서 masked autoencoder를 사용했다. (input sequence의 일부를 0으로 masking하고 recover하도록 모델링)</li>
  <li>다만 이걸 그대로 Multivariate time series에 가져오면 두 가지 문제가 발생한다.
    <ul>
      <li>첫째, single time step에 masking하면 맞추기가 너무 쉽다. (interpolating 하면 끝) 그래서 다양한 크기의 group of time series를 랜덤하게 masking하는 기존의 방법을 사용했다.</li>
      <li>둘째, 각 time step을 D차원으로 representation하면 \(z_t \in \mathbb R^D\)가 되니, parameter matrix \(\mathbf W\)의 차원이 \((L\cdot D) \times (M\cdot T)\)이 되어 \(L, D, M, T\) 중 하나만 커지더라도 oversieze가 된다. 그래서 PatchTST에서는 \(D \times P\) size의 linear layer를 사용하였고, patch 단위로 masking을 했다.</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>사용한 데이터셋은 9개(ETTm1/2, ETTh1/2, ILI, Weather, Traffic, Exchange, Electrictiy)이고 비교한 모델은 6개(FEDformer, Autoformer, Informer, Pyraformer, LogTrans +LTSF-Linear)이다.
<img src="/assets/img/timeseries/AreTF/table12.jpeg" alt="사진3" />
<img src="/assets/img/timeseries/PatchTST/table34.jpeg" alt="사진4" /></li>
  <li>PatchTST/64는 input patches 64개, look-back window size L=512이다.
  PatchTST/42는 input patches 42개, look-back window size L=336이다.
  두 버전 모두 patch length P = 16, stride S = 8이다.
  Masking ratio = 40%이다.</li>
  <li>실험 결과 long-term forecasting에서 다른 transformer-based models 및 DLinear보다 성능이 뛰어났다.
<img src="/assets/img/timeseries/PatchTST/table56.jpeg" alt="사진5" /></li>
  <li>Transfer learning task에서도 다른 모델들보다 성능이 뛰어났다.
<img src="/assets/img/timeseries/PatchTST/table7.jpeg" alt="사진6" /></li>
  <li>Ablation study 결과 patching과 channel-independence 모두 성능에 중요한 역할을 하고 있음을 알 수 있다. 특히 patching의 motivation은 앞서 언급한 것처럼 직관적이다.
<img src="/assets/img/timeseries/PatchTST/fig2.jpeg" alt="사진7" /></li>
  <li>논문 <a href="/timeseries/2024-02-16-AreTF">Are Transformer Effective for Time Series Forecasting?</a>에서 transformer-based models의 경우 look-back windows size가 커져도 예측 성능이 좋아지지 않는다고 주장했고, 이는 temporal information을 잘 못잡아내는 것이 맞다. 하지만 PatchTST는 look-back windows가 길어질수록 성능이 좋아지므로 해당사항이 없다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>PatchTST의 key components 2가지는 : Patching과 Channel-independence이다.</li>
  <li>PatchTST는 longer look-back windows의 benefit을 가질 수 있으면서 local semantic information을 파악할 수 있다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://arxiv.org/abs/2211.14730)]]></summary></entry><entry><title type="html">(LTSF-Linear) Are Transformer Effective for Time Series Forecasting?</title><link href="http://localhost:4000/timeseries/2024-02-16-AreTF/" rel="alternate" type="text/html" title="(LTSF-Linear) Are Transformer Effective for Time Series Forecasting?" /><published>2024-02-16T00:00:00+09:00</published><updated>2024-02-19T01:13:12+09:00</updated><id>http://localhost:4000/timeseries/AreTF</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-16-AreTF/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Transformer-based solutions는 긴 시퀀스 속에 있는 semantic correlations를 잘 추출하기 때문에 long-term time series forecasting(LTSF)에 쓰인다.</li>
  <li>하지만 이러한 permutation-invariant self-attention의 특성상 temporal information loss가 불가피하다.</li>
  <li>그러므로 simple one-layer linear models (LTSF-Linear)를 제안한다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Transformer의 핵심인 multi-head self-attention은 <code class="language-plaintext highlighter-rouge">permutation-invariant</code> = <code class="language-plaintext highlighter-rouge">anti-order</code>한 semantic correlations를 찾아낸다.</li>
  <li>하지만 time series는 순서 자체가 굉장히 중요한데, NLP를 위한 transformer가 LTST에 효과적일까?</li>
  <li>결론적으로, 실험결과 : 복잡한 transformer 구조보다 단순한 LTSF-Linear의 성능이 더 뛰어났다.</li>
</ul>

<h2 id="2-prelininaries--tsf-problem-formulation">2. Prelininaries : TSF Problem Formulation</h2>
<ul>
  <li>본 논문에서 사용할 notation은 간단하다. \(\cal X = \{ X_1^t, ..., X_C^t\}_{t=1}^L\)을 보고 \(\cal \hat X = \{\hat X_1^t, ..., \hat X_C^t\}_{t=L+1}^{L+T}\)를 예측하는 것이다. (\(C\)는 변수의 개수, \(L\)은 look-back window의 size, \(T\)는 예측하고자 하는 time steps)</li>
</ul>

<h2 id="3-transformer-based-ltsf-solutions">3. Transformer-based LTSF solutions</h2>
<ul>
  <li>Vanilla Transformer는 quadratic time/memory complexity, error accumulation by the autoregressive decoder로 인한 한계가 있고, 이걸 해결하기 위한 많은 transformer-based models가 있는데, 요약하면 아래 그림과 같다.
<img src="/assets/img/timeseries/AreTF/fig1.jpeg" alt="사진1" /></li>
  <li>Autoformer, LogTrans, Pyraformer, FEDformer, Informer 등 transformer를 time series에 활용하기 위한 다양한 시도들이 있었는데, 파이프라인 단계별로 간단하게만 보겠다.
    <ul>
      <li>(a) Preprocessing : normalization with zero-mean, seasonal-trend decomposition, …</li>
      <li>(b) Embedding : fixed positional encoding, channel projection embedding, learnable temporal embedding, …</li>
      <li>(c) Encoder : Logsparse mask(LogTrans), pyramidal attention(Pyraformer), ProbSparse(Informer), Frequency enhanced block(FEDformer), series-wise auto-correlation(Autoformer), …</li>
      <li>(d) Decoder : IMS(Iterative Multi-step forecasting, 단일 시점 예측 반복하여 여러 시점 예측) 대신 DMS(Direct Multi-step forecasting, 각 미래 시점 예측 위해 별도 모델) 사용</li>
    </ul>
  </li>
  <li>하지만 앞서 언급한 것처럼 permutation-invariant한 semantic correlations은 temporal relation과 다르다.</li>
</ul>

<h2 id="4-an-embarrassingly-simple-baseline">4. An Embarrassingly Simple Baseline</h2>
<ul>
  <li>기존의 transformer-based LTSF solutions는 non-transformer에 비해 성능이 좋다는 실험 결과를 내놓았지만, 그건 IMS를 사용한 non-transformer와 달리 DMS를 사용했기 때문이다.</li>
  <li>이걸 뒷받침하는 근거가 본 논문에서 제시하는 DMS 모델인 LTSF-Linear인데, \(\hat X_i = WX_i\) where \(W \in \mathbb R^{T\times L}\) 구조이다.</li>
  <li>LTSF-Linear는 DLinear, NLinear 두 종류가 있다.
    <ul>
      <li>DLinear : trend component와 seasonal component로 decompose하고 각각을 one-layer에 넣어 다시 sum한다.</li>
      <li>NLinear : Input의 각 값에서 마지막 값을 빼고 linear layer를 통과한 뒤 다시 더해주는 simple normalization
<img src="/assets/img/timeseries/AreTF/fig2.jpeg" alt="사진2" /></li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>
<ul>
  <li>사용한 데이터셋은 9개(ETTm1/2, ETTh1/2, ILI, Weather, Traffic, Exchange, Electrictiy)이고 비교한 모델은 5개(FEDformer, Autoformer, Informer, Pyraformer, LogTrans)이다. 아래는 quantitative and qualitative results이다.
<img src="/assets/img/timeseries/AreTF/table12.jpeg" alt="사진3" />
<img src="/assets/img/timeseries/AreTF/fig3.jpeg" alt="사진4" /></li>
  <li>Q) Can existing LTSF-Transformers extract temporal relations well from longer input sequences?
    <ul>
      <li><img src="/assets/img/timeseries/AreTF/fig4.jpeg" alt="사진5" /></li>
      <li>Look-back window size가 커짐에 따라, transformer-based models의 성능은 크게 좋아지지 않지만, LTSF-Linear의 성능은 유의미하게 좋아진다.</li>
    </ul>
  </li>
  <li>Q) Are the self-attention scheme effective for LTSF?
    <ul>
      <li><img src="/assets/img/timeseries/AreTF/table4.jpeg" alt="사진6" /></li>
      <li>Informer에서 one linear layer까지 구조를 점점 단순하게 할수록 성능이 좋아졌다. 즉 복잡한 모듈이 불필요하다.</li>
    </ul>
  </li>
  <li>Q) Can existing LTSF-Transformers preserve temporal order well?
    <ul>
      <li><img src="/assets/img/timeseries/AreTF/table5.jpeg" alt="사진7" /></li>
      <li>데이터를 섞거나 반 나눠서 순서를 바꿨을 때, 다른 transformer-based models은 성능이 크게 떨어지지 않았지만, LTSF-Linear의 경우 유의미하게 떨어졌다.</li>
      <li>그러므로 LTSF-Linear가 다른 transformer-based models보다 temporal order를 잘 보존한다.</li>
    </ul>
  </li>
  <li>Q) Is training data size a limiting factor for existing LTSF- Transformers?
    <ul>
      <li><img src="/assets/img/timeseries/AreTF/table7.jpeg" alt="사진8" /></li>
      <li>Full dataset(ori.)로 학습했을 때보다 1년치 데이터(short)로 학습했을 때 성능이 더 좋았다.</li>
      <li>단순하게 긴 training data가 필요한게 아니라, whole-year data에 더 명확한 temporal features가 있다는 걸 알 수 있다.</li>
    </ul>
  </li>
</ul>

<h2 id="6-conclusion-and-future-work">6. Conclusion and Future Work</h2>
<ul>
  <li>본 논문의 contribution은 linear model을 제안한 데에 있는 것이 아니라, 비교 실험을 통해 왜 LTSF-transformers가 효율적이지 못한지에 대한 질문을 던지는 데에 있다.</li>
  <li>본 논문의 비교 실험 결과를 통해 transformer의 구조에 대해 더 잘 이해하고 효율적으로 사용하기 위한 방법을 고민해볼 수 있다.</li>
  <li>본 논문에서 제시하는 LTSF-Linear는 change points로 인한 temporal dynamics를 포착하기 어려운 등 한계점이 분명하기 때문에, 앞으로의 연구를 위해 단순하면서 경쟁력 있는 기준선 정도로 생각하면 되겠다.</li>
</ul>

<h2 id="추가">추가</h2>
<ul>
  <li>본 논문이 발표되기 전에 transformer-based time series models가 많이 나왔다. Informer, Autoformer, Pyraformer, Fedformer 등이다. 본 논문은 이러한 solutions을 반박하면서 conv를 제안하였다. 이제 본 논문을 반박하면서 다시 transformer를 제시하는 PatchTST를 읽어보러 가자.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2022](https://arxiv.org/abs/2205.13504)]]></summary></entry><entry><title type="html">깃블로그 만들기 6 : 흔히 발생할 수 있는 에러</title><link href="http://localhost:4000/gitblog/2024-02-15-gitblog6/" rel="alternate" type="text/html" title="깃블로그 만들기 6 : 흔히 발생할 수 있는 에러" /><published>2024-02-15T00:00:00+09:00</published><updated>2024-02-17T00:26:39+09:00</updated><id>http://localhost:4000/gitblog/gitblog6</id><content type="html" xml:base="http://localhost:4000/gitblog/2024-02-15-gitblog6/"><![CDATA[<ul>
  <li>이번에는 Hydejack 테마를 사용하면서 내가 자주 했던 실수들을 살펴보겠다. 처음에는 에러의 이유를 몰라서 gitblog repository를 몇 번이나 삭제하고 다시 만들면서 시간을 많이 썼는데, 앞으로는 그러지 않기 위해 남긴다.</li>
</ul>

<h2 id="1-git-commit-push할-때-_configyml의-theme-수정">1. git commit, push할 때 _config.yml의 theme 수정</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">깃 블로그 만들기 2 : 테마 적용하기</code> 10. 에서 git에 commit, push하기 전에 <code class="language-plaintext highlighter-rouge">_config.yml</code>파일에 수정을 해야 한다고 말했다. 무슨 말이냐면 내가 로컬에서 깃블로그를 수정하고 <code class="language-plaintext highlighter-rouge">bundle exec jekyll serve</code>로 서버에서 확인을 한 뒤 이제 push를 통해 반영하기 위해서는 터미널에 아래처럼 입력해야 한다는 것이다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sed</span> <span class="o">-</span><span class="n">i</span> <span class="sh">''</span> <span class="sh">'</span><span class="s">s/theme: jekyll-theme-hydejack/# theme: jekyll-theme-hydejack/</span><span class="sh">'</span> <span class="n">_config</span><span class="p">.</span><span class="n">yml</span>
<span class="n">sed</span> <span class="o">-</span><span class="n">i</span> <span class="sh">''</span> <span class="sh">'</span><span class="s">s/# remote_theme: hydecorp\/hydejack@v9/remote_theme: hydecorp\/hydejack@v9/</span><span class="sh">'</span> <span class="n">_config</span><span class="p">.</span><span class="n">yml</span>

<span class="n">ga</span> <span class="p">.</span>
<span class="n">git</span> <span class="n">commit</span> <span class="o">-</span><span class="n">m</span> <span class="err">“</span><span class="n">update</span><span class="err">”</span>
<span class="n">gp</span> <span class="o">--</span><span class="nb">set</span><span class="o">-</span><span class="n">upstream</span> <span class="n">origin</span> <span class="n">main</span>
<span class="n">gp</span>

<span class="n">sed</span> <span class="o">-</span><span class="n">i</span> <span class="sh">''</span> <span class="sh">'</span><span class="s">s/^# theme: jekyll-theme-hydejack/theme: jekyll-theme-hydejack/</span><span class="sh">'</span> <span class="n">_config</span><span class="p">.</span><span class="n">yml</span>
<span class="n">sed</span> <span class="o">-</span><span class="n">i</span> <span class="sh">''</span> <span class="sh">'</span><span class="s">s/^remote_theme: hydecorp\/hydejack@v9/# remote_theme: hydecorp\/hydejack@v9/</span><span class="sh">'</span> <span class="n">_config</span><span class="p">.</span><span class="n">yml</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ga .</code>는 <code class="language-plaintext highlighter-rouge">git add .</code>, <code class="language-plaintext highlighter-rouge">gp</code>는 <code class="language-plaintext highlighter-rouge">git push</code>를 의미한다.</li>
  <li>위 2줄은 <code class="language-plaintext highlighter-rouge">_config.yml</code>파일에서 <code class="language-plaintext highlighter-rouge">theme: jekyll-theme-hydejack</code> (이하 <code class="language-plaintext highlighter-rouge">theme</code>) 부분을 주석 처리하고 <code class="language-plaintext highlighter-rouge">remote_theme: hydecorp/hydejack@v9</code> (이하 <code class="language-plaintext highlighter-rouge">remote_theme</code>)부분의 주석 처리를 해제하는 명령어이다.</li>
  <li>
    <p>중앙 4줄은 변경 사항들을 git에 push하는 명령어이고, 아래 2줄은 원래대로 <code class="language-plaintext highlighter-rouge">_config.yml</code>파일에서 <code class="language-plaintext highlighter-rouge">theme</code> 부분의 주석을 해제하고 <code class="language-plaintext highlighter-rouge">remote_theme</code> 부분을 주석 처리하는 명령어이다.</p>
  </li>
  <li>그러면 <code class="language-plaintext highlighter-rouge">theme</code> 부분이 주석 처리되고, <code class="language-plaintext highlighter-rouge">remote_theme</code> 부분이 주석 해제된 채로 (즉 git push하는 상태) 터미널에 <code class="language-plaintext highlighter-rouge">bundle exec jekyll serve</code>를 입력하면 어떻게 될까? 아래처럼 에러가 발생하여 서버에 페이지를 띄울 수가 없다.</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog6/gitblog6_1.png" alt="사진1" /></p>

<ul>
  <li>그러면 반대로 <code class="language-plaintext highlighter-rouge">theme</code> 부분이 주석 해제되고, <code class="language-plaintext highlighter-rouge">remote_theme</code> 부분이 주석 처리된 채로 (즉 로컬 서버에 띄울 수 있는 상태)에서 git push를 하면 어떻게 될까? 아래처럼 github에 에러가 발생한다.</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog6/gitblog6_2.png" alt="사진2" /></p>

<ul>
  <li>어떤 에러인지 들어가보면 아래 사진처럼 <code class="language-plaintext highlighter-rouge">Build with Jekyll</code>에 에러가 발생했고, 자세히 보면 테마를 찾을 수 없다고 한다.</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog6/gitblog6_3.png" alt="사진3" />
<img src="/assets/img/gitblog/gitblog6/gitblog6_4.png" alt="사진4" /></p>

<ul>
  <li>그러므로 이러한 에러가 발생한다면 <code class="language-plaintext highlighter-rouge">_config.yml</code> 파일을 확인해보자.</li>
</ul>

<h2 id="2-게시글-파일-이름-에러">2. 게시글 파일 이름 에러</h2>

<p><img src="/assets/img/gitblog/gitblog3/gitblog3_3.png" alt="사진5" /></p>

<ul>
  <li>나는 원래 위 사진처럼 게시글 파일 이름을 <code class="language-plaintext highlighter-rouge">2024-02-12-깃블로그 만들기 4 : 새로운 게시글 작성하기.md </code>처럼 작성일자와 제목으로 관리해왔다. 이렇게 블로그에 표시될 게시글 제목과 게시글 파일 이름을 동일하게 하면 내가 로컬에서 관리하기 쉽기 때문이다. 하지만 역시 git push 했을 때 페이지가 정상적으로 뜨지 않는 에러가 발생했다. 통상적으로 파일 이름에 한글과 띄워쓰기를 쓰지 않는다는 점에서 혹시나 하는 마음에 아래 사진처럼 게시글 파일 이름을 모두 변경하였더니, 정상적으로 페이지가 떴다.</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog6/gitblog6_5.png" alt="사진6" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="gitblog" /><summary type="html"><![CDATA[이번에는 Hydejack 테마를 사용하면서 내가 자주 했던 실수들을 살펴보겠다. 처음에는 에러의 이유를 몰라서 gitblog repository를 몇 번이나 삭제하고 다시 만들면서 시간을 많이 썼는데, 앞으로는 그러지 않기 위해 남긴다.]]></summary></entry><entry><title type="html">(ModernTCN) A Modern Pure Convolution Structure for General Time Series Analysis</title><link href="http://localhost:4000/timeseries/2024-02-14-ModernTCN/" rel="alternate" type="text/html" title="(ModernTCN) A Modern Pure Convolution Structure for General Time Series Analysis" /><published>2024-02-14T00:00:00+09:00</published><updated>2024-02-17T00:26:39+09:00</updated><id>http://localhost:4000/timeseries/ModernTCN</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-14-ModernTCN/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>최근 Transformer-based 모델과 MLP-based 모델이 time series에서 우위에 있지만, 본 논문에서는 convolution을 time series에서 사용하는 모델 ModernTCN을 제안한다.</li>
  <li>Time series의 5개 mainstream task (long-term and short-term forecasting, imputation, classification and anomaly detection)에서 SOTA</li>
  <li>Convolution의 sharing params로 효율적이면서도 넓은 receptive fields를 가진다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Transformer-based 모델과 MLP-based 모델이 우위에 있는 이유는 global한 efective receptive fields (ERFs)가 cross-time dependency를 파악하기 때문이다.</li>
  <li>하지만 지금까지 convolution in time series 연구들은 그저 모델의 구조를 복잡하게 해왔고, 이와 다르게 본 논문에서는 convolution 자체를 업데이트해서 ERF를 키운다.</li>
  <li>왜냐하면 CV에서는 이미 Transformer을 보고 convolution을 optimizing하려고 시도하고 있기 때문이다.</li>
  <li>시계열에서 convolution을 쓴다는 것은 cross-time and cross-variable dependency를 포착하겠다는 의미이다.</li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>
<ul>
  <li>MICN(2023), SCINet(2023) 등 최근까지도 convolution을 time series에 활용하려는 시도가 많았지만 long-term dependency가 중요한 time series에서 limited ERFs는 transformer를 이길 수가 없었다.</li>
  <li>CNN(2017) 이후 ViTs(2020)이 등장했고, ViTs를 따라잡기 위해 다양한 modern convolution이 등장하는데, 예를 들면 conv block을 transformer와 비슷하게 하거나 (ConvNeXt, 2022), kernel size를 51 \(\times\) 51로 늘려버리기도 했다. (SLaK, 2022)</li>
  <li>본 논문에서는 conv를 time series에 쓰기 위해 1D conv를 수정한 ModernTCN을 제안한다.</li>
</ul>

<h2 id="3-moderntcn">3. ModernTCN</h2>

<h3 id="31-modernize-the-1d-convolution-block">3.1. Modernize the 1D Convolution block</h3>
<p><img src="/assets/img/timeseries/modernTCN/fig2.jpeg" alt="사진1" /></p>

<ul>
  <li>1D conv를 Figure-2:(b)처럼 DWConv(depth-wise)와 ConvFFN(feed-forward NN)으로 re-design하였다.
    <ul>
      <li>DWconv는 transformer의 self-attention와 같은 역할 : learning the temporal information among tokens on a <strong>per-feature</strong> basis</li>
      <li>ConvFFN은 transformer의 FFN과 같은 역할 : learn the new feature representation of each token <strong>independently</strong></li>
    </ul>
  </li>
  <li>위 디자인은 temporal and feature information을 분리한다. 이것이 jointly mix하던 tranditional conv와의 차이점이다.</li>
  <li>하지만 multivariate time series에서는 cross-variable information도 중요하니 추가적인 수정이 필요하긴 하다.</li>
</ul>

<h3 id="32-time-series-related-modifications">3.2. Time series related Modifications</h3>
<ul>
  <li>CV에서는 RGB \(\to\) D-dim embedding 하지만, 그대로 특정 t시점에서 M개 변수 \(\to\) D-dim embedding하면 안된다.
    <ul>
      <li>RGB차이보다 t시점에서 M개 변수 사이의 차이가 더 크고, cross-variable dependency를 반영 못하기 때문</li>
    </ul>
  </li>
  <li>그래서 아래와 같은 방식으로 patchify embedding을 거친다.
    <ul>
      <li>1) \(X_{in} \in \mathbb R^{M\times L}\)을 \(X_{in} \in \mathbb R^{M\times 1\times L}\)로 unsqueeze</li>
      <li>2) \(X_{in}\) 뒤에 \(P-S\)만큼 패딩 (\(P\)는 patch size, \(S\)는 stride)</li>
      <li>3) 1D conv를 통과, 각 patch는 D차원으로 embedding</li>
      <li>그림으로 표현하면 아래와 같다.
<img src="/assets/img/timeseries/modernTCN/myfig1.jpeg" alt="사진2" /></li>
      <li>예시로 이해해보자. patch size가 10이고 stride가 2이므로 총 50개의 patch를 보게 되므로 N=50이 된다. 
<img src="/assets/img/timeseries/modernTCN/myfig2.png" alt="사진3" /></li>
    </ul>
  </li>
  <li>DWConv는 feature와 variable 모두에 대해 independent하게, 그리고 kernel을 크게 해서 ERFs를 넓게 가져가 temporal information을 포착하도록 했다.</li>
  <li>ConvFFN은 information across feature and variable을 섞는 역할을 해야 하는데, 연산의 효율을 위해 jointly하게 학습하기보다는 두 개의 ConvFFN으로 decople했다.
    <ul>
      <li>ConvFFN 1 : learning the new feature representations per variable</li>
      <li>ConvFFN 2 : learning the cross-variable dependency per feature</li>
    </ul>
  </li>
</ul>

<h3 id="33-overall-structure">3.3 Overall Structure</h3>
<ul>
  <li>\(\mathbf{Z}=\operatorname{Backbone}(\mathbf X_{emb})\), Backbone(\(\cdot\))은 ModernTCN을 쌓아서 만든 구조이다.</li>
  <li>즉 다음과 같이 표현할 수 있다.</li>
  <li>\(\mathbf{Z}_{i+1}=\operatorname{Block}\left(\mathbf{Z}_i\right)+\mathbf{Z}_i\) , 즉 \(\mathbf{Z}_i= \begin{cases}\mathbf{X}_{e m b} &amp; , i=1 \\ \operatorname{Block}\left(\mathbf{Z}_{i-1}\right)+\mathbf{Z}_{i-1} &amp; , i&gt;1\end{cases}\), 이 때 Blcok(\(\cdot\))은 ModernTCN block이다.</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<p><img src="/assets/img/timeseries/modernTCN/fig3.jpeg" alt="사진4" /></p>
<ul>
  <li>Time series의 5가지 mainstream analysis task에서 performance, efficiency 측면에서 SOTA를 달성했다.</li>
</ul>

<h2 id="5-model-analysis">5. Model Analysis</h2>
<ul>
  <li>Performance, efficiency 측면에서 ModernTCN은 SOTA를 달성했다.</li>
  <li>Conv-based time series model TimesNet(2023)도 ModernTCN만큼 성능이 좋은데, 그 이유가 두 모델 모두 CV 분야에서 convolution을 사용하는 아이디어에서 영감을 얻었기 때문이다.</li>
  <li>다만, TimesNet은 conv를 사용하기 위해 1D time series를 2D 공간으로 보낸거고, ModernTCN은 conv 자체를 1D time series에 사용할 수 있도록 modernize했기 때문에 training speed가 빠르다.</li>
</ul>

<h2 id="6-conclusion-and-future-work">6. Conclusion and Future Work</h2>
<ul>
  <li>본 논문의 contribution은 단순히 conv-based model로 transformer-based 모델보다 좋은 성능을 냈다 정도가 아니라, 다시 한 번 time series에 다양한 conv-based models가 등장할 수 있음을 의미한다는 것이다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[ICLR 2024]]></summary></entry><entry><title type="html">(FTS-Diffusion) Generative Learning for Financial Time Series with Irregular and Scale-invariant Patterns</title><link href="http://localhost:4000/timeseries/2024-02-13-FTS-Diffusion/" rel="alternate" type="text/html" title="(FTS-Diffusion) Generative Learning for Financial Time Series with Irregular and Scale-invariant Patterns" /><published>2024-02-13T00:00:00+09:00</published><updated>2024-02-15T16:47:11+09:00</updated><id>http://localhost:4000/timeseries/FTS-Diffusion</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-02-13-FTS-Diffusion/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Financial deep learning 모델을 훈련시키기 위한 데이터가 부족한데, 그렇다고 synthetic data를 만들어내려 하니 irregular and scale-invariant patterns 때문에 어려움이 있음</li>
  <li>패턴이 irregular하다는 말은 패턴이 발생하는 간격이 일정하지 않아서 예측하기 어렵다는 것</li>
  <li>패턴이 scale-invariant하다는 말은 scale을 변화시켜도 형태가 유지된다는 말인데, 특정 패턴이 다양한 너비(폭)나 높이(진폭)로 나타날 수 있다는 말이다. 또한 프랙탈 구조처럼 축소해도 비슷한 패턴이 보이게 된다.</li>
  <li>본 논문에서는 irregular and scale-invariant patterns을 학습하고 생성하는 모델 FTS-Diffusion을 제시한다.</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/fig1.jpeg" alt="사진1" />
<img src="/assets/img/timeseries/fts-diff/fig2.jpeg" alt="사진2" /></p>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>FTS-Diffusion은 3가지의 modules로 구성된다.</li>
  <li>Pattern recognition - Pattern generation - Pattern evolution</li>
  <li>패턴을 인식하고, 패턴을 생성한 뒤, 패턴을 이어붙여서 하나의 time series를 만든다는 것이다. 기존 time series generation 모델들이 어려워하던 irregular and scale-invariant 패턴을 모델링할 수 있다.</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>
<ul>
  <li>본 논문에서 제시하는 모델은 time series를 생성하는 모델이다. 생성 모델은 크게 VAE 계열, GAN 계열, Diffusion 계열이 있다.</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/gm.jpeg" alt="사진3" /></p>

<ul>
  <li>일반적으로 좋은 생성 모델을 3가지 특성으로 정의하는데, 세 가지 모두 뛰어난 모델은 없고 상대적인 장단점이 존재한다.</li>
  <li>Diffusion이 속도는 상대적으로 느리지만 높은 퀄리티와 다양성 측면에서 뛰어나 많은 주목 받았고, 본 논문에서도 diffusion을 사용한다.</li>
</ul>

<h2 id="3-problem-statement">3. Problem Statement</h2>
<ul>
  <li>시계열 \(X=\{x_1,...,x_M\}\)은 \(M\)개의 segments로 이루어지고 \(x_m=\{x_{m,1},...,x_{m,t_m}\}\) 각 segment의 길이는 \(t_m\)</li>
  <li>conditional distribution \(f(\cdot\mid p,\alpha,\beta)\)에서 샘플링을 하는 것이고, \(p\)는 패턴, \(\alpha\)는 duration, \(\beta\)는 magnitude이다.</li>
  <li>tuple \((p,\alpha,\beta)\)는 하나의 state이고, 패턴끼리의 dynamic across를 모델링하기 위해 Markov chain을 사용한다. 즉 transition probability \(Q(p_j,\alpha_j,\beta_j \mid p_i,\alpha_i,\beta_i)\)를 통해 time series를 생성한다.</li>
  <li>이제 FTS-Diffusion의 각 모듈을 다시 살펴보면 아래와 같다.
    <ul>
      <li>Pattern recognition : 패턴 \(p\)를 인식하고 반복되는 패턴의 구조 \(\cal P\) 학습</li>
      <li>Pattern Generation : conditional distribution \(f(\cdot\mid p,\alpha,\beta), \forall p \in \cal P\) 학습</li>
      <li>Pattern Evolution : pattern transition probability \(Q(p_j,\alpha_j,\beta_j \mid p_i,\alpha_i,\beta_i)\) 학습</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/fig3.jpeg" alt="사진4" /></p>

<h2 id="4-framework">4. Framework</h2>

<h3 id="1-pattern-recognition">(1) Pattern recognition</h3>
<ul>
  <li>전체 time series를 여러 개의 segments로 나누고 비슷한 segments끼리 묶어 K개의 clusters를 만드는 알고리즘 (Scale-Invariant Subsequence Clustering, SISC)</li>
  <li>SISC는 각 segment의 length는, 가장 가까운 centriod와의 거리가 최소가 되는 segment length로 결정한다.</li>
  <li>이 때 거리 metric은 일반적으로 사용하는 euclidean이 아니라 length나 magnitude에 구애받지 않는 dynamic time wraping (DTW)를 사용하였다.</li>
  <li>centroid initialization은 처음 1개만 랜덤하게 고른 뒤 먼 segment일수록 다음 centroid가 될 확률이 높도록 하였다. (k-Center-Greedy와 비슷)</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/fig4.jpeg" alt="사진5" /></p>

<h3 id="2-pattern-generation">(2) Pattern generation</h3>
<ul>
  <li>패턴에 gaussian noise를 씌우고 denoising gradient를 학습하는 DDPM의 방식을 사용하여 패턴을 생성하였다.</li>
  <li>Diffusion으로 생성된 패턴을 (scaling) autoencoder에 통과시켜 원하는 length로 transform한다.</li>
  <li>Objective를 아래 식으로 사용하여 diffusion 모델과 autoencoder를 같이 학습시킨다.
\(\mathcal{L}(\theta)=\mathbb{E}_{\boldsymbol{x}_m}\left[\left\|\boldsymbol{x}_m-\hat{\boldsymbol{x}}_m\right\|_2^2\right]+\mathbb{E}_{\boldsymbol{x}_m^0, i, \epsilon}\left[\left\|\epsilon^i-\epsilon_\theta\left(\boldsymbol{x}_m^{i+1}, i, \boldsymbol{p}\right)\right\|_2^2\right]\)</li>
</ul>

<h3 id="3-pattern-generation">(3) Pattern generation</h3>
<ul>
  <li>Pattern evolution network \(\phi\)는 현재 state가 주어졌을 때 다음 state에 올 패턴들의 확률을 학습한다.
\((\hat p_{m+1}, \hat \alpha_{m+1}, \hat \beta_{m+1}) = \phi(p_m, \alpha_m, \beta_m)\)</li>
  <li>Pattern evolution objective는 아래와 같다.
\(\mathcal{L}(\phi)=\mathbb{E}_{\boldsymbol{x}_m}\left[\ell_{C E}\left(p_{m+1}, \hat{p}_{m+1}\right)+\left\|\alpha_{m+1}-\hat{\alpha}_{m+1}\right\|_2^2+\left\|\beta_{m+1}-\hat{\beta}_{m+1}\right\|_2^2\right]\)</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<ul>
  <li>S&amp;P500, GOOG, ZC=F(옥수수 선물) 데이터를 활용하였고, 자산 가격은 non-stationary random walk를 따른다고 알려져있으므로, 통계적 특성을 가지는 수익률(return)을 사용하였다.</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/table1.jpeg" alt="사진6" /></p>

<ul>
  <li>위 결과는 실제 return의 분포와 synthesized 분포의 적합도를 테스트하는 KS test와 AD test 결과이다.</li>
</ul>

<p><img src="/assets/img/timeseries/fts-diff/fig6.jpeg" alt="사진7" /></p>

<ul>
  <li>Mixed data(생성된 synthesized data + 실제 observed data)로 training을 하고 real data로 test를 했을 때에도(TMTR, TATR), mixed data의 비율이 달라졌을 때에도 예측 성능이 일정하다는 것으로부터 FTS-Diffusion으로 생성한 synthesized data가 observed data와 유사하다고 볼 수 있다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>Pattern recognition : SISC designed to identify patterns</li>
  <li>Pattern generation : diffusion-based network to synthesize the segments of patterns</li>
  <li>Pattern evolution : assemble generated segments with proper temporal evoution</li>
</ul>

<h2 id="implementation">Implementation</h2>
<ul>
  <li>under review</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[ICLR 2024]]></summary></entry><entry><title type="html">깃블로그 만들기 5 : 블로그 테마 색 변경하기</title><link href="http://localhost:4000/gitblog/2024-02-12-gitblog5/" rel="alternate" type="text/html" title="깃블로그 만들기 5 : 블로그 테마 색 변경하기" /><published>2024-02-12T00:00:00+09:00</published><updated>2024-02-16T13:24:32+09:00</updated><id>http://localhost:4000/gitblog/gitblog5</id><content type="html" xml:base="http://localhost:4000/gitblog/2024-02-12-gitblog5/"><![CDATA[<ul>
  <li>이번에는 블로그 테마는 그대로 두고 로고, 사이드바 이미지를 비롯해 전체적인 색감을 내가 원하는 디자인으로 변경한다.</li>
</ul>

<h2 id="로고-및-사이드바-이미지-변경--_configyml">로고 및 사이드바 이미지 변경 : _config.yml</h2>

<ul>
  <li>로고 이미지와 사이드바 이미지는 <code class="language-plaintext highlighter-rouge">_config.yml</code> 파일에서 변경할 수 있다. 먼저 이미지를 <code class="language-plaintext highlighter-rouge">assets</code> 폴더의 내가 원하는 위치에 저장한 뒤 <code class="language-plaintext highlighter-rouge">_config.yml</code>에서 아래처럼 생긴 부분을 수정하면 된다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># A (square) logo for your site.
# If provided, it will be shown at the top of the sidebar.
# It also used by the `jekyll-seo-tag` plugin.
</span><span class="n">logo</span><span class="p">:</span>                  <span class="o">/</span><span class="n">assets</span><span class="o">/</span><span class="n">img</span><span class="o">/</span><span class="n">me</span><span class="o">/</span><span class="n">logo</span><span class="p">.</span><span class="n">jpg</span>
</code></pre></div></div>

<ul>
  <li>사이드바 이미지를 변경하기 위해서는 역시 <code class="language-plaintext highlighter-rouge">_config.yml</code> 파일에서 <code class="language-plaintext highlighter-rouge">accent_image</code>에 저장된 사이드바 이미지의 경로를 지정해주면 된다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sidebar image and theme color of the site.
# accent_image:          /assets/img/sidebar-bg.jpg
</span><span class="n">accent_image</span><span class="p">:</span>          <span class="o">/</span><span class="n">assets</span><span class="o">/</span><span class="n">img</span><span class="o">/</span><span class="n">me</span><span class="o">/</span><span class="n">sidebar</span><span class="p">.</span><span class="n">jpg</span>
<span class="n">accent_color</span><span class="p">:</span>          <span class="nf">rgb</span><span class="p">(</span><span class="mi">79</span><span class="p">,</span><span class="mi">177</span><span class="p">,</span><span class="mi">186</span><span class="p">)</span>

<span class="c1"># This is used for the `theme-color` meta tag,
# which changes the background color of the browser UI in certain browsers.
# Defaults to `accent_color`.
</span><span class="n">theme_color</span><span class="p">:</span>           <span class="nf">rgb</span><span class="p">(</span><span class="mi">236</span><span class="p">,</span><span class="mi">231</span><span class="p">,</span><span class="mi">222</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/img/gitblog/gitblog5/gitblog5_1.png" alt="사진1" /></p>

<p>정상적으로 변경된 것을 확인할 수 있다. 하지만 상단 부분에 남아있는 청록색이 사이드바의 컬러와 어울리지 않고, 게시글 밑줄 컬러 역시 이전 테마의 청록색이 남아있다.</p>

<h2 id="테마-밖-컬러-및-게시글-밑줄-컬러-변경">테마 밖 컬러 및 게시글 밑줄 컬러 변경</h2>

<ul>
  <li>아래 코드에서 <code class="language-plaintext highlighter-rouge">accent_image</code>가 사이드바 이미지였고, <code class="language-plaintext highlighter-rouge">accent_color</code>는 게시글 밑줄 컬러이다. 그리고 테마 밖(?) 컬러는 <code class="language-plaintext highlighter-rouge">theme_color</code>로 지정할 수 있다. 게시글 밑줄 컬러 <code class="language-plaintext highlighter-rouge">accent_color</code>는 적당히 어두운 회색으로, 테마 밖 컬러 <code class="language-plaintext highlighter-rouge">theme_color</code>는 사이드바와 연결되는 컬러로 지정했다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sidebar image and theme color of the site.
# accent_image:          /assets/img/sidebar-bg.jpg
</span><span class="n">accent_image</span><span class="p">:</span>          <span class="o">/</span><span class="n">assets</span><span class="o">/</span><span class="n">img</span><span class="o">/</span><span class="n">me</span><span class="o">/</span><span class="n">sidebar</span><span class="p">.</span><span class="n">jpg</span>
<span class="n">accent_color</span><span class="p">:</span>          <span class="nf">rgb</span><span class="p">(</span><span class="mi">94</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">94</span><span class="p">)</span>

<span class="c1"># This is used for the `theme-color` meta tag,
# which changes the background color of the browser UI in certain browsers.
# Defaults to `accent_color`.
</span><span class="n">theme_color</span><span class="p">:</span>           <span class="nf">rgb</span><span class="p">(</span><span class="mi">230</span><span class="p">,</span> <span class="mi">217</span><span class="p">,</span> <span class="mi">195</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/img/gitblog/gitblog5/gitblog5_2.png" alt="사진2" /></p>

<ul>
  <li>상단에 보이는 테마 밖 컬러와 게시글 밑줄 컬러가 변경되었다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="gitblog" /><summary type="html"><![CDATA[이번에는 블로그 테마는 그대로 두고 로고, 사이드바 이미지를 비롯해 전체적인 색감을 내가 원하는 디자인으로 변경한다.]]></summary></entry><entry><title type="html">깃 블로그 만들기 4 : 새로운 게시글 작성하기</title><link href="http://localhost:4000/gitblog/2024-02-12-gitblog4/" rel="alternate" type="text/html" title="깃 블로그 만들기 4 : 새로운 게시글 작성하기" /><published>2024-02-12T00:00:00+09:00</published><updated>2024-02-16T15:16:48+09:00</updated><id>http://localhost:4000/gitblog/gitblog4</id><content type="html" xml:base="http://localhost:4000/gitblog/2024-02-12-gitblog4/"><![CDATA[<p>이전 게시물에서 새로운 게시물을 작성하는 방법을 간단하게 설명하였지만, 하나의 게시글 안에 다양한 다른 게시글들을 모아놓거나, 게시글들의 순서를 변경하는 등 더 자유로운 게시글 관리를 위해서는 아래의 방법을 따라하면 된다.</p>

<h2 id="_configyml에서-사이드바-메뉴-추가하기">_config.yml에서 사이드바 메뉴 추가하기</h2>

<p><img src="/assets/img/gitblog/gitblog3/gitblog3_1.png" alt="그림1" /></p>

<p>만약 사이드바에 <code class="language-plaintext highlighter-rouge">gitblog</code>라는 카테고리를 만들고 그 안에 깃 블로그와 관련된 게시글들을 모으고 싶다면, <code class="language-plaintext highlighter-rouge">_config.yml</code> 파일의 <code class="language-plaintext highlighter-rouge">menu</code> 항목에서 <code class="language-plaintext highlighter-rouge">gitblog</code>를 추가하면 된다.</p>

<h2 id="게시글을-보관할-폴더-만들기">게시글을 보관할 폴더 만들기</h2>

<p><img src="/assets/img/gitblog/gitblog3/gitblog3_2.png" alt="그림2" /></p>

<p>이제 <code class="language-plaintext highlighter-rouge">/username.github.io/</code>에 <code class="language-plaintext highlighter-rouge">gitblog</code>라는 폴더를 만들고 그 안에는 <code class="language-plaintext highlighter-rouge">_posts</code>라는 폴더와 <code class="language-plaintext highlighter-rouge">README.md</code>라는 파일을 위치시킨다. <code class="language-plaintext highlighter-rouge">_posts</code>은 게시글들이 보관될 위치이고, <code class="language-plaintext highlighter-rouge">README.md</code>는 <code class="language-plaintext highlighter-rouge">gitblog</code>라는 카테고리에 들어왔을 때 보여질 페이지이므로 <code class="language-plaintext highlighter-rouge">_posts</code>에 있는 게시글들을 나열하면 되겠다.</p>

<h2 id="readmemd-작성하기">README.md 작성하기</h2>

<ul>
  <li>아래처럼 <code class="language-plaintext highlighter-rouge">README.md</code>를 작성하면 되는데, 일반적으로 여러 게시글을 작성할 것이기 때문에, 아래 사진을 참고하면 된다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="n">게시글</span> <span class="n">제목</span><span class="p">]{:.</span><span class="n">heading</span><span class="p">.</span><span class="n">flip</span><span class="o">-</span><span class="n">title</span><span class="p">}</span>
<span class="p">[</span><span class="n">게시글</span> <span class="n">제목</span><span class="p">]:</span> <span class="n">게시글</span> <span class="n">경로</span>
</code></pre></div></div>

<p><img src="/assets/img/gitblog/gitblog3/gitblog3_3.png" alt="그림3" /></p>

<ul>
  <li>추가 : 만약 이렇게 했을 때 안되면 아래 사진처럼 경로에 <code class="language-plaintext highlighter-rouge">/_posts</code>를 삭제하고, 뒤에 <code class="language-plaintext highlighter-rouge">.md</code>를 삭제해보는 것이 방법이 될 수 있다. 나의 경우에는 bundle을 업데이트했더니 404 에러가 떠서, 아래 사진처럼 바꿔주었더니 다시 페이지가 작동되었다. (정확한 업데이트 내용은 모른다.)</li>
</ul>

<p><img src="/assets/img/gitblog/gitblog3/gitblog3_7.png" alt="그림7" /></p>

<h2 id="게시글-작성하기">게시글 작성하기</h2>

<p><img src="/assets/img/gitblog/gitblog3/gitblog3_4.png" alt="그림4" /></p>

<p>위 사진처럼 <code class="language-plaintext highlighter-rouge">--- ---</code> 안에 게시글의 format을 설정하고 그 아래에 내용을 작성하게 된다. 먼저 format에서는 게시글이므로 <code class="language-plaintext highlighter-rouge">layout</code>은 post로 설정한다. 게시글의 제목은 <code class="language-plaintext highlighter-rouge">title</code>이 아니라 format 밖에서 <code class="language-plaintext highlighter-rouge">#</code>으로 적어준다. <code class="language-plaintext highlighter-rouge">related_posts</code>에서는 본 게시글 마지막에 연관 글로 보여줄 게시글을 표시할 수 있다. 비울 경우에는 임의의 게시글이 표시되므로 해당 기능을 원하지 않는다면 <code class="language-plaintext highlighter-rouge">_</code>로 설정하면 된다. <code class="language-plaintext highlighter-rouge">description</code> 이나 마지막으로 수정한 날짜를 표시하는 기능은 사용하지 않았다.</p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="gitblog" /><summary type="html"><![CDATA[이전 게시물에서 새로운 게시물을 작성하는 방법을 간단하게 설명하였지만, 하나의 게시글 안에 다양한 다른 게시글들을 모아놓거나, 게시글들의 순서를 변경하는 등 더 자유로운 게시글 관리를 위해서는 아래의 방법을 따라하면 된다.]]></summary></entry></feed>