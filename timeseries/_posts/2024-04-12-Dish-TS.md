---
layout: post
related_posts:
  _
title: 
description: >
  [AAAI 2023](https://arxiv.org/pdf/2302.14829.pdf)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting (AAAI 2023)

## Abstract

- Distribution shift in TS : series distribution changes over time
- 기존 연구들은 the quantification of distribution 정도
- Distribution shift in TS는 2개 카테고리
  - **intra-space shift** : the distribution within the input-space keeps shifted over time
  - **inter-space shift** : that the distribution is shifted btw/ input-space and output-space
- Dish-TS : neural paradigm for alleviating distribution shift in TSF
  - CONET : can be any NN, input sequences into learnable distribution coefficients
  - Dual-CONET : separately learn the distribution of input- and output-space

## 1. Introduction

![사진1](/assets/img/timeseries/Dish-TS/fig1.png)

- TS의 non-stationarity(distribution shift over time)는 예측 성능을 방해
- **intra-space shift** :  TS distribution changes over time
- **inter-space shift** :  Distribution shift btw/ input-space (lookbacks) and output-space (horizons)
- 대표적인 alleviate distribution shift solution : RevIN.
  - Quantifying true distribution with fixed statistics (e.g., mean and std.)
    - But, unreliable (limited in expressiveness for representing the true distribution)
    - Different sampling frequencies provide different statistics
  -  Strong assumption : the lookbacks and horizons share the same statistical properties
    - But,  always a variation in distribution btw/ input-space and output-space

- Dish-TS는 RevIN으로부터 영감을 받은 만큼 전체적인 구조는 유사하다
  - two-stage process : normalizing $$\to$$ forecasting $$\to$$ denormalizing
  - CONET : window $$\to$$ two learnable coefficients:
    - a level coefficient and a scaling coefficient
    - to illustrate series overall scale and fluctuation
  - Dual-CONET
    - BACKCONET : coefficients to estimate the distribution of input-space (lookbacks)
    - HORICONET : coefficients to infer the distribution of output-space (horizons)
  - Prior-knowledge : HORICONET 학습할 때 prior 줘서 output-space를 잘 infer(predict) 하도록

## 2. Related Work

- Models for Time Series Forecasting
  - ARMA, BEATS, Transformer, Informer, Autoformer, ...
- Distribution Shift in Time Series Forecasting
  - Adaptive Norm :  puts z-score normalization on series by the computed global statistics
  - DAIN (Passalis et al. 2019) :  applies nonlinear NN to adaptively normalize the series
  - RevIN (Kim et al. 2022) : instance normalization to reduce series shift
- 대부분의 연구가 static statistics 사용해서 normalizing한다. (inter-space shift 고려 안함)

## 3. Problem Formulations

- Time Series Forecasting

  - Formula : $$\left(x_{t: t+H}^{(1)}, \cdots, x_{t: t+H}^{(N)}\right)^T=\mathscr{F}_{\Theta}\left(\left(x_{t-L: t}^{(1)}, \cdots, x_{t-L: t}^{(N)}\right)^T\right)$$​ 
  - $$\mathscr{F}_{\Theta}: \mathbb{R}^{L \times N} \rightarrow \mathbb{R}^{H \times N}$$,   $$\Theta$$ :  forecasting model parameters

- Distribution Shift in Time Series

  - intra-space shift : $$\mid d\left(\mathcal{X}_{\text {input }}^{(i)}(u), \mathcal{X}_{\text {input }}^{(i)}(v)\right)\mid >\delta$$​
  - inter-space shift : $$\mid d\left(\mathcal{X}_{\text {input }}^{(i)}(u), \mathcal{X}_{\text {output }}^{(i)}(u)\right)\mid >\delta$$

  - $$\mathcal{X}_{\text {input }}^{(i)}(u)$$는 $$t=u$$ 시점으로부터 과거 방향으로 $$L$$ 길이의 lookback window
  - $$\mathcal{X}_{\text {output }}^{(i)}(u)$$는 $$t=u$$ 시점으로부터 미래 방향으로 $$H$$ 길이의 horizon window

## 4. Dish-TS

### 4.1. Overview

![사진2](/assets/img/timeseries/Dish-TS/fig2.png)

- CONET :  input series $$\to$$​​ coefficients (for distribution measurement)
- RevIN처럼 two-stage process
  - BACKCONET : transformed the lookbacks (before forecasting model)
  - HORICONET : transformed the forecasting results
    - HORICONET can be trained in a prior knowledgeinduced fashion (4.4에서 설명)

### 4.2. Dual-Conet Framework

- 기존 연구들은 mean, std로 distribution을 measure $$\to$$​ unreliable (Different frequencies different statistics)
- 기본 CONET구조
  - $$\varphi, \xi=\operatorname{CONET}(x)$$ : any NN (can non-linear mapping)
    - $$\varphi \in \mathbb{R}^1$$ : level coefficient (overall scale of input series)
    - $$\xi \in \mathbb{R}^1$$ : scaling coefficient (fluctuation scale)

- Mutivariate forecasting을 위한 Dual-CONET
  - $$\begin{aligned}
    & \varphi_{b, t}^{(i)}, \xi_{b, t}^{(i)}=\operatorname{BACKCONET}\left(x_{t-L: t}^{(i)}\right), i=1, \cdots, N \\
    & \varphi_{h, t}^{(i)}, \xi_{h, t}^{(i)}=\operatorname{HORICONET}\left(x_{t-L: t}^{(i)}\right), i=1, \cdots, N
    \end{aligned}$$​
    - $$\varphi_{b, t}^{(i)}, \xi_{b, t}^{(i)} \in \mathbb{R}^1$$ : level, scaling coefficients for lookbacks
    - $$\varphi_{h, t}^{(i)}, \xi_{h, t}^{(i)} \in \mathbb{R}^1$$ : level, scaling coefficients for horizons
  - BACKCONET과 HORICONET 둘다 input이 "t 시점에서 L 길이의 historical series"이다 !
- Integrating Dual-Conet into Forecasting
  - Final transformed forecasting results : $$\hat{x}_{t: t+H}^{(i)}=\xi_{h, t}^{(i)} \mathscr{F}_{\Theta}\left(\frac{1}{\xi_{b, t}^{(i)}}\left(x_{t-L: t}^{(i)}-\varphi_{b, t}^{(i)}\right)\right)+\varphi_{h, t}^{(i)}$$

### 4.3. A Simple and Intuitive Instance of Conet

- 실제 CONET에서의 연산은 다음과 같다. 
- lookback level coefficient : $$\varphi_{b, t}^{(i)}=\sigma\left(\sum_{\tau=1}^{\operatorname{dim}\left(\mathbf{v}_{b, i}^{\ell}\right)} \mathbf{v}_{b, i \tau}^{\ell} x_{\tau-L+t}^{(i)}\right),$$
- horizons level coefficient : $$\varphi_{h, t}^{(i)}=\sigma\left(\sum_{\tau=1}^{\operatorname{dim}\left(\mathbf{v}_{h, i}^{\ell}\right)} \mathbf{v}_{h, i \tau}^{\ell} x_{\tau-L+t}^{(i)}\right)$$​
- lookback scaling coefficient : $$\xi_{b, t}^{(i)}=\sqrt{\mathbb{E}\left(x_t^{(i)}-\varphi_{b, t}^{(i)}\right)^2}$$ 
- horizons scaling coefficient : $$\xi_{h, t}^{(i)}=\sqrt{\mathbb{E}\left(x_t^{(i)}-\varphi_{h, t}^{(i)}\right)^2}$$

### 4.4. Prior Knowledge-Induced Training Strategy

- HORICONET은 미래의 정보인 $$\mathcal{X}_{\text {output }}^{(i)}$$의 분포를 infer(predict)해야 하기 때문에 intractable하다.
- 그러므로 prior(mean of horizons)을 soft-target으로 줘서 학습의 난이도를 낮춘다.
- final loss는 $$\sum_{k=1}^K \sum_{i=1}^N[\left(\hat{x}_{t_k: t_k+H}^{(i)}-x_{t_k: t_k+H}^{(i)}\right)^2+\underbrace{\left.\alpha\left(\frac{1}{H} \sum_{t=t_k+1}^{t_k+H} x_t^{(i)}-\varphi_{h, t_k}^{(i)}\right)^2\right]}_{\text {Prior Knowledge Guidance }}$$이다.
  - MSE term에 prior knowledge를 $$\alpha$$의 weight로 준다.

## 5. Experiment

![사진3](/assets/img/timeseries/Dish-TS/table1.png)

![사진4](/assets/img/timeseries/Dish-TS/table2.png)

![사진5](/assets/img/timeseries/Dish-TS/table3.png)

Dish-TS를 적용하면 Informer, Autoformer, N-BEATS의 성능이 향상되고, 그 정도는 RevIN보다 크다.

## 6. Conclusion

-  Systematically summarize the distribution shift in time series forecasting
  - as intra-space shift and interspace shift.
- Dish-TS better alleviates the two shift
  -  prior knowledge-induced training strategy, for effectiveness