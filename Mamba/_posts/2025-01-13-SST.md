---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2404.14757)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting (Arxiv 2024)

## Abstract

- Time series forecasting에서 **heterogeneity** between long-range and short-range time series를 간과하면 안됨
  - distinct objectives tailored to different ranges 필요
  - (time series can be decomposed into global patterns and local variations)

- 본 논문에서 multi-scale hybrid Mamba-Transformer experts model STATE SPACE TRANSFORMER(SST) 제안
  - **Mamba** as an expert to extract **global patterns** in **coarse**-grained long-range time series
    - selectively retain long-term patterns and filter out fluctuation
  - **Local Window Transformer (LWT)**, the other expert to focus on capturing **local** variations in **fine**-grained short-range time series
    - locality-awareness capability
  - 각각을 합치는 건 long-short router
    - dynamically adjusts contributions of the two experts

![그림1](/assets/img/Mamba/SST/fig1.png)

## 1. Introduciton

- the lack of distinction btw **long**-range and **short**-range time series
  - 때문에 예측 성능이 방해
  - **long**-range time series는 global patterns이 중요
    - local deviations는 negatively impact forecasting accuracy
  - 반대로 short-range는 local variations가 중요
    - global patterns는 less evident within limited time frames

- **long**-range vs. **short**-range 구분 자체가 애매모호함 (intertwine)
  - New metric to measure the difference 필요
  - 둘을 integrating하는 것이 non-trivial task.
    - relative importance between patterns and variations 잘해야 함
- 이를 위해 **multi-resolution** framework 필요
  - **larger patches** and **longer stride** for **long**-range to obtain **low-resolution**
  - **smaller patches** and **shorter stride** for **short** range lead to **high-resolution**
  - 지금까지의 Mixture-of-Experts (MoEs)는 experts의 역할이 애매했음 !

## 2. Related Work

pass

## 3. Preliminaries

- Look-back window $$\mathcal{L}=\left(\mathrm{x}_1, \mathrm{x}_2, . ., \mathrm{x}_L\right) \in \mathbb{R}^{L \times M}$$
  - Long-range time series $$\mathcal{L}$$ denotes the full range of look-back window $$\mathcal{L}[:]$$
  - shortrange time series $$\mathcal{S} \in \mathbb{R}^{S \times M}$$ denotes the partial latest range $$\mathcal{L}[-S:], S<L$$.

![그림1](/assets/img/Mamba/SST/fig3.png)

- Mamba : retain repeated upand-down patterns in long range
- Transformer : the missing local inductive bias hinders its ability to further capture local variations

## 4. Methodology

![그림1](/assets/img/Mamba/SST/fig4.png)

- 4 modules: **multi-scale patcher**, **global patterns expert**, **local variations expert**, and long-short router
  - **Multi-scale patcher**: **transforms input time series (TS) into different resolutions** according to ranges
  - **Global pattern expert**: is dedicated to finding **long**-term patterns in a **low**-resolution TS
  - **Local variations expert**: aims to capture **short**-term variations in a **high**-resolutionTS
  - **Long-short router**: dynamically learns **contributions** of the two experts.

### Multi-Scale Resolutions

![그림1](/assets/img/Mamba/SST/fig2.png)

- **Global patterns** emerge when viewed at a **broader**-scale granularity
- **Local variations** become clearer when examined at a **finer**-scale granularity
- $$\to$$ **low** resolution for **long**-rangeTS and **high** resolution for short-**range** TS

### Resolution

- $$R_{P T S}=N \sqrt{P}=\left(\left\lfloor\frac{L-P}{S t r}\right\rfloor+1\right) \sqrt{P} \approx \frac{\sqrt{P}}{S t r}$$.

### Hybrid Mamba-Transformer Experts

- Inspired by Mixture-of-Experts (MoEs)
- 기존 MoEs와 다르게, assign global patterns and local variations roles to two experts.
  - extract long-term patterns and filter out small variations in long-range TS

![그림1](/assets/img/Mamba/SST/fig8.png)

- **Global Patterns Expert**
  - 먼저 encodes long-range PTS $$\mathbf{x}_{p L}^{(i)} \in \mathbb{R}^{N_L \times P_L}$$ into high-dimension space $$\mathbf{x}_L^{(i)} \in \mathbb{R}^{N_L \times D}$$ in the encoding layer
  - 다음으로 Mamba block이 select input-dependent patterns and filter out irrelevant variations
- **Local Variations Expert**
  - Transformer는 lack of locality inductive bias and quadratic complexity
  - 따라서 local window Transformer(LWT)로 사용
- **Mixture of Experts**
  - Long-Short Router
    - learning the **relative contributions** of the two specialized experts
  - Formally, the router projects input TS $$\mathcal{L} \in R^{L×M}$$ into the D-dim
  - 그 다음 transformed by a flatten layer into a vector $$z_R$$
  - 그 다음 A linear layer (with a softmax) to output two values $$p_T, p_S \in(0.1)$$
- **Forecasting Module**
  - long range embedding $$\mathbf{z}_L^{(i)}$$과short-range embedding $$\mathbf{z}_S^{(i)}$$를 flatten
    - into a single-row vector
    - and concatenates them with respective weights $p_L$ and $p_S$.
  - Long-short range fusion representation : $$\mathbf{z}_{L S}^i \in \mathbb{R}^{\left(N_S+N_L\right) L D}$$
  - 마지막으로 이걸 linear head에 넣어서 $$\hat{\mathbf{x}}^{(i)}=\left\{\hat{x}_{L+1}, \hat{x}_{L+2}, \ldots, \hat{x}_{L+F}\right\} \in \mathbb{R}^{F \times 1}$$ (for individual variate $$i$$)를 얻음

- **Linear Complexity Analysis**
  - the total complexity of SST is $$O\left(\frac{L}{N_L}+\frac{w S}{N_S}\right)$$

## 5. Experiments

- Time Series Forecasting Results

![그림1](/assets/img/Mamba/SST/table1.png)

- Ablation Studies

![그림1](/assets/img/Mamba/SST/fig6.png)

## 6. Conclusion and Future Work

- Global patterns should be extracted from longrange,
  - while local variations are more effectively captured inshort range.
- A multi-scale hybrid Mamba-Transformer framework SST를 제안
  - **Mamba**, serving as a **global** patterns expert, focuses on extracting **long**-term patterns in **low** resolution
  - **LWT**, as a **local** variations expert, addresses subtle nuances in **short**-range time series in **high** resolution
- with scaling linearly $$O(L)$$ with time series length $$L$$

