---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/abs/2404.16112)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges (Arxiv 2024)

## Abstract

- Sequence modelingì—ì„œ RNN, LSTMì„ ì‚¬ìš©í–ˆì—ˆìŒ
- Transformerê°€ í›Œë¥­í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ
  - but $$O(N^2)$$ complexity,inductive bias handlingì´ ì–´ë ¤ì›€
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” State Space Model (SSM)ë¥¼ í¬ê²Œ 3ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
  -  Gating architectures
  - Structural architectures
  - Recurrent architectures

## 1. Introduciton

- RNN
  - look at only the last state and current input for predicting the next state
  - gradient calculations being limited to the hidden state and current input
  - exploding or vanishing gradient problem
  - lack sufficient memory for long sequences 
- LSTM
  - complexity with their gating mechanisms
  - exhibit challenges in transfer learning
- Transformer
  - enable each token to interact with every other token in the input sequence
  - but $$O(N^2)$$ complexity

- State Space Model (SSM)
  - Understanding of State Space Models (SSMs) : mathematical fundamentals
  - Categorization and Recent Advances of SSMs : systematic categorization
  - Application of SSMs Across Domains : utility in domains
  - Performance Comparison of SSMs with Transformers : SSMê³¼ Transformer ë¹„êµ

## 2. Basics of State Space Model

- High-orderë¥¼ first-order derivativesì™€ vector quantitiesë¡œ representation
- Dynamics of  damped mass-spring system : $$m \frac{d^2 y(t)}{d t^2}+c \frac{d y(t)}{d t}+k y(t)=u(t)$$
  - $$u(t)$$ : ì§ˆëŸ‰ì— ì‘ìš©í•˜ëŠ” ì™¸ë¶€ í˜
  - $$y(t)$$ : ìˆ˜ì§ ìœ„ì¹˜
  - $$x(t)$$ : ì´ ë°©ì •ì‹ì„ 1ì°¨ ë¯¸ë¶„ê³¼ ë²¡í„° ì–‘ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ ë„ì…í•˜ëŠ” ë²¡í„°

### 2.1. Spring Mass-Damper system

- State Variables
  - $$x_1$$ : equilibriumìœ¼ë¡œë¶€í„° ì§ˆëŸ‰ì˜ ìœ„ì¹˜
  - $$\dot{x_1}$$ : ì§ˆëŸ‰ì˜ ì†ë„
- System Dynamics
  - ë‰´í„´ì˜ ì œ 2ë²•ì¹™ìœ¼ë¡œ í‘œí˜„í•˜ë©´ $$m \ddot{x}_1=-k x_1-c \dot{x}_1$$
  - $$\ddot{x_1}$$ëŠ” ì§ˆëŸ‰ì˜ ê°€ì†ë„, $$-kx_1$$ì€ ìœ„ì¹˜ì— ë¹„ë¡€í•˜ëŠ” ìŠ¤í”„ë§ì˜ í˜,
  - $$c\dot{x_1}$$ì€ ì†ë„ì— ë¹„ë¡€í•˜ëŠ” damping force (ìš´ë™ ì—ë„ˆì§€ ê°ì‡ ì‹œí‚¤ëŠ” í˜)
- State-Space Formulation
  - State vector $$x \in \mathbb R^n$$ : ì‹œìŠ¤í…œì˜ ë‚´ë¶€ ìƒíƒœ ë³€ìˆ˜
  - Input vector $$u\in \mathbb R^m$$ : ì‹œìŠ¤í…œì— ëŒ€í•œ ì œì–´ ë˜ëŠ” ì™¸ë¶€ ì…ë ¥
  - Output vector $$y \in \mathbb R^p$$ : ê´€ì‹¬ ìˆëŠ” ì¸¡ì • ê°€ëŠ¥í•œ ì–‘
  - System dynamics : ì¼ì°¨ ë¯¸ë¶„ ë°©ì •ì‹ìœ¼ë¡œ í‘œí˜„ $$\dot{\mathbf x}=\mathbf A \mathbf x+\mathbf B \mathbf u$$
    - $$\mathbf x=\left[x_1, \dot{x}_1\right]^T$$ëŠ” state vector, $$\mathbf u$$ëŠ” input,
    - $$\mathbf A\in\mathbb R^{n \times n}$$ëŠ” dynamic matrix $$\mathbf A=\left[\begin{array}{cc}
      0 & 1 \\
      -\frac{k}{m} & -\frac{c}{m}
      \end{array}\right]$$
    - $$\mathbf B \in \mathbb R^{n \times m}$$ì€ input matrix $$\mathbf B=\left[\begin{array}{c}
      0 \\
      \frac{1}{m}
      \end{array}\right] \dot{\mathbf x}$$
    - Output equation : $$\mathbf{y}=\mathbf{C x}+\mathbf{D u}$$
      - $$\mathbf{C} \in \mathbb{R}^{p \times n}$$ëŠ” output or sensor matrix
      - $$\mathbf{D} \in \mathbb{R}^{p \times m}$$ëŠ” feedthrough matrix

### 2.2. State Space Model

- Definition
  - Discrete-time dynamical system : 
    - $$x(t+1)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t=0,1,2,$$...
    - $$x(t) \in \mathbb{R}^n$$ : tì‹œì ì—ì„œ state
    - $$u(t) \in\mathbb{R}^p$$ : control variables
    - $$y(t) \in\mathbb{R}^k$$â€‹ : specific outputs of interest
  - Continuous-time model
    - $$\frac{d}{d t} x(t)=A x(t)+B u(t), \quad y(t)=C x(t)+D u(t), \quad t \geq 0$$.

![ê·¸ë¦¼1](/assets/img/mamba/Mamba360/fig3.png)

- Model Formulation
  - Complexity ë•Œë¬¸ì— Multi-head self-attention ëŒ€ì‹  SSM ì‚¬ìš©
  - Continuous-time Latent State spaceëŠ” linear ordinary differential equationìœ¼ë¡œ í‘œí˜„
    - $$\begin{aligned} \dot{x}(t) & =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
      y(t) & =\boldsymbol{C} x(t)+\boldsymbol{D} u(t) \end{aligned}$$, 
    - evolution parameter $$A \in \mathcal{R}^{N \times N}$$
    - projection parameter $$B \in \mathcal{R}^{N \times 1} \text { and } C \in \mathcal{R}^{N \times 1}$$
- Discrete-time SSM
  - continuous parameters $$A, B, C$$ ë¥¼ discreteí•˜ê²Œ ë°”ê¾¸ê¸° ìœ„í•´ time-scale parameter $$\Delta$$ ì‚¬ìš©
  - ì¦‰ $$\bar{A}=f_A(\Delta, A), \bar{B}=f_B(\Delta, A, B)$$â€‹
  - $$\begin{array}{lll}
    x_k=\overline{\boldsymbol{A}} x_{k-1}+\overline{\boldsymbol{B}} u_k & \overline{\boldsymbol{A}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1}(\boldsymbol{I}+\Delta / 2 \cdot \boldsymbol{A}) & \\
    y_k=\overline{\boldsymbol{C}} x_k & \overline{\boldsymbol{B}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1} \Delta \boldsymbol{B} & \overline{\boldsymbol{C}}=\boldsymbol{C}\end{array}$$â€‹.
  - ì›ë˜ëŠ” ìœ„ì²˜ëŸ¼ ìƒê²¼ìŒ
- Convolutional Kernel Representation
  - í•˜ì§€ë§Œ ìœ„ ì‹ì€ sequential nature ë•Œë¬¸ì— trainableí•˜ì§€ ì•ŠìŒ
  - ê·¸ë˜ì„œ ì•„ë˜ì²˜ëŸ¼ continuous convolutionì„ ì‚¬ìš©
  - $$\begin{array}{lll}
    x_0=\overline{\boldsymbol{B}} u_0 & x_1=\overline{\boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{B}} u_1 & x_2=\overline{\boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{B}} u_2 \\
    y_0=\overline{\boldsymbol{C} B} u_0 & y_1=\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_0+\overline{\boldsymbol{C} B} u_1 & y_2=\overline{\boldsymbol{C} \boldsymbol{A}}^2 \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_1+\overline{\boldsymbol{C} B} u_2
    \end{array}$$.
  - vectorizeí•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ
  - $$\begin{aligned}
    y_k & =\overline{\boldsymbol{C A}}^k \overline{\boldsymbol{B}} u_0+\overline{\boldsymbol{C A}}^{k-1} \overline{\boldsymbol{B}} u_1+\cdots+\overline{\boldsymbol{C} \boldsymbol{A B}} u_{k-1}+\overline{\boldsymbol{C} \boldsymbol{B}} u_k \\
    y & =\overline{\boldsymbol{K}} * u \\
    \overline{\boldsymbol{K}} \in \mathbb{R}^L: & =\mathcal{K}_L(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}}):=\left(\overline{\boldsymbol{C}}^i \overline{\boldsymbol{B}}\right)_{i \in[L]}=\left(\overline{\boldsymbol{C B}}, \overline{\boldsymbol{C} \boldsymbol{A B}}, \ldots, \overline{\boldsymbol{C}}^{L-1} \overline{\boldsymbol{B}}\right) .
    \end{aligned}$$.

## 3. Recent Advances in State Space Models

- Transformerì˜ limitations :
  - Computational Complexity
  - Large Memory Requirements : for storing embeddings and intermediate actiavations
  - Fixed Sequence Length : du to positional embeddings
  - Attention Mechanism Scalability : quadratic scaling with input length
  - Lack of Causality in Standard Attention : not inherently capture causality
- SSMì˜ categorization : ì–´ë–»ê²Œ long sequenceë¥¼ ë‹¤ë£° ê²ƒì¸ê°€
  - Structured SSMs : based on S4 and variants
  - Recurrent SSMs : based on RNNs and variants
  - Gated SSMs : leveraging gating techniques
  - Miscellaneous SSMs : ê¸°íƒ€ ë‹¤ì–‘í•œ ë°©ë²•ë“¤

### 3.1. Structured SSMs

- S4, HiPPO, H3, Liquid-S4 ë“±...

- long-range dependencyë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²• ì‚¬ìš© : 

  - polynomial projection operators

  - multi-input multi-output systems

  - and convolutional kernels

### 3.1.1. Structured State Space Sequence (S4)

- Higher-Order Polynomial Project Operator (HiPPO)
  - State and input transition matricesë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ memorize
-  Diagonal Plus Low-Rank Parametrization
  - SSM matrix (A)ì˜ rankë¥¼ ë‚®ê²Œ í•´ì„œ diagonalizability and stability ë³´ì¥
- Efficient (convolutional) Kernel Computation
  - FFTì™€ iFFT ì‚¬ìš©í•´ì„œ complexityë¥¼ $$ğ‘‚(ğ‘ log(ğ‘))$$ë¡œ ë§Œë“¬

### 3.1.2. High-Order Polynomial Projection Operators (HiPPO)

- S4ì— ì‚¬ìš©ëœ í–‰ë ¬ì˜ ìˆ˜í•™ì ì¸ í•´ì„ì„ ì œê³µ
- 4ê°€ì§€ì˜ ë³€í˜•ì„ ì‚¬ìš©í•˜ëŠ”ë°,
  - the truncated Fourier basis polynomial (Hippo-FouT)
  - based on Lagurre polynomials(LagT)
  - based on Legendre polynomials(LegT)
  - based on Legendre polynomials with a sliding window(LegS)

### 3.1.3. Hungry Hungry HiPPO (H3)

- SSMì—ì„œì˜ 2ê°œì˜ challenges
  - ì²«ì§¸, difficulty in recalling earlier tokens
    - ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ì´ì „ í† í°ì„ ê¸°ì–µí•˜ëŠ” ë° ì–´ë ¤ì›€
  - ë‘˜ì§¸, difficult in comparing the tokens across different sequences
    - ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ì—ì„œ í† í°ì„ ë¹„êµí•˜ëŠ” ë° ì–´ë ¤ì›€
- ê·¹ë³µí•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì˜ 3ê°€ì§€ í•µì‹¬ ìš”ì†Œ
  - Multiplicative Interactionsê°€ ìˆëŠ” Stacked SSMs
    -  stacking two SSMs with multiplicative interactions between their input and output projections
  - í•™ìŠµ íš¨ìœ¨ì„±ì„ ìœ„í•œ FlashConv
    - FFTë¥¼ ì‚¬ìš©í•˜ì—¬  training efficiency í–¥ìƒ
  - Scalingì„ ìœ„í•œ State-Passing
    - effectively splits the input into the largest possible chunks that can fit

### 3.1.4. Global Convolution

- ì›ë˜ëŠ” inputë§Œí¼ ê¸´ conv kernelì„ hidden state matrixì— ê³±í–ˆëŠ”ë° - ë¶ˆì•ˆì •í•¨
- ì´ conv kernelì„ parametrizingí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆ
- ì¼ë°˜ì ìœ¼ë¡œ conv kernelì€ FFTë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ëŠë¦´ ìˆ˜ê°€ ìˆì–´ì„œ IO-aware algorithm ì‚¬ìš©

### 3.1.7. LDStack

- RNNì´ ë‹¤ì¤‘ ì…ë ¥ ë‹¤ì¤‘ ì¶œë ¥(MIMO)  Linear Dynamical System(LDS)ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆìŒ
- ì´ ë•Œ Parallel scanì´ ì‚¬ìš©ë¨
- ì¦‰  Single Input Multiple Outputs (SIMO) LDSë¥¼ í•©ì³ì„œ  MIMO LDSë¥¼ approximate
  - essential characteristicsë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ê³„ì‚°ì€ simpleí•´ì§
- LDSë¥¼ time-varying state space modelsë¡œ ë³¼ ìˆ˜ ìˆìŒ

### 3.1.8 S5

- RNNì„ ë‹¤ì¤‘ ì…ë ¥ ë‹¤ì¤‘ ì¶œë ¥ ì„ í˜• ë™ì  ì‹œìŠ¤í…œ(LDS)ìœ¼ë¡œ ëª¨ë¸ë§í•œ LDStackì„ state space models (SSMs)ìœ¼ë¡œ í™•ì¥
- LDStackê³¼ ë‹¬ë¦¬, S5 ê³„ì¸µì€ ì—¬ëŸ¬ ì…ë ¥ ë° ì¶œë ¥ì„ ë™ì‹œì— ì²˜ë¦¬

## 3.2. Gated SSMs

- FFT ì—°ì‚° ìµœì í™”ë¥¼ ìœ„í•´ gating unitsë¥¼ ì‚¬ìš©
- Toepliz NNì€ position-encoded Toeplitz matrixë¡œ token mixing
- MambaëŠ” gated MLPë¡œ SSMì˜ compoutational inefficiency ê·¹ë³µí•˜ê³ ì í•¨
- (ë¬´ìŠ¨ ë§ ?) ë” ì½ì–´ë³´ì

### 3.2.3. Toeplitz Neural Network (TNN)

- Transformerì˜ **attention-mechanism**ê³¼ **positional embedding**ì„ ê°œì„ 
- position-encoded Toeplitz matrixë¥¼ ì‚¬ìš©í•˜ì—¬ token-pair ê´€ê³„ íŒŒì•…
  - space-time complexityë¥¼ $$O(NlogN)$$ìœ¼ë¡œ ì¤„ì„
  - Relative Position Encoder (RPE)ë¡œ ìƒëŒ€ì  ìœ„ì¹˜ë¥¼ ìƒì„±í•´ì„œ parametersê°€ input lengthì— ë…ë¦½ì ì´ê²Œ í•¨

### 3.2.4. Mamba

- Transformerì˜ quadratic computational and memory complexityì— ì£¼ëª©
- íŠ¹íˆ SSMì€  addressing tasks (selective copying, induction head)ì—ì„œ ë¹„íš¨ìœ¨ì ì´ì—ˆìŒ
- Mambaê°€ ì´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì€ :
  - novel parametrization approach for SSMs based on input characteristics
  - incorporating a simple selection mechanism
  - efficient hardware-aware algorithm based on selective scan
  - gated technique to reduce the dimensionality of global kernel operations
  - combine gated MLP[93] with the SSM module

## 4. Applications of State Space Models

### 4.1. Language Domain (long sequence)

- ì›ë˜ëŠ” Transformer ë§ì´ ì¼ëŠ”ë° $$O(N^2)$$ quadratic complexity $$\to$$ long sequence ë¶ˆê°€ëŠ¥
- ê·¸ë˜ì„œ  State Space Models (SSMs)ì´ ë“±ì¥
  - input dataë¥¼ fixed-size latent stateì— í‘œí˜„
  - í•˜ì§€ë§Œ ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ capability to retrieve and copyì—ì„œ trade-off

![ê·¸ë¦¼1](/assets/img/mamba/Mamba360/table2.png)

### 4.2. Vision domain

- Vision Mambaë‚˜ SiMBAì™€ ê°™ì€ Vision-specific Mamba
  - utilize bidirectional and visual state space models
- SiMBA
  - sequence length and channel dimensionsì´ ê¼­ perfect square dimensionsì´ ì•„ë‹ˆì–´ë„ ë¨
  - pyramid version of the transformer architecture (ì„±ëŠ¥ í–¥ìƒ)

![ê·¸ë¦¼1](/assets/img/mamba/Mamba360/table3.png)

### 4.7. Time Series Domain

- ì˜›ë‚ ì—ëŠ” ARIMA ì“°ë‹¤ê°€ Transformer ë“±ì¥í•˜ë©´ì„œ variantsê°€ ë§ì´ ë‚˜ì˜´
  - Informer, FEDFormer, PatchTST...
  - í•˜ì§€ë§Œ ì—¬ì „íˆ attention complexity ë•Œë¬¸ì— long-range dependency ëª»ì¡ìŒ
- ê·¸ë˜ì„œ SSM ëª¨ë¸ì¸ Timemachine, SiMBA, MambaMix ë“±ì¥

![ê·¸ë¦¼1](/assets/img/mamba/Mamba360/table11.png)

![ê·¸ë¦¼1](/assets/img/mamba/Mamba360/table14.png)

## 6. Conclusion

- SSMì€ 3ê°€ì§€ ë²”ì£¼ë¡œ ë¶„ë¥˜ ê°€ëŠ¥ (structured, gated, and recurrent)
- ì•„ì§ Transformerê°€ ë” ì˜í•˜ëŠ” ì˜ì—­ì´ ìˆê¸´ í•˜ì§€ë§Œ (ë§¥ë½ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‘ì—… ë“±)
  - SiMBAëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì™€ Mamba ì•„í‚¤í…ì²˜ë¥¼ ê²°í•©í•´ì„œ Time seriesì—ì„œ SOTA
- SSMì„ large networkë¡œ ì•ˆì •ì ìœ¼ë¡œ í™•í•˜ëŠ” ê²ƒì´ ì•„ì§ í•´ê²°ë˜ì§€ ì•Šì€ ë¬¸ì œ