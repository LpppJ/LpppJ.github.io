---
layout: post
related_posts:
  _
title: 
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# The Law of Total Probability / Total Expectation / Total Variance / Total Covariance (Gumbel-Max trick, ...)

- The Law of Total Probability
  $$P(A)=\sum P\left(A \mid B_n\right) P\left(B_n\right)$$
- The Law of Total Expectation
  $$\mathrm{E}(X)=\mathrm{E}(\mathrm{E}(X \mid Y))$$
- The Law of Total Variance
  $$\operatorname{Var}(Y)=\mathrm{E}[\operatorname{Var}(Y \mid X)]+\operatorname{Var}(\mathrm{E}[Y \mid X])$$
- The Law of Total Covariance
  $$\operatorname{cov}(X, Y)=\mathrm{E}(\operatorname{cov}(X, Y \mid Z))+\operatorname{cov}(\mathrm{E}(X \mid Z), \mathrm{E}(Y \mid Z))$$

## The Law of Total Probability

$$P(A)=\sum P\left(A \mid B_n\right) P\left(B_n\right)$$
- Example) **Gumbel-Max trick**
- 일반적으로 계산이 어려운 normalizing constant $$\sum \exp(\theta)$$를 계산하지 않으면서 categorical distribution에서 sample을 생성하는 technique
- 다음과 같은 상황을 가정한다.
- $$X \sim Cat(\pi)$$는 $$1, ..., K$$중 하나의 class에 속하는 R.V (각 class에 속할 확률은 $$\pi_k$$)

$$\begin{array}{c|ccccc} \hline k & 1 & 2 & \cdots & K-1 & K \\ \hline P(X=k) & \pi_1 & \pi_2 & \cdots & \pi_{K-1} & \pi_K \\ \hline \end{array}$$

- Probability $$\pi$$는 다음과 같이 계산한다. $$\pi_i=\frac{\exp \left(\theta_i\right)}{\sum_{j \in[K]} \exp \left(\theta_j\right)} \quad \text { where }[K]=\{1,2, \ldots, K\}$$

- 이 경우 normalizaing constant $$\sum_{j \in[K]} \exp \left(\theta_j\right)$$를 계산해야 한다.
- Gumbel-Max trick은 normalizaing constant를 계산하지 않는다.

- **$$X$$대신 $$Y=\arg \max _{j \in[K]}\left\{\theta_j+G_j\right\} \sim \operatorname{Cat}(\pi)$$을 사용하자 !**

  - $$G_1, ..., G_k$$는 i.i.d standard Gumbel R.V
  - 즉 $$G$$의 CDF가 $$F(x)=e^{-e^{-x}} \text { for all } x \in \mathbb{R}$$
  - **$$P(Y=i)=\pi_i \text { for each } i \in[K] \text {. }$$를 보이면 된다 !**

- Proof

: $$\begin{aligned}
    P(Y=i) & =P\left(\theta_i+G_i \geq \max _{j \in[K] \backslash\{i\}}\left\{\theta_j+G_j\right\}\right)\\&=\mathbb{E}\left[P\left(\theta_i+G_i \geq \max _{j \in[K] \backslash\{i\}}\left\{\theta_j+G_j\right\} \mid G_i\right)\right] \\ & \text{by the law of total probability}\\
    & =\int_{-\infty}^{\infty} f_{G_i}(x) P\left(\theta_i+G_i \geq \max _{j \in[K] \backslash\{i\}}\left\{\theta_j+G_j\right\} \mid G_i=x\right) d x \\
    &=\int_{-\infty}^{\infty} f_{G_i}(x) P\left(\theta_i+x \geq \max _{j \in[K] \backslash\{i\}}\left\{\theta_j+G_j\right\}\right) d x \\ &=\int_{-\infty}^{\infty} f_{G_i}(x) A\ dx
    \end{aligned}$$

  where $$\begin{aligned}A &= P\left(\theta_i+x \geq \max _{j \in[K] \backslash\{i\}}\left\{\theta_j+G_j\right\}\right)\\ & =\prod_{j \in[K] \backslash\{i\}} P\left(G_j \leq x+\theta_i \theta_j\right)\\&=\prod_{j \in[K] \backslash\{i\}} e^{-e^{-x-\theta_i+\theta_j}} \\ &=e^{-B} \end{aligned}$$

  where $$\begin{aligned}B &= e^{-x+\theta_i+\theta_1}+e^{-x+\theta_i+\theta_2}+...\\&= e^{-x-\theta_1} \sum_{j \in[K] \backslash\{i\}} e^{\theta_j} \\ &=e^{-x} \frac{\sum_{j \in[K] \backslash\{i\}} e^{\theta_j}}{e^{-\theta_i}} \\ &= e^{-x}\frac{1-\pi_i}{\pi_i}\end{aligned}$$

  - therefore, $$\begin{aligned} A &= e^{-B} = e^{-e^{-x} \times \frac{1-\pi_i}{\pi_i}}\end{aligned}$$​

  - finally,  $$\begin{aligned} P(Y=i) & =\int_{-\infty}^{\infty} f_{G_i}(x) e^{-e^{-x} \times \frac{1-\pi_i}{\pi_i}} d x \\& =\int_{-\infty}^{\infty} e^{-x-e^{-x}} e^{-e^{-x} \times \frac{1-\pi_i}{\pi_i}} d x \\ & =\int_{-\infty}^{\infty} e^{-x-e^{-x} \pi_i^{-1}} d x \\ & =\pi_i \int_{-\infty}^{\infty} e^{-\left[\left(x-\log \frac{1}{\pi_i}\right)+e^{\left.-\left(x-\log \frac{1}{\pi_i}\right)\right]}\right.} d x=\pi_i \\ &\because \int e^{-(x+A+e^{(-x+A)})}dx = 1\end{aligned}$$

## The Law of Total Expectation

$$\mathrm{E}(X)=\mathrm{E}(\mathrm{E}(X \mid Y))$$
- Proof

: $$\begin{aligned}
    \mathrm{E}[\mathrm{E}(X \mid Y)] & =\mathrm{E}\left[\sum_{x \in \mathcal{X}} x \cdot \operatorname{Pr}(X=x \mid Y)\right] \\
    & =\sum_{y \in \mathcal{Y}}\left[\sum_{x \in \mathcal{X}} x \cdot \operatorname{Pr}(X=x \mid Y=y)\right] \cdot \operatorname{Pr}(Y=y) \\ & =\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} x \cdot \operatorname{Pr}(X=x \mid Y=y) \cdot \operatorname{Pr}(Y=y) \\ & =\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} x \cdot \operatorname{Pr}(X=x, Y=y) \\& =\sum_{x \in \mathcal{X}} x \sum_{y \in \mathcal{Y}} \operatorname{Pr}(X=x, Y=y)\\  & =\sum_{x \in \mathcal{X}} x \cdot \operatorname{Pr}(X=x) \\ & =\mathrm{E}(X) \end{aligned}$$

- Example)
  - $$\begin{aligned} X \mid P &\sim \operatorname{Binomial}(n, P), \\ P &\sim \operatorname{Beta}(\alpha, \beta) \end{aligned}$$이면
  - $$\mathbb{E}[X]=\mathbb{E}[\mathbb{E}(X \mid P)]=\mathbb{E}[n P]=n \frac{\alpha}{\alpha+\beta}$$이다.

## The Law of Total Variance

- $$\operatorname{Var}(Y)=\mathrm{E}[\operatorname{Var}(Y \mid X)]+\operatorname{Var}(\mathrm{E}[Y \mid X])$$ (EVVE)

- Proof

: $$\begin{aligned}
  Var[X]&=\mathbb{E}\left[(X-\mathbb{E}[X])^2\right]\\&=\mathbb{E}\left[(X-\mathbb{E}[X \mid Y]+\mathbb{E}[X \mid Y]-\mathbb{E}[X])^2\right]\\& =\mathbb{E}\left[(X-\mathbb{E}[X \mid Y])^2\right]+\mathbb{E}\left[(\mathbb{E}[X \mid Y]-\mathbb{E}[X])^2\right] \\&\quad +2 \mathbb{E}([X-\mathbb{E}(X \mid Y)]\left[\mathbb{E}(X \mid Y)-\mathbb{E}(X)\right])\\&=\mathbb{E}\left[(X-\mathbb{E}[X \mid Y])^2\right]+\mathbb{E}\left[(\mathbb{E}[X \mid Y]-\mathbb{E}[X])^2\right] \\ &=\mathbb{E}[Var(X\mid Y)] + Var[\mathbb{E}(X \mid Y)]\\ & \because \mathbb{E}[X-\mathbb{E}(X \mid Y)] = \mathbb{E}[\mathbb{E}[X-\mathbb{E}(X \mid Y)]\mid Y]=0
  \end{aligned}$$

- Example)
  - $$\begin{aligned} X \mid Y &\sim \operatorname{Binomial}(n, P), \\ Y &\sim \operatorname{Beta}(\alpha, \beta) \end{aligned}$$이면

  - $$\begin{aligned}Var(X)&=\mathrm{E}[\operatorname{Var}(X \mid Y)]+\operatorname{Var}(\mathrm{E}[X \mid Y])\end{aligned}$$​

: $$\begin{aligned}
  \mathbb{E}[\operatorname{Var}(X \mid Y)] & =n\left(\mathbb{E}[Y]-\mathbb{E}\left[Y^2\right]\right)\\&=n\left(\frac{\alpha}{\alpha+\beta}-\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}-\frac{\alpha^2}{(\alpha+\beta)^2}\right) \\ & =n \frac{\alpha \beta}{(\alpha+\beta)(\alpha+\beta+1)} \\ \operatorname{Var}[\mathbb{E}(X \mid Y)]&=\operatorname{Var}[n P]=n^2 \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \\ \therefore \operatorname{Var}[X]&=n \frac{\alpha \beta(\alpha+\beta+n)}{(\alpha+\beta)^2(\alpha+\beta+1)}
  \end{aligned}$$​

## The Law of Total Variance

- $$\operatorname{cov}(X, Y)=\mathrm{E}(\operatorname{cov}(X, Y \mid Z))+\operatorname{cov}(\mathrm{E}(X \mid Z), \mathrm{E}(Y \mid Z))$$.

- $$\begin{aligned}\operatorname{Cov}\left(X_i, X_j\right) &\leq \sqrt{\operatorname{Var}\left(X_i\right) \operatorname{Var}\left(X_j\right)} \leq \operatorname{Var}\left(X_i\right) / 2+\operatorname{Var}\left(X_j\right) / 2 \\ &\text{by }x y \leq x^2 / 2+y^2 / 2 \end{aligned}$$. 