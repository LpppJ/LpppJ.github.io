---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2408.15997)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)

## Abstract

- Transformer는 높은 계산 비용(quadratic computational cost)이 문제,
- Mamba는 longterm forecasting에서 성능이 effective하지 않음 (potential information loss 때문)
- 본 논문에서 제시하는 **Mixture of Universals (MoU)**의 components:
  - **Mixture of Feature Extractors (MoF)**: adaptive patch representations
    - (for **short**-term dependency)
  - **Mixture of Architectures (MoA)**:  Mamba, FeedForward, Convolution, and Self-Attention 연결한 것
    - (for  **long**-term dependency )

## 1. Introduction

- **PatchTST**:  patch embedding에서 uniform linear transformations 사용 $$\to$$ varying semantic contexts 손실
  - Vision에서 Dynamic Convolution, Conditional Convolution (for informative representations)등장했지만
  - Time series에서는 poor performances

- **Transformer**: 장기 의존성은 잘 처리하지만 계산 비용이 큼
- **Mamba**: 효율적이지만 정보 손실로 인해 장기 예측에서 성능이 떨어질 수 있음

![그림1](/assets/img/Mamba/MoU/fig2.png)

- **MoF(Mixture of Feature Extractors)**:
  - multiple sub-extractors로 구성되어 있고
  - sparse activation을 사용해서 input patch에 적합한 sub-extractor만 활성화
  - learning of diverse contexts and **minimal parameter increase** !

- **MoA(Mixture of Architectures)** Mamba에서 시작해 국소적인 관점에서 전역적인 Self-Attention 계층으로 확장하며 장기 의존성을 효율적으로 캡처하는 계층적 구조.
  -  hierarchical structure를 가진 encoder
    - **Mamba layer** that selects and learns key dependencies using a Selective State-Space Model (SSM). 
    - **FeedForward transition layer** and a **Convolution-layer** that broadens the receptive field to capture longer dependencies.
    - **Self-Attention layer** integrates information globally to fully capture long-term dependencies

## 2. Approach

### 2.1 Problem Setting and Model Structure

- $$\mathbf{X}_{\text {input }}=\left[\mathbf{X}^1, \mathbf{X}^2, \ldots, \mathbf{X}^M\right] \in \mathbb{R}^{M \times L}$$ where $$\mathbf{X}^i=\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_L\right] \in \mathbb{R}^L$$
- $$\hat{\mathbf{X}}_{\text {output }}=\left[\hat{\mathbf{X}}^1, \hat{\mathbf{X}}^2, \ldots, \hat{\mathbf{X}}^M\right] \in \mathbb{R}^{M \times T}$$.
- Goal : learning a function $$\mathcal{F}: \mathbf{X} \rightarrow \hat{\mathbf{X}}$$

- Overall process는 아래와 같음. 
  - 처음에는 raw time series $$\mathbf{X} \in \mathbb{R}^L$$에서 시작 (variate independence setting이라서 channel =1)
  - $$N$$개의 patch tokens를 만듬 : 
    - $$\mathbf{X}_p=\operatorname{Patch}(\mathbf{X}) \in \mathbb{R}^{N \times P}$$ with fixed size $$P$$, stride $$S$$
  - **MoF** module에 넣어서 adaptive representations를 얻음 :
    - $$\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}$$.
    - MoF는 parameters를 조절하면서 계산 비용 최적화 (**2.2**에서 자세히 설명)
  - 이제 **MoA**에 넣어서 long-term dependencies (among tokens) 잡음
    - $$\mathbf{X}_{\mathrm{rep}}=\operatorname{MoA}\left(\mathbf{X}_{\mathrm{rep}}\right)\in \mathbb{R}^{N \times D}$$.
    - MoA는 long-term encoder based on the Mixture of Architectures (**2.3**에서 자세히 설명)
  - 마지막으로 linear projector에 넣어서 final prediction 얻음
    - $$\hat{\mathbf{X}}=\mathbf{P}\left(\operatorname{Flatten}\left(\mathbf{X}_{\mathrm{rep}^{\prime}}\right)\right)\in \mathbb{R}^{M \times T}$$.

### 2.2 Mixture of Feature Extractors

- MoF의 목적은 **patch의 representative embedding**을 만드는 것

![그림1](/assets/img/Mamba/MoU/fig3.png)

- Sub-extractors $$\left\{F_1, F_2, \ldots, F_c\right\}$$가 있고 각각은 independent linear mapping
- MoF를 통과하면 $$\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}$$를 얻음
  - where $$R_i(\cdot)$$는  input-relevant router (sub-extractor를 sparse하게 활성화)
    - 즉 $$R\left(\mathbf{X}_p\right)_i=\operatorname{Softmax}\left(\operatorname{Top}_{\mathrm{k}}\left(H\left(\mathbf{X}_p\right)_i, k\right)\right)$$ 
      - $$\operatorname{Softmax}(\cdot)$$는  $$\operatorname{Topk}(\cdot, k)$$에 의해 선택된 상위 $$k$$개의 점수를 정규화
      - $$H\left(X_p\right)=\left[H\left(\mathbf{X}_p\right)_1, H\left(\mathbf{X}_p\right)_2, \ldots H\left(\mathbf{X}_p\right)_c\right]$$ 는 sub-extractors의 score vector
      - 여기서 $$H\left(\mathbf{X}_p\right)_i=\left(\mathbf{X}_p \cdot W_g\right)_i+\text { SN } \cdot \text { Softplus }\left(\left(\mathbf{X}_p \cdot W_{\text {noise }}\right)_i\right)$$
        - where $$W_g$$는 linear layer이고 두번째 항은 load balancin을 위한 tunable noise를 넣는 것
- 이렇게 하면 patch token를 $$c$$개의 서로 다른 patterns들의 조합으로 분할할 수 있음
  - each pattern은 최적의 sub-extractors에 의해 뽑힌 것이니
  - most representative embedding (for patches with divergent context)라고 할 수 있음

### 2.3. Mixture of Architectures

- MoA의 목적은 **comprehensive long-term dependencies**를 모델링하는 것

![그림1](/assets/img/Mamba/MoU/fig4.png)

- **Mamba, FeedForward, Convolution**, and **Self-Attention layer**로 구성되어
  - 각각이 다른 관점에서 long-term dependencie를 학습
  - **gradually expanding perspective**를 통해 effectiveness and efficiency 둘 다 챙김
- **Mamba-layer in Time Series** : **relevant data를 선택하고 time-variant dependencies를 학습하는 곳**
  - input은 MoF의 output $$\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}$$인데 $$x$$로 denote
  - $$\begin{gathered}
    \boldsymbol{x}^{\prime}=\sigma(\text { Conv1D }(\text { Linear }(\boldsymbol{x}))) \\
    \boldsymbol{z}=\sigma(\text { Linear }(\boldsymbol{x}))
    \end{gathered}$$, where $$\sigma$$는 $$SiLU$$ activation
  - 다음으로 $$\begin{gathered}
    \boldsymbol{y}^{\prime}=\operatorname{Linear}\left(\operatorname{SelectiveSSM}\left(\boldsymbol{x}^{\prime}\right) \otimes \boldsymbol{z}\right) \\
    \boldsymbol{y}=\operatorname{LayerNorm}\left(\boldsymbol{y}^{\prime}+\boldsymbol{x}\right)
    \end{gathered}$$, where $$\begin{gathered}
    \text { SelectiveSSM }\left(\boldsymbol{x}_t^{\prime}\right)=\boldsymbol{y}_t \\
    \boldsymbol{y}_t=C \boldsymbol{h}_t, \quad \boldsymbol{h}_t=\bar{A} \boldsymbol{h}_{t-1}+\bar{B} \boldsymbol{x}_t^{\prime}
    \end{gathered}$$
    - $$h_t$$는 latent state, $$y_t$$는 output representation
    - The discrete matrices는 $$\begin{gathered}
      B_t=S_B\left(\boldsymbol{x}_t^{\prime}\right), \quad C_t=S_C\left(\boldsymbol{x}_t^{\prime}\right) \\
      \Delta_t=\operatorname{softplus}\left(S_{\Delta}\left(\boldsymbol{x}_t^{\prime}\right)\right)
      \end{gathered}$$
      - $$S$$들은 linear layers이고, $$\begin{gathered}
        f_A\left(\Delta_t, A\right)=\exp \left(\Delta_t A\right) \\
        f_B\left(\Delta_t, A, B_t\right)=\left(\Delta_t A\right)^{-1}\left(\exp \left(\Delta_t A\right)-I\right) \cdot \Delta B_t \\
        \bar{A}_t=f_A\left(\Delta_t, A\right), \quad \bar{B}_t=f_B\left(\Delta_t, A, B_t\right)
        \end{gathered}$$
  - 좀 복잡한데 위에 있는 fig4 보는 것이 낫겠다.
- **FeedForward-layer** : **non-linearity를 강화하는 곳**
  - $$\boldsymbol{x}_{\mathrm{ffn}}=\text { FeedForward }\left(\boldsymbol{y}_t ; w_1, \sigma, w_2\right)$$, where
    - $$w_1$$ and $$w_2$$ are parameters, $$\sigma$$ is activation function

- **Convolution-layer** : **MoA의 receptive field를 넓히는 곳**
  - partial long-term dependencies를 담고 있는 tokens끼리의 정보 교환을 촉진
  - $$\boldsymbol{x}_{\mathrm{conv}}= \operatorname{Conv}\left(\boldsymbol{x}_{\mathrm{ffn}} ; \mathbf{k}, s, p, c_{\mathrm{out}}\right)$$,
    - where $$\mathbf{k}$$ is the kernel size, $$s$$ is the stride, $$p$$ is the padding,
    - and $$c_{\text {out }}$$ is the number of output channels
- **Self-Attention-layer** : **global perspective에서 포괄적인 long-term dependencies를 파악하는 곳**
  - : $$\begin{aligned} & x_{\mathrm{att}}=\operatorname{FeedForward}(\operatorname{Attention}(Q, K, V)) \\ & \operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V \\ & Q=x_{\text {conv }} W_Q, K=x_{\text {conv }} W_K, V=x_{\text {conv }} W_V\end{aligned}$$
- **Partial-to-global Design for Time Series**
  - **gradually expanding perspective**라는 말의 의미는
    - Mamba layer : Selective SSM을 사용하여 시간적으로 변화하는 의존성을 처리
    - FeedForward layer : 이러한 부분 의존성을 더 복잡한 표현으로 전환
    - Convolution layer : 수용 영역을 확장하여 보다 넓은 시간적 관계를 학습
    - Self-Attention layer : 로컬화된 정보를 통합하여 장기 의존성에 대한 포괄적인 이해
  - 를 거치면서 선택적으로 일부 의존성에 초점을 맞춘 후, 이를 점차 확장하여 전체적(global) 관점으로 발전시킨다는 뜻

### 2.4. Computational Complexity and Model Parameter

- Tokens $$T$$개가 주어졌을 때 top-$$k$$ experts를 선택한다고 하면
  - $$C_{\mathrm{MOU}}=\underbrace{k T \times d^2}_{\text {MoF }}+\underbrace{T \times d^2}_{\text {Mamba }}+\underbrace{T \times d^2}_{\text {FFN }}+\underbrace{k T d^2}_{\text {Conv }}+\underbrace{T^2 \times d+T \times d^2}_{\text {Transformer }}$$, 
    - where $$\mathbf{k}$$ is the kernel size in the convolutional layer,
    - $$d$$ is the dimension of the vector representations
  - Transformer 블록을 제외하면 선형적인 복잡도

![그림1](/assets/img/Mamba/MoU/fig1.png)

## 3. Experiments

### 3.1. Dataset

- 7 commonly used datasets 
- Pass

### 3.2. Baselines and Setup

- Mamba-based Models (S-Mamba)
- Linear-based Models (D-Linear)
- Convolution-based Models (ModernTCN)
- Transformer-based Models (PatchTST)

### 3.3. Main Results

![그림1](/assets/img/Mamba/MoU/table1.png)

### 3.4. Ablation Study

![그림1](/assets/img/Mamba/MoU/table2.png)

- 3개의 adaptive feature extractors를 비교했을 때 MoF(in MoU)가 가장 좋았으며
  - Dyconv가 parameters 수를 크게 증가시키기 때문에 time series patch와 같은 작은 데이터셋에는 적합하지 않음
  - SE-M의  calibration strategy는 representation을 normalized gating vector에 곱하는 방식이라서 다양한 컨텍스트 정보를 처리하는 데에는 한계 
- 특히 MoF가 uniform transformation method (Linear)보다 좋다는 점이 주목할만함

![그림1](/assets/img/Mamba/MoU/table3.png)

- AA, MM, MFA, AAA, MMA, AMM, MAM, AMA, AFM, AFCM

  - 여기서 A, M, F, C는 각각 Self-Attention, Mamba, FeedForward, Convolution
  - 글자의 순서는 레이어의 배치 순서

- M-A 순서(MAM, AMA, MMA) > M-A 순서를 가지지 않은 모델(AMM)

  - Mamba를 Self-Attention 이전에 배치하는 것이 장기 의존성을 캡처하는 데 더 효과적

  - A-M 순서보다 M-A 순서가 장기 의존성 학습에서 더 중요한 역할

- F-C 순서 > F

  - Convolution 레이어가 Mamba 레이어의 수용 영역을 확장하여
  - Mamba 레이어의 부분적 관점과 Self-Attention 레이어의 글로벌 관점을 연결하는 중간 관점을 제공한다고 해석됨

### 3.5. Model Analysis

- Does MoF actually learn contexts within patches?
- What is learned by the layers of MoA? 

![그림1](/assets/img/Mamba/MoU/fig56.png)

## 5 Conclusion

- Mixture of Universals (MoU)
  - Mixture of Feature Extractors (MoF)
    - an adaptive method specifically designed to enhance time series patch representations for capturing short-term dependencies
  - Mixture of Architectures (MoA)
    - hierarchically integrates multiple architectures in a structured sequence to model long-term dependencies from a hybrid perspective