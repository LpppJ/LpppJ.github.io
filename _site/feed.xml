<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-01-08T22:03:07+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">MAT: Integration of Mamba and Transformer for Long-Short Range TSF (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2025-01-08-MAT/" rel="alternate" type="text/html" title="MAT: Integration of Mamba and Transformer for Long-Short Range TSF (Arxiv 2024)" /><published>2025-01-08T00:00:00+09:00</published><updated>2025-01-08T22:03:06+09:00</updated><id>http://localhost:4000/mamba/MAT</id><content type="html" xml:base="http://localhost:4000/mamba/2025-01-08-MAT/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Time series는 sparse semantic features를 가짐
    <ul>
      <li>Transformer만으로 다루기에는 어려움이 있고</li>
      <li>Mamba의 selective input and parallel computing 필요</li>
    </ul>
  </li>
  <li>Mamba and Transformer models의 장단점을 이해하고 combined approach 제안</li>
  <li>the long-range dependency capabilities of <strong>Mamba</strong>
    <ul>
      <li>and the short-range characteristics of <strong>Transformers</strong></li>
      <li>called <strong>MAT</strong></li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>Linear</strong> scalability를 가지는 모델들은 성능이 아쉬움
    <ul>
      <li>SciNet
        <ul>
          <li>relying solely on time points</li>
          <li>obscure context-based choices and overlook long-range dependencies</li>
          <li>그러므로 iTransformer처럼 entire window로 다루는 것이 맞음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Transformer</strong>-based models는 high computational complexity
    <ul>
      <li>Informer, FEDformer,and Autoformer
        <ul>
          <li>mixed-channel approach (2d matrix defined by the <strong>number of channels and the length of histories</strong>)</li>
          <li>beneficial when channels exhibit significant correlations,</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SSM
    <ul>
      <li>handle very long sequences with linear complexity</li>
      <li>context-aware selectivity through hidden attention mechanisms</li>
    </ul>
  </li>
  <li>본 논문에서는
    <ul>
      <li>Mamba coupled with attentionmechanisms called Transformer
        <ul>
          <li>(with multi-scale contexts)</li>
        </ul>
      </li>
      <li><strong>long</strong>-term forecasting capabilities and <strong>short</strong>-range dependencies in MTS data
        <ul>
          <li>while maintaining <strong>linear</strong> scalability and minimal memory usage</li>
        </ul>
      </li>
      <li>bottom-up strategy
        <ul>
          <li>generating contextual cues at two distinct scales through linear mapping</li>
          <li>At each of these levels four MAT modules</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-methodology">3. Methodology</h2>

<ul>
  <li>input sequence \(\mathbf{x}=\left[x_1, \ldots, x_L\right]\) 보고 future values \(\left[x_{L+1}, \ldots, x_{L+T}\right]\) 예측</li>
</ul>

<h3 id="ac-preliminary-transformer-mamba">A~C. Preliminary, Transformer, Mamba</h3>

<p>pass</p>

<h3 id="d-mat">D. MAT</h3>
<p><img src="/assets/img/Mamba/MAT/fig3.png" alt="그림1" /></p>

<ul>
  <li>Four combined Mambas and Transformer
    <ul>
      <li>to extract <strong>long-short</strong> range contextual information
        <ul>
          <li><strong>long</strong>-term forecasting capability of <strong>Mamba</strong></li>
          <li>and <strong>short</strong> range dependency of <strong>Transformer</strong>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Normalizing</strong>(RevIN)</li>
  <li><strong>Embedding</strong> : \(\mathbf{x}^{(1)}=E M B_1\left(\mathbf{x}^{(0)}\right), \mathbf{x}^{(2)}=E M B_2\left(\mathbf{x}^{(1)}\right)\).
    <ul>
      <li>\(E M B_1: \mathbb{R}^{M \times L} \rightarrow \mathbb{R}^{M \times n_1}\) and \(E M B_2: \mathbb{R}^{M \times n 1} \rightarrow \mathbb{R}^{M \times n_2}\)</li>
      <li>\(n_1\) and \(n_2\), are chosen from the set \(\{512,256,128,64,32\}\) s.t. \(n_1&gt;n_2\).</li>
    </ul>
  </li>
  <li>사실 <a href="https://lpppj.github.io/mamba/2024-10-29-timemachine">TimeMachine</a>이랑 비슷한데
    <ul>
      <li><strong>not only</strong> the <strong>long</strong>-term prediction capability of the <strong>Mamba</strong>,</li>
      <li><strong>but also</strong> the <strong>short</strong>-range dependency learned from the <strong>Transformer</strong></li>
      <li>이 구조가 pivotal component in the modern LLM</li>
    </ul>
  </li>
  <li><strong>Output Prediction</strong> : \(\overline{\mathbf{x}}^{(1)}=\operatorname{Proj}_1\left(\overline{\mathbf{x}}^{\left(F_1\right)}\right), \overline{\mathbf{x}}=\operatorname{Proj}_2\left(\overline{\mathbf{x}}^{(1)} \oplus \hat{\mathbf{x}}^{\left(F_2\right)}\right)\)
    <ul>
      <li>\(\text{Proj}_1\):  \(\mathbb{R}^{M \times n_2} \rightarrow \mathbb{R}^{M \times n_1}\) to obtained \(\overline{\mathrm{x}}^1\),</li>
      <li>\(\mathrm{Proj}_2\):  \(\mathbb{R}^{M \times n_1} \rightarrow \mathbb{R}^{M \times T}\) to yield \(\overline{\mathbf{x}}\).</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>fixed \(L=96\) and \(T=\{96,192,336,720\}\)</li>
  <li>Default parameters for all Mambas were set as follows:
    <ul>
      <li>Dimension factor \(D=\) 256,</li>
      <li>local convolutional width =2,</li>
      <li>and state expand factor \(N=\) 1,</li>
    </ul>
  </li>
  <li>For the transformer module,
    <ul>
      <li>the multi head number is set \(H=\) 8,</li>
      <li>the Batch Size in the training process is set as Batch =32.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MAT/table1.png" alt="그림1" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Transformers struggle with long-term dependencies and sparse semantic features,
    <ul>
      <li>Mamba excels through selective input handling and parallel computing.</li>
    </ul>
  </li>
  <li>MAT, a combined approach leveraging Mamba’s long-range capabilities
    <ul>
      <li>and Transformers’ short-range strengths</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2409.08530)]]></summary></entry><entry><title type="html">DTMamba : Dual Twin Mamba for Time Series Forecasting (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2025-01-06-DTMamba/" rel="alternate" type="text/html" title="DTMamba : Dual Twin Mamba for Time Series Forecasting (Arxiv 2024)" /><published>2025-01-06T00:00:00+09:00</published><updated>2025-01-08T22:03:05+09:00</updated><id>http://localhost:4000/mamba/DTMamba</id><content type="html" xml:base="http://localhost:4000/mamba/2025-01-06-DTMamba/"><![CDATA[<h2 id="introduction">Introduction</h2>

<ul>
  <li>RNN-based methods
    <ul>
      <li>vanishing gradients</li>
      <li>cannot be parallelized effectively,</li>
      <li>leading tolower computational efficiency</li>
    </ul>
  </li>
  <li>TCN-based methods
    <ul>
      <li>limited modeling capabilities for long-term dependencies.</li>
    </ul>
  </li>
  <li>Transformers
    <ul>
      <li>quadratic complexity</li>
    </ul>
  </li>
  <li>Simple linear models
    <ul>
      <li>short in terms of performance.</li>
    </ul>
  </li>
  <li>Mamba
    <ul>
      <li>inear timeseries modeling approach using State Space Models
        <ul>
          <li>RNN’s sequential processing capability</li>
          <li>CNN’s global information processing capability</li>
        </ul>
      </li>
      <li>selection mechanism within the SSM framework
        <ul>
          <li>focus on essentialinformation while filtering out irrelevant details</li>
        </ul>
      </li>
      <li>incorporates a summary of all preceding information</li>
    </ul>
  </li>
  <li>Dual Twin Mamba(DTMamba)
    <ul>
      <li>RevIN - two TMamba blocks(Residual networks) - projection layer - reverse RevIN</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>Traditional methods
    <ul>
      <li>poor modeling performance</li>
    </ul>
  </li>
  <li>RNN-based models
    <ul>
      <li>vanishing gradients, limited parallelization capabilities,</li>
    </ul>
  </li>
  <li>Transformer-based methods
    <ul>
      <li>self-attention : suitable for modelinglong-term time series data</li>
      <li>quadratic complexity</li>
    </ul>
  </li>
  <li>Linear-based methods
    <ul>
      <li>solely rely on pastobserved temporal patterns</li>
    </ul>
  </li>
  <li>TCN-based methods
    <ul>
      <li>larger receptivefied</li>
      <li>but limited modeling capabilities forlong-term dependencies</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-methods">3. Proposed Methods</h2>

<h3 id="31-problem-statement">3.1 Problem Statement</h3>

<ul>
  <li>\(X=\left\{x_1, \ldots, x_T\right\} \in \mathbb{R}^{T \times N}\)를 보고 \(\hat{X}=\left\{\hat{x}_{T+1}, \ldots, \hat{x}_{T+S}\right\} \in \mathbb{R}^{S \times N}\) 예측</li>
</ul>

<h3 id="32-normalization">3.2 Normalization</h3>

<ul>
  <li>\(X \in \mathbb{R}^{T \times N}\)  into \(X^0=\left\{x_1^0, \ldots, x_T^0\right\} \in \mathbb{R}^{T \times N}\), via \(X^0=\operatorname{RevIN}(X)\).</li>
</ul>

<h3 id="33-channel-independence--reversed-channel-independence">3.3 Channel Independence &amp; Reversed Channel Independence</h3>

<ul>
  <li>model overfitting 때문에 함</li>
  <li>\(\text{Batch}\left(X^0\right)\):  (Batch_Size, Lookback length, Dimension) \(= (B, T, N)\) 를 reshape
    <ul>
      <li>Batch \(\left(X^{\boldsymbol{I}}\right)= \text{ChannelIndepence}\left(\operatorname{Batch}\left(X^0\right)\right) : (B \times N, 1, T)\)</li>
    </ul>
  </li>
  <li>다시 되돌릴 때에는 $\operatorname{Batch}\left(X^P\right):(B \times N, 1, S)$를 reshape
    <ul>
      <li>Batch \((\hat{\boldsymbol{X}})=\text{ ChannelIndepence}^{-1}\left(\operatorname{Batch}\left(X^P\right)\right) : (B, S, N)\)</li>
    </ul>
  </li>
</ul>

<h3 id="34-twin-mamba">3.4 Twin Mamba</h3>

<p><img src="/assets/img/Mamba/DTMamba/fig1.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/DTMamba/fig2.png" alt="그림1" /></p>

<ul>
  <li>Embedding layer - residual</li>
  <li>Dropout layer - twin Mambas</li>
</ul>

<h3 id="341-embedding-layers">3.4.1. Embedding Layers</h3>

<ul>
  <li>linear layer as the Embedding layer \(\to\) global feature representation</li>
  <li>\(X^E = \text{Embedding}(X^I):(B \times N, 1, ni)\).</li>
  <li>Embedding layer는 TMamba Block 안에 있는데
    <ul>
      <li>TMamba Block이 두 개니까 각각의 Embedding layer를 Embedding 1,2라 함</li>
    </ul>
  </li>
</ul>

<h3 id="342-residual">3.4.2. Residual</h3>

<ul>
  <li>proposed by ResNet</li>
  <li>to prevent overfitting, stable training</li>
</ul>

<h3 id="343-dropout">3.4.3. Dropout</h3>

<ul>
  <li>To prevent overfitting</li>
  <li>\(X^E\) into \(X^D:(B \times N, 1, n i)\) (dimension은 그대로)</li>
</ul>

<h3 id="344-mamba">3.4.4. Mamba</h3>

<ul>
  <li>S4 learns how to map an input \(x(t)\) to an output \(y(t)\) through an intermediate state \(h(t)\)</li>
  <li>TMamba Block에서는 2개의 Mamba 사용됨 (multi-level feature learning)
    <ul>
      <li>low-level temporal features &amp; high-level temporal patterns</li>
    </ul>
  </li>
</ul>

<h3 id="35-projection-layer">3.5. Projection Layer</h3>

<p><img src="/assets/img/Mamba/DTMamba/algorithm1.png" alt="그림1" /></p>

<ul>
  <li>지금까지 2개의 TMamba Block와 residuals \(R^1\), \(R^2\) 더해서 hidden information을 얻음</li>
  <li>Output의 shape에 맞게 FC layer 태운 뒤
    <ul>
      <li>앞서 언급한 것처럼 \(\text{ ChannelIndepence}^{-1}\)와 \(\text{RevIN}^{-1}\)을 수행</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiment">4. Experiment</h2>

<h3 id="41-settings">4.1. Settings</h3>

<p>pass</p>

<h3 id="42-long-term-forecasting">4.2. Long-term Forecasting</h3>

<p><img src="/assets/img/Mamba/DTMamba/table1.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/DTMamba/table2-1.png" alt="그림1" />
<img src="/assets/img/Mamba/DTMamba/table2-2.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/DTMamba/table3.png" alt="그림1" /></p>

<h3 id="43-hyperparameter-sensitivity-analysis-and-ablation-study">4.3. Hyperparameter Sensitivity Analysis and Ablation Study</h3>

<p><img src="/assets/img/Mamba/DTMamba/table4.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/DTMamba/table5.png" alt="그림1" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2405.07022)]]></summary></entry><entry><title type="html">MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting (ICLR 2025)</title><link href="http://localhost:4000/mamba/2024-11-27-MambaTS/" rel="alternate" type="text/html" title="MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting (ICLR 2025)" /><published>2024-11-27T00:00:00+09:00</published><updated>2025-01-08T22:03:03+09:00</updated><id>http://localhost:4000/mamba/MambaTS</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-27-MambaTS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformers : quadratic complexity and permutation invariant bias</li>
  <li>Mamba의 한계 4가지를 개선한 <strong>MambaTS</strong> 제시
    <ul>
      <li>variable scan : 모든 변수의 과거 정보를 함께 arrange</li>
      <li>causal convolution (in Mamba block) 필요없음 \(\to\) Temporal Mamba Block(TMB)</li>
      <li>dropout mechanism (for selective parameters of TMB)</li>
      <li>variable permutation training : variable scan order sensitivity 문제 해결</li>
    </ul>
  </li>
  <li>추가적으로 variable-aware scan
    <ul>
      <li>훈련 과정에서 변수 관계를 동적으로 발견하고,</li>
      <li>추론 시 모든 노드를 방문하는 최단 경로 문제를 해결하여 최적의 변수 스캔 순서를 디코딩</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer의 문제는
    <ul>
      <li>quadratic complexity</li>
      <li>look-back window가 커진다고 성능이 향상되는 것이 아님</li>
      <li>permutation invariant bias의 effectiveness 의문</li>
    </ul>
  </li>
  <li>Mamba : SSM + selection mechanism + hardware-aware design
    <ul>
      <li>Transformer based methods(PatchTST 및 iTransformer)에서</li>
      <li>Transformer 블록을 Mamba 블록으로 교체했더니</li>
      <li>학습 속도가 1.3배 더 빠르고, GPU 메모리 사용량이 각각 5.3배 및 7.0배 감소</li>
      <li><strong>하지만 Mamba가 Transformer보다 성능적 우위를 보이지는 못함</strong></li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig1.png" alt="그림1" /></p>

<ul>
  <li>그러므로 Abstract에서 소개한 MambaTS를 제시</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="long-term-time-series-forecasting">Long-term time series forecasting</h3>

<ul>
  <li>variable-mixing : dependencies across time and variables
    <ul>
      <li>RNN, TCN, Transformer, MLP…</li>
      <li>현재는 patching으로 quadratic complexity 개선하고자 함</li>
    </ul>
  </li>
  <li>Variable-independent : the assumption of variable independence
    <ul>
      <li>문제를 과도하게 단순화해서 부적절할 수도 있긴 함</li>
    </ul>
  </li>
</ul>

<h3 id="state-space-models">State Space Models</h3>

<ul>
  <li>Mamba : SSM + selection mechanism + hardware-aware design
    <ul>
      <li>Mamba의 scan order sensitivity를 다루기 위해 Vison 도메인에서는</li>
      <li><strong>bidirectional scanning</strong>(Vision Mamba), <strong>multi-directional scanning</strong>(VMamba, Mamba-nd), <strong>automatic direction scanning</strong>(Local Mamba)</li>
      <li>하지만 temporal problem에는 활발한 연구 X</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 MambaTS는</li>
  <li>VAST(Variable-Aware Scan along Time)으로 표현력 강화</li>
</ul>

<h2 id="3-preliminaries">3. Preliminaries</h2>

<p>pass</p>

<h2 id="4-model-architecture">4. Model Architecture</h2>

<p><img src="/assets/img/Mamba/MambaTS/fig2.png" alt="그림1" /></p>

<ul>
  <li>\(\left(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_L \right)\), where \(\mathbf{x}_i \in \mathbb{R}^K\)를 보고 \(\left(\mathbf{x}_{L+1}, \cdots, \mathbf{x}_{L+2}, \cdots, \mathbf{x}_{L+T}\right)\) 예측하고자 함</li>
</ul>

<h3 id="41-overall-architecture">4.1. Overall Architecture</h3>

<ul>
  <li><strong>Patching and Tokenization</strong>
    <ul>
      <li>PatchTST처럼 각 변수를 \(M=L / s\)개의 patches로 나누어 각 patch를 \(D\)차원으로 mapping</li>
    </ul>
  </li>
  <li><strong>Variable Scan along Time</strong>
    <ul>
      <li>\(K\) 개의 변수를 임베딩함으로써 \(K \times M\) 개의 tokens 얻음</li>
      <li>Variable Scan Along Time(VST) : 첫 시점에서 모든 변수의 토큰 \(\to\) 두 번째 시점에서 모든 변수의 토큰 \(\to\) … 마지막 시점에서 모든 변수의 토근 순서로 정렬</li>
      <li>그림으로 표현하면 아래와 같음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig2-1.png" alt="그림1" /></p>

<ul>
  <li><strong>Encoder</strong>
    <ul>
      <li>\(N\)개의 Temporal Mamba Block(TMB)로 구성됨</li>
      <li>각 TMB는 2개의 branches
        <ul>
          <li>하나는 sequence modeling에 집중, 하나는 비선형성에 집중</li>
          <li>식으로 표현하면 \(h_t=\operatorname{SSM}\left(\operatorname{Dropout}\left(\operatorname{Linear}\left(x_t\right)\right)\right)+\sigma\left(\right. Linear \left.\left(x_t\right)\right)\)이고</li>
          <li>그림으로 표현하면 아래와 같음</li>
          <li>왼쪽 Mamba block에서 Conv 없애고 dropout 추가함</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig2-2.png" alt="그림1" /></p>

<ul>
  <li><strong>Prediction Head</strong>
    <ul>
      <li>Encoder가 global dependencies를 잡으니까 channel-independent decoding</li>
    </ul>
  </li>
  <li><strong>Instance Normalization</strong>
    <ul>
      <li>RevIN 사용</li>
    </ul>
  </li>
  <li><strong>Loss Function</strong>
    <ul>
      <li>MSE 사용 : \(\mathcal{L}=\mathbb{E}_{\mathbf{x}} \frac{1}{M} \sum_{i=1}^M\left\|\hat{\mathbf{x}}_{L+1: L+T}^{(i)}-\mathbf{x}_{L+1: L+T}^{(i)}\right\|_2^2\)</li>
    </ul>
  </li>
</ul>

<h3 id="42--variable-permutation-training">4.2.  Variable Permutation Training</h3>

<ul>
  <li>channel orders의 영향을 줄이고 local context interactions을 늘리기 위해</li>
  <li>variable permutation training (VPT) strategy를 사용
    <ul>
      <li>\(K \times M\)개의 tokens가 있으면 each time step에서 \(K\)개를 섞고 디코딩 후에 복원</li>
    </ul>
  </li>
</ul>

<h2 id="43-variable-aware-scan-along-time">4.3. Variable-Aware Scan along Time</h2>

<ul>
  <li>최적의 <strong>변수 스캔 순서</strong>를 결정하려면 변수간 관계를 알아야 하고</li>
  <li>
    <p>이를 위해 Variable-Aware Scan along Time (VAST) 제안</p>
  </li>
  <li><strong>Training</strong>
    <ul>
      <li>변수 \(K\)개, directed graph adjacency matrix \(P \in \mathbb{R}^{K \times K}\)를 만듬
        <ul>
          <li>\(p_{i, j}\) : cost from node \(i\) to node \(j\)</li>
          <li>다양한 variable scan order 탐색해서 Loss 계산 할 것</li>
        </ul>
      </li>
      <li>이제 node index sequence \(V=\left\{v_1, v_2, \ldots, v_K\right\}\)가 나옴
        <ul>
          <li>\(v_k\)는 shuffled sequnce의 새로운 index</li>
        </ul>
      </li>
      <li>\(K-1\) 개의 transition tuples \(\left\{\left(v_1, v_2\right),\left(v_2, v_3\right), \ldots,\left(v_{K-1}, v_K\right)\right\}\)이 도출됨</li>
      <li>각 sample마다 network의 \(t\)-th iteration동안 training loss \(l^{(t)}\) 계산되고
        <ul>
          <li><strong>directed graph adjacency matrix</strong> \(P \in \mathbb{R}^{K \times K}\)를 update (with exponential moving average)</li>
          <li>\(p_{v_k, v_{k+1}}^{(t)}=\beta p_{v_k, v_{k+1}}^{(t-1)}+(1-\beta) l^{(t)}\).</li>
          <li>\(\beta\)는 rate of the moving average (hyperparameter)</li>
        </ul>
      </li>
      <li>샘플 배치 간의 영향을 제거하기 위해 위 식을 batch 버전으로 확장
        <ul>
          <li>즉 \(\bar{l}^{(t)}=l^{(t)}-\mu\left(l^{(t)}\right)\)를 사용하여 centralization (\(\mu\)는 mean function)</li>
          <li>\(p_{v_k, v_{k+1}}^{(t)}=\beta p_{v_k, v_{k+1}}^{(t-1)}+(1-\beta) \bar{l}^{(t)}\)이 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Inference</strong>
    <ul>
      <li>Training동안 directed graph adjacency matrix \(P\) 는 optimal variable scan order 결정하는 데에 사용</li>
      <li>즉 Asymmetric Traveling Salesman Problem, ATSP 문제가 되고
        <ul>
          <li>heuristic-based simulated annealing algorithm 사용해서 경로 디코딩</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<p><img src="/assets/img/Mamba/MambaTS/table2.png" alt="그림1" /></p>

<h3 id="51-main-results">5.1. Main results</h3>

<p><img src="/assets/img/Mamba/MambaTS/table3.png" alt="그림1" /></p>

<h3 id="52--ablation-studies-and-analyses">5.2.  Ablation studies and analyses</h3>

<ul>
  <li>Component Ablation</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/table4.png" alt="그림1" /></p>

<ul>
  <li>Dropout Ablation</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig3.png" alt="그림1" /></p>

<ul>
  <li>VAST Ablation</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/table5.png" alt="그림1" /></p>

<h3 id="53-model-analysis">5.3 Model Analysis</h3>

<ul>
  <li>Increasing Lookback Window</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig5.png" alt="그림1" /></p>

<ul>
  <li>Efficiency Analys</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/table6.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Variable Scan along Time (VST)
    <ul>
      <li>to organize the historical information of all variables,</li>
      <li>forming a global retrospective sequence</li>
    </ul>
  </li>
  <li>Temporal Mamba Block (TMB)
    <ul>
      <li>causal convolution in Mamba 제거</li>
      <li>dropout regularization 추가</li>
    </ul>
  </li>
  <li>Variable Permutation Training (VPT)
    <ul>
      <li>local context interaction 능력 향상</li>
    </ul>
  </li>
  <li>Variable-Aware Scan along Time (VAST)
    <ul>
      <li>훈련 중 변수 간 관계를 동적으로 발견</li>
      <li>ATSP solver로 the optimal variable scann order 결정</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[ICLR 2025](https://arxiv.org/pdf/2405.16440)]]></summary></entry><entry><title type="html">Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2024-11-25-MoU/" rel="alternate" type="text/html" title="Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)" /><published>2024-11-25T00:00:00+09:00</published><updated>2025-01-08T22:03:02+09:00</updated><id>http://localhost:4000/mamba/MoU</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-25-MoU/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer는 높은 계산 비용(quadratic computational cost)이 문제,</li>
  <li>Mamba는 longterm forecasting에서 성능이 effective하지 않음 (potential information loss 때문)</li>
  <li>본 논문에서 제시하는 <strong>Mixture of Universals (MoU)</strong>의 components:
    <ul>
      <li><strong>Mixture of Feature Extractors (MoF)</strong>: adaptive patch representations
        <ul>
          <li>(for <strong>short</strong>-term dependency)</li>
        </ul>
      </li>
      <li><strong>Mixture of Architectures (MoA)</strong>:  Mamba, FeedForward, Convolution, and Self-Attention 연결한 것
        <ul>
          <li>(for  <strong>long</strong>-term dependency )</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>PatchTST</strong>:  patch embedding에서 uniform linear transformations 사용 \(\to\) varying semantic contexts 손실
    <ul>
      <li>Vision에서 Dynamic Convolution, Conditional Convolution (for informative representations)등장했지만</li>
      <li>Time series에서는 poor performances</li>
    </ul>
  </li>
  <li><strong>Transformer</strong>: 장기 의존성은 잘 처리하지만 계산 비용이 큼</li>
  <li><strong>Mamba</strong>: 효율적이지만 정보 손실로 인해 장기 예측에서 성능이 떨어질 수 있음</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig2.png" alt="그림1" /></p>

<ul>
  <li><strong>MoF(Mixture of Feature Extractors)</strong>:
    <ul>
      <li>multiple sub-extractors로 구성되어 있고</li>
      <li>sparse activation을 사용해서 input patch에 적합한 sub-extractor만 활성화</li>
      <li>learning of diverse contexts and <strong>minimal parameter increase</strong> !</li>
    </ul>
  </li>
  <li><strong>MoA(Mixture of Architectures)</strong> Mamba에서 시작해 국소적인 관점에서 전역적인 Self-Attention 계층으로 확장하며 장기 의존성을 효율적으로 캡처하는 계층적 구조.
    <ul>
      <li>hierarchical structure를 가진 encoder</li>
      <li><strong>Mamba layer</strong> that selects and learns key dependencies using a Selective State-Space Model (SSM).</li>
      <li><strong>FeedForward transition layer</strong> and a <strong>Convolution-layer</strong> that broadens the receptive field to capture longer dependencies.</li>
      <li><strong>Self-Attention layer</strong> integrates information globally to fully capture long-term dependencies</li>
    </ul>
  </li>
</ul>

<h2 id="2-approach">2. Approach</h2>

<h3 id="21-problem-setting-and-model-structure">2.1 Problem Setting and Model Structure</h3>

<ul>
  <li>\(\mathbf{X}_{\text {input }}=\left[\mathbf{X}^1, \mathbf{X}^2, \ldots, \mathbf{X}^M\right] \in \mathbb{R}^{M \times L}\) where \(\mathbf{X}^i=\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_L\right] \in \mathbb{R}^L\)</li>
  <li>\(\hat{\mathbf{X}}_{\text {output }}=\left[\hat{\mathbf{X}}^1, \hat{\mathbf{X}}^2, \ldots, \hat{\mathbf{X}}^M\right] \in \mathbb{R}^{M \times T}\).</li>
  <li>
    <p>Goal : learning a function \(\mathcal{F}: \mathbf{X} \rightarrow \hat{\mathbf{X}}\)</p>
  </li>
  <li>Overall process는 아래와 같음.
    <ul>
      <li>처음에는 raw time series \(\mathbf{X} \in \mathbb{R}^L\)에서 시작 (variate independence setting이라서 channel =1)</li>
      <li>\(N\)개의 patch tokens를 만듬 :
        <ul>
          <li>\(\mathbf{X}_p=\operatorname{Patch}(\mathbf{X}) \in \mathbb{R}^{N \times P}\) with fixed size \(P\), stride \(S\)</li>
        </ul>
      </li>
      <li><strong>MoF</strong> module에 넣어서 adaptive representations를 얻음 :
        <ul>
          <li>\(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\).</li>
          <li>MoF는 parameters를 조절하면서 계산 비용 최적화 (<strong>2.2</strong>에서 자세히 설명)</li>
        </ul>
      </li>
      <li>이제 <strong>MoA</strong>에 넣어서 long-term dependencies (among tokens) 잡음
        <ul>
          <li>\(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoA}\left(\mathbf{X}_{\mathrm{rep}}\right)\in \mathbb{R}^{N \times D}\).</li>
          <li>MoA는 long-term encoder based on the Mixture of Architectures (<strong>2.3</strong>에서 자세히 설명)</li>
        </ul>
      </li>
      <li>마지막으로 linear projector에 넣어서 final prediction 얻음
        <ul>
          <li>\(\hat{\mathbf{X}}=\mathbf{P}\left(\operatorname{Flatten}\left(\mathbf{X}_{\mathrm{rep}^{\prime}}\right)\right)\in \mathbb{R}^{M \times T}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-mixture-of-feature-extractors">2.2 Mixture of Feature Extractors</h3>

<ul>
  <li>MoF의 목적은 <strong>patch의 representative embedding</strong>을 만드는 것</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig3.png" alt="그림1" /></p>

<ul>
  <li>Sub-extractors \(\left\{F_1, F_2, \ldots, F_c\right\}\)가 있고 각각은 independent linear mapping</li>
  <li>MoF를 통과하면 \(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\)를 얻음
    <ul>
      <li>where \(R_i(\cdot)\)는  input-relevant router (sub-extractor를 sparse하게 활성화)
        <ul>
          <li>즉 \(R\left(\mathbf{X}_p\right)_i=\operatorname{Softmax}\left(\operatorname{Top}_{\mathrm{k}}\left(H\left(\mathbf{X}_p\right)_i, k\right)\right)\)
            <ul>
              <li>\(\operatorname{Softmax}(\cdot)\)는  \(\operatorname{Topk}(\cdot, k)\)에 의해 선택된 상위 \(k\)개의 점수를 정규화</li>
              <li>\(H\left(X_p\right)=\left[H\left(\mathbf{X}_p\right)_1, H\left(\mathbf{X}_p\right)_2, \ldots H\left(\mathbf{X}_p\right)_c\right]\) 는 sub-extractors의 score vector</li>
              <li>여기서 \(H\left(\mathbf{X}_p\right)_i=\left(\mathbf{X}_p \cdot W_g\right)_i+\text { SN } \cdot \text { Softplus }\left(\left(\mathbf{X}_p \cdot W_{\text {noise }}\right)_i\right)\)
                <ul>
                  <li>where \(W_g\)는 linear layer이고 두번째 항은 load balancin을 위한 tunable noise를 넣는 것</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이렇게 하면 patch token를 \(c\)개의 서로 다른 patterns들의 조합으로 분할할 수 있음
    <ul>
      <li>each pattern은 최적의 sub-extractors에 의해 뽑힌 것이니</li>
      <li>most representative embedding (for patches with divergent context)라고 할 수 있음</li>
    </ul>
  </li>
</ul>

<h3 id="23-mixture-of-architectures">2.3. Mixture of Architectures</h3>

<ul>
  <li>MoA의 목적은 <strong>comprehensive long-term dependencies</strong>를 모델링하는 것</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig4.png" alt="그림1" /></p>

<ul>
  <li><strong>Mamba, FeedForward, Convolution</strong>, and <strong>Self-Attention layer</strong>로 구성되어
    <ul>
      <li>각각이 다른 관점에서 long-term dependencie를 학습</li>
      <li><strong>gradually expanding perspective</strong>를 통해 effectiveness and efficiency 둘 다 챙김</li>
    </ul>
  </li>
  <li><strong>Mamba-layer in Time Series</strong> : <strong>relevant data를 선택하고 time-variant dependencies를 학습하는 곳</strong>
    <ul>
      <li>input은 MoF의 output \(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\)인데 \(x\)로 denote</li>
      <li>\(\begin{gathered}
\boldsymbol{x}^{\prime}=\sigma(\text { Conv1D }(\text { Linear }(\boldsymbol{x}))) \\
\boldsymbol{z}=\sigma(\text { Linear }(\boldsymbol{x}))
\end{gathered}\), where \(\sigma\)는 \(SiLU\) activation</li>
      <li>다음으로 \(\begin{gathered}
\boldsymbol{y}^{\prime}=\operatorname{Linear}\left(\operatorname{SelectiveSSM}\left(\boldsymbol{x}^{\prime}\right) \otimes \boldsymbol{z}\right) \\
\boldsymbol{y}=\operatorname{LayerNorm}\left(\boldsymbol{y}^{\prime}+\boldsymbol{x}\right)
\end{gathered}\), where \(\begin{gathered}
\text { SelectiveSSM }\left(\boldsymbol{x}_t^{\prime}\right)=\boldsymbol{y}_t \\
\boldsymbol{y}_t=C \boldsymbol{h}_t, \quad \boldsymbol{h}_t=\bar{A} \boldsymbol{h}_{t-1}+\bar{B} \boldsymbol{x}_t^{\prime}
\end{gathered}\)
        <ul>
          <li>\(h_t\)는 latent state, \(y_t\)는 output representation</li>
          <li>The discrete matrices는 \(\begin{gathered}
B_t=S_B\left(\boldsymbol{x}_t^{\prime}\right), \quad C_t=S_C\left(\boldsymbol{x}_t^{\prime}\right) \\
\Delta_t=\operatorname{softplus}\left(S_{\Delta}\left(\boldsymbol{x}_t^{\prime}\right)\right)
\end{gathered}\)
            <ul>
              <li>\(S\)들은 linear layers이고, \(\begin{gathered}
f_A\left(\Delta_t, A\right)=\exp \left(\Delta_t A\right) \\
f_B\left(\Delta_t, A, B_t\right)=\left(\Delta_t A\right)^{-1}\left(\exp \left(\Delta_t A\right)-I\right) \cdot \Delta B_t \\
\bar{A}_t=f_A\left(\Delta_t, A\right), \quad \bar{B}_t=f_B\left(\Delta_t, A, B_t\right)
\end{gathered}\)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>좀 복잡한데 위에 있는 fig4 보는 것이 낫겠다.</li>
    </ul>
  </li>
  <li><strong>FeedForward-layer</strong> : <strong>non-linearity를 강화하는 곳</strong>
    <ul>
      <li>\(\boldsymbol{x}_{\mathrm{ffn}}=\text { FeedForward }\left(\boldsymbol{y}_t ; w_1, \sigma, w_2\right)\), where
        <ul>
          <li>\(w_1\) and \(w_2\) are parameters, \(\sigma\) is activation function</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Convolution-layer</strong> : <strong>MoA의 receptive field를 넓히는 곳</strong>
    <ul>
      <li>partial long-term dependencies를 담고 있는 tokens끼리의 정보 교환을 촉진</li>
      <li>\(\boldsymbol{x}_{\mathrm{conv}}= \operatorname{Conv}\left(\boldsymbol{x}_{\mathrm{ffn}} ; \mathbf{k}, s, p, c_{\mathrm{out}}\right)\),
        <ul>
          <li>where \(\mathbf{k}\) is the kernel size, \(s\) is the stride, \(p\) is the padding,</li>
          <li>and \(c_{\text {out }}\) is the number of output channels</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Self-Attention-layer</strong> : <strong>global perspective에서 포괄적인 long-term dependencies를 파악하는 곳</strong>
    <ul>
      <li>: \(\begin{aligned} &amp; x_{\mathrm{att}}=\operatorname{FeedForward}(\operatorname{Attention}(Q, K, V)) \\ &amp; \operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V \\ &amp; Q=x_{\text {conv }} W_Q, K=x_{\text {conv }} W_K, V=x_{\text {conv }} W_V\end{aligned}\)</li>
    </ul>
  </li>
  <li><strong>Partial-to-global Design for Time Series</strong>
    <ul>
      <li><strong>gradually expanding perspective</strong>라는 말의 의미는
        <ul>
          <li>Mamba layer : Selective SSM을 사용하여 시간적으로 변화하는 의존성을 처리</li>
          <li>FeedForward layer : 이러한 부분 의존성을 더 복잡한 표현으로 전환</li>
          <li>Convolution layer : 수용 영역을 확장하여 보다 넓은 시간적 관계를 학습</li>
          <li>Self-Attention layer : 로컬화된 정보를 통합하여 장기 의존성에 대한 포괄적인 이해</li>
        </ul>
      </li>
      <li>를 거치면서 선택적으로 일부 의존성에 초점을 맞춘 후, 이를 점차 확장하여 전체적(global) 관점으로 발전시킨다는 뜻</li>
    </ul>
  </li>
</ul>

<h3 id="24-computational-complexity-and-model-parameter">2.4. Computational Complexity and Model Parameter</h3>

<ul>
  <li>Tokens \(T\)개가 주어졌을 때 top-\(k\) experts를 선택한다고 하면
    <ul>
      <li>\(C_{\mathrm{MOU}}=\underbrace{k T \times d^2}_{\text {MoF }}+\underbrace{T \times d^2}_{\text {Mamba }}+\underbrace{T \times d^2}_{\text {FFN }}+\underbrace{k T d^2}_{\text {Conv }}+\underbrace{T^2 \times d+T \times d^2}_{\text {Transformer }}\),
        <ul>
          <li>where \(\mathbf{k}\) is the kernel size in the convolutional layer,</li>
          <li>\(d\) is the dimension of the vector representations</li>
        </ul>
      </li>
      <li>Transformer 블록을 제외하면 선형적인 복잡도</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig1.png" alt="그림1" /></p>

<h2 id="3-experiments">3. Experiments</h2>

<h3 id="31-dataset">3.1. Dataset</h3>

<ul>
  <li>7 commonly used datasets</li>
  <li>Pass</li>
</ul>

<h3 id="32-baselines-and-setup">3.2. Baselines and Setup</h3>

<ul>
  <li>Mamba-based Models (S-Mamba)</li>
  <li>Linear-based Models (D-Linear)</li>
  <li>Convolution-based Models (ModernTCN)</li>
  <li>Transformer-based Models (PatchTST)</li>
</ul>

<h3 id="33-main-results">3.3. Main Results</h3>

<p><img src="/assets/img/Mamba/MoU/table1.png" alt="그림1" /></p>

<h3 id="34-ablation-study">3.4. Ablation Study</h3>

<p><img src="/assets/img/Mamba/MoU/table2.png" alt="그림1" /></p>

<ul>
  <li>3개의 adaptive feature extractors를 비교했을 때 MoF(in MoU)가 가장 좋았으며
    <ul>
      <li>Dyconv가 parameters 수를 크게 증가시키기 때문에 time series patch와 같은 작은 데이터셋에는 적합하지 않음</li>
      <li>SE-M의  calibration strategy는 representation을 normalized gating vector에 곱하는 방식이라서 다양한 컨텍스트 정보를 처리하는 데에는 한계</li>
    </ul>
  </li>
  <li>특히 MoF가 uniform transformation method (Linear)보다 좋다는 점이 주목할만함</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/table3.png" alt="그림1" /></p>

<ul>
  <li>
    <p>AA, MM, MFA, AAA, MMA, AMM, MAM, AMA, AFM, AFCM</p>

    <ul>
      <li>여기서 A, M, F, C는 각각 Self-Attention, Mamba, FeedForward, Convolution</li>
      <li>글자의 순서는 레이어의 배치 순서</li>
    </ul>
  </li>
  <li>
    <p>M-A 순서(MAM, AMA, MMA) &gt; M-A 순서를 가지지 않은 모델(AMM)</p>

    <ul>
      <li>
        <p>Mamba를 Self-Attention 이전에 배치하는 것이 장기 의존성을 캡처하는 데 더 효과적</p>
      </li>
      <li>
        <p>A-M 순서보다 M-A 순서가 장기 의존성 학습에서 더 중요한 역할</p>
      </li>
    </ul>
  </li>
  <li>
    <p>F-C 순서 &gt; F</p>

    <ul>
      <li>Convolution 레이어가 Mamba 레이어의 수용 영역을 확장하여</li>
      <li>Mamba 레이어의 부분적 관점과 Self-Attention 레이어의 글로벌 관점을 연결하는 중간 관점을 제공한다고 해석됨</li>
    </ul>
  </li>
</ul>

<h3 id="35-model-analysis">3.5. Model Analysis</h3>

<ul>
  <li>Does MoF actually learn contexts within patches?</li>
  <li>What is learned by the layers of MoA?</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig56.png" alt="그림1" /></p>

<h2 id="5-conclusion">5 Conclusion</h2>

<ul>
  <li>Mixture of Universals (MoU)
    <ul>
      <li>Mixture of Feature Extractors (MoF)
        <ul>
          <li>an adaptive method specifically designed to enhance time series patch representations for capturing short-term dependencies</li>
        </ul>
      </li>
      <li>Mixture of Architectures (MoA)
        <ul>
          <li>hierarchically integrates multiple architectures in a structured sequence to model long-term dependencies from a hybrid perspective</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2408.15997)]]></summary></entry><entry><title type="html">SEQUENTIAL ORDER-ROBUST MAMBA FOR TIME SERIES FORECASTING (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2024-11-14-SOR-Mamba/" rel="alternate" type="text/html" title="SEQUENTIAL ORDER-ROBUST MAMBA FOR TIME SERIES FORECASTING (Arxiv 2024)" /><published>2024-11-14T00:00:00+09:00</published><updated>2025-01-08T22:03:01+09:00</updated><id>http://localhost:4000/mamba/SOR-Mamba</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-14-SOR-Mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Mamba : near-linear complexity in processing sequential data
    <ul>
      <li><strong>하지만 일반적으로 Time series에서 변수의 순서는 존재하지 않기 때문에</strong></li>
      <li><strong>Mamba에서 channel dependencies (CD)를 잡다보면 sequential order bias가 발생</strong></li>
    </ul>
  </li>
  <li>그러므로 본 논문에서는 <strong>SOR-Mamba</strong>를 제안
    <ul>
      <li><strong>Regularization strategy</strong>
        <ul>
          <li>to minimize the discrepancy btw two embedding vectors (reversed channel orders)</li>
          <li>\(\to\)  robustness to channel order</li>
        </ul>
      </li>
      <li><strong>Eliminates the 1D-convolution</strong>
        <ul>
          <li>(originally designed to capture local information in sequential data)</li>
        </ul>
      </li>
      <li><strong>Channel correlation modeling (CCM)</strong>
        <ul>
          <li>pretraining task aimed at preserving <strong>correlations between channels</strong>
            <ul>
              <li>from the data space to the latent space</li>
              <li>in order to enhance the ability to capture CD</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer
    <ul>
      <li>ability to capture long-term dependencies but, quadratic computational complexity</li>
      <li>reduce the complexity 하려다보니 performance degradations</li>
    </ul>
  </li>
  <li><strong>SSM</strong>
    <ul>
      <li>employing <strong>convolutional</strong> operations to process sequences with linear complexity</li>
    </ul>
  </li>
  <li><strong>Mamba</strong>
    <ul>
      <li>incorporating a <strong>selective</strong> mechanism to prioritize important information</li>
      <li>balance btw performance and computational efficiency</li>
    </ul>
  </li>
  <li>Temporal dependencies (TD), channel dependencies (CD) 둘 다 잡아야 하는데,
    <ul>
      <li>iTransformer에서는 CD는 complex attention mechanisms으로,</li>
      <li>TD는 simple multi-layer perceptrons (MLPs)으로 했었음</li>
    </ul>
  </li>
  <li>Mamba는 <strong>sequential order bias</strong>가 있다보니 <strong>Bidirectional Mamba로는 충분하지 않음</strong></li>
  <li>그렇다고 MambaTS처럼 channel을 섞자니 추가적인 작업이 소요됨</li>
</ul>

<p><img src="/assets/img/Mamba/SOR-Mamba/fig1.png" alt="그림1" /></p>

<ul>
  <li>그래서 <strong>Sequential Order-Robust Mamba for TS forecasting (SOR-Mamba)</strong>를 제안
    <ul>
      <li>(간단한 설명은 abstract에 잘 정리되어 있으므로 pass)</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li><strong>CD-Mamba Block</strong>
    <ul>
      <li>1D Conv 제거</li>
      <li>Time series의 channels는 애초에 inherent sequential order가 존재하지 않기 때문</li>
    </ul>
  </li>
  <li><strong>Regulararization with CD-Mamba Block</strong>
    <ul>
      <li>Reversed channel order로 발생하는 두 embedding vectors의 차이를 줄이도록 학습</li>
      <li>\(\to\) Enhancing robustness to channel order !</li>
    </ul>
  </li>
  <li><strong>Channel correlation modeling</strong>
    <ul>
      <li>Data space에서의 correlation (btw channels)과</li>
      <li>Embedding space에서의 correlation (btw channels)의 차이를 줄이도록 학습</li>
    </ul>
  </li>
</ul>

<h2 id="3-preliminaries">3. Preliminaries</h2>

<p>pass</p>

<h2 id="4-methodology">4. Methodology</h2>

<p><img src="/assets/img/Mamba/SOR-Mamba/fig2.png" alt="그림1" /></p>

<h3 id="41-architecture-of-sor-mamba">4.1. Architecture of SOR-Mamba</h3>

<ul>
  <li>Embedding layer: \(\mathbf{x} \in \mathbb{R}^{L \times C}\) into \(\mathbf{Z} \in \mathbb{R}^{C \times D}\) using a single linear layer.</li>
  <li>Mamba for CD: CD-Mamba block (<em>1D-conv</em> 제거)</li>
  <li>MLP for TD: (with layer normalization (LN))</li>
  <li>Prediction head: linear prediction head, resulting in \(\hat{\mathbf{y}} \in \mathbb{R}^{H \times C}\)</li>
</ul>

<h3 id="42-regularization-with-cd-mamba-block">4.2. Regularization with CD-Mamba Block</h3>

<ul>
  <li>\(L_{\mathrm{reg}}(\mathbf{z})=d\left(\mathbf{z}_1, \mathbf{z}_2\right)\), 여기서는 \(d\)를 MSE 사용</li>
  <li>최종적으로 \(L(\mathbf{x}, \mathbf{y})=L_{\mathrm{fcst}}(\mathbf{x}, \mathbf{y})+\lambda \cdot \sum_{i=1}^m L_{\mathrm{reg}}\left(\mathbf{z}^{(i)}\right)\)</li>
</ul>

<h3 id="43-channel-correlation-modeling">4.3. Channel Correlation modeling</h3>

<ul>
  <li>Temporal Dependencies보다는 Channel Dependencies를 강조하는 pre-training task</li>
  <li>CCM: preserve the (Pearson) correlation between channels from the data space to the latent space</li>
</ul>

<p><img src="/assets/img/Mamba/SOR-Mamba/fig3.png" alt="그림1" /></p>

<ul>
  <li>input token on the data space에서의 correlation matrices가 있고
    <ul>
      <li>output token on the latent space에의 correlation matrices가 있으면</li>
      <li>loss function for CCM은 distance between these two matrices:
        <ul>
          <li>\(L_{\mathrm{CCM}}(\mathbf{x})=d\left(\mathbf{R}_{\mathbf{x}}, \mathbf{R}_{\mathbf{z}}\right)\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-experimental-settings">5.1. Experimental settings</h3>

<p>pass</p>

<h3 id="52-time-series-forecasting">5.2. Time series Forecasting</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table2.png" alt="그림1" /></p>

<h3 id="53-transfer-learning">5.3. Transfer Learning</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table4.png" alt="그림1" /></p>

<ul>
  <li>SimMTM에 transfer learning 한 걸 S-Mamba와 비교</li>
</ul>

<h3 id="54-ablation-study">5.4. Ablation Study</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table5.png" alt="그림1" /></p>

<h2 id="6-analysis">6. Analysis</h2>

<h3 id="61-sequential-order-bias">6.1. Sequential order bias</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/fig4.png" alt="그림1" /></p>

<ul>
  <li>Channels가 많을수록, correlated 되어있을수록, Sequential order bias 큼</li>
</ul>

<h3 id="62-effect-of-regularization">6.2. Effect of regularization</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table6.png" alt="그림1" /></p>

<h3 id="63-effect-of-1d-conv">6.3. Effect of 1D-conv</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table7.png" alt="그림1" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2410.23356)]]></summary></entry><entry><title type="html">Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2024-11-12-BiMamba+/" rel="alternate" type="text/html" title="Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)" /><published>2024-11-12T00:00:00+09:00</published><updated>2025-01-08T22:02:59+09:00</updated><id>http://localhost:4000/mamba/BiMamba+</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-12-BiMamba+/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>LTSF :  long-term dependencies capturing and <strong>sparse semantic characteristics</strong></li>
  <li>Mamba
    <ul>
      <li>the selective capability on input data</li>
      <li>the hardware-aware parallel computing algorithm</li>
    </ul>
  </li>
  <li><strong>Mamba+ block</strong>
    <ul>
      <li>by adding a forget gate inside Mamba,</li>
      <li>to selectively combine the new features with the historical features in a complementary manner</li>
    </ul>
  </li>
  <li><strong>Bi-Mamba+</strong>
    <ul>
      <li>apply Mamba+ both forward and backward</li>
    </ul>
  </li>
  <li>MTS는 시나리오마다 dependency가 다름
    <ul>
      <li>(varying emphasis on intra- or inter-series dependencies)</li>
      <li>\(\to\) series-relation-aware decider
        <ul>
          <li>: controls the utilization of channel-independent or channel-mixing tokenization strategy</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer-based models
    <ul>
      <li>implicitly models the <strong>inter-series dependencies</strong> through channel-mixing embeddings</li>
      <li>However the <strong>quadratic complexity</strong> of the self-attention mechanism</li>
      <li>Informer, Autoformer : sparse attention
        <ul>
          <li>But balancing computational efficiency and predicting performance는 본질적 해결 X</li>
          <li>게다가 not explicitly capture the inter-series dependencies</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>state-space models (SSM) : design of selective scanning</li>
  <li><strong>Long-term time series modeling</strong> : patching manner (patch-wise tokens)으로 하겠다</li>
  <li><strong>Emphasis on intra- or inter-series dependencies</strong> : 데이터셋마다 intra- or inter-sequence dependencies 둘 중 뭐가 중요한지가 다름</li>
  <li>그래서 <strong>Mamba+</strong>를 디자인함
    <ul>
      <li>adding a forget gate in Mamba,</li>
      <li>to selectively combine the new features with the historical features in a complementary manner</li>
      <li>therefore preserving historical information in a longer range</li>
    </ul>
  </li>
  <li><strong>Mamba+</strong>에 기반한 <strong>Bidirectional Mamba+ (BiMamba+)</strong>를 제안
    <ul>
      <li>to model the MTS data from both forward and backward,
        <ul>
          <li>enhancing the model’s robustness and ability to capture interactions between time series elements</li>
        </ul>
      </li>
      <li>Series-Relation-Aware (SRA) decider
        <ul>
          <li>measures the proportion of highly correlated series pairs in the MTS data</li>
          <li>to automatically choose channel-independent or channelmixing tokenization strategies</li>
        </ul>
      </li>
      <li>patch-wise tokens
        <ul>
          <li>contain richer semantic information</li>
          <li>and encourage the model to learn the long-term dependencies</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="time-series-forecasting">Time Series Forecasting</h3>

<ul>
  <li>Transformer-based models : quadratic complexity to the length of the sequence
    <ul>
      <li>Informer(Zhou et al. 2021) : ProbSparse mechanism</li>
      <li>Autoformer(Wu et al. 2021) : time series decomposition</li>
      <li>Pyraformer(Liu et al. 2021) : pyramidal attention module</li>
      <li>FEDformer(Zhou et al. 2022) : frequency enhanced Transformer through frequency domain mapping</li>
      <li>PatchTST(Nie et al. 2023) : divides each univariate sequence into patches
        <ul>
          <li>and uses patch-wise self-attention to model temporal dependencies</li>
        </ul>
      </li>
      <li>Crossformer(Zhang and Yan 2023) : Cross-Dimension attention</li>
      <li>iTransformer(Liu et al. 2023) : inverts the attention layers
        <ul>
          <li>to straightly model inter-series dependencies</li>
          <li>But, the tokenization approach is simply passing the whole sequence through a Multilayer Perceptron (MLP),
            <ul>
              <li>which overlooks the complex evolutionary patterns inside the time series</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="ssm-based-models">SSM-based models</h3>

<ul>
  <li>RNN-based models :
    <ul>
      <li>maintain a hidden state which is updated with each input element</li>
      <li>limits the training speed and leads to forgetting long-term information</li>
    </ul>
  </li>
  <li>CNN-based models :
    <ul>
      <li>parallel computing and have faster training speed</li>
      <li>limits the inference speed and overlook the long-term global information</li>
    </ul>
  </li>
  <li>State Space Models (SSM) :
    <ul>
      <li>trained in parallel like CNN and inferences fastly like RNN</li>
    </ul>
  </li>
  <li>Mamba
    <ul>
      <li>parameterized matrices and a hardware-aware parallel computing algorithm to SSM</li>
      <li><strong>S-Mamba</strong>(Wang et al. 2024)</li>
      <li>embeds each univariate time series like iTransformer</li>
      <li>and feeds the embeddings into Mamba blocks
        <ul>
          <li>to model the relationships of different time series</li>
        </ul>
      </li>
      <li>However, the tokenization approach may overlook the complex evolutionary patterns</li>
      <li><strong>MambaMixer</strong>(Behrouz et al. 2024)
        <ul>
          <li>adjusts the Mamba block to bidirectional</li>
          <li>and uses two improved blocks to capture inter/intra-series dependencies simultaneously</li>
          <li>However, the gating branch is used to filter new features
            <ul>
              <li>(of both forward and backward directions)</li>
              <li>which may cause challenges for extracting new features</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>TimeMachine</strong>(Ahamed and Cheng 2024)
        <ul>
          <li>a multi-scale quadruple-Mamba architecture
            <ul>
              <li>to unify the handling of channel-mixing and channelindependence situations</li>
            </ul>
          </li>
          <li>However, simply based on the length of historical observations and variable number of different datasets
            <ul>
              <li>the characteristics of the MTS data are not fully considered.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<h3 id="31-preliminaries">3.1. Preliminaries</h3>

<ul>
  <li>Long-term multivariate time series forecasting
    <ul>
      <li>\(\mathbf{X}_{i n}=\left[x_1, x_2, \ldots, x_L\right] \in \mathbb{R}^{L \times M}\), we predict the future values \(\mathbf{X}_{\text {out }}=\left[x_{L+1}, x_{L+2}, \ldots, x_{L+H}\right] \in \mathbb{R}^{H \times M}\)</li>
    </ul>
  </li>
  <li>State Space Models
    <ul>
      <li>using first-order differential equations, \(h^{\prime}(t)=\mathbf{A} h(t)+\mathbf{B} x(t), \quad y(t)=\mathbf{C} h(t)\)
        <ul>
          <li>where \(\mathbf{A} \in \mathbb{R}^{N \times N}, \mathbf{B} \in \mathbb{R}^{D \times N} \text { and } \mathbf{C} \in \mathbb{R}^{N \times D}\)</li>
        </ul>
      </li>
      <li>can be discretized :
        <ul>
          <li>\(\begin{aligned}
&amp; \overline{\mathbf{A}}=\exp (\Delta \mathbf{A}), \\
&amp; \overline{\mathbf{B}}=(\Delta \mathbf{A})^{-1}(\exp (\Delta \mathbf{A})-\mathbf{I}) \cdot \Delta \mathbf{B}
\end{aligned}\),</li>
          <li>\(h_k=\overline{\mathbf{A}} h_{k-1}+\overline{\mathbf{B}} x_k, \quad y_k=\mathbf{C} h_k\),</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>S4</strong>(Gu et al. 2021b) :
    <ul>
      <li>HIPPO Matrix(Gu et al. 2020) to the initialization of matrix A</li>
    </ul>
  </li>
  <li><strong>Mamba</strong>(Gu and Dao 2023) :
    <ul>
      <li>parameterizes the matrices \(\mathbf{B}, \mathbf{C}\) and \(\Delta\) in a data-driven manner,</li>
      <li>introducing a selection mechanism into S4 model</li>
    </ul>
  </li>
</ul>

<h3 id="32-overview">3.2. Overview</h3>

<p><img src="/assets/img/Mamba/BiMamba+/fig1.png" alt="그림1" /></p>

<ul>
  <li>step 1) calculate the tokenization strategy indicator through the SRA decider</li>
  <li>step 2) divide the input series into patches and generate patch-wise tokens
    <ul>
      <li>based on the tokenization strategy indicator</li>
    </ul>
  </li>
  <li>step 3) obtained tokens are fed into multiple Bi-Mamba+ encoders</li>
  <li>step 4) a flatten head and a linear projector to get the final output</li>
</ul>

<h3 id="33-instance-normalization">3.3. Instance Normalization</h3>

<ul>
  <li>the input sequence의 non-stationary statistics를 제거하기 위해 RevIN (Kim et al. 2022) 사용</li>
</ul>

<h3 id="34-token-generalization">3.4. Token Generalization</h3>

<ul>
  <li>SRA Decider
    <ul>
      <li>Channel-independence / dependence는 데이터셋마다 다름</li>
      <li>\(\to\) automatic tokenization process
        <ul>
          <li>데이터셋마다 \(T=\left\{t^1, t^2, \ldots, t^M\right\}\)에 대해
            <ul>
              <li>Spearman correlation coefficients of different series \(t^i \text { and } t^j\) 계산 \(\rho_{i, j}\)</li>
              <li>\(\rho_{i, j}=1-\frac{6 \sum_{k=0}^n\left(\operatorname{Rank}\left(t_k^i\right)-\operatorname{Rank}\left(t_k^j\right)\right)^2}{n\left(n^2-1\right)}\),
                <ul>
                  <li>where \(n\) : the number of observations</li>
                  <li>\(\operatorname{Rank}\left(t_k^i\right)\) : the rank level of the \(k\)-th element in the specific time series \(t^i\)</li>
                </ul>
              </li>
              <li>threshold \(\lambda\)를 정하고 relevant series \(\rho_{\max }^\lambda\) and \(\rho_{\max }^0\)를 센 다음
                <ul>
                  <li>relation ratio \(r=\rho_{\max }^\lambda / \rho_{\max }^0 \geq 1-\lambda\)이면 channel-mixing</li>
                  <li>그렇지 않으면 channelindependent strategy</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/alg1.png" alt="그림1" /></p>

<h3 id="35-tokenization-process">3.5. Tokenization Process</h3>

<ul>
  <li>PatchTST처럼 \(x_{1: L}^i\)를 \(J = \left\lceil\frac{L-P}{S}+1\right\rceil\)개의 patch로 나누고
    <ul>
      <li>(S는 stride, P는 patch에 들어가는 시점의 개수)</li>
      <li>channel-independent strategy에서는 각 patch를 D차원으로 embedding
        <ul>
          <li>\(\to\) \(\mathbb{E}_{\text {ind }} \in \mathbb{R}^{M \times J \times D}\)</li>
        </ul>
      </li>
      <li>channel-mixing strategy에서는 같은 시점의 다른 변수들도 group으로 만들고 각 group을 tokenization
        <ul>
          <li>\(\to \mathbb{E}_{\operatorname{mix}} \in\mathbb{R}^{J \times M \times D}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="36-mamba-block">3.6. Mamba+ Block</h3>

<p><img src="/assets/img/Mamba/BiMamba+/mamba.png" alt="그림1" /></p>

<ul>
  <li><strong>기존 Mamba</strong>는 2개의 branches를 사용
    <ul>
      <li>\(b_1\)에서는 1d-conv와 SSM을 통과, 다른 하나 \(b_2\)에서는 그냥 SiLU activation 통과</li>
      <li>\(b_1\)의 SSM 안에 HIPPO가 있긴 해도 \(b_2\) 때문에 최근 정보가 더 우선시되는 문제</li>
    </ul>
  </li>
  <li>그래서 <strong>Mamba+ block</strong>에서는
    <ul>
      <li>forget gate \(\text{gate}_f=1-\text{gate}_{b_2}\)를 추가</li>
      <li>\(\text{gate}_f\)와 \(\text{gate}_{b_2}\)는 new features와 forgotten historical features를 선택적 결합
        <ul>
          <li>\(\to\) preserving historical information !</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/fig2.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/BiMamba+/alg2.png" alt="그림1" /></p>

<h3 id="37-bidirectional-mamba-encoder">3.7. Bidirectional Mamba+ Encoder</h3>

<ul>
  <li>Channel-mixing이면 \(\mathbb{E}_x^{(l)} \in \mathbb{R}^{J \times M \times D}\) and \(\mathbb{E}_x^{(0)}=\mathbb{E}_{m i x}\)
    <ul>
      <li>otherwise \(\mathbb{E}_x^{(l)} \in \mathbb{R}^{M \times J \times D}\) and \(\mathbb{E}_x^{(0)}=\mathbb{E}_{\text {ind }}\)</li>
    </ul>
  </li>
  <li>2개의 Bi-Mamba+ block, 각각 forward and backward
    <ul>
      <li>각각의 input을 \(\mathbb{E}_{x, d i r}^{(l)}\) where \(\operatorname{dir} \in\{\text{forward,backward}\}\)이라 하면</li>
      <li>\(\mathbb{E}_x^{(l+1)}=\sum_{\text {dir }}^{\{\text {forward,backward }\}} \mathcal{F}\left(\mathbb{E}_{y, \text { dir }}^{(l)}, \mathbb{E}_{x, d i r}^{(l)}\right)\)가 다음 layer의 input이 됨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/alg3.png" alt="그림1" /></p>

<h3 id="38-loss-function">3.8. Loss Function</h3>

<ul>
  <li>MSE : \(\mathcal{L}(Y, \hat{Y})=\frac{1}{\mid Y \mid} \sum_{i=1}^{\mid Y \mid}\left(y_{(i)}-\hat{y}_{(i)}\right)^2\)</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<h3 id="41-datasets">4.1. Datasets</h3>

<p><img src="/assets/img/Mamba/BiMamba+/table1.png" alt="그림1" /></p>

<h3 id="42-baseline-models">4.2. Baseline Models</h3>

<ul>
  <li>Autoformer (Wu et al. 2021)
    <ul>
      <li>series decomposition technique with Auto-Correlation mechanism</li>
    </ul>
  </li>
  <li>PatchTST (Nie et al. 2023)
    <ul>
      <li>patching and channel independent techniques</li>
    </ul>
  </li>
  <li>Crossformer (Zhang and Yan 2023)
    <ul>
      <li>PatchTST + Attention layer (for capture inter-series dependencies)</li>
    </ul>
  </li>
  <li>iTransformer (Liu et al. 2023)
    <ul>
      <li>inverts the modeling method of Transformer</li>
    </ul>
  </li>
  <li>DLinear (Zeng et al. 2023)
    <ul>
      <li>decomposes time series into two different components</li>
    </ul>
  </li>
  <li>TimesNet (Wu et al. 2022)
    <ul>
      <li>transforming the 1-D time series into a set of 2-D tensors</li>
    </ul>
  </li>
  <li>WITRAN (Jia et al. 2024)
    <ul>
      <li>RNN structure that process the univariate input sequence
        <ul>
          <li>in the 2-D space with a fixed scale</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>CrossGNN (Huang et al. 2024)
    <ul>
      <li>time series in a multi-scale way</li>
      <li>GNN to capture both cross-scale and cross-series dependencies</li>
    </ul>
  </li>
  <li>S-Mamba (Wang et al. 2024)
    <ul>
      <li>generates embeddings for each time series through a simple MLP layer</li>
      <li>and uses Mamba to extract inter-series dependencies</li>
    </ul>
  </li>
</ul>

<h3 id="43-experimental-settings">4.3. Experimental Settings</h3>

<ul>
  <li>\(L=96\) for all models on all datasets, \(H \in\{96,192,336,720\}\)</li>
  <li>\(S=\frac{1}{2} P\) and use patch length \(P=\frac{1}{4} L\)</li>
  <li>SRA decider, we set \(\lambda=0.6\)</li>
  <li>for <strong>Bi-Mamba+, PatchTST and Crossformer</strong> that use patching technique, we set \(D=128\) for Weather, Traffic, Electricity, Solar and \(D=64\) for ETT datasets,
    <ul>
      <li>while for <strong>S-Mamba and iTransformer</strong> that map the whole sequence to tokens, we set \(D=512\) for Weather, Traffic, Electricity, Solar and \(D=256\) for ETT datasets.</li>
    </ul>
  </li>
  <li>As for parameters within Mamba+ block은 Ahamed and Cheng 2024; Wang et al. 2024처럼</li>
  <li><strong>convolutional kernel size</strong> d_conv =2 and <strong>hidden state expansion</strong> expand =1 on all datasets.</li>
  <li><strong>hidden dimension</strong> d_state =16 for Weather, Electricity and Traffic and d_state =8 for ETT datasets.</li>
  <li><strong>encoder layer</strong> \(l \in\{1,2,3\}\),</li>
  <li><strong>learning rate</strong>는 \([5 \mathrm{e}-5,1 \mathrm{e}-4,2 \mathrm{e}-4,5 \mathrm{e}-4,1 \mathrm{e}-3, 2 \mathrm{e}-3,5 \mathrm{e}-3]\)</li>
</ul>

<h3 id="44-main-results">4.4. Main Results</h3>

<p><img src="/assets/img/Mamba/BiMamba+/table3.png" alt="그림1" /></p>

<h3 id="45-ablation-study">4.5. Ablation Study</h3>

<p><img src="/assets/img/Mamba/BiMamba+/table4.png" alt="그림1" /></p>

<ul>
  <li>(a) w/o SRA-I which use channel-independent strategy only</li>
  <li>(b) w/o SRA-M which use channelmixing strategy only</li>
  <li>(c) w/o Bi which use forward direction Mamba block only</li>
  <li>(d) w/o Residual that removes the residual connection</li>
  <li>(e) S-Mamba</li>
  <li>
    <p>(f) PatchTST used for the benchmark models</p>
  </li>
  <li><strong>filter threshold</strong> \(\lambda\)에 따른 tokenization strategy indicator</li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/fig3.png" alt="그림1" /></p>

<ul>
  <li>length of patches \(P\)에 따른 MSE</li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/fig4.png" alt="그림1" /></p>

<h3 id="46-model-efficiency">4.6. Model efficiency</h3>

<p><img src="/assets/img/Mamba/BiMamba+/fig7-1.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/BiMamba+/fig7-2.png" alt="그림1" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li><strong>Bi-Mamba+</strong></li>
  <li>adding forget gate in Mamba
    <ul>
      <li>to selectively combine the added new features with the forgotten historical features in a complementary manner,</li>
      <li>therefore preserving historical information in a longer range</li>
    </ul>
  </li>
  <li>dividing the time series into patches
    <ul>
      <li>for inter-series dependencies at a finer granularity</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2404.15772)]]></summary></entry><entry><title type="html">S-Mamba: Is Mamba Effective for Time Series Forecasting? (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2024-11-07-s-mamba/" rel="alternate" type="text/html" title="S-Mamba: Is Mamba Effective for Time Series Forecasting? (Arxiv 2024)" /><published>2024-11-07T00:00:00+09:00</published><updated>2025-01-08T22:02:58+09:00</updated><id>http://localhost:4000/mamba/s-mamba</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-07-s-mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Time series forecasting (TSF)에서 Transformer는 quadratic complexity</li>
  <li><strong>Simple-Mamba (S-Mamba)</strong> :
    <ul>
      <li>Tokenize the time points of each variate autonomously via a linear layer</li>
      <li>Bi-directional Mamba layer is utilized to extract inter-variate correlations</li>
      <li>Feed-Forward Network is set to learn temporal dependencies</li>
      <li>Generation of forecast outcomes through a linear mapping layer</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>
    <p>Transformer-based</p>

    <ul>
      <li>quadratic computational complexity</li>
      <li>reduce the computational complexity \(\to\)  loss of information \(\to\) performance degradations</li>
    </ul>
  </li>
  <li>
    <p>Linear model</p>

    <ul>
      <li>solely on linear numerical calculations \(\to\) do not incorporate in-context information</li>
    </ul>
  </li>
  <li>
    <p>SSM</p>

    <ul>
      <li>convolutional calculation to capture sequence information</li>
      <li>eliminate hidden states (for parallel computing) \(\to\) near-linear complexity</li>
      <li>But unable to identify and filter content</li>
    </ul>
  </li>
  <li>
    <p>Mamba</p>

    <ul>
      <li>selective mechanism into SSM</li>
    </ul>
  </li>
  <li>
    <p><strong>Simple-Mamba (S-Mamba)</strong> :</p>

    <ul>
      <li>
        <ol>
          <li>time points of each variate are tokenized by a linear layer</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>Mamba VC (Inter-variate Correlation) Encoding layer encodes the VC</li>
        </ol>

        <ul>
          <li>by utilizing a bidirectional Mamba</li>
        </ul>
      </li>
      <li>
        <ol>
          <li>FeedForward Network (FFN) TD (Temporal Dependency) Encoding Layer extracts the TD</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>mapping layer is utilized to output the forecast results</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-preliminaries">3. Preliminaries</h2>

<h3 id="31-problem-statement">3.1. Problem Statement</h3>

<ul>
  <li>\(U_{\text {in }}=\left[u_1, u_2, \ldots, u_L\right] \in \mathbb{R}^{L \times V}\)를 보고 \(U_{\text {out }}=\left[u_{L+1}, u_{L+2}, \ldots, u_{L+T}\right] \in \mathbb{R}^{T \times V}\)를 예측
    <ul>
      <li>각각의 \(u_n=\left[p_1, p_2, \ldots, p_V\right]\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-state-space-models">3.2. State Space Models</h3>

<ul>
  <li>The continuous sequence is discretized by a step size \(\Delta\), and the discretized SSM model :
    <ul>
      <li>\(\begin{aligned}
h_t &amp; =\overline{\boldsymbol{A}} h_{t-1}+\overline{\boldsymbol{B}} x_t, \\
y_t &amp; =\boldsymbol{C} h_t,
\end{aligned}\)      where
        <ul>
          <li>\(\overline{\boldsymbol{A}}=\exp (\Delta \boldsymbol{A}) \text { and } \overline{\boldsymbol{B}}=(\Delta \boldsymbol{A})^{-1}(\exp (\Delta \boldsymbol{A})-I) \cdot \Delta \boldsymbol{B}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="33-mamba-block">3.3. Mamba Block</h3>

<p>: <strong>Data-dependent selection mechanism into the S4</strong> &amp; <strong>Incorporates hardware-aware parallel algorithms</strong></p>

<p><img src="/assets/img/Mamba/s-mamba/alg1.png" alt="그림1" /></p>

<ul>
  <li>
    <p>Mamba Layer</p>

    <ul>
      <li>Input : \(X \in \mathbb{R}^{B \times V \times D}\)</li>
      <li>expands the hidden dimension to \(ED\) through linear projection \(\to x, z\)를 얻음</li>
      <li>the projection using convolutional functions and a SiLU \(\to x'\)를 얻음</li>
      <li>generates the state representation \(y\)</li>
      <li>\(y\) is combined with a residual connection from \(z\) after activation,
        <ul>
          <li>and the final output \(y_t\) at time step $t$ is obtained</li>
        </ul>
      </li>
      <li>with state expansion factor \(N\),
        <ul>
          <li>a size of convolutional kernel \(k\),</li>
          <li>and a block expansion factor \(E\)</li>
        </ul>
      </li>
      <li>The final output of the Mamba block is \(Y \in \mathbb{R}^{B \times V \times D}\).</li>
    </ul>
  </li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<ul>
  <li>1st layer : the Linear Tokenization Layer (tokenizes the time series with a linear layer)</li>
  <li>2nd layer :  the Mamba intervariate correlation (VC) Encoding layer (using a bidirectional Mamba block)</li>
  <li>3rd layer : the FFN Temporal Dependencies (TD) Encoding Layer (learns the temporal sequence information)
    <ul>
      <li>Feed-Forward Network : generates future series representations</li>
    </ul>
  </li>
  <li>4th layer : Projection Layer, is only mapping for forecasting</li>
</ul>

<p><img src="/assets/img/Mamba/s-mamba/alg2.png" alt="그림1" /></p>

<h3 id="41-linear-tokenization-layer">4.1. Linear Tokenization Layer</h3>

<ul>
  <li>\(U=\operatorname{Linear}\left(\operatorname{Batch}\left(U_{\text {in }}\right)\right), \quad\) where \(U\) is the output of this layer</li>
</ul>

<h3 id="42-mamba-vc-encoding-layer">4.2. Mamba VC Encoding Layer</h3>

<ul>
  <li>여기서는 유사한 trend를 보이는 변수들을 연결해서 VC를 찾고 싶음</li>
  <li>Transformer는 그냥 모든 변수들끼리 다 연결하니까 정확하긴 한데 변수 개수 따라 complexity 늘어남</li>
  <li>Mamba는 complexity는 near-linear이지만
    <ul>
      <li><strong>Selection mechanism이 uni-directional해서 앞쪽의 변수만 볼 수 있음</strong></li>
      <li>그래서 2개의 Mamba를 서로 다른 방향으로 흐르도록 놓음 (Bi-directional Mamba)</li>
      <li>\(\begin{aligned}&amp;\overrightarrow{\boldsymbol{Y}}=\overrightarrow{\operatorname{Mamba\operatorname {Block}}(\boldsymbol{U}),} \\
&amp; \overleftarrow{\boldsymbol{Y}}=\overleftarrow{\operatorname{Mamba} \operatorname{Block}}(\boldsymbol{U}) .
\end{aligned}\)이고 \(\boldsymbol{Y}=\overrightarrow{\boldsymbol{Y}}+\overleftarrow{\boldsymbol{Y}}\)로 Aggregate, with residual : \(\boldsymbol{U}^{\prime}=\boldsymbol{Y}+\boldsymbol{U}\)</li>
    </ul>
  </li>
</ul>

<h3 id="43-ffn-td-encoding-layer">4.3. FFN TD Encoding Layer</h3>

<ul>
  <li>
    <ol>
      <li>Normalization layer : enhance convergence and training stability</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>FFN layer : encodes observed time series (encodes TD by keeping the sequential relationships)</li>
    </ol>

    <ul>
      <li>decodes future series representations (adjust the future series representations)</li>
    </ul>
  </li>
</ul>

<h3 id="44-projection-layer">4.4. Projection Layer</h3>

<ul>
  <li>FFN TD Encoding layer의 output인 tokenized temporal information이
    <ul>
      <li>linear mapping을 통해서 reconstructed for predictive outcome</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-datasets-and-baselines">5.1. Datasets and Baselines</h3>

<p><img src="/assets/img/Mamba/s-mamba/table1.png" alt="그림1" /></p>

<ul>
  <li>SOTA와 비교 :
    <ul>
      <li>iTransformer: analyzes the time series information of each <strong>individual variate</strong> and then fuses the information of all variates.</li>
      <li>PatchTST: segments time series into <strong>subseries patches</strong> as input tokens and uses channel-independent shared embeddings and weights</li>
      <li>Crossformer: cross-attention mechanism that allows the model to <strong>interact with information between different time steps</strong></li>
      <li>FEDformer: a <strong>frequency-enhanced Transformer</strong> for utilizaing a sparse representation</li>
      <li>Autoformer: <strong>decomposition architecture</strong> that incorporates an auto-correlation mechanism</li>
      <li>RLinear: reversible normalization and channel independence into <strong>pure linear structure</strong></li>
      <li>TiDE: Multi-layer Perceptron (MLP) based encoderdecoder model</li>
      <li>DLinear: <strong>simple one-layer linear</strong> model with decomposition architecture</li>
      <li>TimesNet: a task-general backbone, <strong>transforms 1D time series into 2D tensors</strong></li>
    </ul>
  </li>
</ul>

<h3 id="52-overall-performance">5.2. Overall Performance</h3>

<p><img src="/assets/img/Mamba/s-mamba/table2.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/s-mamba/table3.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/s-mamba/table4.png" alt="그림1" /></p>

<ul>
  <li>S-Mamba가 traffic-related, Electricity, and Solar-Energy에서 성능이 좋음
    <ul>
      <li>변수들이 periodic한 데이터셋들.</li>
      <li>즉 period variates are more likely to contain learnable VC.</li>
      <li>Mamba VC Fusion Layer가 잘 잡은 것</li>
    </ul>
  </li>
  <li>ETT, and Exchange datasets에서는 성능이 매우 좋지는 않았음
    <ul>
      <li>변수 개수가 적은 데이터셋들 (predominantly of an aperiodic nature)</li>
      <li><strong>weak</strong> VCs between these variates 때문에 Mamba VC Encoding layer가 noise를 가져옴</li>
    </ul>
  </li>
  <li>Weather는 변수도 적고 aperiodic한데 왜 성능이 좋나
    <ul>
      <li>변수들의 Trend가 동시에 나타나는 도메인이라서 Mamba VC Encoding layer가 잘 작동</li>
      <li>Trend가 large sections로 나타나기 때문에 FFN이 이런 거 잘 잡음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/s-mamba/fig4.png" alt="그림1" /></p>

<h3 id="53-model-efficiency">5.3. Model Efficiency</h3>

<p><img src="/assets/img/Mamba/s-mamba/fig5.png" alt="그림1" /></p>

<h3 id="54-ablation-study">5.4. Ablation Study</h3>

<p><img src="/assets/img/Mamba/s-mamba/table5.png" alt="그림1" /></p>

<h3 id="55-can-variate-order-affect-the-performance-of-s-mamba">5.5. Can Variate Order Affect the Performance of S-Mamba?</h3>

<ul>
  <li>S-Mamba는 independent channel이라서 variates order 안중요했음
    <ul>
      <li>하지만 Mamba VC Encoding Layer는 variates order에 따라 initial bias 발생할 수 있음</li>
    </ul>
  </li>
  <li>그래서 Fourier transform해서 variates를 periodic and aperiodic groups으로 나누고
    <ul>
      <li>periodic variates =  reliable information / aperiodic variates = potential noise으로 가정</li>
    </ul>
  </li>
  <li>그래서 reliable information를 가지고 있는(그럴 것이라고 생각되는) periodic variates를 앞쪽에 배치</li>
</ul>

<p><img src="/assets/img/Mamba/s-mamba/fig6.png" alt="그림1" /></p>

<h3 id="56-can-mamba-outperform-advanced-transformers">5.6. Can Mamba Outperform Advanced Transformers?</h3>

<ul>
  <li>Transformer의 Encoder layer를 Mamba로 교체
    <ul>
      <li>Autoformer, Flashformer and Flowformer</li>
      <li>\(\to\)  Auto-M, FlashM and Flow-M 이라고 부르겠음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/s-mamba/fig7.png" alt="그림1" /></p>

<h3 id="57-can-mamba-help-benefit-from-increasing-lookback-length">5.7. Can Mamba Help Benefit from Increasing Lookback Length?</h3>

<ul>
  <li>Transformer-based model은:
    <ul>
      <li>lookback sequence length \(L\)이 늘어나도 성능이 비례해서 좋아지는 건 아님</li>
      <li><strong>sequential order를 신경쓰지 않아서 그렇다</strong></li>
    </ul>
  </li>
  <li>Mamba는:
    <ul>
      <li>certain sequential attributes가 잘 유지되는 편</li>
      <li>Mamba block을 Transformer-based model의 Encoder와 decoder 사이에 배치하면
        <ul>
          <li>Encoder layer의 output에 (decoder layer에 가기 전에)</li>
          <li>positional encoding처럼 어떤 정보를 추가해주는 역할을 하는 것</li>
        </ul>
      </li>
      <li>그것을 Reformer, Informer, and Transformer와 비교해서
        <ul>
          <li>Refor-M, Infor-M, and Trans-M라고 부르고 비교</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/s-mamba/fig8.png" alt="그림1" /></p>

<ul>
  <li>S-Mamba와 iTransformer 둘다 \(L\)이 길어지면 어느 정도 성능이 좋아지긴 함
    <ul>
      <li>하지만 이건 두 모델 모두 가지고 있는 FFN TD Encoding Layer 때문으로 보임</li>
    </ul>
  </li>
  <li>S-Mamba가 iTransformer와 비교해서 일관되게 성능이 좋아지는 편인데
    <ul>
      <li>이건 Mamba VC Encoding layer와 Transformer의 VC Encoding layer의 차이 !</li>
    </ul>
  </li>
</ul>

<h3 id="58-is-mamba-generalizable-in-tsf">5.8. Is Mamba Generalizable in TSF?</h3>

<ul>
  <li>Transformer는 generalization capabilities가 좋은 편이라서
    <ul>
      <li>iTransformer의 경우 40%의 변수만을 가지고 나머지 변수들은 masking해도</li>
      <li>성능이 큰 폭으로 나빠지지는 않음</li>
    </ul>
  </li>
  <li>Mamba의 경우에도 40%의 변수만 보고 나머지 변수들은 masking 했을 때
    <ul>
      <li>iTransformer에 크게 뒤쳐지지 않음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/s-mamba/fig9.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li><strong>Simple-Mamba(S-Mamba)</strong>
    <ul>
      <li>inter-variate correlation (VC) encoding은</li>
      <li>Transformer 대신 <strong>bi-directional</strong> Mamba block으로 하고</li>
      <li>(더 낮은 overhead로 VC를 파악)</li>
      <li>Temporal Dependencies (TD)는
        <ul>
          <li>Feed-Forward Network로 extract</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Mamba는 advanced-Transformer만큼의 <strong>stability</strong>도 있고
    <ul>
      <li><strong>generalization</strong> capabilities도 뛰어난 편</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2403.11144v3)]]></summary></entry><entry><title type="html">TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)</title><link href="http://localhost:4000/mamba/2024-10-29-timemachine/" rel="alternate" type="text/html" title="TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (ECAI 2024)" /><published>2024-10-29T00:00:00+09:00</published><updated>2025-01-08T22:02:56+09:00</updated><id>http://localhost:4000/mamba/timemachine</id><content type="html" xml:base="http://localhost:4000/mamba/2024-10-29-timemachine/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Long-term time-series forecasting(LTSF)에서 우리는 long-term dependencies를 capture하는데
    <ul>
      <li>linear scalability와 computational efficiency를 유지해야 함</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 TimeMachine은 Mamba를 활용하여
    <ul>
      <li>the unique properties of time series를 발견하여
        <ul>
          <li>multi-scales에서 salient contextual cues를 만들고</li>
        </ul>
      </li>
      <li>quadruple-Mamba architecture를 합쳐서
        <ul>
          <li>channel-mixing and channel-independence를 한 번에 통합</li>
        </ul>
      </li>
      <li>서로 다른 scales에서의 global / local contexts를 effective하게 selection할 수 있게 함</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>LTSF에서 Capturing long-term dependencies가 핵심</li>
  <li><strong>Linear model</strong> : DLinear, TiDE
    <ul>
      <li>may not well capture long-range correlations</li>
    </ul>
  </li>
  <li><strong>Transformer-based</strong> : iTransformer, PatchTST, Crossformer
    <ul>
      <li>suffer from the quadratic complexity</li>
    </ul>
  </li>
  <li><strong>state-space models (SSMs)</strong>
    <ul>
      <li>inferring over very long sequences</li>
      <li>context-aware selectivity</li>
      <li>LTSF에서도 활용될 수 있는가
        <ul>
          <li>highly content- and context-selective SSM이 최근에 많이 나오고 있고</li>
          <li>effectively representing the context in time series에 쓸 수 있을 것</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Transforemr-based approach에서는 each observation이나 sub-series, 아니면 time series를 token(patch)로 만드는데
    <ul>
      <li>SSM에서 이러한 접근을 그대로 쓰면 성능이 안나옴</li>
      <li><strong>그래서 salient contextual cues tailored to SSM을 extract하는 것이 먼저 !</strong></li>
    </ul>
  </li>
  <li>기존에는 channel-mixing way도 있고 (ex. Informer, FEDformer, and Autoformer, …)
    <ul>
      <li>channel-independence way도 있는데, (ex. PatchTST, TiDE, …)</li>
      <li>본 논문에서는 unified architecture : applicable to both scenarios!</li>
    </ul>
  </li>
  <li>그리고 Time series에는 downsampling해도 temporal relations가 유지된다는 특징이 있으니
    <ul>
      <li>모든 time points를 token으로 만드는 건 redundant하고, PatchTST처럼 patch를 사용하는 건 good</li>
      <li>하지만 pre-defined small patch는 fixed resolution에서의 context만 제공</li>
      <li>그러니 iTransformer처럼 whole look-back window를 token으로 만드는 것이 낫고</li>
      <li>하지만 iTransformer처럼 channel-independence에서는 select sub-token contents가 잘 안됨</li>
      <li>그러므로, SSM 쓰면 더 잘 될 것</li>
    </ul>
  </li>
  <li>그러니 본 논문에서는 TimeMachine을 제안
    <ul>
      <li>MTS를 2개의 scale에서 context-aware prediction하기 위해 SSM을 사용
        <ul>
          <li>high, low resolution이라는 2개의 scale마다 2개의 mamba를 사용.
            <ul>
              <li>하나는 global perspectives for the channel-mixing</li>
              <li>다른 하나는 both global and local perspectives for the channel-independence</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>이렇게 4개의 SSM modules를 사용해서 channel-independent, -dependent를 통합하고
        <ul>
          <li>즉 btw-channel correlation이 “있으면” 잡아내고 “없으면” independent 처럼</li>
          <li>다양한 scales에서 global and local contextual information을 효율적으로 selection</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li>Non-Transformer-based Supervised Approaches
    <ul>
      <li>Classical methods : ARIMA, VARMAX, GARCH, RNN, …</li>
      <li>MLP-based models : DLinear, TiDE, RLinear, …</li>
      <li>CNN-based : TimesNet, Scinet, …</li>
    </ul>
  </li>
  <li>Transformer-based Supervised Learning methods
    <ul>
      <li>iTransformer, PatchTST, Crossformer, FEDformer, stationary, Flowformer, and Autoformer</li>
      <li>time series를 token series로 만들고 self-attention</li>
      <li>하지만 quadratic time and memory complexity</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-method">3. Proposed Method</h2>

<ul>
  <li>input sequence \(\mathbf{x}=\left[x_1, \ldots, x_L\right]\)
    <ul>
      <li>\(x_t \in \mathcal{R}^M\) representing a vector of \(M\) channels at time point \(t\)</li>
    </ul>
  </li>
  <li><strong>Normalization</strong>
    <ul>
      <li>the original MTS \(\mathbf{x}\) into \(\mathbf{x}^0=\left[\mathbf{x}_1^{(0)}, \cdots, \mathbf{x}_L^{(0)}\right] \in \mathcal{R}^{M \times L}\), via \(\mathbf{x}^{(0)}=\operatorname{Normalize}(\mathbf{x})\).</li>
      <li>Here, Normalize \((\cdot)\) represents a normalization operation RevIN</li>
    </ul>
  </li>
  <li><strong>Channel Mixing vs. Channel Independence</strong>
    <ul>
      <li>PatchTST에서는 Channel Independence가 좋다고 하지만
        <ul>
          <li>그건 length에 비해 channels가 많지 않을 때 이야기고,</li>
          <li>channels가 많을 때에는 Channel Mixing이 더 낫다</li>
        </ul>
      </li>
      <li>TimeMachine은 “potentially” inter-channel correlation을 잡고
        <ul>
          <li>Channel Independence일 때에는 independence를 찾음</li>
        </ul>
      </li>
      <li>input의 shape은 BML, output은 BMT</li>
    </ul>
  </li>
  <li><strong>Embedded Representations</strong>
    <ul>
      <li>2-stage embedded representation</li>
      <li>\(\mathbf{x}^{(1)}=E_1\left(\mathbf{x}^{(0)}\right), \quad \mathbf{x}^{(2)}=E_2\left(D O\left(\mathbf{x}^{(1)}\right)\right)\), where
        <ul>
          <li>\(E_1: \mathbb{R}^{M \times L} \rightarrow \mathbb{R}^{M \times n_1}\) and \(E_2: \mathbb{R}^{M \times n_1} \rightarrow \mathbb{R}^{M \times n_2}\)은 MLP</li>
          <li>DO는 dropout, (MLP 쓰니까 overfitting 방지)</li>
        </ul>
      </li>
      <li>이렇게 input length에 상관없이 fixed-length tokens로 embedding</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/timemachine/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p><strong>Integrated Quadruple Mambas</strong> (fig1 보면서 이해하면 좋음)</p>

    <ul>
      <li>
        <p>\(E_1, E_2\) 각각의 embedding level에서 2개의 mamba를 사용</p>

        <ul>
          <li>\(E_1\) level에서 사용되는 2개의 mamba의 input은 \(D O\left(\mathbf{x}^{(1)}\right)\)</li>
          <li>\(E_2\) level에서 사용되는 2개의 mamba의 input은 \(D O\left(\mathbf{x}^{(2)}\right)\)</li>
        </ul>
      </li>
      <li>
        <p>첫 번째 mamba block 안에서는 2개의 FC-layers가 linear projection하고</p>

        <ul>
          <li>하나만 1d causal conv와 SiLU activation 통과, 그리고 structured SSM으로 간다</li>
          <li>그 다음 남은 하나의 linear projection을 더하고 FC-layer를 한 번 더 태움</li>
          <li>이때 <strong>continuous-time SSM</strong>은 input sequence \(u(t)\)를 latente state \(h(t)\)를 통해 output \(v(t)\)로 보낸다.
            <ul>
              <li>즉 \(d h(t) / d t=A h(t)+B u(t), \quad v(t)=C h(t)\)
                <ul>
                  <li>\(h(t)\) is \(N\)-dimensional (\(N\)은 state expansion factor)</li>
                  <li>\(u(t)\) is \(D\)-dimensional (\(D\)는 dimension factor)</li>
                  <li>\(v(t)\)의 dimension도 \(D\)</li>
                  <li>\(A\), \(B\), \(C\)는 coefficient matrices of proper size</li>
                </ul>
              </li>
              <li><strong>여기서 \(A\), \(B\), \(C\), 그리고 hidden state를 time interval \(\Delta\)에 대한 함수로 놓음</strong></li>
              <li>이것이 모델을 input에 adaptive하게 context selectivity를 강화하는 방법
                <ul>
                  <li>즉 \(h_k=\bar{A} h_{k-1}+\bar{B} u_k, \quad v_k=C h_k\)</li>
                  <li>where \(h_k, u_k\), and \(v_k\) are respectively samples of \(h(t), u(t)\), and \(v(t)\) at time \(k \Delta\),</li>
                  <li>\(\bar{A}=\exp (\Delta A), \quad \bar{B}=(\Delta A)^{-1}(\exp (\Delta A)-I) \Delta B\).</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>(continuous 말고) <strong>SSM</strong>은 \(B\), \(C\), \(\Delta\)가 input에 따라 달라짐
            <ul>
              <li>\(B, C \leftarrow \operatorname{Linear}_N(u)\), \(\Delta \leftarrow \text{softplus}(parameter +Linear _D\left(\right. Linear \left.\left._1(u)\right)\right)\)</li>
              <li>coefficient matrices는 current token을 보고 정보를 selectively propagate하게 함</li>
              <li><strong>channel-mixing case</strong>에서는 각 univariate가 token(dim=\(n_2\))이 되고
                <ul>
                  <li><strong>Inner mambas</strong>에서는 \(BMn_2\)이 나오는데</li>
                  <li>Left / right inner mamba의 k번째 변수의 output은 \(v_{L, k}, v_{R, k} \in \mathcal{R}^{n_2}\)
                    <ul>
                      <li>둘을 더하고 embedding된 \(\mathbf{x}^{(2)}\)을 skip connection하면 \(\mathbf{x}^{(3)}=\mathbf{v}_L \bigoplus \mathbf{v}_R \bigoplus \mathbf{x}^{(2)}\) (Element-wise addition)</li>
                      <li>그 다음 linear mapping \(P_1: \mathbf{x}^{(3)} \rightarrow\mathbf{x}^{(4)} \in \mathcal{R}^{M \times n_1}\)</li>
                    </ul>
                  </li>
                  <li><strong>Outer mambas</strong>에서도 비슷하게
                    <ul>
                      <li>\(v_{L, k}^*, v_{R, k}^* \in \mathcal{R}^{n_1}\)구하고 \(\mathbf{x}^{(5)} \in \mathcal{R}^{M \times n_1}\)랑 해서 셋이 더함</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li><strong>channel-independence</strong>에서는 처음에 \(B M L \mapsto(B \times M) 1 L\) 이렇게 reshape을 해서 마치 match가 \(BM\)개이고 univariates인 것처럼 처리
                <ul>
                  <li>Outer든 inner이든
                    <ul>
                      <li>mamba 하나는 input dim =1, token length =\(n_1\) or \(n_2\),</li>
                      <li>다른 하나는 input dim =\(n_1\) or \(n_2\), token length =1</li>
                    </ul>
                  </li>
                  <li>이렇게 하면 global context and local context 동시에 학습 가능하고</li>
                  <li>fine and coarse scales with high- and low-resolution 각각의 context 추출</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Channel mixing은 변수 개수가 많을 때 하고 independence랑 switch하려면</p>

        <ul>
          <li>input sequence를 그냥 transposed하면 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Output Projection</p>

    <ul>
      <li>MLP 쓰고 \(P_1\) performs a mapping \(\mathcal{R}^{M \times n_2} \rightarrow \mathcal{R}^{M \times n_1}\), \(P_2\)는 \(\mathbb{R}^{M \times 2 n_1} \rightarrow \mathbb{R}^{M \times T}\)</li>
      <li>Residual connection도 fig1처럼 해주고</li>
      <li>Outer Mambas에서 나온 \(\mathbf{x}^{(5)}\)랑 Inner Mambas에서 나온 \(\mathbf{x}^{(4)}\)를 concat해서 사용하게 됨
        <ul>
          <li>즉 \(\mathbf{x}^{(6)}=\mathbf{x}^{(5)} \|\left(\mathbf{x}^{(4)} \bigoplus \mathbf{x}^{(1)}\right)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-result-analysis">4. Result Analysis</h2>

<h3 id="41-datasets">4.1. Datasets</h3>

<ul>
  <li>seven benchmark datasets extensively used for LTSF:
    <ul>
      <li>Weather, Traffic, Electricity, and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2)</li>
    </ul>
  </li>
</ul>

<h3 id="42-experimental-environment">4.2. Experimental Environment</h3>

<ul>
  <li>본 논문에서 제시하는 TimeMachine을 11 SOTA models와 비교 :
    <ul>
      <li>including iTransformer, PatchTST, DLinear, RLinear, Autoformer, Crossformer, TiDE, Scinet, TimesNet, FEDformer, and Stationary</li>
    </ul>
  </li>
</ul>

<h3 id="43-quantitative-results">4.3. Quantitative Results</h3>

<p><img src="/assets/img/Mamba/timemachine/fig2.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/timemachine/fig3.png" alt="그림1" /></p>

<h3 id="44-qualitative-result">4.4. Qualitative Result</h3>

<p><img src="/assets/img/Mamba/timemachine/table2.png" alt="그림1" /></p>

<h2 id="5--hyperparameter-sensitivity-analysis-and-ablation-study">5.  Hyperparameter Sensitivity Analysis and Ablation Study</h2>

<h3 id="51-effect-of-mlps-parameters-n1-n2">5.1. Effect of MLPs’ Parameters (n1, n2)</h3>

<ul>
  <li>MLP의 size인 \(n_1, n_2\)를 다양하게 해봤는데 별 차이 없음 (fig5)
    <ul>
      <li>MLP에 heavily dependent하지 않다는 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/timemachine/fig5.png" alt="그림1" /></p>

<h3 id="52-sensitivity-of-dropouts">5.2. Sensitivity of Dropouts</h3>

<ul>
  <li>Dropout ratio 적당하게 0.7 사용</li>
</ul>

<h3 id="53-ablation-of-residual-connections">5.3. Ablation of Residual Connections</h3>

<ul>
  <li>Residual connections 쓰는 것이 좋더라</li>
</ul>

<h3 id="54-effects-of-mambas-local-convolutional-width">5.4. Effects of Mambas’ Local Convolutional Width</h3>

<ul>
  <li>각 Mamba 안에도 parameters가 있을거니까 local convolutional kernel widths를 2 and 4로실험 해봤더니 2가 낫더라</li>
</ul>

<h3 id="55-ablation-on-state-expansion-factor-of-mambas">5.5. Ablation on State Expansion Factor of Mambas</h3>

<p><img src="/assets/img/Mamba/timemachine/fig6.png" alt="그림1" /></p>

<ul>
  <li>State Expansion Factor를 8부터 256까지 해봤는데 256이 제일 좋아서 defualt로 설정</li>
</ul>

<h3 id="56-ablation-on-mamba-dimension-expansion-factor">5.6. Ablation on Mamba Dimension Expansion Factor</h3>

<ul>
  <li>dimension expansion factor (\(E\))도 있었는데, 크게하면 메모리는 많이 먹는데 성능 향상으로 이어지지는 않아서 그냥 1로 둔다</li>
</ul>

<h2 id="6-strengths-and-limitations">6. Strengths and Limitations</h2>

<ul>
  <li>memory efficiency and stable performance across varying look-back and prediction lengths !</li>
  <li>Weather에서 1등 못한게 limitation. (….?ㅋㅋ)</li>
</ul>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>LTSF with linear scalability and small memory footprints !</li>
  <li>integrated quadruple-Mamba architecture
    <ul>
      <li>to predict with rich global and local contextual cues at multiple scales</li>
      <li>\(\to\) unifies channel-mixing and channel-independence situations</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[ECAI 2024](https://arxiv.org/abs/2403.09898)]]></summary></entry><entry><title type="html">Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Arxiv 2023)</title><link href="http://localhost:4000/mamba/2024-10-28-mamba/" rel="alternate" type="text/html" title="Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Arxiv 2023)" /><published>2024-10-28T00:00:00+09:00</published><updated>2025-01-08T22:02:54+09:00</updated><id>http://localhost:4000/mamba/mamba</id><content type="html" xml:base="http://localhost:4000/mamba/2024-10-28-mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>많은 subquadratic-time architectures (linear attention, gated convolution and recurrent models, and structured state space models (SSMs))가 Transformer의 연산 효율성을 해결하기 위해 제안되었지만
    <ul>
      <li>content-based reasoning에서는 여전히 약한 모습</li>
    </ul>
  </li>
  <li>그래서 본 논문에서 제시하는 Mamba는
    <ul>
      <li><strong>SSM parameters를 input의 함수 형태</strong>로 놓아서 모델이 selectively propagate or forget information 할 수 있도록 함</li>
      <li>그리고 recurrent 모드로 학습을 진행하게 되면 중간 Hidden State 크기가 매우 커질 수 있기 때문에
        <ul>
          <li><strong>hardware-aware parallel algorithm</strong>을 사용하여 hidden State를 메모리에 저장하지 않고 병렬적으로 scan 연산함</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>Selection Mechanism</strong>
    <ul>
      <li>parameterizing the SSM parameters based on the input
        <ul>
          <li>\(\to\) 필요한 정보만 기억하고 필요없는 정보 filter out</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Hardware-aware Algorithm</strong>
    <ul>
      <li>연산 커널을 결합하는 방식(커널 융합)으로 메모리 입출력 과정을 최적화하고 오버헤드를 줄임</li>
      <li>고속 메모리(SRAM)를 활용해 느린 GPU 메모리(HBM) 의존도를 줄여 연산 속도를 높이겠다는 것</li>
      <li>backpropagation 할 때에는 hidden state를 저장하지 않고 필요할 때마다 재계산함으로써 메모리 사용량을 최소화</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/mamba_/fig1.png" alt="그림1" /></p>

<ul>
  <li><strong>Architecture</strong>
    <ul>
      <li>기존 SSM architectures와 Transformer의 MLP blocks을 합쳐서 Mamba를 만듬</li>
    </ul>
  </li>
</ul>

<h2 id="2-state-space-models">2. State Space Models</h2>

<ul>
  <li>Structured state space sequence models (S4)
    <ul>
      <li>inspired by a particular continuous system :
        <ul>
          <li>1-dimensional function or sequence $x(t) \in \mathbb{R} \mapsto y(t) \in \mathbb{R}$ through an implicit latent state $h(t) \in \mathbb{R}^N$.</li>
          <li>\(\begin{aligned} h^{\prime}(t) &amp; =A h(t)+B x(t) \\ y(t) &amp; =C h(t)\end{aligned}\) (1)</li>
          <li>4개의 parameters \((\Delta, A, B, C)\)로 정의됨 (아직 input의 함수 형태가 아님)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Discretization</strong>
    <ul>
      <li>첫번째 단계는 “continuous parameters” \((\Delta, A, B)\)를 “discrete parameters” \((\bar{A}, \bar{B})\)로 바꾸는 것
        <ul>
          <li>fixed formulas \(\overline{A}=f_A(\Delta, A)\) and \(\overline{B}=f_B(\Delta, A, B)\)를 사용</li>
          <li>\(\left(f_A, f_B\right)\)는 discretization rule</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Computation</strong>
    <ul>
      <li>\((\Delta, A, B, C) \mapsto(\bar{A}, \bar{B}, C)\) 변환이 끝났으면 그 다음에는 다음 두 가지 형태의 computation 가능
        <ul>
          <li>a linear recurrence :
            <ul>
              <li>\(\begin{aligned} h_t &amp; =\overline{A} h_{t-1}+\overline{B} x_t \\ y_t &amp; =C h_t\end{aligned}\) (2) 또는</li>
            </ul>
          </li>
          <li>a global convolution :
            <ul>
              <li>\(\begin{aligned} \bar{K} &amp; =\left(C \bar{B}, C \overline{A B}, \ldots, C \bar{A}^k \bar{B}, \ldots\right) \\ y &amp; =x * \bar{K}\end{aligned}\) (3)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Linear Time Invariance (LTI)</strong>
    <ul>
      <li>위 (1) (2) (3) 모델들은 model’s dynamics가 time-invariant</li>
      <li>하지만 본 논문에서는 이러한 LTI property가 근본적인 한계가 있음을 밝히고
        <ul>
          <li>LTI를 제거하면서도 efficiency bottlenecks를 극복함을 제시함</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Structure and Dimensions</strong>
    <ul>
      <li>\(A\) matrix를 사용하기 때문에 SSM인거고, 이때 \(A \in \mathbb{R}^{N \times N}, B \in \mathbb{R}^{N \times 1}, C \in \mathbb{R}^{1 \times N}\)</li>
      <li>total hidden state has dimension은: \(𝐷𝑁\) per input</li>
      <li>the sequence length requires \(𝑂(𝐵𝐿𝐷𝑁)\)</li>
    </ul>
  </li>
</ul>

<h2 id="3-selective-state-space-models">3. Selective State Space Models</h2>

<ul>
  <li>3.1절에서는 selection mechanism을 소개하고</li>
  <li>3.2절에서는 어떻게 selection mechanism이 SSM과 같이 쓰일 수 있는지 보고</li>
  <li>3.3절에서는 hardware-aware algorithm을 알아보고</li>
  <li>3.4절에서는 simple SSM을 attention이나 MLP없이 알아보고</li>
  <li>3.5절에서는 additional properties of selection mechanisms를 논의한다</li>
</ul>

<h3 id="31-motivation-selection-as-a-means-of-compression">3.1. Motivation: Selection as a Means of Compression</h3>

<ul>
  <li>sequence modeling의 본질적인 문제는 <strong>small state에 context를 압축</strong>하는 것
    <ul>
      <li>Trade off : 압축을 안하면 inefficient하고(Transformer) efficient하면 압축을 너무 많이 하고(RNN)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/mamba_/fig2.png" alt="그림1" /></p>

<ul>
  <li>Synthetic task 2가지
    <ul>
      <li><strong>Selective Copying</strong>
        <ul>
          <li>필요한 tokens와 아닌 것들을 구별하기 위해 content-aware reasoning하는 task</li>
        </ul>
      </li>
      <li><strong>Induction Heads</strong>
        <ul>
          <li>다음에 뭐가 올지 추론하기 위해 context-aware reasoning하는 task</li>
        </ul>
      </li>
      <li>이 두 가지는 위에서 소개한 LTI mode로는 하기 어렵다</li>
    </ul>
  </li>
  <li>결국에는 efficient하려면 small state를 가져야 하는데, 그걸 “잘” 하려면 selectivity를 “잘” 해야 함</li>
</ul>

<h3 id="32--improving-ssms-with-selection">3.2.  Improving SSMs with Selection</h3>

<ul>
  <li>본 논문에서 소개하는 selection mechanism은 model의 parameters를 input-dependent하게 만드는 것
    <ul>
      <li>\(\Delta, B, C\)을 length dimension \(L\)로 만듬 (즉 time-invariant에서 time-varying으로)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/mamba_/algorithm12.png" alt="그림1" /></p>

<h3 id="33-efficient-implementation-of-selective-ssms">3.3 Efficient Implementation of Selective SSMs</h3>

<ul>
  <li>Convolution이나 attention처럼 GPU-friendly하게 Selective SSM을 만들고 싶은 거고</li>
  <li>즉 시간에 따라 필요한 정보를 선택적으로 처리하겠다는 것. 그러면 더 빠르게 긴 시퀀스를 처리</li>
</ul>

<p><strong>3.3.1 Motivation of Prior Models</strong></p>

<ul>
  <li>paying speed and memory costs 없이 maximize hidden state dimension하고 싶음</li>
  <li>Recurrent mode는 hidden이 input보다 훨씬 커서 메모리 사용량이 많음
    <ul>
      <li>그래서 input의 shape (=output의 shape)과 같은 conv를 쓰겠다</li>
    </ul>
  </li>
  <li>기존 LTI는 데이터 특성을 잘 반영 못했지만 Mamba는 순환적 요소와 컨볼루션적 요소를 동시에 사용해 모델의 효율성을 극대화 !</li>
</ul>

<p><strong>3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion</strong></p>

<ul>
  <li>LTI의 한계를 극복하는 selection mechanism을 소개:</li>
  <li>문제는 (1) the sequential nature of recurrence, and (2) the large memory usage
    <ul>
      <li>(2) the large memory usage는 kernel fusion으로 해결
        <ul>
          <li>scan input \((\bar{A}, \bar{B})\) of size \((B, L, D, N)\)을 HBM에 저장하는 것이 아니라</li>
          <li>the SSM parameters \((\triangle, A, B, C)\)의 final output \((B, L, D)\)만 저장</li>
          <li>discretization이랑 recurrence는 SRAM에서 수행</li>
        </ul>
      </li>
      <li>(1) the sequential nature of recurrence는 recomputation으로 해결
        <ul>
          <li>intermediate states를 저장하지 않는데 이건 backpropagation에서 필요하니까</li>
          <li>그냥 다시 계산함 (recomputation)</li>
          <li>그 결과 FlashAttention과 유사한 memory efficiency</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="34-a-simplified-ssm-architecture">3.4 A Simplified SSM Architecture</h3>

<ul>
  <li>Mamba는 linear attention과 MLP를 결합해서 gated attention unit(GAP)처럼 만듬</li>
  <li>model dimension을 D에서 expansion factor E를 사용해서 늘려줌
    <ul>
      <li>대부분의 model parameters \(3ED^2\)개 \(2ED^2\)개가 input projection에, \(ED^2\)개가 output projection에 있음</li>
      <li>반면 SSM 안에는 parameters가 별로 없는데, Mamba는 이걸 반복해서 사용하기 때문에 효율적이다</li>
    </ul>
  </li>
</ul>

<h3 id="35-properties-of-selection-mechanisms">3.5 Properties of Selection Mechanisms</h3>

<ul>
  <li>The selection mechanism은 RNN이나 CNN에 쓸 수 있는 broader concept임</li>
</ul>

<p><strong>3.5.1 Connection to Gating Mechanisms</strong></p>

<ul>
  <li>SSM의 게이트 역할을 하는 \(\Delta\)가 RNN의 게이트와 유사하게 작동
    <ul>
      <li>입력된 정보 중 어떤 것을 유지하고 어떤 것을 버릴지 결정하는 역할인 점도 비슷</li>
      <li>When $N=1, A=-1, B=1, s_{\Delta}=\operatorname{Linear}(x)$, and $\tau_{\Delta}=$ softplus,</li>
      <li>\(\begin{aligned} &amp; g_t=\sigma\left(\operatorname{Linear}\left(x_t\right)\right) \\ &amp; h_t=\left(1-g_t\right) h_{t-1}+g_t x_t\end{aligned}\) .</li>
      <li>이런 식으로 \(g_t\)가 현재 입력 \(x_t\)가 얼마나 중요한지 표현하게 하고
        <ul>
          <li>\(g_t\)가 1에 가까울수록 \(x_t\)를 많이 반영, 0에 가까울수록 이전 state \(h_{t-1}\)을 많이 반영</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>3.5.2 Interpretation of Selection Mechanisms</strong></p>

<ul>
  <li><strong>Variable Spacing</strong>
    <ul>
      <li>Selectivity의 역할은 filtering out irrelevant noise tokens that may occur between inputs of interest</li>
    </ul>
  </li>
  <li><strong>Filtering Context</strong>
    <ul>
      <li>context가 길어진다고 성능이 좋아지는 것이 아님. 대부분의 모델이 너무 긴 sequence에서 불필요한 정보를 제거하지 못해서 성능 저하가 발생</li>
      <li>selective model은 state를 언제든 초기화 할 수 있으니 긴 sequence가 들어왔을 때 성능이 더 좋아지도록 작동</li>
    </ul>
  </li>
  <li><strong>Boundary Resetting</strong>
    <ul>
      <li>LTI는 sequence의 경계에서 정보가 섞이는 문제가 있었는데, selective SSM은 그런 문제 없음
        <ul>
          <li>언제든지 state를 초기화할 수 있으니 그냥 boundaries에서 초기화 하면 됨 (\(g_t=1\))</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Interpretation of \(\Delta\)</strong>
    <ul>
      <li>\(\Delta\)가 크면 다 잊고 현재 정보를 위주로 state를 만드는거고, 작으면 이전 state를 유지</li>
    </ul>
  </li>
  <li><strong>Interpretation of A</strong>
    <ul>
      <li>사실 \(\bar{A}=\exp (\Delta A)\)도 \(\Delta\)를 통해 만들어지니 크게 건드리지 말고 단순하게 둔다</li>
    </ul>
  </li>
  <li><strong>Interpretation of 𝑩 and 𝑪.</strong>
    <ul>
      <li>결국 Selectivity의 역할은 filtering out.</li>
      <li>B와 C는 입력을 상태로 전달할지, 상태를 출력으로 내보낼지를 결정</li>
      <li>모델이 state(context)를 더 세밀하게 제어할 수 있음</li>
    </ul>
  </li>
</ul>

<h3 id="36-additional-model-details">3.6 Additional Model Details</h3>

<p>pass</p>

<h2 id="4-empirical-evaluation">4. Empirical Evaluation</h2>

<h3 id="41-synthetic-tasks">4.1 Synthetic Tasks</h3>

<p><strong>4.1.1 Selective Copying</strong></p>

<p><strong>4.1.2 Induction Heads</strong></p>

<p><img src="/assets/img/Mamba/mamba_/table12.png" alt="그림1" /></p>

<h3 id="42-language-modeling">4.2. Language Modeling</h3>

<p><img src="/assets/img/Mamba/mamba_/table3.png" alt="그림1" /></p>

<h3 id="45-speed-and-memory-benchmarks">4.5 Speed and Memory Benchmarks</h3>

<p><img src="/assets/img/Mamba/mamba_/fig8.png" alt="그림1" /></p>

<h3 id="46-model-ablations">4.6. Model Ablations</h3>

<p><img src="/assets/img/Mamba/mamba_/table6.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/mamba_/table78.png" alt="그림1" /></p>

<h2 id="5-discussion">5. Discussion</h2>

<p>Pass</p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>a selection mechanism to structured state space models
    <ul>
      <li>to perform context-dependent reasoning</li>
      <li>Without attention ! (simple attention-free architecture)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2312.00752)]]></summary></entry><entry><title type="html">T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)</title><link href="http://localhost:4000/timeseries/2024-10-27-T-PATCHGNN/" rel="alternate" type="text/html" title="T-PATCHGNN: Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach (ICML 2024)" /><published>2024-10-27T00:00:00+09:00</published><updated>2024-10-27T11:03:31+09:00</updated><id>http://localhost:4000/timeseries/T-PATCHGNN</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-10-27-T-PATCHGNN/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>transforms each univariate irregular time series into a series of transformable patches</li>
      <li>local semantics capture와, inter-time series correlation modeling는 하면서</li>
      <li>avoiding sequence <strong>length explosion</strong> in aligned IMTS (무슨 의미인지 1. introduction (3)에서 설명)</li>
    </ul>
  </li>
  <li>Time-adaptive graph neural networks으로 time-varying adaptive graphs를 학습해서
    <ul>
      <li>dynamic intertime series correlation를 표현</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Multivariate Time Series (IMTS)의 특징은 irregular sampling intervals and missing data</li>
  <li>Irregularity within the series and asynchrony 때문에 다루기 어려움
    <ul>
      <li>ODE로 풀려고 한 적은 있지만 numerical integration process으로 인해 computationally expensive</li>
    </ul>
  </li>
  <li>IMTS forecasting의 어려움에는 3가지 이유가 있음</li>
  <li>첫번째는 (1) irregularity in intra-time series dependency modeling
    <ul>
      <li><strong>varying time intervals</strong> between adjacent observations이 the consistent flow of time series data를 방해</li>
    </ul>
  </li>
  <li>두번째는 (2) asynchrony in intertime series correlation modeling
    <ul>
      <li><strong>misaligned at time</strong> due to irregular sampling or missing data.</li>
    </ul>
  </li>
  <li>가장 중요한 건 (3) sequence length explosion with the increase of variables
    <ul>
      <li>아래 fig1처럼 “단 하나의 변수라도 기록된 time stamp”는 모두 존재하는 걸로 해버리면, 변수 개수가 늘어남에 다라 time stamps의 수가 너무 많아지는 문제. (이러한 방법을 canonical pre-alignment representation이라고 부름)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TPatchGNN/fig1.png" alt="그림1" /></p>

<ul>
  <li>그래서 본 논문에서 제시하는 T-PATCHGNN의 장점은
    <ul>
      <li>첫째로 The independent patching process for each univariate irregular time series으로 representation에서 sequence length explosion의 risk를 없애고</li>
      <li>둘째로 local semantics를 잘 잡기 위해 putting each individual observation into patches with richer context</li>
      <li>셋째로 transformable patching 후에 IMTS is naturally aligned in a consistent patch-level temporal resolution</li>
    </ul>
  </li>
  <li>본 논문의 contribution은 :
    <ul>
      <li>New transformable patching method to transform each univariate irregular time series of IMTS into a series of variable-length yet time-aligned patches</li>
      <li>transformable patching outcomes을 바탕으로,  time-adaptive graph neural networks를 제안</li>
      <li>building a benchmark for IMTS forecasting evaluation</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2, Related Works</h2>

<h3 id="21-irregular-multivariate-time-series-forecasting">2.1. Irregular Multivariate Time Series Forecasting</h3>

<p>pass</p>

<h3 id="22-irregular-multivariate-time-series-representation">2.2. Irregular Multivariate Time Series Representation</h3>

<ul>
  <li>기존에는 time-aligned manner로 IMTS를 representation (pre-alignment representation method)
    <ul>
      <li>즉 하나의 변수라고 기록된 time stamp는 존재하는 걸로 생각하니</li>
      <li>sequence length that equals the number of all unique time stamps in IMTS</li>
      <li>예를 들어 변수 1은 1,3,5 시점에 기록되고 변수 2는 2,4,6 시점에 기록되면 unique time stamps의 개수는 6이 됨</li>
      <li>sequence length explosion problem 발생</li>
    </ul>
  </li>
</ul>

<h3 id="23-graph-neural-networks-for-multivariate-time-series">2.3. Graph Neural Networks for Multivariate Time Series</h3>

<ul>
  <li>
    <p>2018년 DCRNN, STGCN은 pre-defined graph structures를 사용해서 실제로 쓰기 어려웠고</p>
  </li>
  <li>2019년부터 data로부터 graph structures를 학습하는 방식을 사용
    <ul>
      <li>하지만 IMTS에서는 잘 작동을 안 함. mimisalignment at times으로 인해 inter-time series correlation modeling이 잘 안 됨</li>
    </ul>
  </li>
  <li>Raindrop(2021)[<a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">paper review</a>]
    <ul>
      <li>이 문제를 propagation the asynchronous observations at all the timestamps로 해결하려고 했지만  sequence length explosion problem을 피할 수 없음</li>
    </ul>
  </li>
</ul>

<h2 id="3-preliminary">3. Preliminary</h2>

<h3 id="31-problem-definition">3.1. Problem Definition</h3>

<h3 id="definition-1">Definition 1</h3>

<ul>
  <li>Irregular Multivariate Time Series
    <ul>
      <li>\(\mathcal{O}=\left\{\mathbf{o}_{1: L_n}^n\right\}_{n=1}^N=\left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\), where</li>
      <li>\(N\)개의 변수가 있고 \(n\)번째 변수는 \(L_n\)개의 observations가 있고, \(n\)번째 변수의 \(i\)번째 변수의 값은 \(t_i^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="definition-2">Definition 2</h3>

<ul>
  <li>Forecasting Query \(q_j^n\)
    <ul>
      <li>\(j\)-th query on \(n\)-th variable to predict its corresponding value at a future time \(q_j^n\)</li>
    </ul>
  </li>
</ul>

<h3 id="problem-1">Problem 1</h3>

<ul>
  <li>Irregular Multivariate Time Series Forecasting
    <ul>
      <li>IMTS \(\mathcal{O} =  \left\{\left[\left(t_i^n, x_i^n\right)\right]_{i=1}^{L_n}\right\}_{n=1}^N\)와 Forecasting query \(\mathcal{Q}=\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)가 있을 때,</li>
      <li>problem은 accurately forecast recorded values \(\hat{\mathcal{X}}=\left\{\left[\hat{x}_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\) in correspondence to the forecasting queries</li>
      <li>\(\mathcal{F}(\mathcal{O}, \mathcal{Q}) \longrightarrow \hat{\mathcal{X}}\)로 표현됨</li>
    </ul>
  </li>
</ul>

<h3 id="32-canonical-pre-alignment-representation-for-imts">3.2. Canonical Pre-Alignment Representation for IMTS</h3>

<ul>
  <li>2.2. Irregular Multivariate Time Series Representation 참고</li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<p><img src="/assets/img/timeseries/TPatchGNN/fig2.png" alt="그림1" /></p>

<h3 id="41-irregular-time-series-patching">4.1. Irregular Time Series Patching</h3>

<ul>
  <li>모든 univariate TS에 같은 patching operation을 하니까 변수 index 표기는 생략</li>
</ul>

<h3 id="411-transformable-patching">4.1.1. TRANSFORMABLE PATCHING</h3>

<ul>
  <li>Time series patching이 forecasting에 좋은 방법이라는 건 알려진 사실. benefits in :
    <ul>
      <li>capturing local semantic information,</li>
      <li>reducing computation and memory usage,</li>
      <li>modeling longer-range historical observations</li>
    </ul>
  </li>
  <li>일반적으로 time series patching은 하나의 patch에 같은 숫자의 observations가 있는데,
    <ul>
      <li>IMTS에서 time intervals는 다양하기 때문에 이러한 방식이 적절하지 않음</li>
    </ul>
  </li>
  <li>그래서 patch에 같은 개수의 observataions가 아니라, unified time horizon이 들어가도록 함
    <ul>
      <li>patch 안에 들어가는 observations의 개수는 다를 수 있지만, ex) 2시간인 건 동일하도록</li>
    </ul>
  </li>
  <li>patch는 \(\left[\mathbf{o}_{l_p: r_p}\right]_{p=1}^P\)로 표현되고 \(P\)</li>
</ul>

<h3 id="412-patch-encoding">4.1.2. PATCH ENCODING</h3>

<ul>
  <li><strong>Continuous time embedding</strong>
    <ul>
      <li>\(\phi(t)[d]=\left\{\begin{array}{lll}
\omega_0 \cdot t+\alpha_0, &amp; \text { if } &amp; d=0 \\
\sin \left(\omega_d \cdot t+\alpha_d\right), &amp; \text { if } &amp; 0&lt;d&lt;D_t
\end{array}\right.\).
        <ul>
          <li>where the \(\omega_d\) and \(\alpha_d\) are learnable parameters and \(D_t\) is embedding’s dimension</li>
        </ul>
      </li>
      <li>Concatenation하면 observations in the patch:
        <ul>
          <li>\(\mathbf{z}_{l_p: r_p}=\left[z_i\right]_{i=l_p}^{r_p}=\left[\phi\left(t_i\right) \| x_i\right]_{i=l_p}^{r_p}\).</li>
          <li>이건 하나의 patch에 대한 표현이 되는 것 !</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Transformable time-aware convolution</strong>
    <ul>
      <li>input sequence의 길이에 맞게 (adaptively), generated parameters와 transformable filter size를 사용</li>
      <li>\(\mathbf{f}_d=\left[\frac{\exp \left(\mathbf{F}_d\left(z_i\right)\right)}{\sum_{j=1}^{L_p} \exp \left(\mathbf{F}_d\left(z_j\right)\right)}\right]_{i=1}^{L_p}\)으로 표현됨
        <ul>
          <li>where \(L_p\) is the sequence length of patch \(\mathbf{z}_{l_p: r_p}, \mathbf{f}_d \in \mathbb{R}^{L_p \times D_{i n}}\) is the derived filter for \(d\)-th feature map, \(D_{i n}\) is dimension of inputs, and \(\mathbf{F}_d\) denotes the meta-filter that can be instantiated by learnable neural networks</li>
          <li>이건 filter의 parameters를 along the temporal dimension으로 normalizaing해서 consistent scaling 하겠다는 것</li>
        </ul>
      </li>
      <li>위 식으로 \(D-1\)개의 filters를 사용해서 <strong>latent patch embedding</strong> \(h_p^c \in \mathbb{R}^{D-1}\)를 얻음 :
        <ul>
          <li>\(h_p^c=\left[\sum_{i=1}^{L_p} \mathbf{f}_d[i]^{\top} \mathbf{z}_{l_p: r_p}[i]\right]_{d=1}^{D-1}\).</li>
          <li>이건  encoded transformable patches:
            <ul>
              <li>variable-length sequences에 따라 flexibility를 가지고</li>
              <li>parameterization for varying time intervals을 하면서</li>
              <li>additional learnable filter parameters 없이 더 긴 시퀀스를 처리할 수 있음</li>
            </ul>
          </li>
          <li>마지막으로 \(h_p=\left[h_p^c \| m_p\right]\) 이렇게 patch에 masking을 덧붙여주는데,
            <ul>
              <li>\(m_p\)는 이 patch 안에 observations가 하나 이상 있다~를 indicator로 표현</li>
            </ul>
          </li>
          <li>최종적으로 \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)를 얻는다.</li>
          <li>이건 \(P\)개의 patch를 \(D-1\)차원으로 표현하고 마지막에는 masking으로 indicator를 붙인 것</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="42-intra--and-inter-time-series-modeling">4.2. Intra- and Inter-Time Series Modeling</h3>

<ul>
  <li>이제 이  transformable patching을 irregular time series를 intra- and inter-time series modeling하는지 알아보자</li>
</ul>

<h3 id="421-transformer-to-model-sequential-patches">4.2.1. TRANSFORMER TO MODEL SEQUENTIAL PATCHES</h3>

<ul>
  <li>위에서 구한 \(\mathbf{h}_{1: P}=\left[h_p\right]_{p=1}^P \in \mathbb{R}^{P \times D}\)를 Transformer에 넣는다.</li>
  <li>먼저 positional encoding을 하고
    <ul>
      <li>\(\mathbf{x}_{1: P}^{t f, n}=\mathbf{h}_{1: P}^n+\mathbf{P E}_{1: P}\).</li>
    </ul>
  </li>
  <li>Q, K, V를 만들어서 MHA를 통과한다.
    <ul>
      <li>\(\mathbf{q}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^Q\) / \(\mathbf{k}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^K\) / \(\mathbf{v}_h^n=\mathbf{x}_{1: P}^{t f, n} \mathbf{W}_h^V\) where \(\mathbf{W}_h^Q, \mathbf{W}_h^K, \mathbf{W}_h^V \in \mathbb{R}^{D \times(D / H)}\)</li>
      <li>\(\mathbf{h}_{1: P}^{t f, n}=\|_{h=1}^H \operatorname{Softmax}\left(\frac{\mathbf{q}_h^n \mathbf{k}_h^{n T}}{\sqrt{D / H}}\right) \mathbf{v}_h^n \in \mathbb{R}^{P \times D}\),</li>
    </ul>
  </li>
</ul>

<h3 id="422-time-varying-adaptive-graph-structure-learning">4.2.2. TIME-VARYING ADAPTIVE GRAPH STRUCTURE LEARNING</h3>

<ul>
  <li>한 변수를 예측하기 위해서 다른 변수의 정보는 매우 유용할 수가 있음</li>
  <li>하지만 IMTS에서는 misaligned at times으로 인해 correlation modeling이 어려움
    <ul>
      <li>그렇다고 Raindrop처럼 하기엔 e sequence length explosion problem이 발생</li>
    </ul>
  </li>
  <li>그래서 <strong>transformable patching</strong>으로 해결
    <ul>
      <li>patch를 observations의 개수가 아니라 시간 길이를 기준으로 끊다보니</li>
      <li>각 변수는 같은 숫자의 patches로 이루어지니까</li>
      <li>time-adaptive graph neural networks로 inter-time series correlation를 modeling할 수 있음</li>
    </ul>
  </li>
  <li>즉 IMTS의  dynamic correlations를 파악하기 위해서는
    <ul>
      <li>series of time-varying adaptive graphs를 학습하겠다는 것이고</li>
      <li>지금 문제는 variable embedding이 training에서는 update 가능하지만 inference에서는 static</li>
      <li>그러니 learnable \(\mathbf{E}_1^s, \mathbf{E}_2^s \in \mathbb{R}^{N \times D_g}\)를 사용해서</li>
      <li>우리가 지금까지 만들었던  time-varying patch embedding \(\mathbf{H}_p^{t f}=\left[\mathbf{h}_p^{t f, n}\right]_{n=1}^N \in \mathbb{R}^{N \times D}\)을
        <ul>
          <li>static variable embedding으로 만들면 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>그 gated adding operation은 다음과 같음
    <ul>
      <li>\(\begin{gathered}
\mathbf{E}_{p, k}=\mathbf{E}_k^s+g_{p, k} * \mathbf{E}_{p, k}^d, \\
\mathbf{E}_{p, k}^d=\mathbf{H}_p^{t f} \mathbf{W}_k^d, \\
g_{p, k}=\operatorname{ReLU}\left(\tanh \left(\left[\mathbf{H}_p^{t f} \| \mathbf{E}_k^s\right] \mathbf{W}_k^g\right)\right) \\
k=\{1,2\}
\end{gathered}\), where
        <ul>
          <li>\(\mathbf{W}_k^d \in \mathbb{R}^{D \times D_g}, \mathbf{W}_k^g \in \mathbb{R}^{\left(D+D_g\right) \times 1}\) are learnable parameters</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이제 time-varying adaptive graph structure를 다음과 같이 얻음 : \(\mathbf{A}_p=\operatorname{Softmax}\left(\operatorname{ReLU}\left(\mathbf{E}_{p, 1} \mathbf{E}_{p, 2}^T\right)\right)\)</li>
</ul>

<h3 id="423-gnns-to-model-inter-time-series-correlation">4.2.3. GNNS TO MODEL INTER-TIME SERIES CORRELATION</h3>

<ul>
  <li>다음으로 dynamic inter-time series correlation at a patch-level resolution을 얻음
    <ul>
      <li>\(\mathbf{H}_p=\operatorname{ReLU}\left(\sum_{m=0}^M\left(\mathbf{A}_p\right)^m \mathbf{H}_p^{t f} \mathbf{W}_m^{g n n}\right) \in \mathbb{R}^{N \times D}\).</li>
      <li>where $M$ is the number of layers for GNNs, and $\mathbf{W}_m^{g n n} \in$ $\mathbb{R}^{D \times D}$ are learnable parameters at $m$-th layer.</li>
    </ul>
  </li>
</ul>

<h3 id="43-imts-forecasti">4.3. IMTS Forecasti</h3>

<ul>
  <li>이제 final latent representation을 얻는다 :
    <ul>
      <li>\(\mathbf{H}=\text { Flatten }\left(\left[\mathbf{H}_p\right]_{p=1}^P\right) \mathbf{W}^f \in \mathbb{R}^{N \times D_o}\), where  \(\mathbf{W}^f \in \mathbb{R}^{P D \times D_o}\) are learnable parameters.</li>
      <li>각 변수마다 이 representation을 얻는다</li>
    </ul>
  </li>
  <li>n-번째 변수의 final latent representation \(\mathbf{H}^n \in \mathbf{H}\)과, forecasting query \(\left\{\left[q_j^n\right]_{j=1}^{Q_n}\right\}_{n=1}^N\)를 가지고 MLP에 넣는다</li>
  <li>
    <p>\(\hat{x}_j^n=\operatorname{MLP}\left(\left[\mathbf{H}^n \| \phi\left(q_j^n\right)\right]\right)\).</p>
  </li>
  <li>모델은 각 변수의 예측의 MSE를 줄이는 방향으로 학습
    <ul>
      <li>\(\mathcal{L}=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2\).</li>
    </ul>
  </li>
</ul>

<h3 id="44-analysis-on-scalabil">4.4. Analysis on Scalabil</h3>

<ul>
  <li>The average sequence length : \(L_{t p}=L_{a v g} \leq L_{\max } \leq L_{c p r} \leq N \times L_{a v g}\), where
    <ul>
      <li>\(L_{\text {avg }}=\frac{1}{N} \sum_{n=1}^N L_n\).</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-experimental-setup">5.1. Experimental Setup</h3>

<ul>
  <li>Dataset :
    <ul>
      <li>PhysioNet, MIMIC, Human Activity, and USHCN</li>
      <li>training, validation, and test sets adhering to ratios of 60%, 20%, and 20%</li>
    </ul>
  </li>
  <li>Evaluation Metric :
    <ul>
      <li>\(\begin{aligned}
\text { MSE }&amp;=\frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left(\hat{x}_j^n-x_j^n\right)^2, \\\text { MAE }&amp;=
 \frac{1}{N} \sum_{n=1}^N \frac{1}{Q_n} \sum_{j=1}^{Q_n}\left|\hat{x}_j^n-x_j^n\right| .
\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h4 id="52-main-results">5.2. Main Results</h4>

<p><img src="/assets/img/timeseries/TPatchGNN/table1.png" alt="그림1" /></p>

<h3 id="53-ablation-study">5.3. Ablation Study</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table2.png" alt="그림1" /></p>

<h3 id="54-scalability-and-efficiency-analysis">5.4. Scalability and Efficiency Analysis</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/table3.png" alt="그림1" /></p>

<h3 id="55-effect-of-patch-size">5.5. Effect of Patch Size</h3>

<p><img src="/assets/img/timeseries/TPatchGNN/fig4.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Transformable Patching Graph Neural Networks (T-PATCHGNN)
    <ul>
      <li>achieved the alignment between asynchronous IMTS
        <ul>
          <li>by transforming each univariate irregular time series into a series of transformable patches with varying observation counts but maintaining unified time horizon resolution.</li>
          <li>without a canonical pre-alignment representation process, preventing the aligned sequence length from explosively growing</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2024](https://openreview.net/pdf?id=UZlMXUGI6e)]]></summary></entry></feed>