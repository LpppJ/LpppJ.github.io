---
layout: post
related_posts:
  _
title: 
description: >
  [ICLR 2024](https://arxiv.org/abs/2312.16424)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# SoftCLT: Soft Contrastive Learning for Time Series (ICLR 2024)

## Abstract
- TS instance만으로 contrasting하거나, 하나의 TS의 adjacent timestamps만으로 contrasting하면 TS의 inherent correlation을 제대로 반영하지 못한다.
- **SoftCLT** : instance-wise & temporal Contrastive loss를 사용하여, 0 또는 1이 아닌 0 ~ 1 사이의 값으로 assign
  - Instance-wise contrastive loss : Data space에서 두 시계열의 distance
  - Temporal contrastive loss : 서로 다른 시점에 대한 loss

## 1. Introduciton
- Self-supervised learning - Contrastive learning에서는 두 instance의 유사도로 pair를 설정하기보다는, 각 데이터에 대해 2개의 view를 augmentation하고 둘을 positive pair로 설정한다. (다른 데이터에서 augmentation된 view와는 negative)
- TS의 inherent correlations는 유사한 instance 뿐만 아니라, 인접한 timestamps에도 있다.
- 설령 같은 data에서 augmentation 되었거나, 같은 timestamp의 value에만 positive(1)로 설정하더라도, 다른 data에서 augmentation 된 다른 timestamp의 value에는 다 똑같의 negative(0)으로 설정하는 것은 optimal하지 않다.
- [InfoNCE](https://arxiv.org/pdf/1807.03748.pdf)의 loss처럼 positive pair 뿐만 아니라 negative pair에 대해서도 weight를 고려하는 Soft Contrastive Learning for TS를 제안
![사진1](/assets/img/timeseries/softclt/table1.png)

## 2. Related Work
- Self-supervised learning : 많은 양의 unlabeled data를 활용하는 pretext task를 수행하는 모델을 훈련시키고, 해당 모델을 downstream task의 앞쪽에 가져와서 사용한다. pretext task로는 `next token prediction`, `masked token prediction`, `jigsaw puzzles`, `rotation prediction` 등
- Contrastive learning in TS 
  - [T-Loss](https://arxiv.org/pdf/1901.10738.pdf) : TS에서 subseries 샘플링, subseries가 속한 TS와는 positive 다른 TS와는 negative
  - [Self-Time](https://arxiv.org/pdf/2011.13548.pdf) : augmented sample로 inter-sample relation 학습, temporal distance로 label을 만들고 classification 해서 intra-temporal relation 학습
  - [TNC](https://arxiv.org/pdf/2106.00750.pdf) : 정규분포 window로 정의한 temporal neighborhood를 positive로 설정
  - [TS-TCC](https://arxiv.org/pdf/2106.14112.pdf) : augmentations가 서로의 미래 시점을 예측하도록 해서 temporal contrastive loss 설정
  - [Mixing-up](https://arxiv.org/pdf/2203.09270.pdf) : 2개의 TS를 섞어서 새로운 TS를 만드는데 mixing weights를 predict
  - [CoST](https://arxiv.org/pdf/2202.01575.pdf) : time, frequency domain의 contrastive losses를 사용하여 representation learning
  - [TimeCLR](https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705122X00075/1-s2.0-S0950705122002726/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCDSwasc4kb8LU0bgnJMwyHRmJ5Xp0qOkMGkgGRC101hgIgQu4zuleiKDecSs%2FYiwU6McTbx88zb7ZGNMt6fPGxxoAqvAUI0v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDF3vdKai06uU77VSpSqQBVBf8gEuvX4RtLZKv9hE3KxXK6cBABJ3MlBBkOHOQG9wKZz7o6d1XmLn2FZSjY4%2FX7pPyQcDRmkq2n%2FJq8%2Fsui0EeAASo4iyS840z2aCXativJcKRglZNYHjwhkBAax3A8xvkoahV70%2BnaX2GHQ5TWPuaUEwQkfUD1Y%2BKJW17Llm6SzL9NFiUMu9oRVuGLNfzeDz8H916pyvPQXFS5ltLq8PmDSlDFSwpmuvjRZvcGJVQCRYJ9QAqN4pjSkBwmRJtZ1zxeYYLfQU88%2FhHpCuqKY%2Fo5PlZqitF%2Fi5tN9wcjv%2BOaUu0e3H8K8qknd2hQhfYZ3mcE319ttggfYVT4PxT6jQv2hH%2BtWO%2BQVZ32moiyr2q2dfvqndSZ%2BcslmaJMEEGPfVbkcqz6OuRXKg9c6wHw%2BzGjJ0qF8lctSoDbLhT5IOZWG%2FmNF%2BMVvU5Wgorfa2swiBav99Hgn3vrf74u8mLsY5T0vx4NEyG%2BNVyPbgKqGOHsQAejW06Vq4ik5UIzPpsQ5HV4XPKK%2Fqymlem1XN6PxFHQeaf3vs0y8kVwp0rvnrfnnQ4LSrKfZc%2FNdpLU%2FXHGx%2BkUvIAHtVRX4a%2BC3sP8xSXKWTctA458XV7b5O7K5sXlyS36%2F3xnTrjC0NcJv80e3dYPRDhg3knQfYgDe2geRGQuU2COYu%2FKksZiGgBXhSAln%2Bud9LWeLVphgzjSipja81DInKiCBNOHOlulkaoOh0WcdIOFQeAQ2q6v4DeoIE1D8tTL5JNgWDLUk8sxQGq9zspYADXCEhc9Ke4hgL%2FuFvRA6Q12rCsxcWroPVYAf2el010OB%2BHSiqaCHw6xiykfcfjw7oO7bINDuWwMASTZbTgTVETF0hx1VpaYUnUbObMOCH4K8GOrEBiMpaUIWZ9pgzWe8VL74b8Keg8P4qJal5QND0KgcUzoVtZv4JMAmxEuey5Xggo3VjcjrCYsQ3sGOrJ9OJ570LbYowhBvMl7GSojd2kqdTrDSXd400eFg4uwE4Vb35B7htjgxzcxpZJeKmMPHzoEJdnMzI61T%2Fkl8%2FoIh3I9dws%2BUKd1pmrot0rGKx7EM68SBELBX2rcQ1SfmvbLKJAKBimMCFZrVhl4BAeMfpNyKSnkQM&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240318T093805Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYRLOY57VC%2F20240318%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0e72e500b607b8e5538d7a3d24ad28c78f5bc82c4b10c8f43305eb800f767c1b&hash=4ee078d9d743db9b7aed88f3efc12b11dbe7e497747d269063317fe1b35fad70&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0950705122002726&tid=spdf-1cc819cf-9b45-4e50-834c-160b0fdd0a1e&sid=cc5998cf8485c74f0b497fc9e83d21092089gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05125c535357540259&rr=86643f6fd90f3079&cc=kr) : phase-shift and amplitude change augmentation based on DTW
- Soft contrastive learning
  - [NNCLR](https://arxiv.org/pdf/2104.14548.pdf) : 각 view마다 feature space에서 k-neighbors를 찾아서 추가적인 positive pair를 고려
  - non-TS 도메인에서는 soft assignment를 계산할 때 embedding space에서 했지만, TS 도메인에서는 data space에서 계산한다.
- Masked modeling in TS
  - [TST](https://arxiv.org/pdf/2010.02803.pdf) : masked modeling paradigm을 TS에 도입
  - [PatchTST](https://arxiv.org/pdf/2211.14730.pdf) :  masked subseries-level patche를 예측해서 local semantic information을 효율적으로 계산
  - [SimMTM](https://arxiv.org/pdf/2302.00861.pdf) : 여러 개의 masked TS로부터 reconstruction

## 3. Methodology
- instance-wise contrastive loss : inter-sample relationship 학습. Distance btw TS on data space
- temporal contrastive loss : intra-temporal relationship 학습. 하나의 TS에서 서로 다른 timestamps의 차이
![사진2](/assets/img/timeseries/softclt/fig1.png)

### 3.1. Problem Definition
- 하나의 batch에는 N개의 TS : $$\mathcal{X}=\left\{x_1, \ldots, x_N\right\}$$가 있고
  - $$f_\theta = x_i \in \mathbb{R}^{T \times D} \to r_i=\left[r_{i, 1}, \ldots, r_{i, T}\right]^{\top} \in \mathbb{R}^{T \times M}$$를 학습
  - $$D$$는 input feature dim, $$M$$은 embedded feature dim
  - 
### 3.2. Soft Instance-wise Contrastive learning
- Vision에서는 pixel-by-pixel distance가 similarity와 관련이 없기 때문에 embedding space에서 similar instance를 학습하지만, TS에서는 data space에서의 거리가 similarity가 된다.
- soft assignment for a pair of data indice $$(i, i')$$ : $$w_I\left(i, i^{\prime}\right)=2 \alpha \cdot \sigma\left(-\tau_I \cdot D\left(x_i, x_{i^{\prime}}\right)\right)$$
  - $$D(\cdot, \cdot)$$ : min-max normalized distance metric
  - $$\tau_I$$ : hyperparameter controlling the sharpness
  - $$\alpha$$ : the upper bound in the range of [0, 1]. 완전 똑같은 TS의 assignment
  - augmented view끼리가 아니라 original TS끼리 계산하는 것
- Contrasive loss는 cross-entropy loss로 해석될 수 있으므로([i-Mix](https://arxiv.org/pdf/2010.08887.pdf)), softmax probability of the relative similarity는 $$p_I\left(\left(i, i^{\prime}\right), t\right)=\frac{\exp \left(r_{i, t} \circ r_{i^{\prime}, t}\right)}{\sum_{j=1, j \neq i}^{2 N} \exp \left(r_{i, t} \circ r_{j, t}\right)}$$
- Soft instance-wise contrastive loss for $$x_i$$, at $$t$$는 $$\ell_I^{(i, t)}=-\log p_I((i, i+N), t)-\sum_{j=1, j \neq\{i, i+N\}}^{2 N} w_I(i, j \bmod N) \cdot \log p_I((i, j), t)$$
  - 첫째 term은 positive pair에 대한 loss, 둘째 term은 나머지에 대한 loss인데 $$w_I\left(i, i^{\prime}\right)$$로 weighted.
  - $$\forall w_I\left(i, i^{\prime}\right)=0$$이면 hard instance-wise contrastive loss이므로 일반화 버전이다.
  - 
### 3.3. Soft Temporal Contrastive learning
- soft assignment for a pair of timestamps $$(t, t')$$ : $$w_T\left(t, t^{\prime}\right)=2 \cdot \sigma\left(-\tau_T \cdot\mid t-t^{\prime}\mid\right)$$
  - $$\tau_I$$ : hyperparameter controlling the sharpness
- : 인접한 timestamp의 values는 비슷할 것이라는 직관
- **Hierarchical loss** : [TS2Vec](https://arxiv.org/pdf/2106.10466.pdf)의 방식처럼 maxpooling을 해서 loss 계산
![사진3](/assets/img/timeseries/softclt/fig2.png)
- Softmax probability of the relative similarity는 $$p_T\left(i,\left(t, t^{\prime}\right)\right)=\frac{\exp \left(r_{i, t} \circ r_{i, t^{\prime}}\right)}{\sum_{s=1, s \neq t}^{2 T} \exp \left(r_{i, t} \circ r_{i, s}\right)}$$,
- Soft temporal contrastive loss for $$x_i$$ at $$t$$는 $$\ell_T^{(i, t)}=-\log p_T(i,(t, t+T))-\sum_{s=1, s \neq\{t, t+T\}}^{2 T} w_T(t, s \bmod T) \cdot \log p_T(i,(t, s))$$
  - 마찬가지로 $$\forall w_T\left(t, t^{\prime}\right)=0$$이면 hard temporal contrastive loss이므로 일반화 버전이다.
- 본 논문에서 제시하는 SoftCLT의 Final loss : $$\mathcal{L}=\frac{1}{4 N T} \sum_{i=1}^{2 N} \sum_{t=1}^{2 T}\left(\lambda \cdot \ell_I^{(i, t)}+(1-\lambda) \cdot \ell_T^{(i, t)}\right)$$
  - $$\lambda$$ : hyperparameter controlling the contribution of each loss
  
## 4. Experiments
- (1) Classification with UTS, MTS (2) Semi-supervised classification (3) Transfer learning in in-domain and cross-domain (4) Anomaly detection
### 4.1. Classification
![사진4](/assets/img/timeseries/softclt/fig23.png)

### 4.2. Semi-supervised classification
![사진5](/assets/img/timeseries/softclt/table3.png)

### 4.3. Transfer learning
![사진6](/assets/img/timeseries/softclt/table4.png)

### 4.4. Anomaly detection
![사진7](/assets/img/timeseries/softclt/table5.png)

### 4.5. Ablation study
![사진8](/assets/img/timeseries/softclt/table6.png)

  - (a) : soft assignment를 instance-wise와 temporal에 모두 적용했을 때 성능이 가장 좋다.
  - (b) : $$W_T$$를 계산하는 방법들에 따른 비교. sigmoid를 사용하는 근거가 된다.
  - (c) : $$\alpha=0.5$$ 정도로 해서 같은 TS의 similarity of the pairs를 적절히 크게 할 때 성능이 좋다.
  - (d) : Distance function에 따른 성능 비교. DTW와 TAM의 성능이 같지만 더 일반적인 DTW 사용했다.
### 4.6. Analysis
- `Comparison with soft CL methods in computer vision` : 앞서 언급했듯이 embedding space에서 similarity를 계산하는 vision domain과 다르게, TS는 data space에서 계산하면 성능이 더 좋다.
- `Robustness to seasonality` : Seasonality in TS는 extract하기 어렵지도 않고 고려 안해도 성능이 좋아서 직접적으로 고려하지 않았다.
- `Instance-wise relationships` : layer가 깊어짐에 따라 SoftCLT가 Hard CL보다 TS instance 사이의 관계를 잘 보존한다.
- `Temporal relationships` : 시간(training epoch)에 따라서도 t-SNE를 비교했을 때, Hard CL은 진한 색(large tarining epoch)을 잘 구분하지 못하는데, large training epoch에는 fine-grained relationship이 학습된다. 즉 Hard CL은 coarse-grained relationship은 잘 학습하지만 SoftCLT는 fine-grained relationship도 잘 학습한다.
   
## 5. Conclusion
- SoftCLT : soft assignments based on the instance-wise and temporal relationships on the data space