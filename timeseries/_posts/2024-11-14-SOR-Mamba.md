---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024](https://arxiv.org/pdf/2410.23356)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Sequential Order-Robust Mamba for Time Series Forecasting (Arxiv 2024)

## Abstract

- Mamba : near-linear complexity in processing sequential data
  - **하지만 일반적으로 Time series에서 변수의 순서는 존재하지 않기 때문에**
  - **Mamba에서 channel dependencies (CD)를 잡다보면 sequential order bias가 발생**

- 그러므로 본 논문에서는 **SOR-Mamba**를 제안
  - **Regularization strategy**
    - to minimize the discrepancy btw two embedding vectors (reversed channel orders)
    - $$\to$$  robustness to channel order
  - **Eliminates the 1D-convolution**
    - (originally designed to capture local information in sequential data)
  - **Channel correlation modeling (CCM)**
    - pretraining task aimed at preserving **correlations between channels**
      - from the data space to the latent space
      - in order to enhance the ability to capture CD

## 1. Introduction

- Transformer
  - ability to capture long-term dependencies but, quadratic computational complexity
  - reduce the complexity 하려다보니 performance degradations
- **SSM**
  - employing **convolutional** operations to process sequences with linear complexity
- **Mamba**
  - incorporating a **selective** mechanism to prioritize important information
  - balance btw performance and computational efficiency
- Temporal dependencies (TD), channel dependencies (CD) 둘 다 잡아야 하는데,
  - iTransformer에서는 CD는 complex attention mechanisms으로,
  - TD는 simple multi-layer perceptrons (MLPs)으로 했었음

- Mamba는 **sequential order bias**가 있다보니 **Bidirectional Mamba로는 충분하지 않음** (table1)
- 그렇다고 MambaTS처럼 channel을 섞자니 추가적인 작업이 소요됨

table1

- 그래서 **Sequential Order-Robust Mamba for TS forecasting (SOR-Mamba)**를 제안
  - (간단한 설명은 abstract에 잘 정리되어 있으므로 pass)

## 2. Related Works

- **CD-Mamba Block**
  - 1D Conv 제거
  - Time series의 channels는 애초에 inherent sequential order가 존재하지 않기 때문
- **Regulararization with CD-Mamba Block**
  - Reversed channel order로 발생하는 두 embedding vectors의 차이를 줄이도록 학습
  - $$\to$$ Enhancing robustness to channel order !
-  **Channel correlation modeling**
   -  Data space에서의 correlation (btw channels)과
   -  Embedding space에서의 correlation (btw channels)의 차이를 줄이도록 학습