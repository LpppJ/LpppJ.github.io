<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v9.1.6 <https://hydejack.com/>
-->







<head>
  






  
    
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>(Code Review, ICLR 2022) Raindrop | LpppJ</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="(Code Review, ICLR 2022) Raindrop" />
<meta name="author" content="GW Jeong" />
<meta property="og:locale" content="en" />
<meta name="description" content="Raindrop github" />
<meta property="og:description" content="Raindrop github" />
<link rel="canonical" href="http://localhost:4000/pytorch/2024-09-24-raindrop/" />
<meta property="og:url" content="http://localhost:4000/pytorch/2024-09-24-raindrop/" />
<meta property="og:site_name" content="LpppJ" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-09-24T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="(Code Review, ICLR 2022) Raindrop" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"GW Jeong"},"dateModified":"2024-09-24T18:34:03+09:00","datePublished":"2024-09-24T00:00:00+09:00","description":"Raindrop github","headline":"(Code Review, ICLR 2022) Raindrop","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/pytorch/2024-09-24-raindrop/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/me/logo.jpg"},"name":"GW Jeong"},"url":"http://localhost:4000/pytorch/2024-09-24-raindrop/"}</script>
<!-- End Jekyll SEO tag -->


  

  



  <meta name="theme-color" content="rgb(230, 217, 195)">


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="LpppJ">
<meta name="apple-mobile-web-app-status-bar-style" content="default">

<meta name="application-name" content="LpppJ">

<meta name="generator" content="Hydejack v9.1.6" />


<link rel="alternate" href="http://localhost:4000/pytorch/2024-09-24-raindrop/" hreflang="en">

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="LpppJ" />


<link rel="shortcut icon"    href="/assets/icons/favicon.ico">
<link rel="apple-touch-icon" href="/assets/icons/icon-192x192.png">

<link rel="manifest" href="/assets/site.webmanifest">

<link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com">



<link rel="preload" href="/assets/img/swipe.svg" as="image" id="_hrefSwipeSVG">






<script>!function(r,c){"use strict";function a(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}r.loadJS=function(e,t){var n=c.createElement("script"),e=(n.src=e,t&&a(n,"load",t,{once:!0}),c.scripts[0]);return e.parentNode.insertBefore(n,e),n},r._loaded=!1,r.loadJSDeferred=function(e,t){var n=c.createElement("script");function o(){r._loaded=!0,t&&a(n,"load",t,{once:!0});var e=c.scripts[0];e.parentNode.insertBefore(n,e)}return n.src=e,r._loaded?o():a(r,"load",o,{once:!0}),n},r.setRel=r.setRelStylesheet=function(e){a(c.getElementById(e),"load",function(){this.rel="stylesheet"},{once:!0})}}(window,document);
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
!function(w) {
  w._baseURL = '/';
  w._publicPath = '/assets/js/';
  w._noPushState = false;
  w._noDrawer = false;
  w._noNavbar = false;
  w._noToc = false;
  w._noSearch = false;
  w._search = {
    DATA_URL: '/assets/sitedata.json?no-cache',
    STORAGE_KEY: 'mini-search/',
    INDEX_KEY: 'index--2025-01-08T22:03:07+09:00',
  };
  w._clapButton = true;
}(window);</script>


<script async src="/assets/bower_components/MathJax/es5/tex-mml-chtml.js" id="_MathJax"></script>


<!--[if gt IE 8]><!---->

  




<link rel="stylesheet" href="/assets/css/hydejack-9.1.6.css" id="_stylePreload">
<link rel="stylesheet" href="/assets/icomoon/style.css" id="_iconsPreload">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap" id="_fontsPreload">



  <style id="_pageStyle">

html{--accent-color: rgb(94, 97, 94);--accent-color-faded: rgba(94, 97, 94, 0.5);--accent-color-highlight: rgba(94, 97, 94, 0.1);--accent-color-darkened: #4b4e4b;--theme-color: rgb(230, 217, 195)}
</style>


<!--<![endif]-->




</head>

<body class="no-break-layout">
  


<hy-push-state
  id="_pushState"
  replace-selector="#_main"
  link-selector="a[href]:not([href^='/assets/']):not(.external):not(.no-push-state)"
  script-selector="script"
  duration="500"
  hashchange
>
  
  
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <div class="nav-btn-bar">
      <a id="_menu" class="nav-btn no-hover" href="#_drawer--opened">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <div class="nav-span"></div>
    </div>
  </div>
</div>
<hr class="sr-only" hidden />

  <main
  id="_main"
  class="content layout-post"
  role="main"
>
  <nav id="breadcrumbs" class="screen-only"><ul>
  
  
    <li><a href="/">home</a></li>
    
      <li>
        
          <span>/</span>
          
          
          <a href="/pytorch/">pytorch</a>
        
      </li>
    
      <li>
        
          <span>/</span>
          <span>2024-09-24-raindrop</span>
        
      </li>
    
  
</ul></nav>
  










<article id="post-pytorch-raindrop" class="page post mb6" role="article">
  <header>
    <h1 class="post-title flip-project-title">
      
        (Code Review, ICLR 2022) Raindrop
      
    </h1>

    <div class="post-date">
      
      <span class="ellipsis mr1">
        <time datetime="2024-09-24T00:00:00+09:00">24 Sep 2024</time> in <span>Pytorch</span> 
      </span>
      
    </div>

    
    

    



  
    <p class="note-sm" >
      <a href="https://github.com/mims-harvard/Raindrop">Raindrop github</a>

    </p>
  


  </header>

  
    <p><a href="https://arxiv.org/abs/2110.05357">(Paper) Graph-Guided Network for Irregularly Sampled Multivariate Time Series</a></p>

<p><a href="https://lpppj.github.io/timeseries/2024-02-09-Raindrop">(Paper Review, ICLR 2022) Raindrop</a></p>

<h2 id="1-git-clone">1. Git clone</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig1.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig2.png" alt="사진1" /></p>

<ul>
  <li>먼저 터미널에 git clone과 requirements를 입력하여 install 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">python Raindrop.py</code>로 P19, P12, PAM 데이터셋에 대한 성능을 볼 수 있다.</li>
</ul>

<h2 id="2-raindroppy">2. Raindrop.py</h2>

<h3 id="21-data-preparing">2.1. Data Preparing</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig3.png" alt="사진1" />
<img src="/assets/img/pytorch/raindrop_code/fig4.png" alt="사진1" /></p>

<ul>
  <li>parser를 통해 aurgments를 만들고</li>
  <li>본 논문에서 제시하는 model은 irregular time series를 다룬다.
    <ul>
      <li>그러므로 <code class="language-plaintext highlighter-rouge">missing ratio</code>, 즉 feature를 masking하는 비율을 미리 결정해준다. (option)</li>
      <li>일단은 0(no missing)으로 두고 코드를 이해해보자</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig5.png" alt="사진1" /></p>

<ul>
  <li>사전에 정한 <code class="language-plaintext highlighter-rouge">missing ratio</code>를 사용한다.</li>
  <li>epoch 수와 learning rate도 미리 정한다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig6.png" alt="사진1" /></p>

<ul>
  <li>P19 데이터셋을 사용한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">d_static</code>과 <code class="language-plaintext highlighter-rouge">d_inp</code>는 시간에 따라 변하지 않는(정적) / 변하는(동적) 변수의 개수</li>
      <li><code class="language-plaintext highlighter-rouge">static_info</code>는 <code class="language-plaintext highlighter-rouge">d_static</code> 변수가 있는지 없는지 (bool)</li>
      <li><code class="language-plaintext highlighter-rouge">max_len</code>은, batch 내 샘플마다 시계열의 길이가 다른데 최대 길이
        <ul>
          <li>만약 <code class="language-plaintext highlighter-rouge">max_len</code>보다 짧다면 그 부분은 다 0으로 기록되어있다.</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">n_classes</code>는 샘플에 속하는 class의 개수</li>
    </ul>
  </li>
  <li>
    <p>다른 데이터셋을 사용한다면 위의 변수들은 달라질 수 있다.</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">d_ob</code>는 각 변수를 몇 차원으로 표현할지를 의미한다.</li>
  <li>그래서 <code class="language-plaintext highlighter-rouge">d_model</code>은 동적 변수의 개수인 <code class="language-plaintext highlighter-rouge">d_inp</code>에 <code class="language-plaintext highlighter-rouge">d_ob</code>를 곱한 값이 된다.</li>
  <li><code class="language-plaintext highlighter-rouge">nhid</code>는 FFN의 dimension인데 <code class="language-plaintext highlighter-rouge">d_model</code>의 2배를 사용</li>
  <li><code class="language-plaintext highlighter-rouge">nlayers</code>는 layer의 개수, <code class="language-plaintext highlighter-rouge">nhead</code>는 MHA(multi-head attention)에서 heads 개수이고 모두 2개를 사용</li>
  <li><code class="language-plaintext highlighter-rouge">dropout</code>은 TransformerEncoderLayer에서 사용하는 dropout ratio</li>
  <li><code class="language-plaintext highlighter-rouge">aggreg</code>는 나중에 각 배치마다, 각 시점을 vector로 표현할텐데 그걸 모든 시점에 대해 합칠 때 <strong>평균</strong>을 사용</li>
  <li><code class="language-plaintext highlighter-rouge">MAX</code>는 positional encoder에 들어가는 MAX parameter인데
    <ul>
      <li>막상 positional encoder 코드를 보면 <code class="language-plaintext highlighter-rouge">MAX</code>라는 변수를 사용하지 않으니 신경 안써도 된다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig7.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n_run</code>은 데이터셋에 대해 몇 번을 실험해서 기록할지를 의미한다.</li>
  <li><code class="language-plaintext highlighter-rouge">n_splits</code>는 데이터가 5등분 되어있어서 5를 사용한다.</li>
  <li>그리고 본 model을 평가하기 위한 성능 지표를 기록할 arrays를 만들어놓는다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig8.png" alt="사진1" /></p>

<ul>
  <li>그리고 불러온 데이터셋을 train(0.8) / valid(0.1) / test(0.1)로 나누고 label(y)도 따로 준비한다.</li>
  <li>P19 데이터셋의 경우 train에는 31042개의 샘플이 있다. (샘플은 한 명의 환자 정도로 생각할 수 있다.)
    <ul>
      <li>그리고 각 샘플은 <code class="language-plaintext highlighter-rouge">torch.size([# of timesetps,  # of features])</code>인 tensor이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig9.png" alt="사진1" /></p>

<ul>
  <li>T는 time steps의 수, F는 (동적) 변수의 개수이고, D는 (정적) 변수의 개수가 된다.
    <ul>
      <li>동적 변수는 <code class="language-plaintext highlighter-rouge">Ptrain</code>의 <code class="language-plaintext highlighter-rouge">arr</code>에, 정적 변수는 <code class="language-plaintext highlighter-rouge">Ptrain</code>의 <code class="language-plaintext highlighter-rouge">extended_static</code>에 따로 준비하고 있다.</li>
    </ul>
  </li>
  <li>그리고 normalization을 위해 모든 변수들의 평균과 표준편차를 얻는다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">getStats</code> 함수에는 사용하는데 특이사항 없으므로 skip</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig10.png" alt="사진1" /></p>

<ul>
  <li>각각의 shape은 아래와 같다.</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>의 동적 변수들의 개수가 34개였는데 68이 된 이유는 :
    <ul>
      <li>같은 크기의 Mask를 옆에 이어붙였기 때문이다.</li>
      <li>Mask는 <code class="language-plaintext highlighter-rouge">M = 1*(input_tensor &gt; 0) + 0*(input_tensor &lt;= 0)</code>이다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>은 31042개의 텐서인데, 각 tensor는 해당 샘플의 길이를 알려준다.
    <ul>
      <li>즉 만약 0번째 샘플의 길이가 40이라면, 0번째 tensor는<code class="language-plaintext highlighter-rouge">[1, 2, ..., 40, 0, 0, ...]</code>이다.</li>
      <li>일단 숫자는 <code class="language-plaintext highlighter-rouge">max_len</code>개인데 해당 샘플의 길이까지만 index를 기록하고 뒷부분은 zero padding</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">y_train</code>은 각 샘플의 정답 label이다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig11.png" alt="사진1" /></p>

<ul>
  <li>이제 <code class="language-plaintext highlighter-rouge">global_structure</code>를 정의하는데, 각각의 동적 변수가 상호작용하는지를 0, 1로 표현
    <ul>
      <li>adjacency matrix의 역할을 한다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">missing_ratio</code>가 존재했다면 몇몇 feature를 masking한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>이 <code class="language-plaintext highlighter-rouge">sample</code>이면 각 샘플(환자)마다 독립적으로 특성을 무작위 제거</li>
      <li><code class="language-plaintext highlighter-rouge">feature_removal_level</code>이 <code class="language-plaintext highlighter-rouge">set</code>이면 미리 계산된 density scores를 사용하여 제거할 특성을 결정하고 모든 샘플에서 제거</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig12.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Ptrain</code>의 shape을 <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps,  batch_size,  # of features(w/masking)])</code>으로,</li>
  <li><code class="language-plaintext highlighter-rouge">Ptrain_time</code>의 shape을 <code class="language-plaintext highlighter-rouge">torch.size([# of timesteps, batch_size])</code>로 setting</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig13.png" alt="사진1" /></p>

<ul>
  <li>앞서 소개한 parameters를 한 번 출력해보았다.</li>
  <li>지금은 masking ratio가 0이다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig14.png" alt="사진1" /></p>

<ul>
  <li>parameters의 descriptions는 위와 같다.</li>
</ul>

<h3 id="22-model-setting">2.2. Model setting</h3>

<p><img src="/assets/img/pytorch/raindrop_code/fig15.png" alt="사진1" /></p>

<ul>
  <li>이제 model, criterion, optimiazer, scheduler를 정의한다.</li>
  <li>model은 2d tensor로 표현된 샘플마다 classification하도록 설계되었으므로 CrossEntropyLoss를 사용</li>
  <li>아직 input을 model에 넣은 건 아님.
    <ul>
      <li>input이 model에 들어가면 어떤 과정을 거치는지는 아래 3. <code class="language-plaintext highlighter-rouge">models_rd.py</code>에서 보도록 한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig16.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">idx_0</code>은 <code class="language-plaintext highlighter-rouge">y</code>가 0인 samples의 index, <code class="language-plaintext highlighter-rouge">idx_1</code>은 반대</li>
  <li>label이 1인 샘플의 개수가 적은 unbalancing 문제를 해결하기 위해 3배로 늘림 (왜 <strong>3</strong>배인지는 모름)</li>
  <li>batch_size가 128인데 label이 0과 1인 samples를 절반씩 채울테니
    <ul>
      <li>n_batches는 개수가 더 적은 label 기준으로 모든 samples를 한 번씩 다 볼 수 있도록 설정했다.</li>
      <li>사실 label이 1인 samples를 3배 했으니 label이 1인 샘플을 3번씩 보는 꼴이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig17.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig18.png" alt="사진1" /></p>

<ul>
  <li>이제 epoch를 시작하는데, label이 0인 샘플과 1인 샘플에서 무작위로 <code class="language-plaintext highlighter-rouge">batch_size/2</code>개씩 가져온다.</li>
  <li>사실 label이 1인 samples를 3배 했으니 여기서는 중복된 샘플이 나올 가능성이 있다.</li>
  <li>model에 들어갈 input tensors의 shape을 미리 확인해두자</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig19.png" alt="사진1" /></p>

<ul>
  <li>이제 model에 들어가고 통상적인 backpropagation을 거친다.</li>
  <li>model에 들어가면 어떤 일이 일어나는지 알아보자.</li>
</ul>

<h2 id="3-models_rdpy">3. models_rd.py</h2>

<h3 id="31-init">3.1. <strong>init</strong></h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>이 상당히 많지만 지금 다 알 필요는 없다.</li>
  <li><code class="language-plaintext highlighter-rouge">forward</code>에서 사용할 때 다시 올라와서 보면 될 듯</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig20.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig21.png" alt="사진1" /></p>

<p><img src="/assets/img/pytorch/raindrop_code/fig22.png" alt="사진1" /></p>

<h2 id="32-forward">3.2. forward</h2>

<p><img src="/assets/img/pytorch/raindrop_code/fig23.png" alt="사진1" /></p>

<ul>
  <li>P19 데이터셋의 경우 input shape은 주황색 주석과 같다.</li>
  <li>src로 들어오는 P의 경우 34개의 변수였는데 같은 크기의 Mask를 옆에 이어붙인 것이니 다시 분리
    <ul>
      <li>각각을 missing_mask, src라고 부름</li>
    </ul>
  </li>
  <li>그 다음 34개의 변수를 <code class="language-plaintext highlighter-rouge">d_ob</code>(여기선 4)번 반복해서 src의 representation capacity를 키워주고
    <ul>
      <li>ReLu를 통과시켜서 non-linearity를 표현할 수 있게 한다.</li>
      <li>그 다음 dropout을 거친다.</li>
    </ul>
  </li>
  <li>결국 <code class="language-plaintext highlighter-rouge">h</code>는 src를 확장시키고 learnable weights와 ReLu를 곱해 모델이 학습할 수 있는 형태로 만든 것</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig24.png" alt="사진1" /></p>

<ul>
  <li>이제 batch에 있는 각 sample마다 mask를 만든다.</li>
  <li>sample에 값이 있으면 mask에는 False가 되고 값이 없으면 mask가 True가 된다.</li>
  <li>mask의 길이는 60으로 고정이지만 sample마다 길이가 다르기 때문에 어디까지 False이고 언제부터 True인지는 sample마다 다르다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig25.png" alt="사진1" /></p>

<ul>
  <li>다음으로 <code class="language-plaintext highlighter-rouge">global_structure</code>를 adjacency matrix로 사용한다.
    <ul>
      <li>shape은 동적 변수의 개수 <code class="language-plaintext highlighter-rouge">d_inp</code> x <code class="language-plaintext highlighter-rouge">d_inp</code>가 되므로 각 동적 변수를 연결 여부를 (0,1)로 표현한다.</li>
      <li>epoch가 진행되면서 바뀔 수도 있으니 대각성분은 항상 1로 update해준다.</li>
    </ul>
  </li>
  <li>그 다음 edge_index와 edge_weights를 미리 구해놓는다.
    <ul>
      <li>연결된 nodes의 index와 그 weights를 의미함</li>
    </ul>
  </li>
  <li>그 다음 batch에 있는 각 sample마다 (동적) 변수들의 global structure(edge)를 고려한 representation을 저장할 공간 <code class="language-plaintext highlighter-rouge">output</code>을 미리 만들어놓는다.
    <ul>
      <li>각 sample마다 <code class="language-plaintext highlighter-rouge">torch([# of time steps,  d_inp x d_ob])</code> shape의 tensor가 들어갈 예정이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig26.png" alt="사진1" /></p>

<ul>
  <li>이제 아까 만든 <code class="language-plaintext highlighter-rouge">h</code>를 <code class="language-plaintext highlighter-rouge">x</code>로 받아서 (<code class="language-plaintext highlighter-rouge">x=h</code>) 하나의 sample에 대한 <code class="language-plaintext highlighter-rouge">h</code>를 <code class="language-plaintext highlighter-rouge">stepdata</code> 가져온다</li>
  <li><code class="language-plaintext highlighter-rouge">p_t</code>는 각 timestep을 <code class="language-plaintext highlighter-rouge">d_pe = 16</code>차원 vector로 embedding한 것이다. (init 참고)</li>
  <li>이제 <code class="language-plaintext highlighter-rouge">stepdata</code>를 <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code>로 reshape한다.
    <ul>
      <li>왜냐하면 feature끼리 attention을 수행하기 때문에 각 feature를 하나의 vector로 만들 필요가 있기 때문</li>
    </ul>
  </li>
  <li>이제 각 feature를 vector로 만든 걸 <code class="language-plaintext highlighter-rouge">ob_propagation</code>으로 정의된 attention layer에 넣는다.
    <ul>
      <li>그러면 같은 shape <code class="language-plaintext highlighter-rouge">torch([# of features,  (# of time steps)x(d_ob)])</code> tensor가 return되지만</li>
      <li>해당 sample의 각각의 features를 Observation Propagation을 거쳐 representation한 결과이다.</li>
      <li><code class="language-plaintext highlighter-rouge">Ob_propagation.py</code>에 있고, 코드를 따로 첨부하지는 않겠으나 아래와 같은 과정을 거친다.
        <ul>
          <li>1) Message Passing: node 간에 정보를 전달하는 mechanism 구현</li>
          <li>2) Attention Mechanism: 각 node가 이웃 node로부터 받는 메시지의 중요도를 학습</li>
          <li>3) Egde weights: graph의 edge에 weight를 적용하여 정보 전달의 강도를 조절</li>
          <li>4) Edge prune: 중요도가 낮은 edge를 제거하여 computation efficiency 높임</li>
          <li>5) Feature Transform: linear Transform과 activation ftn으로 node의 feature를 변환</li>
          <li>6) Aggregation: 이웃 node로부터 받은 메시지를 합침</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig27.png" alt="사진1" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ob_propagation-layer</code>를 한 번 더 통과시키고 shape을 맞춰서 <code class="language-plaintext highlighter-rouge">output</code>의 sample index 자리에 넣는다.
    <ul>
      <li>그리고 alpha_all에는 그 attention weights를 넣는다.
        <ul>
          <li>34개의 features끼리의 attention이니 34\(\times\)34\(=\)1156개의 숫자가 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>모든 samples에 대해서 완료하여 <code class="language-plaintext highlighter-rouge">output</code>이 완성되면 distance를 구한다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig28.png" alt="사진1" /></p>

<ul>
  <li>다음으로 time embedding을 concat한다.</li>
  <li>이러면 shape이 <code class="language-plaintext highlighter-rouge">torch.size([60, 128, 152])</code>가 되는데, 각 sample마다(128), 하나의 시점을 152차원 vector로 표현한 것이다.
    <ul>
      <li>이 152는 (동적) 변수 34개를 34\(\times\)4 = 136차원으로 표현하고, time embedding 16차원을 붙인 것</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig29.png" alt="사진1" /></p>

<ul>
  <li>이걸 transformer encoder에 통과시키고</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig30.png" alt="사진1" /></p>

<ul>
  <li>aggregate 하는데, 이 때 모든 시점에 대해 평균을 내준다. (<code class="language-plaintext highlighter-rouge">aggreg == mean</code>)</li>
  <li>그러면 각 sample은 모든 시점과 모든 변수를 통합하여 152차원 벡터로 표현된 결과가 나온다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig31.png" alt="사진1" /></p>

<ul>
  <li>마지막으로 (정적) 변수를 embedding한 emb를 붙여서 2-layer MLP에 넣으면</li>
  <li>각 sample에 대한 classification이 완료된다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig32.png" alt="사진1" /></p>

<ul>
  <li>Training에 따른 validation set acccuracy가 출력된다.</li>
</ul>

<p><img src="/assets/img/pytorch/raindrop_code/fig33.png" alt="사진1" /></p>

<ul>
  <li>그리고 classification report가 출력된다.</li>
</ul>

<p>끝 !</p>

<ul>
  <li>참고로 나의 경우에는 <code class="language-plaintext highlighter-rouge">from torch_scatter import gather_csr, scatter, segment_csr</code>가 안되어서 아래와 같이 주석 처리하고
    <ul>
      <li>pytorch를 보고 함수를 직접 작성하여 사용하였다.</li>
      <li><a href="https://github.com/rusty1s/pytorch_scatter/blob/master/torch_scatter/scatter.py">pytorch_scatter 참고</a></li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># from torch_scatter import gather_csr, scatter, segment_csr
</span><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">+</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
            <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">(),</span> <span class="n">other</span><span class="p">.</span><span class="nf">dim</span><span class="p">()):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">other</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">src</span>

<span class="k">def</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">index</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">dim_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">dim_size</span>
        <span class="k">elif</span> <span class="n">index</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">dim_size</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="n">index_dim</span> <span class="o">=</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">index_dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index_dim</span> <span class="o">+</span> <span class="n">src</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">index_dim</span><span class="p">:</span>
        <span class="n">index_dim</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="nf">size</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">src</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">index_dim</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="n">count</span><span class="p">[</span><span class="n">count</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="p">.</span><span class="nf">is_floating_point</span><span class="p">():</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">true_divide_</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">div_</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">floor</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">torch_scatter</span><span class="p">.</span><span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="nb">reduce</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">
    |

    .. image:: https://raw.githubusercontent.com/rusty1s/pytorch_scatter/
            master/docs/source/_figures/add.svg?sanitize=true
        :align: center
        :width: 400px

    |

    Reduces all values from the :attr:`src` tensor into :attr:`out` at the
    indices specified in the :attr:`index` tensor along a given axis
    :attr:`dim`.
    For each value in :attr:`src`, its output index is specified by its index
    in :attr:`src` for dimensions outside of :attr:`dim` and by the
    corresponding value in :attr:`index` for dimension :attr:`dim`.
    The applied reduction is defined via the :attr:`reduce` argument.

    Formally, if :attr:`src` and :attr:`index` are :math:`n`-dimensional
    tensors with size :math:`(x_0, ..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})`
    and :attr:`dim` = `i`, then :attr:`out` must be an :math:`n`-dimensional
    tensor with size :math:`(x_0, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})`.
    Moreover, the values of :attr:`index` must be between :math:`0` and
    :math:`y - 1`, although no specific ordering of indices is required.
    The :attr:`index` tensor supports broadcasting in case its dimensions do
    not match with :attr:`src`.

    For one-dimensional tensors with :obj:`reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, the operation
    computes

    .. math::
        \mathrm{out}_i = \mathrm{out}_i + \sum_j~\mathrm{src}_j

    where :math:`\sum_j` is over :math:`j` such that
    :math:`\mathrm{index}_j = i`.

    .. note::

        This operation is implemented via atomic operations on the GPU and is
        therefore **non-deterministic** since the order of parallel operations
        to the same value is undetermined.
        For floating-point variables, this results in a source of variance in
        the result.

    :param src: The source tensor.
    :param index: The indices of elements to scatter.
    :param dim: The axis along which to index. (default: :obj:`-1`)
    :param out: The destination tensor.
    :param dim_size: If :attr:`out` is not given, automatically create output
        with size :attr:`dim_size` at dimension :attr:`dim`.
        If :attr:`dim_size` is not given, a minimal sized output tensor
        according to :obj:`index.max() + 1` is returned.
    :param reduce: The reduce operation (:obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">mul</span><span class="sh">"</span><span class="s">`,
        :obj:`</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="s">`, :obj:`</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="s">` or :obj:`</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="s">`). (default: :obj:`</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">`)

    :rtype: :class:`Tensor`

    .. code-block:: python

        from torch_scatter import scatter

        src = torch.randn(10, 6, 64)
        index = torch.tensor([0, 1, 0, 1, 2, 1])

        # Broadcasting in the first and last dim.
        out = scatter(src, index, dim=1, reduce=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="s">)

        print(out.size())

    .. code-block::

        torch.Size([10, 3, 64])
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">sum</span><span class="sh">'</span> <span class="ow">or</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">add</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mul</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_mean</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">min</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">reduce</span> <span class="o">==</span> <span class="sh">'</span><span class="s">max</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span>

</code></pre></div></div>

  
</article>



  <hr class="dingbat related mb6" />






  
     


  <aside class="about related mt4 mb4" role="complementary">
    
    

<div class="author mt4">
  

  
    


<img
  
    src="https://via.placeholder.com/128x128"
    srcset="/assets/img/me/me.JPG 1x,/assets/img/me/me.JPG 2x"
    
  
  alt="GW Jeong"
  class="avatar"
  
  width="120"
  height="120"
  loading="lazy"
/>

  

  
  
  <h2  class="page-title hr-bottom">
    About
  </h2>

  <p>Bachelor’s degree in Applied Statistics. Yonsei Univ. (2018~2024) <br />
DataScienceLab 8th 학회장 (2022~2023) <br />
Master’s degree in Statitstics. Yonsei Univ. (2024~)</p>


  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>

  </aside>


  

  
  

  
    


  

  
  

  
    

  


  
<footer class="content" role="contentinfo">
  <hr/>
  
    <p><small class="copyright">© Geonwoo Jeong.
</small></p>
  
  
    <p><small>Powered by <a class="external" href="https://hydejack.com/">Hydejack</a> v<span id="_version">9.1.6</span></small></p>
  <hr class="sr-only"/>
</footer>


</main>

  <hy-drawer
  id="_drawer"
  class=""
  side="left"
  threshold="10"
  noscroll
  
>
  <header id="_sidebar" class="sidebar" role="banner">
    




<div class="sidebar-bg sidebar-overlay" style="background-color:rgb(230, 217, 195);background-image:url(/assets/img/me/sidebar.jpg)"></div>

    <div class="sidebar-sticky">
  <div class="sidebar-about">
    
      <a class="no-hover" href="/" tabindex="-1">
        <img src="/assets/img/me/logo.jpg" class="avatar" alt="LpppJ" width="120" height="120" loading="lazy" />
      </a>
    
    <a class="sidebar-title" href="/"><h2 class="h1">LpppJ</h2></a>
    
    
      <p class="">
        DataScience and AI

      </p>
    
  </div>

  <nav class="sidebar-nav heading" role="navigation">
    <span class="sr-only">Navigation:</span>
<ul>
  
    
      
      <li>
        <a
          id="_drawer--opened"
          href="/timeseries/"
          class="sidebar-nav-item "
          
        >
          TimeSeries
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/mamba/"
          class="sidebar-nav-item "
          
        >
          Mamba
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/pytorch/"
          class="sidebar-nav-item "
          
        >
          Pytorch
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/stat/"
          class="sidebar-nav-item "
          
        >
          Statistics
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/project/"
          class="sidebar-nav-item "
          
        >
          Project
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/presentation/"
          class="sidebar-nav-item "
          
        >
          Presentation
        </a>
      </li>
    
  
</ul>

  </nav>

  
  <div class="sidebar-social">
    <span class="sr-only">Social:</span>
<ul>
  
    
      



  

  
  
  
  

  

  

  <li>
    <a href="https://github.com/lpppj" title="GitHub" class="no-mark-external">
      <span class="icon-github"></span>
      <span class="sr-only">GitHub</span>
    </a>
  </li>


    
      



  

  
  
  
  

  

  

  <li>
    <a href="mailto:wjdrjsdn39@yonsei.ac.kr" title="Email" class="no-mark-external">
      <span class="icon-mail"></span>
      <span class="sr-only">Email</span>
    </a>
  </li>


    
  
</ul>

  </div>
</div>
  </header>
</hy-drawer>
<hr class="sr-only" hidden />

</hy-push-state>


  <!--[if gt IE 10]><!---->
  <script nomodule>!function(){var t,n=document.createElement("script");!("noModule"in n)&&"onbeforeload"in n&&(t=!1,document.addEventListener("beforeload",function(e){if(e.target===n)t=!0;else if(!e.target.hasAttribute("nomodule")||!t)return;e.preventDefault()},!0),n.type="module",n.src=".",document.head.appendChild(n),n.remove())}();
</script>
  <script src="/assets/js/hydejack-9.1.6.js" type="module"></script>
  <script src="/assets/js/LEGACY-hydejack-9.1.6.js" nomodule defer></script>
  

  

<!--<![endif]-->
  <!-- <script>
  document.querySelector('hy-push-state').setAttribute('prefetch', '');

  document.querySelectorAll('.sidebar a[href^="/"]').forEach(function (el) { 
    el.addEventListener('click', function (e) {
      if (el.pathname === window.location.pathname) {
        e.preventDefault();
        e.stopPropagation();
        document.querySelector('hy-drawer').close();
      }
    });
  });
</script> -->

<!--
Code for integrating CloudFlare's email protection with Hydejack's single page app loading.
-->
<script>
  document.getElementById('_pushState').addEventListener('hy-push-state-after', function (e) {
    function e(e){
      (console.error?console.error:console.log).call(console,e)
    }

    function t(e){
      return l.innerHTML='<a href="'+e.replace(/"/g,"&quot;")+'"></a>',l.childNodes[0].getAttribute("href")
    }

    function r(e,t){
      var r=e.substr(t,2);return parseInt(r,16)
    }

    function n(e,n){
      for(var o="",c=r(e,n),a=n+2;a<e.length;a+=2){
        var l=r(e,a)^c;
        o+=String.fromCharCode(l)
      }
      return t(o)
    }

    var o="/cdn-cgi/l/email-protection#",
        c=".__cf_email__",
        a="data-cfemail",
        l=document.createElement("div");

    !function(){
      for(var t=document.getElementsByTagName("a"),r=0;r<t.length;r++)
        try{
          var c=t[r],a=c.href.indexOf(o);
          a>-1&&(c.href="mailto:"+n(c.href,a+o.length))
        }catch(t){
          e(t)
        }
    }(),
    function(){
      for(var t=document.querySelectorAll(c),r=0;r<t.length;r++)
        try{
          var o=t[r],l=n(o.getAttribute(a),0),i=document.createTextNode(l);
          o.parentNode.replaceChild(i,o)
        }catch(t){
          e(t)
        }
    }()
  });
</script>





<div hidden>
  
  <h2 class="sr-only">Templates (for web app):</h2>

  <template id="_animation-template">
  <div class="animation-main fixed-top">
    <nav id="breadcrumbs" class="screen-only"><ul>
  
  
</ul></nav>
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

  <template id="_loading-template">
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

  <template id="_error-template">
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

  <template id="_permalink-template">
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="content-hash"></span>
  </a>
</template>

</div>


</body>
</html>
