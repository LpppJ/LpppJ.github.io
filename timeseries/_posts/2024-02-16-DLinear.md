---
layout: post
related_posts:
  _
title: 
description: >
  [AAAI 2022](https://arxiv.org/abs/2205.13504)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# DLinear: Are Transformer Effective for Time Series Forecasting? (AAAI 2022)

## Abstract
- Transformer-based solutions는 긴 시퀀스 속에 있는 semantic correlations를 잘 추출하기 때문에 long-term time series forecasting(LTSF)에 쓰인다.
- 하지만 이러한 permutation-invariant self-attention의 특성상 temporal information loss가 불가피하다.
- 그러므로 simple one-layer linear models (LTSF-Linear)를 제안한다.

## 1. Introduction
- Transformer의 핵심인 multi-head self-attention은 `permutation-invariant` = `anti-order`한 semantic correlations를 찾아낸다.
- 하지만 time series는 순서 자체가 굉장히 중요한데, NLP를 위한 transformer가 LTST에 효과적일까?
- 결론적으로, 실험결과 : 복잡한 transformer 구조보다 단순한 LTSF-Linear의 성능이 더 뛰어났다.

## 2. Prelininaries : TSF Problem Formulation
- 본 논문에서 사용할 notation은 간단하다. $$\cal X = \{ X_1^t, ..., X_C^t\}_{t=1}^L$$을 보고 $$\cal \hat X = \{\hat X_1^t, ..., \hat X_C^t\}_{t=L+1}^{L+T}$$를 예측하는 것이다. ($$C$$는 변수의 개수, $$L$$은 look-back window의 size, $$T$$는 예측하고자 하는 time steps)

## 3. Transformer-based LTSF solutions
- Vanilla Transformer는 quadratic time/memory complexity, error accumulation by the autoregressive decoder로 인한 한계가 있고, 이걸 해결하기 위한 많은 transformer-based models가 있는데, 요약하면 아래 그림과 같다.
![사진1](/assets/img/timeseries/AreTF/fig1.jpeg)
- Autoformer, LogTrans, Pyraformer, FEDformer, Informer 등 transformer를 time series에 활용하기 위한 다양한 시도들이 있었는데, 파이프라인 단계별로 간단하게만 보겠다.
  - (a) Preprocessing : normalization with zero-mean, seasonal-trend decomposition, ...
  - (b) Embedding : fixed positional encoding, channel projection embedding, learnable temporal embedding, ...
  - (c) Encoder : Logsparse mask(LogTrans), pyramidal attention(Pyraformer), ProbSparse(Informer), Frequency enhanced block(FEDformer), series-wise auto-correlation(Autoformer), ...
  - (d) Decoder : IMS(Iterative Multi-step forecasting, 단일 시점 예측 반복하여 여러 시점 예측) 대신 DMS(Direct Multi-step forecasting, 각 미래 시점 예측 위해 별도 모델) 사용
- 하지만 앞서 언급한 것처럼 permutation-invariant한 semantic correlations은 temporal relation과 다르다.

## 4. An Embarrassingly Simple Baseline
- 기존의 transformer-based LTSF solutions는 non-transformer에 비해 성능이 좋다는 실험 결과를 내놓았지만, 그건 IMS를 사용한 non-transformer와 달리 DMS를 사용했기 때문이다.
- 이걸 뒷받침하는 근거가 본 논문에서 제시하는 DMS 모델인 LTSF-Linear인데, $$\hat X_i = WX_i$$ where $$W \in \mathbb R^{T\times L}$$ 구조이다.
- LTSF-Linear는 DLinear, NLinear 두 종류가 있다.
  - DLinear : trend component와 seasonal component로 decompose하고 각각을 one-layer에 넣어 다시 sum한다.
  - NLinear : Input의 각 값에서 마지막 값을 빼고 linear layer를 통과한 뒤 다시 더해주는 simple normalization
  ![사진2](/assets/img/timeseries/AreTF/fig2.jpeg)

## 5. Experiments
- 사용한 데이터셋은 9개(ETTm1/2, ETTh1/2, ILI, Weather, Traffic, Exchange, Electrictiy)이고 비교한 모델은 5개(FEDformer, Autoformer, Informer, Pyraformer, LogTrans)이다. 아래는 quantitative and qualitative results이다.
  ![사진3](/assets/img/timeseries/AreTF/table12.jpeg)
  ![사진4](/assets/img/timeseries/AreTF/fig3.jpeg)
- Q) Can existing LTSF-Transformers extract temporal relations well from longer input sequences?
  - ![사진5](/assets/img/timeseries/AreTF/fig4.jpeg)
  - Look-back window size가 커짐에 따라, transformer-based models의 성능은 크게 좋아지지 않지만, LTSF-Linear의 성능은 유의미하게 좋아진다.
- Q) Are the self-attention scheme effective for LTSF?
  - ![사진6](/assets/img/timeseries/AreTF/table4.jpeg)
  - Informer에서 one linear layer까지 구조를 점점 단순하게 할수록 성능이 좋아졌다. 즉 복잡한 모듈이 불필요하다.
- Q) Can existing LTSF-Transformers preserve temporal order well?
  - ![사진7](/assets/img/timeseries/AreTF/table5.jpeg)
  - 데이터를 섞거나 반 나눠서 순서를 바꿨을 때, 다른 transformer-based models은 성능이 크게 떨어지지 않았지만, LTSF-Linear의 경우 유의미하게 떨어졌다.
  - 그러므로 LTSF-Linear가 다른 transformer-based models보다 temporal order를 잘 보존한다.
- Q) Is training data size a limiting factor for existing LTSF- Transformers?
  - ![사진8](/assets/img/timeseries/AreTF/table7.jpeg)
  - Full dataset(ori.)로 학습했을 때보다 1년치 데이터(short)로 학습했을 때 성능이 더 좋았다.
  - 단순하게 긴 training data가 필요한게 아니라, whole-year data에 더 명확한 temporal features가 있다는 걸 알 수 있다.

## 6. Conclusion and Future Work
- 본 논문의 contribution은 linear model을 제안한 데에 있는 것이 아니라, 비교 실험을 통해 왜 LTSF-transformers가 효율적이지 못한지에 대한 질문을 던지는 데에 있다.
- 본 논문의 비교 실험 결과를 통해 transformer의 구조에 대해 더 잘 이해하고 효율적으로 사용하기 위한 방법을 고민해볼 수 있다.
- 본 논문에서 제시하는 LTSF-Linear는 change points로 인한 temporal dynamics를 포착하기 어려운 등 한계점이 분명하기 때문에, 앞으로의 연구를 위해 단순하면서 경쟁력 있는 기준선 정도로 생각하면 되겠다.

## 추가
- 본 논문이 발표되기 전에 transformer-based time series models가 많이 나왔다. Informer, Autoformer, Pyraformer, Fedformer 등이다. 본 논문은 이러한 solutions을 반박하면서 conv를 제안하였다. 이제 본 논문을 반박하면서 다시 transformer를 제시하는 PatchTST를 읽어보러 가자.