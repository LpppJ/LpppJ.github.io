<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-02-26T13:41:07+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (PMLR 2022)</title><link href="http://localhost:4000/llm/2025-02-26-BLIP/" rel="alternate" type="text/html" title="BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (PMLR 2022)" /><published>2025-02-26T00:00:00+09:00</published><updated>2025-02-26T13:41:07+09:00</updated><id>http://localhost:4000/llm/BLIP</id><content type="html" xml:base="http://localhost:4000/llm/2025-02-26-BLIP/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>대부분의 Vision-Language Pre-training (VLP)는
    <ul>
      <li>either <strong>understanding-based</strong> tasks or generation-based tasks</li>
      <li>scaling up the dataset with <strong>noisy</strong> image-textpairs collected from the web (sub-optimal)</li>
    </ul>
  </li>
  <li>본 논문에서는 BLIP 제안
    <ul>
      <li>transfers flexibly to both vision-language <strong>understanding</strong> and <strong>generation</strong> tasks</li>
      <li>the noisy web data 활용 by bootstrapping the captions
        <ul>
          <li>generates syntheticcaptions and a filter removes the noisy ones.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Existing methods have 2 major <strong>limitations</strong>:
    <ul>
      <li><strong>Model</strong> perspective:
        <ul>
          <li>encoder-based models는 less straightforward to directly transfer to text generation tasks</li>
          <li>encoder-decoder models는 have not beensuccess fully adopted for image-text retrieval tasks</li>
        </ul>
      </li>
      <li><strong>Data</strong> perspective:
        <ul>
          <li>SOTA(CLIP, ALBEF, SimVLM) 다 noisy web text 쓰다보니 suboptimal</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 BLIP의 2 <strong>contribution</strong>:
    <ul>
      <li>Multimodal mixture of Encoder-Decoder (<strong>MED</strong>):
        <ul>
          <li>unimodal encoder도 되고 image-grounded text encoder/decoder 다 됨</li>
          <li>jointly pre-train
            <ul>
              <li><strong>image-text contrastive learning</strong>,</li>
              <li><strong>image-text matching</strong>,</li>
              <li><strong>image-conditioned language modeling</strong></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Captioning and Filtering (<strong>CapFilt</strong>):
        <ul>
          <li>a <strong>captioner</strong>: to produce synthetic captions given web images</li>
          <li>a <strong>filter</strong>: to remove noisy captions from both the original web texts and the synthetic texts</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/llm/BLIP/fig1.png" alt="그림1" /></p>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="21-vision-language-pre-training">2.1. Vision-language Pre-training</h3>

<ul>
  <li>Vision-language pre-training (VLP)들은 data crawled from the web 쓰다보니 suboptimal
    <ul>
      <li>그러므로 web datasets를 잘 활용하는 <strong>CapFilt</strong>를 제안하겠음</li>
    </ul>
  </li>
  <li>문제는 <strong>understanding-based</strong> tasks (e.g. image-text retrieval)and generation-based tasks (e.g. image captioning) 둘다 잘해야 한다는 건데
    <ul>
      <li>기존 encoder-based, encoder-decoder 다 잘 못해서</li>
      <li>본 논문에서 <strong>multimodal mixture ofencoder-decoder model</strong>을 제안하겠음</li>
    </ul>
  </li>
</ul>

<h3 id="22-knowledge-distillation">2.2. Knowledge Distillation</h3>

<ul>
  <li>Self-distillation: teacher and student have <strong>equal sizes</strong></li>
  <li>기존 KD: simply enforce the student to have the <strong>same</strong> class predictions as the teacher,</li>
  <li>CapFilt: the <strong>captioner</strong> distills its knowledge through semantically-rich synthetic captions
    <ul>
      <li>the <strong>filter</strong> distills its knowledge by removing noisy captions</li>
    </ul>
  </li>
</ul>

<h3 id="23-data-augmentation">2.3. Data Augmentation</h3>

<ul>
  <li>synthesize examples for various NLP tasks
    <ul>
      <li>기존: the low-resourcelanguage-only tasks</li>
      <li>BLIP: the advantage of synthetic captions in large-scale vision-language pre-training</li>
    </ul>
  </li>
</ul>

<h2 id="3-method">3. Method</h2>

<ul>
  <li>기본적으로 noisy image-text pairs에서 학습. MED와 CapFilt를 알아보자</li>
</ul>

<p><img src="/assets/img/llm/BLIP/fig2.png" alt="그림1" /></p>

<h3 id="31-model-architecture">3.1. Model Architecture</h3>

<ul>
  <li>Multimodal mixture of encoder-decoder (MED)는 다음 3가지 가능한 multi-task model
    <ul>
      <li>(1) <strong>Unimodal Encoder</strong>: separately encodes imageand text (ViT, BERT)</li>
      <li>(2) <strong>Image-grounded text encoder</strong>: text encoder인데 visual 정보 삽입
        <ul>
          <li>how? Self-attention과 FFN 사이에 cross-attention 하나 추가</li>
        </ul>
      </li>
      <li>(3) <strong>Image-grounded text decoder</strong>: (2)에서 bi-directional self-attention을 causal self-attention으로 대체</li>
    </ul>
  </li>
</ul>

<h3 id="32-pre-training-objectives">3.2. Pre-training Objectives</h3>

<ul>
  <li>2 <strong>understanding</strong>-based objectives &amp; 1 <strong>generation</strong>-based objective</li>
  <li>Each image-text pair는 <strong>one</strong> forward pass <strong>visual</strong> transformer &amp; <strong>three</strong> forward passes <strong>text</strong> transformer</li>
</ul>

<h3 id="image-text-contrastive-loss-itc">Image-Text Contrastive Loss (ITC)</h3>

<ul>
  <li>unimodal encoder를 activate
    <ul>
      <li><strong>visual</strong> transformer and the <strong>text</strong> transformer의 feature space를 align</li>
      <li><strong>positive</strong> image-text pairs가 similar representations 갖도록 (negative는 반대)
        <ul>
          <li>momentum encoder가 features와 soft labels만들고 pairs로 학습</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="image-text-matching-loss-itm">Image-Text Matching Loss (ITM)</h3>

<ul>
  <li>image grounded text encoder를 activate</li>
  <li>image-text multi-modal representation을 통해 vision-language align
    <ul>
      <li>predict whether an image-text pair is positive, given their multi-modal feature</li>
    </ul>
  </li>
</ul>

<h3 id="language-modeling-loss-lm">Language Modeling Loss (LM)</h3>

<ul>
  <li>image grounded text decoder를 activate</li>
  <li>generate textual descriptions given an image
    <ul>
      <li>optimizes a cross entropy loss, to maximize the likelihood of the text, in an autoregressive manner.</li>
    </ul>
  </li>
</ul>

<h3 id="33-capfilt">3.3. CapFilt</h3>

<p><img src="/assets/img/llm/BLIP/fig3.png" alt="그림1" /></p>

<ul>
  <li><strong>captioner</strong>: to generate captions given web images
    <ul>
      <li>image-grounded text <strong>de</strong>coder</li>
      <li>finetuned with the LM objective to <strong>decode texts given images</strong></li>
      <li>web images \(I_w\)보고 synthetic captions \(T_s\) 생성</li>
    </ul>
  </li>
  <li><strong>filter</strong>: to remove noisy image-text pairs
    <ul>
      <li>image-grounded text <strong>en</strong>coder</li>
      <li>finetune with the ITC and ITM objectives to learn <strong>whether a text matches an image</strong></li>
      <li>removes noisy texts in both the original web texts \(T_w\) and the synthetic texts \(T_s\)</li>
    </ul>
  </li>
  <li>both are initialized from the same pre-trained MED, finetuned individually on the COCO dataset</li>
</ul>

<h2 id="4-experiments-and-discussions">4. Experiments and Discussions</h2>

<h3 id="41-pre-training-details">4.1. Pre-training Details</h3>

<ul>
  <li>image transformer is initialized from ViT pre-trained on ImageNet</li>
  <li>the text transformer is initialized from BERT-base</li>
</ul>

<h3 id="42-effect-of-capfilt">4.2. Effect of CapFilt</h3>

<p><img src="/assets/img/llm/BLIP/table1.png" alt="그림1" /></p>

<ul>
  <li>captioner와 filter 둘 중 하나만 써도 성능 개선, 둘 다 쓰면 더더욱 개선</li>
</ul>

<p><img src="/assets/img/llm/BLIP/fig4.png" alt="그림1" /></p>

<ul>
  <li>example captions and their corresponding images
    <ul>
      <li>the captioner to generate new textual descriptions</li>
      <li>the filter to remove noisy captions from both the originalweb texts and the synthetic texts</li>
    </ul>
  </li>
</ul>

<h3 id="43-diversity-is-key-for-synthetic-captions">4.3. Diversity is Key for Synthetic Captions</h3>

<p><img src="/assets/img/llm/BLIP/table2.png" alt="그림1" /></p>

<ul>
  <li>Nucleus sampling is astochastic decoding method
    <ul>
      <li>each token is sampled from a set of tokens whose cumulative probability mass exceeds a threshold \(p\) (\(p=0.9\))</li>
      <li>beam search, a deterministic decoding method와 비교했을 때 더 뛰어남</li>
    </ul>
  </li>
  <li>nucleus sampling generates more <strong>diverse</strong> and surprising captions</li>
  <li>beam search tends to generate <strong>safe</strong> captions that are common inthe dataset</li>
</ul>

<h3 id="44-parameter-sharing-and-decoupling">4.4. Parameter Sharing and Decoupling</h3>

<p><img src="/assets/img/llm/BLIP/table3.png" alt="그림1" /></p>

<ul>
  <li>Pre-training에서 the text encoder and decoder share all parameters</li>
  <li>except for the SA(self-attention) layers</li>
  <li>만약 SA layer도 공유했으면 encoding과 decoding이 충돌해서 성능 안좋아짐</li>
</ul>

<p><img src="/assets/img/llm/BLIP/table4.png" alt="그림1" /></p>

<ul>
  <li>if the captioner and filter share parameters in the same way as pre-training,
    <ul>
      <li><strong>confirmatio-bias</strong>: parameter sharing 때문에 captioner가 만든 noisy captions가 filter out이 잘 안됨</li>
    </ul>
  </li>
</ul>

<h2 id="5-comparison-with-state-of-the-arts">5. Comparison with State-of-the-arts</h2>

<h3 id="51-image-text-retrieval">5.1. Image-Text Retrieval</h3>

<ul>
  <li>image-to-text retrieval (TR) &amp; text-to-image retrieval (IR)
    <ul>
      <li>finetune the pre-trained modelusing ITC and ITM losses</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/llm/BLIP/table5.png" alt="그림1" /></p>

<h3 id="52-image-captioning">5.2. Image Captioning</h3>

<p><img src="/assets/img/llm/BLIP/table7.png" alt="그림1" /></p>

<ul>
  <li>14M 데이터로 pre-train 해도 성능이 준수하고 129M로 pre-train하면 LEMON의 200M 만큼의 성능</li>
</ul>

<h3 id="53-visual-question-answering-vqa">5.3. Visual Question Answering (VQA)</h3>

<p><img src="/assets/img/llm/BLIP/fig5.png" alt="그림1" /></p>

<ul>
  <li>During finetuning, pre-trained model을 re-arrange (그림 5 (a))
    <ul>
      <li>image-question is first encoded into multi modal embeddings</li>
      <li>그 다음 given to an answer decoder</li>
    </ul>
  </li>
</ul>

<h3 id="54-natural-language-visual-reasoning-nlvr">5.4. Natural Language Visual Reasoning (NLVR)</h3>

<ul>
  <li>NLVR: 한 문장이 두 그림 중 어떤 걸 설명하는지 맞추는 것</li>
  <li>사진이 2장이다보니 two cross-attention layers가 있고 (그림 5 (b))
    <ul>
      <li>2 CA layers are intialized from the same pre-trained weights</li>
      <li>각각의 outputs는 merged &amp; fed to the FFN</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/llm/BLIP/table8.png" alt="그림1" /></p>

<h3 id="55-visual-dialog-visdial">5.5. Visual Dialog (VisDial)</h3>

<ul>
  <li>VisDial: VQA의 확장. image-question pair뿐만 아니라 대화 기록, captions보고 예측</li>
  <li>concatenate image and caption embeddings해서 cross-attention 통해 dialog encoder에게 전달</li>
  <li>그러면 ITM loss로 학습된 dialog encoder가 discriminate whether the answer is true or false for a question</li>
</ul>

<h3 id="56-zero-shot-transfer-to-video-language-tasks">5.6. Zero-shot Transfer to Video-Language Tasks</h3>

<ul>
  <li>Strong generalization abilityto video-language tasks
    <ul>
      <li>zero-shot transfer to text-to-video retrieval and video question answering</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/llm/BLIP/table10.png" alt="그림1" /></p>

<p><img src="/assets/img/llm/BLIP/table11.png" alt="그림1" /></p>

<h2 id="6-additional-ablation-study">6. Additional Ablation Study</h2>

<p><img src="/assets/img/llm/BLIP/table12.png" alt="그림1" /></p>

<ul>
  <li>Improvement with CapFilt is not due to longer training</li>
</ul>

<p><img src="/assets/img/llm/BLIP/table13.png" alt="그림1" /></p>

<ul>
  <li>A new model should be trained on the bootstrapped dataset</li>
</ul>

<h2 id="7-conclusion">7. Conclusion</h2>

<ul>
  <li>BLIP pre-trains a multimodal mixture of encoder-decoder model
    <ul>
      <li>using a dataset bootstrapped from large-scale noisy image-text pairs</li>
      <li>by injecting diversesynthetic captions and removing noisy captions</li>
    </ul>
  </li>
  <li>Potential directions that can further enhancethe performance of BLIP:
    <ul>
      <li>Multiple rounds of dataset bootstrapping</li>
      <li>Generate multiple synthetic captions per image to further enlarge the pre-training corpus</li>
      <li>Model ensemble by training multiple different captioners and filter and combining their forces in CapFilt</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="llm" /><summary type="html"><![CDATA[[Arxiv 2024]()]]></summary></entry><entry><title type="html">Byte Latent Transformer: Patches Scale Better Than Tokens (Arxiv 2024)</title><link href="http://localhost:4000/llm/2025-02-22-BLT/" rel="alternate" type="text/html" title="Byte Latent Transformer: Patches Scale Better Than Tokens (Arxiv 2024)" /><published>2025-02-22T00:00:00+09:00</published><updated>2025-02-24T02:08:15+09:00</updated><id>http://localhost:4000/llm/BLT</id><content type="html" xml:base="http://localhost:4000/llm/2025-02-22-BLT/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Byte Latent Transformer (BLT)
    <ul>
      <li><strong>new byte-level LLM architecture</strong></li>
      <li>tokenizer-free</li>
    </ul>
  </li>
  <li>BLT encodes bytes into <strong>dynamically sized patches</strong> (primary units of computation)</li>
  <li>Patches are segmented based on the <strong>entropy of the next byte</strong>
    <ul>
      <li>allocating more compute and model capacity where increased data complexity demands it</li>
    </ul>
  </li>
  <li>Feasibility of scaling models trained on raw bytes without a fixed vocabulary
    <ul>
      <li>By dynamically selecting long patches when data is predictable</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>기존 LLM : end-to-end train… but <strong>except for tokenization !</strong>
    <ul>
      <li>groups bytes into a <strong>static</strong> set of tokens (heuristic)</li>
      <li>\(\to\) bias how a string is compressed</li>
      <li>\(\to\) shortcomings such as
        <ul>
          <li>domain/modality sensitivity</li>
          <li>sensitivity to input noise</li>
          <li>a lack of orthographic knowledge</li>
          <li>and multilingual inequity</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>LLMs에서 long sequence 다루려면 Tokenization은 필수
    <ul>
      <li>training on bytes : costly at scale</li>
      <li>그래서 self-attention or attention-free architectures
        <ul>
          <li>But small models일때나 말이 됨</li>
          <li>왜냐면 Transformer로 효율적으로 하려고 해봐야 어차피  Scale 관점에서 <strong>large FFN on every byte</strong>가 문제</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>본 논문에서 dynamic, learnable method for <strong>grouping bytes into patches</strong> 제안
    <ul>
      <li>mixes byte and patch information</li>
      <li>tokenizaiton과 다르게 no fixed vocabulary for patches</li>
      <li>Arbitrary groups of bytes are mapped to latent patch representations !</li>
    </ul>
  </li>
  <li>기존 Tokenization-based LLMs allocate the <strong>same</strong> amount of compute to <strong>every</strong> token.
    <ul>
      <li>\(\to\) trades efficiency for performance 불가피</li>
      <li>왜냐면 예측의 complexity랑 token이랑 not always correlated</li>
    </ul>
  </li>
  <li>그러니 compute를 필요한 곳에 할당하자
    <ul>
      <li>ending of most words 예측은 쉬워서 large transformer 필요 X (low-entropy)</li>
    </ul>
  </li>
  <li>이 생각을 반영한 BLT:
    <ul>
      <li>3 transformer blocks: <strong>two small byte-level local</strong> models and <strong>a large global latent transformer</strong></li>
      <li>how to dynamically allocate compute = how to group bytes into patches</li>
      <li>BLT segments data based on the <strong>entropy of the next-byte prediction</strong></li>
    </ul>
  </li>
</ul>

<h2 id="2-patching-from-individual-bytes-to-groups-of-bytes">2. Patching: From Individual Bytes to Groups of Bytes</h2>

<p><img src="/assets/img/llm/BLT/fig3.png" alt="그림1" /></p>

<ul>
  <li>Patching : segmenting the sequence of byte \(\boldsymbol{x}=\left\{x_i, \mid i=1, \ldots n\right\}\) into \(p=\left\{p_j \mid j=1, \ldots, m\right\}, m&lt;n\)
    <ul>
      <li>by mapping each \(x_i\) to the set \(\{0,1\}\) where 1 indicates the start of a new patch</li>
      <li>the computational cost = the number of main Transformer execution
        <ul>
          <li>BLT에서는 = the number of patches !</li>
          <li>따라서 average size of a patch, or simply patch size 매우 중요함</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>그러므로 여기서는 patching functions을 다룸
    <ul>
      <li>patching with a fixed number of bytes per patch</li>
      <li>whitespace patching</li>
      <li>dynamically patching with entropies from a small byte LM</li>
    </ul>
  </li>
</ul>

<h3 id="21-strided-patching-every-k-bytes">2.1. Strided Patching Every K Bytes</h3>

<ul>
  <li>일반적으로는 bytes into patches of fixed size \(k\) as done in MegaByte
    <ul>
      <li>changing the average patch size 쉽고 control the FLOP cost 쉬움</li>
      <li>But ! compute is not dynamically allocated
        <ul>
          <li>공백(whitespace) 예측하려고 Transformer 쓴다거나</li>
          <li>byte가 많은 math에 compute를 안쓴다거나…</li>
        </ul>
      </li>
      <li>그러다보니 inconsistent and non-contextual patching of similar byte sequences (같은 걸 다르게 split)</li>
    </ul>
  </li>
</ul>

<h3 id="22-space-patching">2.2. Space Patching</h3>

<ul>
  <li>creates new patches after any space-like bytes (= 공백 이후에 patch 시작)</li>
  <li>natural boundaries for linguistic units in many languages (=언어의 관점에서 자연스러운 접근)</li>
  <li>words are patched in the same way across sequences
    <ul>
      <li>FLOPs are allocated for hard predictions which often follow spaces (공백 바로 뒤)</li>
      <li>ex. “Who composed the Magic Flute?”에 답하기 위해서 바로 맞추라고 하면 어렵지만 M을 주면 Mozart 맞추기 쉬워짐 !</li>
    </ul>
  </li>
  <li>But ! cannot gracefully handle all languages and domains
    <ul>
      <li>무엇보다도 cannot vary thepatch size</li>
    </ul>
  </li>
</ul>

<h3 id="23-entropy-patching-using-next-byte-entropies-from-a-small-byte-lm">2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM</h3>

<ul>
  <li>2.2 같은 rule-based heuristic such as whitespace 말고 data-driven approach 원함
    <ul>
      <li>identify high uncertainty next-byte predictions, 즉 entropy patching</li>
    </ul>
  </li>
  <li>먼저 a small byte-level auto-regressive language model 하나 학습시키고
    <ul>
      <li>그 다음 compute next byte entropies under the LM distribution \(p_e\)  over the byte vocabulary \(\mathcal{V} :\)</li>
      <li>\(H\left(x_i\right)=\sum_{v \in \mathcal{V}} p_e\left(x_i=v \mid x_{&lt;i}\right) \log p_e\left(x_i=v \mid \boldsymbol{x}_{&lt;i}\right)\).</li>
      <li>Entropy가 주어졌을 때 patch boundaries는 어떻게 찾냐
        <ul>
          <li>첫째, entropy가 그냥 threshold보다 높은 지점</li>
          <li>둘째, 갑자기 entropy가 커진 지점</li>
          <li>i.e., \(\begin{aligned}\text{Global Constraint} \quad H\left(x_t\right)&gt;\theta_g \\ \text{Approx. Monotonic Constraint} \quad H\left(x_t\right)-H\left(x_{t-1}\right)&gt;\theta_r\end{aligned}\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="24-the-byte-pair-encoding-bpe-tokenizer-and-incremental-patching">2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching</h3>

<ul>
  <li>대부분의 LLMs (Llama 3 포함) use a subword tokenizer like BPE</li>
  <li>본 논문에서는
    <ul>
      <li>“<strong>tokens</strong>”: byte-groups drawn from a finite vocabulary determined prior totraining</li>
      <li>“<strong>patches</strong>”: dynamically grouped sequences without a fixed vocabulary</li>
      <li>둘의 차이는 with tokens, the model has no direct access to theunderlying byte features</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/llm/BLT/fig4.png" alt="그림1" /></p>

<ul>
  <li>BLT는 the trade off between the vocabulary <strong>size</strong> and <strong>compute</strong>를 re-define함
    <ul>
      <li>원래는 vocabulary size 커짐 = larger tokens = larger final projection layer</li>
      <li>Llama 3도 Llama 2에 비해 average token size를 3.7에서 4.4 bytes로 늘리면서 embedding table size는 4배 됨</li>
    </ul>
  </li>
  <li>BLT는 결국 byte sequence 내에서 지금이 patch boundary인가 아닌지 판단하면 되고
    <ul>
      <li>즉 = Latent Transformer로 more compute 할지말지 판단</li>
      <li>이건 아직 생성 안된 rest of sequence랑 독립</li>
      <li>즉 incremental patching \(f_p\left(x_{&lt;i}\right)=f_p(\boldsymbol{x})_{&lt;i}\)이 아님</li>
      <li>즉 같은 prefix라도 다르게 tokenize 가능</li>
    </ul>
  </li>
</ul>

<h2 id="3-blt-architecture">3. BLT Architecture</h2>

<p><img src="/assets/img/llm/BLT/fig2.png" alt="그림1" /></p>

<ul>
  <li>BLT의 구성은:
    <ul>
      <li><strong>A large</strong> global autoregressive language model
        <ul>
          <li>that operates on patch representations,</li>
        </ul>
      </li>
      <li><strong>Two</strong> smaller local models
        <ul>
          <li>encode sequences of bytes into patches</li>
          <li>decode patch representations back into bytes</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="31-latent-global-transformer-model">3.1 Latent Global Transformer Model</h3>

<ul>
  <li>The Latent Global Transformer is an autoregressive transformer model \(\mathcal{G}\) with \(l_{\mathcal{G}}\) layers
    <ul>
      <li>sequence of latent input patch representations \(p_j\) \(\to\) sequence of output patch representations \(o_j\)로 변환</li>
    </ul>
  </li>
  <li>block-causal attention mask 사용
    <ul>
      <li>현재 문서 내 현재 배치까지 범위에서만 attention</li>
      <li>pre-training, inference에서 FLOP 대부분이라 언제 실행할지가 complexity 결정</li>
    </ul>
  </li>
</ul>

<h3 id="32-local-encoder">3.2. Local Encoder</h3>

<p><img src="/assets/img/llm/BLT/fig5.png" alt="그림1" /></p>

<ul>
  <li>The Local Encoder Model, denoted by \(\mathcal{E}\)
    <ul>
      <li>lightweight transformer-based model with layers \(l_{\mathcal E} &lt;&lt; l_{\mathcal G}\)</li>
      <li>input bytes \(b_i\) \(\to\) patch representation \(p_j\)</li>
      <li>Transformer랑 다른 건 cross-attention layer after each transformer layer (pool byte representations into patch representations 하는 부분) - 3.2.2에서 이해됨</li>
      <li>input sequence of bytes, \(b_i\), are embedded using a \(\mathbb{R}^{256 \times h_{\mathcal E}}\) matrix, denoted as \(x_i\)</li>
      <li>hash-embeddings는 optional</li>
    </ul>
  </li>
  <li>즉 alternating transformer and cross-attention layers가 input bytes \(b_i\)의 representation을 \(\to\) patch representation \(p_j\)</li>
  <li>local block causal attention mask; 사용해서 each byte attends to a fixed window of \(\mathcal w_{\mathcal{E}}\)
    <ul>
      <li>다른 patch 볼 수 있지만 현재 문서 내에서만 !</li>
    </ul>
  </li>
</ul>

<h3 id="321-encoder-hash-n-gram-embeddings">3.2.1. Encoder Hash n-gram Embeddings</h3>

<ul>
  <li>결국 to incorporate information about the preceding bytes가 중요</li>
  <li>byte \(b_i\)를 as individual로, 그리고 as part of a byte n-gram으로 둘다 봄
    <ul>
      <li>Byte-gram: \(g_{i, n}=\left\{b_{i-n+1}, \ldots, b_i\right\}\) (with a fixed size for each size \(n \in\{3,4,5,6,7,8\}\))</li>
      <li>그 다음 individual embedding에 다 더함. i.e.,
        <ul>
          <li>\(e_i  =x_i+\sum_{n=3, \ldots, 8} E_n^{\text {hash }}\left(\operatorname{Hash}\left(g_{i, n}\right)\right)\).</li>
          <li>where \(\operatorname{Hash}\left(g_{i, n}\right)=\operatorname{RollPolyHash}\left(g_{i, n}\right) \% \mid E_n^{\text {hash }}\mid\)</li>
        </ul>
      </li>
      <li>그 다음에 normalizing !</li>
    </ul>
  </li>
</ul>

<h3 id="322-encoder-multi-headed-cross-attention">3.2.2 Encoder Multi-Headed Cross-Attention</h3>

<ul>
  <li>이제 진짜 bytes-sequence를 patch-sequence로 바꿔보자</li>
  <li>Perceiver의 입력 cross-attention과 비슷하지만 patch 크기가 동적 !</li>
  <li>Patch represenation은 관련된 byte만으로 학습됨
    <ul>
      <li>initialization은 byte representation에 pooling</li>
      <li>Cross-Attention을 통해 Patch represenation이 풍부한 맥락을 반영</li>
      <li>\(\begin{aligned} P_{0, j} &amp; =\mathcal{E}_C\left(f_{\text {bytes }}\left(\left(p_j\right)\right), f \text { is a pooling function }\right. \\ P_l &amp; =P_{l-1}+W_o\left(\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\right) \\ \text { where } Q_j &amp; =W_q\left(P_{l-1, j}\right), K_i=W_k\left(h_{l-1, i}\right), V_i=W_v\left(h_{l-1, i}\right) \\ h_l &amp; =\text { Encoder-Transformer-Layer }\left(h_{l-1}\right)\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h3 id="33-local-decoder">3.3. Local Decoder</h3>

<ul>
  <li>Local encoder와 비슷하게 lightweight transformer-based model with \(l_{\mathcal D} &lt;&lt; l_{\mathcal G}\)</li>
  <li>patch representation \(o_j\) \(\to\) raw bytes \(y_i\)</li>
  <li>이번에는 Cross-Attention으로 byte representation으로 변환
    <ul>
      <li>\(\begin{aligned}
&amp; D_0=h_{l_{\varepsilon}} \\
&amp; B_l=D_{l-1}+W_o\left(\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\right) \text {, } \\
&amp; \text { where } Q_i=W_q\left(d_{l-1, i}\right), K_i=W_k\left(\mathcal{D}_C\left(o_j\right)\right), V_i=W_v\left(\mathcal{D}_C\left(o_j\right)\right) \\
&amp; D_l=\text { Decoder-Transformer-layer }{ }_l\left(B_l\right)
\end{aligned}\).</li>
      <li>Encoder는 byte가 K/V, patch가 Q / Decoder는 patch가 K/V, byte가 Q</li>
    </ul>
  </li>
</ul>

<h2 id="4-experimental-setup">4. Experimental Setup</h2>

<h3 id="41-pre-training-datasets">4.1. Pre-training Datasets</h3>

<ul>
  <li>LLaMA 2 (Touvron et al., 2023)으로 BLT model scaling law experiments</li>
  <li>BLT-1Tdㅡ로 Llama3와 비교</li>
</ul>

<h3 id="42-entropy-model">4.2. Entropy Model</h3>

<ul>
  <li>BLT의 Entropy-Based Patching을 위한 language model
    <ul>
      <li>Byte-level auto-regressive transformer 사용</li>
      <li>trained on the same training distribution as the full BLT model</li>
    </ul>
  </li>
</ul>

<h3 id="43-entropy-threshold-and-equalizing-context-length">4.3. Entropy Threshold and Equalizing Context Length</h3>

<ul>
  <li>공정한 비교를 위해 patch size가 동적으로 변하더라도 same average context length i.e.,
    <ul>
      <li>the number of bytes ineach <strong>batch</strong> remains constant <strong>in expectation</strong></li>
      <li>모든 배치가 동일한 패치 개수를 가지도록 하여 large patch size로 인한 memory spikes 방지</li>
    </ul>
  </li>
</ul>

<h3 id="44-entropy-model-context">4.4. Entropy Model Context</h3>

<p><img src="/assets/img/llm/BLT/fig9.png" alt="그림1" /></p>

<ul>
  <li>entropy patching yields progressively <strong>larger patches</strong> in structured content like multiple choice tasks</li>
  <li>반복되다보니 entropy가 낮아지는 것 (lower entropy on the repeated content)</li>
  <li>따라서 <em>new lines</em>에서 entropy context 리셋하고 앞서 설명한 approximate monontonicity constraint 적용</li>
</ul>

<h3 id="45-flops-estimation">4.5. FLOPs Estimation</h3>

<ul>
  <li>주로 FLOPs가 발생하는 곳 3:
    <ul>
      <li>FFN, Self-attention, Output projection !</li>
      <li>\(\begin{aligned} \mathrm{FL}_{\mathrm{BLT}} &amp; =\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{G}}, l_{\mathcal{G}}, m=n_{c t x} / n_p, V=0\right) / n_p \\ &amp; +\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{E}}, l_{\mathcal{E}}, m=w_{\mathcal{E}}, V=0\right) \\ &amp; +\operatorname{Transf.} \mathrm{FL}\left(h_{\mathcal{D}}, l_{\mathcal{D}}, m=w_{\mathcal{D}}, V=256\right) \\ &amp; +\operatorname{Cross} \operatorname{Attn} . \mathrm{FL}\left(h_{\mathcal{E}}, l_{\mathcal{E}}, m=n_p, r=n_p / k\right) \times k / n_p \\ &amp; + \text { Cross Attn. FL }\left(h_{\mathcal{D}}, l_{\mathcal{D}}, m=k, r=k / n_p\right)\end{aligned}\).</li>
    </ul>
  </li>
</ul>

<h3 id="46-bits-per-byte-estimation">4.6. Bits-Per-Byte Estimation</h3>

<ul>
  <li>Perplexity per <strong>token</strong>는 비교가 어렵기 때문에, <strong>Byte</strong>-Level에서 평가하는 Bits-Per-Byte(<strong>BPB</strong>) 사용
    <ul>
      <li>\(\operatorname{BPB}(x)=\frac{\mathcal{L}_{C E}(\boldsymbol{x})}{\ln (2) \cdot n_{\text {bytes }}}\).</li>
    </ul>
  </li>
</ul>

<h3 id="47-transformer-architecture-hyperparameters">4.7. Transformer Architecture Hyperparameters</h3>

<ul>
  <li>Llama 3와 동일한 hyperparameters setting
    <ul>
      <li>Activation: SwiGLU</li>
      <li>Positional embedding: RoPE</li>
      <li>Normalization: RMSNorm</li>
      <li>Attention optimization: Flash Attention</li>
    </ul>
  </li>
</ul>

<h3 id="48-blt-specific-hyperparameters">4.8. BLT-Specific Hyperparameters</h3>

<ul>
  <li>N-gram Hash Embeddings : 3~8</li>
</ul>

<p>pass</p>

<h2 id="5-scaling-trends">5. Scaling Trends</h2>

<h3 id="51-parameter-matched-compute-optimal-scaling-trends">5.1. Parameter Matched Compute Optimal Scaling Trends</h3>

<ul>
  <li>compute-optimal scaling of BLT
    <ul>
      <li>compared in terms of training <strong>FLOPs</strong> and language modeling <strong>performance</strong></li>
    </ul>
  </li>
  <li>Larger patch sizes in BLT (6 or 8 bytes) <strong>reduce inference FLOPs by up to 50%</strong> while maintaining comparable performance to BPE models.</li>
</ul>

<h3 id="52-beyond-compute-optimal-task-evaluations">5.2. Beyond Compute Optimal Task Evaluations</h3>

<p><img src="/assets/img/llm/BLT/table1.png" alt="그림1" /></p>

<ul>
  <li>Evaluations include :<strong>common sense reasoning, world knowledge, and code generation</strong> tasks</li>
  <li>Llama 3 보다 뛰어난 이유는
    <ul>
      <li>Better use of compute through dynamic patching !</li>
      <li>Direct modeling of byte-level information rather than tokens</li>
      <li>Larger patch size로 inference FLOPs reduction !</li>
    </ul>
  </li>
</ul>

<h3 id="53-patches-scale-better-than-tokens">5.3. Patches Scale Better Than Tokens</h3>

<p><img src="/assets/img/llm/BLT/table2.png" alt="그림1" /></p>

<ul>
  <li>모델 크기와 패치 크기를 동시에 증가시키면서도 동일한 학습 및 추론 FLOP 예산을 유지</li>
</ul>

<h2 id="6-byte-modeling-improves-robustness">6. Byte Modeling Improves Robustness</h2>

<h3 id="61-character-level-tasks">6.1. Character-Level Tasks</h3>

<p><img src="/assets/img/llm/BLT/fig7.png" alt="그림1" /></p>

<ul>
  <li>Byte-level models are inherently more robust to <strong>input noise</strong> and better at handling <strong>character-level variations.</strong></li>
</ul>

<p><img src="/assets/img/llm/BLT/table3.png" alt="그림1" /></p>

<h3 id="62-training-blt-from-llama-3">6.2. Training BLT from Llama 3</h3>

<ul>
  <li>Pre-traind tokenizer-based model을 leverage해서 더 빠르게 수렴할 수 있는가</li>
  <li>BLT from Llama 3.1이 from Llama 3보다 좋음 !</li>
</ul>

<p><img src="/assets/img/llm/BLT/table5.png" alt="그림1" /></p>

<h2 id="7-ablations-and-discussion">7. Ablations and Discussion</h2>

<p><img src="/assets/img/llm/BLT/fig8.png" alt="그림1" /></p>

<p><img src="/assets/img/llm/BLT/table6.png" alt="그림1" /></p>

<p><img src="/assets/img/llm/BLT/table78.png" alt="그림1" /></p>

<p><img src="/assets/img/llm/BLT/table9.png" alt="그림1" /></p>

<h2 id="8-related-work">8. Related Work</h2>

<p>Pass</p>

<h2 id="9-limitations-and-futurework">9. Limitations and FutureWork</h2>

<ul>
  <li>Scaling laws were calculated for BPE-level transformers and may lead to suboptimal (data, parameter sizes) ratios in the case of BLT</li>
  <li>실험들이 &lt;1B parameters에서 진행. &gt;8B parameters에서는 확장 가능성만 제시했을 뿐</li>
  <li>Entropy model을 별도로 훈련해서 patching하지만 이것까지도 end-to-end 하고자 함</li>
</ul>

<h2 id="10-conclusion">10. Conclusion</h2>

<ul>
  <li>Fixed-vocabulary tokenization에 의존하지 않는 Byte Latent Transformer (BLT)
    <ul>
      <li>Byte를 learnable, dynamic하게 patching</li>
      <li>소폭의 성능 손실을 감수하는 대신, 추론 시 FLOP 연산량을 최대 50%까지 줄일 수 있음</li>
    </ul>
  </li>
  <li>기존 LLM은 fixed context에서 model size만 증가
    <ul>
      <li>BLT는 동일한 inference FLOP 내에서 model size와 patch size를 동시에 확장할 수 있음</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="llm" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2412.09871)]]></summary></entry><entry><title type="html">ComRes: Semi-Supervised Time Series Forecasting Utilizing Consensus Promotion of Multi-Resolution (ICLR 2025)</title><link href="http://localhost:4000/timeseries/2025-02-17-ComRes/" rel="alternate" type="text/html" title="ComRes: Semi-Supervised Time Series Forecasting Utilizing Consensus Promotion of Multi-Resolution (ICLR 2025)" /><published>2025-02-17T00:00:00+09:00</published><updated>2025-02-23T20:29:55+09:00</updated><id>http://localhost:4000/timeseries/ComRes</id><content type="html" xml:base="http://localhost:4000/timeseries/2025-02-17-ComRes/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>기존의 supervised learning approaches:
    <ul>
      <li>relyon cleaned and labeled data \(\to\) unseen characteristics 못잡음</li>
    </ul>
  </li>
  <li>본 논문에서는 self-supervised approach 제안
    <ul>
      <li>Multi-view setting on augmented data</li>
      <li>without requiring explicit future values as labels</li>
    </ul>
  </li>
  <li><strong>consensus promotion framework</strong>을 통해
    <ul>
      <li>여러 모델이 비슷하게 예측하도록 ! (agreement among multiple single-view models)</li>
    </ul>
  </li>
  <li>추가로 not only improves forecasting <strong>accuracy</strong> but also <strong>mitigates</strong> error accumulation inlong-horizon predictions</li>
  <li>impact of autoregressive and non-autoregressive decoding schemes on error propagation
    <ul>
      <li>\(\to\) extending prediction horizons에 robust</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Time series의 intricate temporal patterns 중에서 <strong>multi-scale</strong> dependencies !
    <ul>
      <li>Pathformer, Scaleformer, TimeMixer, Nhits 등 여러 시도 있었음</li>
      <li>하지만 different scales \(\to\) increase in parameters \(\to\) overfitting risk</li>
      <li>게다가 limited to labeled data</li>
    </ul>
  </li>
  <li>그렇다고 <strong>data augmentation</strong> 하면 underlying dynamics이 바뀌어 버려서
    <ul>
      <li>slight shifts in trends or periodic patterns \(\to\) prediction errors</li>
      <li>그러므로 overfitting risk 줄이고 generalization ability 늘리는 것 중요
        <ul>
          <li>(augmented data에 future values 붙이지 않은 채로 !)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>그러므로 본 논문에서는 enhancing <strong>consensus</strong> among single-view models using <strong>unseen</strong> data
    <ul>
      <li>기존의 co-training method와 비슷하게,</li>
      <li>maximizing <strong>agreement</strong> among multi-view models on unseen data !</li>
    </ul>
  </li>
  <li>추가적으로, <strong>error accumulation in long-horizon forecasting</strong> 이슈 다룸
    <ul>
      <li>기존의 former들은 non-autoregressive decoding scheme</li>
      <li>how non-autoregressive models can be employed to minimize error accumulation ?</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li><strong>Long-term Time Series Forecasting</strong>
    <ul>
      <li>Transformer-based:
        <ul>
          <li>FEDformer(2022), Autoformer(2021): address the quadratic complexity</li>
          <li>PatchTST(2023): patching to enhance local pattern recognition</li>
          <li>iTransformer(2024), Crossformer(2023): attention으로 multivariate correlations</li>
        </ul>
      </li>
      <li>non-Transformer-based 역시 예측 성능 뛰어난ㅁ
        <ul>
          <li>tiDE(2023), MICN(2023), DLinear(2023), TimeMixer(2024)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Multi-scale Modeling for Time Series</strong>
    <ul>
      <li>NHITS(2023): multi-rate data sampling and hierarchical interpolation</li>
      <li>Pyraformer(2022), Scaleformer(2023): pyramidal attention mechanisms</li>
      <li>TimeMixer(2024): decomposable mixing strategy</li>
      <li>Pathformer(2024): features from both different <strong>resolutions</strong> and <strong>temporal</strong> distances</li>
      <li>대부분 ensemble of multiple predictions that rely on labeled data…
        <ul>
          <li>본 논문에서는 semi-supervised learning으로
            <ul>
              <li>challenge of reconciling diverse representations 다룸</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Time Series Data Augmentation</strong>
        <ul>
          <li>classification and anomaly detection에서는
            <ul>
              <li>small perturbations typically do not change the data label</li>
            </ul>
          </li>
          <li>하지만 forecasting에서는 even small perturbations can lead to significant changes in the observations !
            <ul>
              <li>the improvement relies on the quality of the augmented data</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<ul>
  <li>\(L:\left(x_{t-L}, \ldots, x_t\right)\)보고 \(H \text { timestamps }\left(x_{t+1}, \ldots, x_{t+H}\right)\)예측</li>
  <li>Pathformer를 leverage unseen data하도록 확장 !</li>
</ul>

<p><img src="/assets/img/timeseries/ComRes/fig1.png" alt="그림1" /></p>

<ul>
  <li>Pathformer처럼
    <ul>
      <li>multi-scale patch division with various patch sizes</li>
      <li><strong>intra-patch</strong> attention within each divided patch and
        <ul>
          <li><strong>inter-patch</strong> attention across different patches</li>
          <li>그 다음 MLP</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Pathformer와 다르게
    <ul>
      <li><strong>equal weighted</strong> aggregator to combine these multi-scale characteristics</li>
    </ul>
  </li>
  <li>기존 supervised learning에서는 loss:
    <ul>
      <li>\(\mathcal{L}_s=\frac{1}{m+1}\left(\mid\widehat{x_{* t+1: t+H}}-x_{t+1: t+H}\mid^2+\sum_{i=1}^m \mid\widehat{x}_{i t+1: t+H}-x_{t+1: t+H}\mid^2\right)\).</li>
    </ul>
  </li>
</ul>

<h3 id="31-consensus-promotion-learning">3.1. Consensus Promotion Learning</h3>

<ul>
  <li>Augmented data는 potentially noisy \(\to\)  future values 정하는 건 challenging</li>
  <li>그래서 <strong>consistency loss</strong>: encourages mutual agreement amongthe models
    <ul>
      <li>\(\mathcal{L}_u=\frac{1}{m}\left(\sum_{i=1}^m\mid\widehat{x}_{i t+1: t+H}-\widehat{x_{* t+1}}{ }^2 t+H\mid^2\right)\):
        <ul>
          <li>individual-view models to align their predictions with the comprehensive prediction</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>최종적인 unsupervised consistency loss는 \(\mathcal{L}=\mathcal{L}_s+w_u \mathcal{L}_u\)
    <ul>
      <li>\(w_u\): represents the weight ot the unsupervised consistency loss</li>
      <li>\(\mathcal L_s\): minimizes the prediction error with respect to the ground truth</li>
      <li>\(\mathcal L_u\): aligns the individual model predictions</li>
    </ul>
  </li>
</ul>

<h3 id="32-unseen-data-generation">3.2. Unseen Data Generation</h3>

<ul>
  <li>Consensus promotion는 leveraging information from unseen data으로 model generalization을 향상시키는 것이 목표</li>
  <li>그러면 Data augmentation은? explores new areas of the input space이 목표
    <ul>
      <li><strong>time warping, interpolation,and noise injection</strong>으로 하겠다 !</li>
      <li>Interpolation : estimating values between known data points to produce a smoother and more continuous time series.</li>
      <li>Time Warping : DTW처럼 selects a random time range and then compresses(down-samples)</li>
      <li>Noise Injection : selects a random time range and then compresses(down-samples)</li>
    </ul>
  </li>
</ul>

<h3 id="33-extending-prediction-horizons-beyond-training-ranges">3.3. Extending Prediction Horizons Beyond Training Ranges</h3>

<ul>
  <li>현실적으로 autoregressive forecasting is essential !</li>
  <li>하지만 Teacher forcing처럼 autoregressively training 없으면 error accumulation 발생</li>
</ul>

<p><img src="/assets/img/timeseries/ComRes/fig2.png" alt="그림1" /></p>

<ul>
  <li>
    <p>consensus promotion through two approaches:</p>

    <ul>
      <li>
        <p><strong>Block-wise autoregressive</strong>: extends the prediction horizon by utilizing the entire prediction range at once</p>

        <p><strong>Fine-grained autoregressive</strong>: fine-grained autoregressive prediction involves a step-by-step process</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<h3 id="41-time-series-forecasting">4.1. Time Series Forecasting</h3>

<ul>
  <li><strong>MRes (SL)</strong>: MRes trained exclusively on labeled datasets using supervised learning (SL).</li>
  <li><strong>MRes w. augmentation</strong>: MRes trained on pseudo-labeled datasets augmented with Time Warping.</li>
  <li><strong>MRes w. consensus</strong>: MRes trained on labeled datasets with consensus promotion but withoutdata augmentation. Predictions on labeled data are learned not only from the ground truthbut also from the comprehensive prediction.</li>
</ul>

<p><img src="/assets/img/timeseries/ComRes/table1.png" alt="그림1" /></p>

<ul>
  <li>그리고 일관되게 base-line model 보다 성능이 좋음 (아래 그림)</li>
</ul>

<p><img src="/assets/img/timeseries/ComRes/fig3.png" alt="그림1" /></p>

<h3 id="42-error-accumulation-in-auto-regressive-forecasting">4.2. Error Accumulation in Auto-regressive Forecasting</h3>

<p><img src="/assets/img/timeseries/ComRes/table2.png" alt="그림1" /></p>

<ul>
  <li>일단 table1보다 MSE가 커진건 inherent in autoregressive forecasting</li>
  <li>첫째로 CoMRes가 MRes(SL)보다 일관되게 좋음</li>
  <li>둘째로 block-wise autoregressive가 fine-grained autoregressive보다 좋음</li>
</ul>

<p><img src="/assets/img/timeseries/ComRes/fig4.png" alt="그림1" /></p>

<ul>
  <li>미래 96시점 예측하게 학습하고 forecast longer horizons 해보았음</li>
  <li>일단 CoMRes(초록, 빨강)가 MRes(오렌지)보다 stronger capacity to reduce error accumulation</li>
  <li>그리고 미래 720시점 예측하게 학습한 파랑, 초록과 비교해보았음</li>
</ul>

<h3 id="43-ablation-study">4.3. Ablation Study</h3>

<p><img src="/assets/img/timeseries/ComRes/table3.png" alt="그림1" /></p>

<ul>
  <li><strong>Limited-Resource Scenarios</strong> (table3)
    <ul>
      <li>reducing the amount of <strong>labeled</strong> training data 상태로 비교</li>
      <li>train-test 사이에서 temporal variations이 다르면 성능 안좋아짐</li>
      <li>ETTh2 and ETTm2는 labeled data 적어도 성능 좋음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/ComRes/table4.png" alt="그림1" /></p>

<ul>
  <li><strong>Combining Multiple Unseen Data Generation Techniques</strong> (table4)
    <ul>
      <li>Combining all 3 augmentation techniques (Time Warp, Interpolation, and Noise Injection)가 항상 성능 개선으로 이어지는 건 아님
        <ul>
          <li>ETTh1 (높은 변동성): 성능 향상이 거의 X
            <ul>
              <li>data의 변동성이 높을 경우, 추가적인 데이터 변형이 예측 성능에 미치는 영향이 크지 않음</li>
            </ul>
          </li>
          <li>ETTm2 (명확한 주기성):  &lt;720 장기 예측에서는 급격한 성능 저하 발생
            <ul>
              <li>장기적인 추세 변화가 빠른 데이터에서는 augmentation이 부정적인 영향</li>
            </ul>
          </li>
          <li>Weather 데이터셋 (변수 간 강한 상관관계): 성능이 향상
            <ul>
              <li>상관성이 높은 데이터에서는 다양한 증강 기법을 적용하는 것이 모델 학습에 도움</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Multi-view learning strategy for long-term time series forecasting
    <ul>
      <li>superior prediction accuracy and robustness</li>
    </ul>
  </li>
  <li>Augmented unseen data to ensure that models
    <ul>
      <li>not only capturemulti-resolution patterns</li>
      <li>but also provide consistent predictions</li>
    </ul>
  </li>
  <li>(especially in autoregressive setups, where it mitigates the issue of error accumulation.)</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICML 2025](https://openreview.net/pdf?id=bRa4JLPzii)]]></summary></entry><entry><title type="html">SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2025-01-13-SST/" rel="alternate" type="text/html" title="SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting (Arxiv 2024)" /><published>2025-01-13T00:00:00+09:00</published><updated>2025-01-13T16:49:49+09:00</updated><id>http://localhost:4000/mamba/SST</id><content type="html" xml:base="http://localhost:4000/mamba/2025-01-13-SST/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Time series forecasting에서 <strong>heterogeneity</strong> between long-range and short-range time series를 간과하면 안됨
    <ul>
      <li>distinct objectives tailored to different ranges 필요</li>
      <li>(time series can be decomposed into global patterns and local variations)</li>
    </ul>
  </li>
  <li>본 논문에서 multi-scale hybrid Mamba-Transformer experts model STATE SPACE TRANSFORMER(SST) 제안
    <ul>
      <li><strong>Mamba</strong> as an expert to extract <strong>global patterns</strong> in <strong>coarse</strong>-grained long-range time series
        <ul>
          <li>selectively retain long-term patterns and filter out fluctuation</li>
        </ul>
      </li>
      <li><strong>Local Window Transformer (LWT)</strong>, the other expert to focus on capturing <strong>local</strong> variations in <strong>fine</strong>-grained short-range time series
        <ul>
          <li>locality-awareness capability</li>
        </ul>
      </li>
      <li>각각을 합치는 건 long-short router
        <ul>
          <li>dynamically adjusts contributions of the two experts</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/SST/fig1.png" alt="그림1" /></p>

<h2 id="1-introduciton">1. Introduciton</h2>

<ul>
  <li>the lack of distinction btw <strong>long</strong>-range and <strong>short</strong>-range time series
    <ul>
      <li>때문에 예측 성능이 방해</li>
      <li><strong>long</strong>-range time series는 global patterns이 중요
        <ul>
          <li>local deviations는 negatively impact forecasting accuracy</li>
        </ul>
      </li>
      <li>반대로 short-range는 local variations가 중요
        <ul>
          <li>global patterns는 less evident within limited time frames</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>long</strong>-range vs. <strong>short</strong>-range 구분 자체가 애매모호함 (intertwine)
    <ul>
      <li>New metric to measure the difference 필요</li>
      <li>둘을 integrating하는 것이 non-trivial task.
        <ul>
          <li>relative importance between patterns and variations 잘해야 함</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이를 위해 <strong>multi-resolution</strong> framework 필요
    <ul>
      <li><strong>larger patches</strong> and <strong>longer stride</strong> for <strong>long</strong>-range to obtain <strong>low-resolution</strong></li>
      <li><strong>smaller patches</strong> and <strong>shorter stride</strong> for <strong>short</strong> range lead to <strong>high-resolution</strong></li>
      <li>지금까지의 Mixture-of-Experts (MoEs)는 experts의 역할이 애매했음 !</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-preliminaries">3. Preliminaries</h2>

<ul>
  <li>Look-back window \(\mathcal{L}=\left(\mathrm{x}_1, \mathrm{x}_2, . ., \mathrm{x}_L\right) \in \mathbb{R}^{L \times M}\)
    <ul>
      <li>Long-range time series \(\mathcal{L}\) denotes the full range of look-back window \(\mathcal{L}[:]\)</li>
      <li>shortrange time series \(\mathcal{S} \in \mathbb{R}^{S \times M}\) denotes the partial latest range \(\mathcal{L}[-S:], S&lt;L\).</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/SST/fig3.png" alt="그림1" /></p>

<ul>
  <li>Mamba : retain repeated upand-down patterns in long range</li>
  <li>Transformer : the missing local inductive bias hinders its ability to further capture local variations</li>
</ul>

<h2 id="4-methodology">4. Methodology</h2>

<p><img src="/assets/img/Mamba/SST/fig4.png" alt="그림1" /></p>

<ul>
  <li>4 modules: <strong>multi-scale patcher</strong>, <strong>global patterns expert</strong>, <strong>local variations expert</strong>, and long-short router
    <ul>
      <li><strong>Multi-scale patcher</strong>: <strong>transforms input time series (TS) into different resolutions</strong> according to ranges</li>
      <li><strong>Global pattern expert</strong>: is dedicated to finding <strong>long</strong>-term patterns in a <strong>low</strong>-resolution TS</li>
      <li><strong>Local variations expert</strong>: aims to capture <strong>short</strong>-term variations in a <strong>high</strong>-resolutionTS</li>
      <li><strong>Long-short router</strong>: dynamically learns <strong>contributions</strong> of the two experts.</li>
    </ul>
  </li>
</ul>

<h3 id="multi-scale-resolutions">Multi-Scale Resolutions</h3>

<p><img src="/assets/img/Mamba/SST/fig2.png" alt="그림1" /></p>

<ul>
  <li><strong>Global patterns</strong> emerge when viewed at a <strong>broader</strong>-scale granularity</li>
  <li><strong>Local variations</strong> become clearer when examined at a <strong>finer</strong>-scale granularity</li>
  <li>\(\to\) <strong>low</strong> resolution for <strong>long</strong>-rangeTS and <strong>high</strong> resolution for short-<strong>range</strong> TS</li>
</ul>

<h3 id="resolution">Resolution</h3>

<ul>
  <li>\(R_{P T S}=N \sqrt{P}=\left(\left\lfloor\frac{L-P}{S t r}\right\rfloor+1\right) \sqrt{P} \approx \frac{\sqrt{P}}{S t r}\).</li>
</ul>

<h3 id="hybrid-mamba-transformer-experts">Hybrid Mamba-Transformer Experts</h3>

<ul>
  <li>Inspired by Mixture-of-Experts (MoEs)</li>
  <li>기존 MoEs와 다르게, assign global patterns and local variations roles to two experts.
    <ul>
      <li>extract long-term patterns and filter out small variations in long-range TS</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/SST/fig8.png" alt="그림1" /></p>

<ul>
  <li><strong>Global Patterns Expert</strong>
    <ul>
      <li>먼저 encodes long-range PTS \(\mathbf{x}_{p L}^{(i)} \in \mathbb{R}^{N_L \times P_L}\) into high-dimension space \(\mathbf{x}_L^{(i)} \in \mathbb{R}^{N_L \times D}\) in the encoding layer</li>
      <li>다음으로 Mamba block이 select input-dependent patterns and filter out irrelevant variations</li>
    </ul>
  </li>
  <li><strong>Local Variations Expert</strong>
    <ul>
      <li>Transformer는 lack of locality inductive bias and quadratic complexity</li>
      <li>따라서 local window Transformer(LWT)로 사용</li>
    </ul>
  </li>
  <li><strong>Mixture of Experts</strong>
    <ul>
      <li>Long-Short Router
        <ul>
          <li>learning the <strong>relative contributions</strong> of the two specialized experts</li>
        </ul>
      </li>
      <li>Formally, the router projects input TS \(\mathcal{L} \in R^{L×M}\) into the D-dim</li>
      <li>그 다음 transformed by a flatten layer into a vector \(z_R\)</li>
      <li>그 다음 A linear layer (with a softmax) to output two values \(p_T, p_S \in(0.1)\)</li>
    </ul>
  </li>
  <li><strong>Forecasting Module</strong>
    <ul>
      <li>long range embedding \(\mathbf{z}_L^{(i)}\)과short-range embedding \(\mathbf{z}_S^{(i)}\)를 flatten
        <ul>
          <li>into a single-row vector</li>
          <li>and concatenates them with respective weights $p_L$ and $p_S$.</li>
        </ul>
      </li>
      <li>Long-short range fusion representation : \(\mathbf{z}_{L S}^i \in \mathbb{R}^{\left(N_S+N_L\right) L D}\)</li>
      <li>마지막으로 이걸 linear head에 넣어서 \(\hat{\mathbf{x}}^{(i)}=\left\{\hat{x}_{L+1}, \hat{x}_{L+2}, \ldots, \hat{x}_{L+F}\right\} \in \mathbb{R}^{F \times 1}\) (for individual variate \(i\))를 얻음</li>
    </ul>
  </li>
  <li><strong>Linear Complexity Analysis</strong>
    <ul>
      <li>the total complexity of SST is \(O\left(\frac{L}{N_L}+\frac{w S}{N_S}\right)\)</li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<ul>
  <li>Time Series Forecasting Results</li>
</ul>

<p><img src="/assets/img/Mamba/SST/table1.png" alt="그림1" /></p>

<ul>
  <li>Ablation Studies</li>
</ul>

<p><img src="/assets/img/Mamba/SST/fig6.png" alt="그림1" /></p>

<h2 id="6-conclusion-and-future-work">6. Conclusion and Future Work</h2>

<ul>
  <li>Global patterns should be extracted from longrange,
    <ul>
      <li>while local variations are more effectively captured inshort range.</li>
    </ul>
  </li>
  <li>A multi-scale hybrid Mamba-Transformer framework SST를 제안
    <ul>
      <li><strong>Mamba</strong>, serving as a <strong>global</strong> patterns expert, focuses on extracting <strong>long</strong>-term patterns in <strong>low</strong> resolution</li>
      <li><strong>LWT</strong>, as a <strong>local</strong> variations expert, addresses subtle nuances in <strong>short</strong>-range time series in <strong>high</strong> resolution</li>
    </ul>
  </li>
  <li>with scaling linearly \(O(L)\) with time series length \(L\)</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2404.14757)]]></summary></entry><entry><title type="html">MAT: Integration of Mamba and Transformer for Long-Short Range TSF (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2025-01-08-MAT/" rel="alternate" type="text/html" title="MAT: Integration of Mamba and Transformer for Long-Short Range TSF (Arxiv 2024)" /><published>2025-01-08T00:00:00+09:00</published><updated>2025-01-08T22:03:06+09:00</updated><id>http://localhost:4000/mamba/MAT</id><content type="html" xml:base="http://localhost:4000/mamba/2025-01-08-MAT/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Time series는 sparse semantic features를 가짐
    <ul>
      <li>Transformer만으로 다루기에는 어려움이 있고</li>
      <li>Mamba의 selective input and parallel computing 필요</li>
    </ul>
  </li>
  <li>Mamba and Transformer models의 장단점을 이해하고 combined approach 제안</li>
  <li>the long-range dependency capabilities of <strong>Mamba</strong>
    <ul>
      <li>and the short-range characteristics of <strong>Transformers</strong></li>
      <li>called <strong>MAT</strong></li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>Linear</strong> scalability를 가지는 모델들은 성능이 아쉬움
    <ul>
      <li>SciNet
        <ul>
          <li>relying solely on time points</li>
          <li>obscure context-based choices and overlook long-range dependencies</li>
          <li>그러므로 iTransformer처럼 entire window로 다루는 것이 맞음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Transformer</strong>-based models는 high computational complexity
    <ul>
      <li>Informer, FEDformer,and Autoformer
        <ul>
          <li>mixed-channel approach (2d matrix defined by the <strong>number of channels and the length of histories</strong>)</li>
          <li>beneficial when channels exhibit significant correlations,</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SSM
    <ul>
      <li>handle very long sequences with linear complexity</li>
      <li>context-aware selectivity through hidden attention mechanisms</li>
    </ul>
  </li>
  <li>본 논문에서는
    <ul>
      <li>Mamba coupled with attentionmechanisms called Transformer
        <ul>
          <li>(with multi-scale contexts)</li>
        </ul>
      </li>
      <li><strong>long</strong>-term forecasting capabilities and <strong>short</strong>-range dependencies in MTS data
        <ul>
          <li>while maintaining <strong>linear</strong> scalability and minimal memory usage</li>
        </ul>
      </li>
      <li>bottom-up strategy
        <ul>
          <li>generating contextual cues at two distinct scales through linear mapping</li>
          <li>At each of these levels four MAT modules</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<p>pass</p>

<h2 id="3-methodology">3. Methodology</h2>

<ul>
  <li>input sequence \(\mathbf{x}=\left[x_1, \ldots, x_L\right]\) 보고 future values \(\left[x_{L+1}, \ldots, x_{L+T}\right]\) 예측</li>
</ul>

<h3 id="ac-preliminary-transformer-mamba">A~C. Preliminary, Transformer, Mamba</h3>

<p>pass</p>

<h3 id="d-mat">D. MAT</h3>
<p><img src="/assets/img/Mamba/MAT/fig3.png" alt="그림1" /></p>

<ul>
  <li>Four combined Mambas and Transformer
    <ul>
      <li>to extract <strong>long-short</strong> range contextual information
        <ul>
          <li><strong>long</strong>-term forecasting capability of <strong>Mamba</strong></li>
          <li>and <strong>short</strong> range dependency of <strong>Transformer</strong>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Normalizing</strong>(RevIN)</li>
  <li><strong>Embedding</strong> : \(\mathbf{x}^{(1)}=E M B_1\left(\mathbf{x}^{(0)}\right), \mathbf{x}^{(2)}=E M B_2\left(\mathbf{x}^{(1)}\right)\).
    <ul>
      <li>\(E M B_1: \mathbb{R}^{M \times L} \rightarrow \mathbb{R}^{M \times n_1}\) and \(E M B_2: \mathbb{R}^{M \times n 1} \rightarrow \mathbb{R}^{M \times n_2}\)</li>
      <li>\(n_1\) and \(n_2\), are chosen from the set \(\{512,256,128,64,32\}\) s.t. \(n_1&gt;n_2\).</li>
    </ul>
  </li>
  <li>사실 <a href="https://lpppj.github.io/mamba/2024-10-29-timemachine">TimeMachine</a>이랑 비슷한데
    <ul>
      <li><strong>not only</strong> the <strong>long</strong>-term prediction capability of the <strong>Mamba</strong>,</li>
      <li><strong>but also</strong> the <strong>short</strong>-range dependency learned from the <strong>Transformer</strong></li>
      <li>이 구조가 pivotal component in the modern LLM</li>
    </ul>
  </li>
  <li><strong>Output Prediction</strong> : \(\overline{\mathbf{x}}^{(1)}=\operatorname{Proj}_1\left(\overline{\mathbf{x}}^{\left(F_1\right)}\right), \overline{\mathbf{x}}=\operatorname{Proj}_2\left(\overline{\mathbf{x}}^{(1)} \oplus \hat{\mathbf{x}}^{\left(F_2\right)}\right)\)
    <ul>
      <li>\(\text{Proj}_1\):  \(\mathbb{R}^{M \times n_2} \rightarrow \mathbb{R}^{M \times n_1}\) to obtained \(\overline{\mathrm{x}}^1\),</li>
      <li>\(\mathrm{Proj}_2\):  \(\mathbb{R}^{M \times n_1} \rightarrow \mathbb{R}^{M \times T}\) to yield \(\overline{\mathbf{x}}\).</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>fixed \(L=96\) and \(T=\{96,192,336,720\}\)</li>
  <li>Default parameters for all Mambas were set as follows:
    <ul>
      <li>Dimension factor \(D=\) 256,</li>
      <li>local convolutional width =2,</li>
      <li>and state expand factor \(N=\) 1,</li>
    </ul>
  </li>
  <li>For the transformer module,
    <ul>
      <li>the multi head number is set \(H=\) 8,</li>
      <li>the Batch Size in the training process is set as Batch =32.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MAT/table1.png" alt="그림1" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Transformers struggle with long-term dependencies and sparse semantic features,
    <ul>
      <li>Mamba excels through selective input handling and parallel computing.</li>
    </ul>
  </li>
  <li>MAT, a combined approach leveraging Mamba’s long-range capabilities
    <ul>
      <li>and Transformers’ short-range strengths</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2409.08530)]]></summary></entry><entry><title type="html">DTMamba : Dual Twin Mamba for Time Series Forecasting (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2025-01-06-DTMamba/" rel="alternate" type="text/html" title="DTMamba : Dual Twin Mamba for Time Series Forecasting (Arxiv 2024)" /><published>2025-01-06T00:00:00+09:00</published><updated>2025-01-08T22:03:05+09:00</updated><id>http://localhost:4000/mamba/DTMamba</id><content type="html" xml:base="http://localhost:4000/mamba/2025-01-06-DTMamba/"><![CDATA[<h2 id="introduction">Introduction</h2>

<ul>
  <li>RNN-based methods
    <ul>
      <li>vanishing gradients</li>
      <li>cannot be parallelized effectively,</li>
      <li>leading tolower computational efficiency</li>
    </ul>
  </li>
  <li>TCN-based methods
    <ul>
      <li>limited modeling capabilities for long-term dependencies.</li>
    </ul>
  </li>
  <li>Transformers
    <ul>
      <li>quadratic complexity</li>
    </ul>
  </li>
  <li>Simple linear models
    <ul>
      <li>short in terms of performance.</li>
    </ul>
  </li>
  <li>Mamba
    <ul>
      <li>inear timeseries modeling approach using State Space Models
        <ul>
          <li>RNN’s sequential processing capability</li>
          <li>CNN’s global information processing capability</li>
        </ul>
      </li>
      <li>selection mechanism within the SSM framework
        <ul>
          <li>focus on essentialinformation while filtering out irrelevant details</li>
        </ul>
      </li>
      <li>incorporates a summary of all preceding information</li>
    </ul>
  </li>
  <li>Dual Twin Mamba(DTMamba)
    <ul>
      <li>RevIN - two TMamba blocks(Residual networks) - projection layer - reverse RevIN</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>Traditional methods
    <ul>
      <li>poor modeling performance</li>
    </ul>
  </li>
  <li>RNN-based models
    <ul>
      <li>vanishing gradients, limited parallelization capabilities,</li>
    </ul>
  </li>
  <li>Transformer-based methods
    <ul>
      <li>self-attention : suitable for modelinglong-term time series data</li>
      <li>quadratic complexity</li>
    </ul>
  </li>
  <li>Linear-based methods
    <ul>
      <li>solely rely on pastobserved temporal patterns</li>
    </ul>
  </li>
  <li>TCN-based methods
    <ul>
      <li>larger receptivefied</li>
      <li>but limited modeling capabilities forlong-term dependencies</li>
    </ul>
  </li>
</ul>

<h2 id="3-proposed-methods">3. Proposed Methods</h2>

<h3 id="31-problem-statement">3.1 Problem Statement</h3>

<ul>
  <li>\(X=\left\{x_1, \ldots, x_T\right\} \in \mathbb{R}^{T \times N}\)를 보고 \(\hat{X}=\left\{\hat{x}_{T+1}, \ldots, \hat{x}_{T+S}\right\} \in \mathbb{R}^{S \times N}\) 예측</li>
</ul>

<h3 id="32-normalization">3.2 Normalization</h3>

<ul>
  <li>\(X \in \mathbb{R}^{T \times N}\)  into \(X^0=\left\{x_1^0, \ldots, x_T^0\right\} \in \mathbb{R}^{T \times N}\), via \(X^0=\operatorname{RevIN}(X)\).</li>
</ul>

<h3 id="33-channel-independence--reversed-channel-independence">3.3 Channel Independence &amp; Reversed Channel Independence</h3>

<ul>
  <li>model overfitting 때문에 함</li>
  <li>\(\text{Batch}\left(X^0\right)\):  (Batch_Size, Lookback length, Dimension) \(= (B, T, N)\) 를 reshape
    <ul>
      <li>Batch \(\left(X^{\boldsymbol{I}}\right)= \text{ChannelIndepence}\left(\operatorname{Batch}\left(X^0\right)\right) : (B \times N, 1, T)\)</li>
    </ul>
  </li>
  <li>다시 되돌릴 때에는 $\operatorname{Batch}\left(X^P\right):(B \times N, 1, S)$를 reshape
    <ul>
      <li>Batch \((\hat{\boldsymbol{X}})=\text{ ChannelIndepence}^{-1}\left(\operatorname{Batch}\left(X^P\right)\right) : (B, S, N)\)</li>
    </ul>
  </li>
</ul>

<h3 id="34-twin-mamba">3.4 Twin Mamba</h3>

<p><img src="/assets/img/Mamba/DTMamba/fig1.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/DTMamba/fig2.png" alt="그림1" /></p>

<ul>
  <li>Embedding layer - residual</li>
  <li>Dropout layer - twin Mambas</li>
</ul>

<h3 id="341-embedding-layers">3.4.1. Embedding Layers</h3>

<ul>
  <li>linear layer as the Embedding layer \(\to\) global feature representation</li>
  <li>\(X^E = \text{Embedding}(X^I):(B \times N, 1, ni)\).</li>
  <li>Embedding layer는 TMamba Block 안에 있는데
    <ul>
      <li>TMamba Block이 두 개니까 각각의 Embedding layer를 Embedding 1,2라 함</li>
    </ul>
  </li>
</ul>

<h3 id="342-residual">3.4.2. Residual</h3>

<ul>
  <li>proposed by ResNet</li>
  <li>to prevent overfitting, stable training</li>
</ul>

<h3 id="343-dropout">3.4.3. Dropout</h3>

<ul>
  <li>To prevent overfitting</li>
  <li>\(X^E\) into \(X^D:(B \times N, 1, n i)\) (dimension은 그대로)</li>
</ul>

<h3 id="344-mamba">3.4.4. Mamba</h3>

<ul>
  <li>S4 learns how to map an input \(x(t)\) to an output \(y(t)\) through an intermediate state \(h(t)\)</li>
  <li>TMamba Block에서는 2개의 Mamba 사용됨 (multi-level feature learning)
    <ul>
      <li>low-level temporal features &amp; high-level temporal patterns</li>
    </ul>
  </li>
</ul>

<h3 id="35-projection-layer">3.5. Projection Layer</h3>

<p><img src="/assets/img/Mamba/DTMamba/algorithm1.png" alt="그림1" /></p>

<ul>
  <li>지금까지 2개의 TMamba Block와 residuals \(R^1\), \(R^2\) 더해서 hidden information을 얻음</li>
  <li>Output의 shape에 맞게 FC layer 태운 뒤
    <ul>
      <li>앞서 언급한 것처럼 \(\text{ ChannelIndepence}^{-1}\)와 \(\text{RevIN}^{-1}\)을 수행</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiment">4. Experiment</h2>

<h3 id="41-settings">4.1. Settings</h3>

<p>pass</p>

<h3 id="42-long-term-forecasting">4.2. Long-term Forecasting</h3>

<p><img src="/assets/img/Mamba/DTMamba/table1.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/DTMamba/table2-1.png" alt="그림1" />
<img src="/assets/img/Mamba/DTMamba/table2-2.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/DTMamba/table3.png" alt="그림1" /></p>

<h3 id="43-hyperparameter-sensitivity-analysis-and-ablation-study">4.3. Hyperparameter Sensitivity Analysis and Ablation Study</h3>

<p><img src="/assets/img/Mamba/DTMamba/table4.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/DTMamba/table5.png" alt="그림1" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2405.07022)]]></summary></entry><entry><title type="html">MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting (ICLR 2025)</title><link href="http://localhost:4000/mamba/2024-11-27-MambaTS/" rel="alternate" type="text/html" title="MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting (ICLR 2025)" /><published>2024-11-27T00:00:00+09:00</published><updated>2025-01-08T22:03:03+09:00</updated><id>http://localhost:4000/mamba/MambaTS</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-27-MambaTS/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformers : quadratic complexity and permutation invariant bias</li>
  <li>Mamba의 한계 4가지를 개선한 <strong>MambaTS</strong> 제시
    <ul>
      <li>variable scan : 모든 변수의 과거 정보를 함께 arrange</li>
      <li>causal convolution (in Mamba block) 필요없음 \(\to\) Temporal Mamba Block(TMB)</li>
      <li>dropout mechanism (for selective parameters of TMB)</li>
      <li>variable permutation training : variable scan order sensitivity 문제 해결</li>
    </ul>
  </li>
  <li>추가적으로 variable-aware scan
    <ul>
      <li>훈련 과정에서 변수 관계를 동적으로 발견하고,</li>
      <li>추론 시 모든 노드를 방문하는 최단 경로 문제를 해결하여 최적의 변수 스캔 순서를 디코딩</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer의 문제는
    <ul>
      <li>quadratic complexity</li>
      <li>look-back window가 커진다고 성능이 향상되는 것이 아님</li>
      <li>permutation invariant bias의 effectiveness 의문</li>
    </ul>
  </li>
  <li>Mamba : SSM + selection mechanism + hardware-aware design
    <ul>
      <li>Transformer based methods(PatchTST 및 iTransformer)에서</li>
      <li>Transformer 블록을 Mamba 블록으로 교체했더니</li>
      <li>학습 속도가 1.3배 더 빠르고, GPU 메모리 사용량이 각각 5.3배 및 7.0배 감소</li>
      <li><strong>하지만 Mamba가 Transformer보다 성능적 우위를 보이지는 못함</strong></li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig1.png" alt="그림1" /></p>

<ul>
  <li>그러므로 Abstract에서 소개한 MambaTS를 제시</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="long-term-time-series-forecasting">Long-term time series forecasting</h3>

<ul>
  <li>variable-mixing : dependencies across time and variables
    <ul>
      <li>RNN, TCN, Transformer, MLP…</li>
      <li>현재는 patching으로 quadratic complexity 개선하고자 함</li>
    </ul>
  </li>
  <li>Variable-independent : the assumption of variable independence
    <ul>
      <li>문제를 과도하게 단순화해서 부적절할 수도 있긴 함</li>
    </ul>
  </li>
</ul>

<h3 id="state-space-models">State Space Models</h3>

<ul>
  <li>Mamba : SSM + selection mechanism + hardware-aware design
    <ul>
      <li>Mamba의 scan order sensitivity를 다루기 위해 Vison 도메인에서는</li>
      <li><strong>bidirectional scanning</strong>(Vision Mamba), <strong>multi-directional scanning</strong>(VMamba, Mamba-nd), <strong>automatic direction scanning</strong>(Local Mamba)</li>
      <li>하지만 temporal problem에는 활발한 연구 X</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 MambaTS는</li>
  <li>VAST(Variable-Aware Scan along Time)으로 표현력 강화</li>
</ul>

<h2 id="3-preliminaries">3. Preliminaries</h2>

<p>pass</p>

<h2 id="4-model-architecture">4. Model Architecture</h2>

<p><img src="/assets/img/Mamba/MambaTS/fig2.png" alt="그림1" /></p>

<ul>
  <li>\(\left(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_L \right)\), where \(\mathbf{x}_i \in \mathbb{R}^K\)를 보고 \(\left(\mathbf{x}_{L+1}, \cdots, \mathbf{x}_{L+2}, \cdots, \mathbf{x}_{L+T}\right)\) 예측하고자 함</li>
</ul>

<h3 id="41-overall-architecture">4.1. Overall Architecture</h3>

<ul>
  <li><strong>Patching and Tokenization</strong>
    <ul>
      <li>PatchTST처럼 각 변수를 \(M=L / s\)개의 patches로 나누어 각 patch를 \(D\)차원으로 mapping</li>
    </ul>
  </li>
  <li><strong>Variable Scan along Time</strong>
    <ul>
      <li>\(K\) 개의 변수를 임베딩함으로써 \(K \times M\) 개의 tokens 얻음</li>
      <li>Variable Scan Along Time(VST) : 첫 시점에서 모든 변수의 토큰 \(\to\) 두 번째 시점에서 모든 변수의 토큰 \(\to\) … 마지막 시점에서 모든 변수의 토근 순서로 정렬</li>
      <li>그림으로 표현하면 아래와 같음</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig2-1.png" alt="그림1" /></p>

<ul>
  <li><strong>Encoder</strong>
    <ul>
      <li>\(N\)개의 Temporal Mamba Block(TMB)로 구성됨</li>
      <li>각 TMB는 2개의 branches
        <ul>
          <li>하나는 sequence modeling에 집중, 하나는 비선형성에 집중</li>
          <li>식으로 표현하면 \(h_t=\operatorname{SSM}\left(\operatorname{Dropout}\left(\operatorname{Linear}\left(x_t\right)\right)\right)+\sigma\left(\right. Linear \left.\left(x_t\right)\right)\)이고</li>
          <li>그림으로 표현하면 아래와 같음</li>
          <li>왼쪽 Mamba block에서 Conv 없애고 dropout 추가함</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig2-2.png" alt="그림1" /></p>

<ul>
  <li><strong>Prediction Head</strong>
    <ul>
      <li>Encoder가 global dependencies를 잡으니까 channel-independent decoding</li>
    </ul>
  </li>
  <li><strong>Instance Normalization</strong>
    <ul>
      <li>RevIN 사용</li>
    </ul>
  </li>
  <li><strong>Loss Function</strong>
    <ul>
      <li>MSE 사용 : \(\mathcal{L}=\mathbb{E}_{\mathbf{x}} \frac{1}{M} \sum_{i=1}^M\left\|\hat{\mathbf{x}}_{L+1: L+T}^{(i)}-\mathbf{x}_{L+1: L+T}^{(i)}\right\|_2^2\)</li>
    </ul>
  </li>
</ul>

<h3 id="42--variable-permutation-training">4.2.  Variable Permutation Training</h3>

<ul>
  <li>channel orders의 영향을 줄이고 local context interactions을 늘리기 위해</li>
  <li>variable permutation training (VPT) strategy를 사용
    <ul>
      <li>\(K \times M\)개의 tokens가 있으면 each time step에서 \(K\)개를 섞고 디코딩 후에 복원</li>
    </ul>
  </li>
</ul>

<h2 id="43-variable-aware-scan-along-time">4.3. Variable-Aware Scan along Time</h2>

<ul>
  <li>최적의 <strong>변수 스캔 순서</strong>를 결정하려면 변수간 관계를 알아야 하고</li>
  <li>
    <p>이를 위해 Variable-Aware Scan along Time (VAST) 제안</p>
  </li>
  <li><strong>Training</strong>
    <ul>
      <li>변수 \(K\)개, directed graph adjacency matrix \(P \in \mathbb{R}^{K \times K}\)를 만듬
        <ul>
          <li>\(p_{i, j}\) : cost from node \(i\) to node \(j\)</li>
          <li>다양한 variable scan order 탐색해서 Loss 계산 할 것</li>
        </ul>
      </li>
      <li>이제 node index sequence \(V=\left\{v_1, v_2, \ldots, v_K\right\}\)가 나옴
        <ul>
          <li>\(v_k\)는 shuffled sequnce의 새로운 index</li>
        </ul>
      </li>
      <li>\(K-1\) 개의 transition tuples \(\left\{\left(v_1, v_2\right),\left(v_2, v_3\right), \ldots,\left(v_{K-1}, v_K\right)\right\}\)이 도출됨</li>
      <li>각 sample마다 network의 \(t\)-th iteration동안 training loss \(l^{(t)}\) 계산되고
        <ul>
          <li><strong>directed graph adjacency matrix</strong> \(P \in \mathbb{R}^{K \times K}\)를 update (with exponential moving average)</li>
          <li>\(p_{v_k, v_{k+1}}^{(t)}=\beta p_{v_k, v_{k+1}}^{(t-1)}+(1-\beta) l^{(t)}\).</li>
          <li>\(\beta\)는 rate of the moving average (hyperparameter)</li>
        </ul>
      </li>
      <li>샘플 배치 간의 영향을 제거하기 위해 위 식을 batch 버전으로 확장
        <ul>
          <li>즉 \(\bar{l}^{(t)}=l^{(t)}-\mu\left(l^{(t)}\right)\)를 사용하여 centralization (\(\mu\)는 mean function)</li>
          <li>\(p_{v_k, v_{k+1}}^{(t)}=\beta p_{v_k, v_{k+1}}^{(t-1)}+(1-\beta) \bar{l}^{(t)}\)이 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Inference</strong>
    <ul>
      <li>Training동안 directed graph adjacency matrix \(P\) 는 optimal variable scan order 결정하는 데에 사용</li>
      <li>즉 Asymmetric Traveling Salesman Problem, ATSP 문제가 되고
        <ul>
          <li>heuristic-based simulated annealing algorithm 사용해서 경로 디코딩</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<p><img src="/assets/img/Mamba/MambaTS/table2.png" alt="그림1" /></p>

<h3 id="51-main-results">5.1. Main results</h3>

<p><img src="/assets/img/Mamba/MambaTS/table3.png" alt="그림1" /></p>

<h3 id="52--ablation-studies-and-analyses">5.2.  Ablation studies and analyses</h3>

<ul>
  <li>Component Ablation</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/table4.png" alt="그림1" /></p>

<ul>
  <li>Dropout Ablation</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig3.png" alt="그림1" /></p>

<ul>
  <li>VAST Ablation</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/table5.png" alt="그림1" /></p>

<h3 id="53-model-analysis">5.3 Model Analysis</h3>

<ul>
  <li>Increasing Lookback Window</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/fig5.png" alt="그림1" /></p>

<ul>
  <li>Efficiency Analys</li>
</ul>

<p><img src="/assets/img/Mamba/MambaTS/table6.png" alt="그림1" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<ul>
  <li>Variable Scan along Time (VST)
    <ul>
      <li>to organize the historical information of all variables,</li>
      <li>forming a global retrospective sequence</li>
    </ul>
  </li>
  <li>Temporal Mamba Block (TMB)
    <ul>
      <li>causal convolution in Mamba 제거</li>
      <li>dropout regularization 추가</li>
    </ul>
  </li>
  <li>Variable Permutation Training (VPT)
    <ul>
      <li>local context interaction 능력 향상</li>
    </ul>
  </li>
  <li>Variable-Aware Scan along Time (VAST)
    <ul>
      <li>훈련 중 변수 간 관계를 동적으로 발견</li>
      <li>ATSP solver로 the optimal variable scann order 결정</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[ICLR 2025](https://arxiv.org/pdf/2405.16440)]]></summary></entry><entry><title type="html">Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2024-11-25-MoU/" rel="alternate" type="text/html" title="Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need (Arxiv 2024)" /><published>2024-11-25T00:00:00+09:00</published><updated>2025-01-08T22:03:02+09:00</updated><id>http://localhost:4000/mamba/MoU</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-25-MoU/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer는 높은 계산 비용(quadratic computational cost)이 문제,</li>
  <li>Mamba는 longterm forecasting에서 성능이 effective하지 않음 (potential information loss 때문)</li>
  <li>본 논문에서 제시하는 <strong>Mixture of Universals (MoU)</strong>의 components:
    <ul>
      <li><strong>Mixture of Feature Extractors (MoF)</strong>: adaptive patch representations
        <ul>
          <li>(for <strong>short</strong>-term dependency)</li>
        </ul>
      </li>
      <li><strong>Mixture of Architectures (MoA)</strong>:  Mamba, FeedForward, Convolution, and Self-Attention 연결한 것
        <ul>
          <li>(for  <strong>long</strong>-term dependency )</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li><strong>PatchTST</strong>:  patch embedding에서 uniform linear transformations 사용 \(\to\) varying semantic contexts 손실
    <ul>
      <li>Vision에서 Dynamic Convolution, Conditional Convolution (for informative representations)등장했지만</li>
      <li>Time series에서는 poor performances</li>
    </ul>
  </li>
  <li><strong>Transformer</strong>: 장기 의존성은 잘 처리하지만 계산 비용이 큼</li>
  <li><strong>Mamba</strong>: 효율적이지만 정보 손실로 인해 장기 예측에서 성능이 떨어질 수 있음</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig2.png" alt="그림1" /></p>

<ul>
  <li><strong>MoF(Mixture of Feature Extractors)</strong>:
    <ul>
      <li>multiple sub-extractors로 구성되어 있고</li>
      <li>sparse activation을 사용해서 input patch에 적합한 sub-extractor만 활성화</li>
      <li>learning of diverse contexts and <strong>minimal parameter increase</strong> !</li>
    </ul>
  </li>
  <li><strong>MoA(Mixture of Architectures)</strong> Mamba에서 시작해 국소적인 관점에서 전역적인 Self-Attention 계층으로 확장하며 장기 의존성을 효율적으로 캡처하는 계층적 구조.
    <ul>
      <li>hierarchical structure를 가진 encoder</li>
      <li><strong>Mamba layer</strong> that selects and learns key dependencies using a Selective State-Space Model (SSM).</li>
      <li><strong>FeedForward transition layer</strong> and a <strong>Convolution-layer</strong> that broadens the receptive field to capture longer dependencies.</li>
      <li><strong>Self-Attention layer</strong> integrates information globally to fully capture long-term dependencies</li>
    </ul>
  </li>
</ul>

<h2 id="2-approach">2. Approach</h2>

<h3 id="21-problem-setting-and-model-structure">2.1 Problem Setting and Model Structure</h3>

<ul>
  <li>\(\mathbf{X}_{\text {input }}=\left[\mathbf{X}^1, \mathbf{X}^2, \ldots, \mathbf{X}^M\right] \in \mathbb{R}^{M \times L}\) where \(\mathbf{X}^i=\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_L\right] \in \mathbb{R}^L\)</li>
  <li>\(\hat{\mathbf{X}}_{\text {output }}=\left[\hat{\mathbf{X}}^1, \hat{\mathbf{X}}^2, \ldots, \hat{\mathbf{X}}^M\right] \in \mathbb{R}^{M \times T}\).</li>
  <li>
    <p>Goal : learning a function \(\mathcal{F}: \mathbf{X} \rightarrow \hat{\mathbf{X}}\)</p>
  </li>
  <li>Overall process는 아래와 같음.
    <ul>
      <li>처음에는 raw time series \(\mathbf{X} \in \mathbb{R}^L\)에서 시작 (variate independence setting이라서 channel =1)</li>
      <li>\(N\)개의 patch tokens를 만듬 :
        <ul>
          <li>\(\mathbf{X}_p=\operatorname{Patch}(\mathbf{X}) \in \mathbb{R}^{N \times P}\) with fixed size \(P\), stride \(S\)</li>
        </ul>
      </li>
      <li><strong>MoF</strong> module에 넣어서 adaptive representations를 얻음 :
        <ul>
          <li>\(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\).</li>
          <li>MoF는 parameters를 조절하면서 계산 비용 최적화 (<strong>2.2</strong>에서 자세히 설명)</li>
        </ul>
      </li>
      <li>이제 <strong>MoA</strong>에 넣어서 long-term dependencies (among tokens) 잡음
        <ul>
          <li>\(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoA}\left(\mathbf{X}_{\mathrm{rep}}\right)\in \mathbb{R}^{N \times D}\).</li>
          <li>MoA는 long-term encoder based on the Mixture of Architectures (<strong>2.3</strong>에서 자세히 설명)</li>
        </ul>
      </li>
      <li>마지막으로 linear projector에 넣어서 final prediction 얻음
        <ul>
          <li>\(\hat{\mathbf{X}}=\mathbf{P}\left(\operatorname{Flatten}\left(\mathbf{X}_{\mathrm{rep}^{\prime}}\right)\right)\in \mathbb{R}^{M \times T}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-mixture-of-feature-extractors">2.2 Mixture of Feature Extractors</h3>

<ul>
  <li>MoF의 목적은 <strong>patch의 representative embedding</strong>을 만드는 것</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig3.png" alt="그림1" /></p>

<ul>
  <li>Sub-extractors \(\left\{F_1, F_2, \ldots, F_c\right\}\)가 있고 각각은 independent linear mapping</li>
  <li>MoF를 통과하면 \(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\)를 얻음
    <ul>
      <li>where \(R_i(\cdot)\)는  input-relevant router (sub-extractor를 sparse하게 활성화)
        <ul>
          <li>즉 \(R\left(\mathbf{X}_p\right)_i=\operatorname{Softmax}\left(\operatorname{Top}_{\mathrm{k}}\left(H\left(\mathbf{X}_p\right)_i, k\right)\right)\)
            <ul>
              <li>\(\operatorname{Softmax}(\cdot)\)는  \(\operatorname{Topk}(\cdot, k)\)에 의해 선택된 상위 \(k\)개의 점수를 정규화</li>
              <li>\(H\left(X_p\right)=\left[H\left(\mathbf{X}_p\right)_1, H\left(\mathbf{X}_p\right)_2, \ldots H\left(\mathbf{X}_p\right)_c\right]\) 는 sub-extractors의 score vector</li>
              <li>여기서 \(H\left(\mathbf{X}_p\right)_i=\left(\mathbf{X}_p \cdot W_g\right)_i+\text { SN } \cdot \text { Softplus }\left(\left(\mathbf{X}_p \cdot W_{\text {noise }}\right)_i\right)\)
                <ul>
                  <li>where \(W_g\)는 linear layer이고 두번째 항은 load balancin을 위한 tunable noise를 넣는 것</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이렇게 하면 patch token를 \(c\)개의 서로 다른 patterns들의 조합으로 분할할 수 있음
    <ul>
      <li>each pattern은 최적의 sub-extractors에 의해 뽑힌 것이니</li>
      <li>most representative embedding (for patches with divergent context)라고 할 수 있음</li>
    </ul>
  </li>
</ul>

<h3 id="23-mixture-of-architectures">2.3. Mixture of Architectures</h3>

<ul>
  <li>MoA의 목적은 <strong>comprehensive long-term dependencies</strong>를 모델링하는 것</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig4.png" alt="그림1" /></p>

<ul>
  <li><strong>Mamba, FeedForward, Convolution</strong>, and <strong>Self-Attention layer</strong>로 구성되어
    <ul>
      <li>각각이 다른 관점에서 long-term dependencie를 학습</li>
      <li><strong>gradually expanding perspective</strong>를 통해 effectiveness and efficiency 둘 다 챙김</li>
    </ul>
  </li>
  <li><strong>Mamba-layer in Time Series</strong> : <strong>relevant data를 선택하고 time-variant dependencies를 학습하는 곳</strong>
    <ul>
      <li>input은 MoF의 output \(\mathbf{X}_{\mathrm{rep}}=\operatorname{MoF}\left(\mathbf{X}_p\right)=\sum_{i=1}^n R_i\left(\mathbf{X}_p\right) F_i\left(\mathbf{X}_p\right)\in \mathbb{R}^{\mathbf{N} \times \mathbf{D}}\)인데 \(x\)로 denote</li>
      <li>\(\begin{gathered}
\boldsymbol{x}^{\prime}=\sigma(\text { Conv1D }(\text { Linear }(\boldsymbol{x}))) \\
\boldsymbol{z}=\sigma(\text { Linear }(\boldsymbol{x}))
\end{gathered}\), where \(\sigma\)는 \(SiLU\) activation</li>
      <li>다음으로 \(\begin{gathered}
\boldsymbol{y}^{\prime}=\operatorname{Linear}\left(\operatorname{SelectiveSSM}\left(\boldsymbol{x}^{\prime}\right) \otimes \boldsymbol{z}\right) \\
\boldsymbol{y}=\operatorname{LayerNorm}\left(\boldsymbol{y}^{\prime}+\boldsymbol{x}\right)
\end{gathered}\), where \(\begin{gathered}
\text { SelectiveSSM }\left(\boldsymbol{x}_t^{\prime}\right)=\boldsymbol{y}_t \\
\boldsymbol{y}_t=C \boldsymbol{h}_t, \quad \boldsymbol{h}_t=\bar{A} \boldsymbol{h}_{t-1}+\bar{B} \boldsymbol{x}_t^{\prime}
\end{gathered}\)
        <ul>
          <li>\(h_t\)는 latent state, \(y_t\)는 output representation</li>
          <li>The discrete matrices는 \(\begin{gathered}
B_t=S_B\left(\boldsymbol{x}_t^{\prime}\right), \quad C_t=S_C\left(\boldsymbol{x}_t^{\prime}\right) \\
\Delta_t=\operatorname{softplus}\left(S_{\Delta}\left(\boldsymbol{x}_t^{\prime}\right)\right)
\end{gathered}\)
            <ul>
              <li>\(S\)들은 linear layers이고, \(\begin{gathered}
f_A\left(\Delta_t, A\right)=\exp \left(\Delta_t A\right) \\
f_B\left(\Delta_t, A, B_t\right)=\left(\Delta_t A\right)^{-1}\left(\exp \left(\Delta_t A\right)-I\right) \cdot \Delta B_t \\
\bar{A}_t=f_A\left(\Delta_t, A\right), \quad \bar{B}_t=f_B\left(\Delta_t, A, B_t\right)
\end{gathered}\)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>좀 복잡한데 위에 있는 fig4 보는 것이 낫겠다.</li>
    </ul>
  </li>
  <li><strong>FeedForward-layer</strong> : <strong>non-linearity를 강화하는 곳</strong>
    <ul>
      <li>\(\boldsymbol{x}_{\mathrm{ffn}}=\text { FeedForward }\left(\boldsymbol{y}_t ; w_1, \sigma, w_2\right)\), where
        <ul>
          <li>\(w_1\) and \(w_2\) are parameters, \(\sigma\) is activation function</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Convolution-layer</strong> : <strong>MoA의 receptive field를 넓히는 곳</strong>
    <ul>
      <li>partial long-term dependencies를 담고 있는 tokens끼리의 정보 교환을 촉진</li>
      <li>\(\boldsymbol{x}_{\mathrm{conv}}= \operatorname{Conv}\left(\boldsymbol{x}_{\mathrm{ffn}} ; \mathbf{k}, s, p, c_{\mathrm{out}}\right)\),
        <ul>
          <li>where \(\mathbf{k}\) is the kernel size, \(s\) is the stride, \(p\) is the padding,</li>
          <li>and \(c_{\text {out }}\) is the number of output channels</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Self-Attention-layer</strong> : <strong>global perspective에서 포괄적인 long-term dependencies를 파악하는 곳</strong>
    <ul>
      <li>: \(\begin{aligned} &amp; x_{\mathrm{att}}=\operatorname{FeedForward}(\operatorname{Attention}(Q, K, V)) \\ &amp; \operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V \\ &amp; Q=x_{\text {conv }} W_Q, K=x_{\text {conv }} W_K, V=x_{\text {conv }} W_V\end{aligned}\)</li>
    </ul>
  </li>
  <li><strong>Partial-to-global Design for Time Series</strong>
    <ul>
      <li><strong>gradually expanding perspective</strong>라는 말의 의미는
        <ul>
          <li>Mamba layer : Selective SSM을 사용하여 시간적으로 변화하는 의존성을 처리</li>
          <li>FeedForward layer : 이러한 부분 의존성을 더 복잡한 표현으로 전환</li>
          <li>Convolution layer : 수용 영역을 확장하여 보다 넓은 시간적 관계를 학습</li>
          <li>Self-Attention layer : 로컬화된 정보를 통합하여 장기 의존성에 대한 포괄적인 이해</li>
        </ul>
      </li>
      <li>를 거치면서 선택적으로 일부 의존성에 초점을 맞춘 후, 이를 점차 확장하여 전체적(global) 관점으로 발전시킨다는 뜻</li>
    </ul>
  </li>
</ul>

<h3 id="24-computational-complexity-and-model-parameter">2.4. Computational Complexity and Model Parameter</h3>

<ul>
  <li>Tokens \(T\)개가 주어졌을 때 top-\(k\) experts를 선택한다고 하면
    <ul>
      <li>\(C_{\mathrm{MOU}}=\underbrace{k T \times d^2}_{\text {MoF }}+\underbrace{T \times d^2}_{\text {Mamba }}+\underbrace{T \times d^2}_{\text {FFN }}+\underbrace{k T d^2}_{\text {Conv }}+\underbrace{T^2 \times d+T \times d^2}_{\text {Transformer }}\),
        <ul>
          <li>where \(\mathbf{k}\) is the kernel size in the convolutional layer,</li>
          <li>\(d\) is the dimension of the vector representations</li>
        </ul>
      </li>
      <li>Transformer 블록을 제외하면 선형적인 복잡도</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig1.png" alt="그림1" /></p>

<h2 id="3-experiments">3. Experiments</h2>

<h3 id="31-dataset">3.1. Dataset</h3>

<ul>
  <li>7 commonly used datasets</li>
  <li>Pass</li>
</ul>

<h3 id="32-baselines-and-setup">3.2. Baselines and Setup</h3>

<ul>
  <li>Mamba-based Models (S-Mamba)</li>
  <li>Linear-based Models (D-Linear)</li>
  <li>Convolution-based Models (ModernTCN)</li>
  <li>Transformer-based Models (PatchTST)</li>
</ul>

<h3 id="33-main-results">3.3. Main Results</h3>

<p><img src="/assets/img/Mamba/MoU/table1.png" alt="그림1" /></p>

<h3 id="34-ablation-study">3.4. Ablation Study</h3>

<p><img src="/assets/img/Mamba/MoU/table2.png" alt="그림1" /></p>

<ul>
  <li>3개의 adaptive feature extractors를 비교했을 때 MoF(in MoU)가 가장 좋았으며
    <ul>
      <li>Dyconv가 parameters 수를 크게 증가시키기 때문에 time series patch와 같은 작은 데이터셋에는 적합하지 않음</li>
      <li>SE-M의  calibration strategy는 representation을 normalized gating vector에 곱하는 방식이라서 다양한 컨텍스트 정보를 처리하는 데에는 한계</li>
    </ul>
  </li>
  <li>특히 MoF가 uniform transformation method (Linear)보다 좋다는 점이 주목할만함</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/table3.png" alt="그림1" /></p>

<ul>
  <li>
    <p>AA, MM, MFA, AAA, MMA, AMM, MAM, AMA, AFM, AFCM</p>

    <ul>
      <li>여기서 A, M, F, C는 각각 Self-Attention, Mamba, FeedForward, Convolution</li>
      <li>글자의 순서는 레이어의 배치 순서</li>
    </ul>
  </li>
  <li>
    <p>M-A 순서(MAM, AMA, MMA) &gt; M-A 순서를 가지지 않은 모델(AMM)</p>

    <ul>
      <li>
        <p>Mamba를 Self-Attention 이전에 배치하는 것이 장기 의존성을 캡처하는 데 더 효과적</p>
      </li>
      <li>
        <p>A-M 순서보다 M-A 순서가 장기 의존성 학습에서 더 중요한 역할</p>
      </li>
    </ul>
  </li>
  <li>
    <p>F-C 순서 &gt; F</p>

    <ul>
      <li>Convolution 레이어가 Mamba 레이어의 수용 영역을 확장하여</li>
      <li>Mamba 레이어의 부분적 관점과 Self-Attention 레이어의 글로벌 관점을 연결하는 중간 관점을 제공한다고 해석됨</li>
    </ul>
  </li>
</ul>

<h3 id="35-model-analysis">3.5. Model Analysis</h3>

<ul>
  <li>Does MoF actually learn contexts within patches?</li>
  <li>What is learned by the layers of MoA?</li>
</ul>

<p><img src="/assets/img/Mamba/MoU/fig56.png" alt="그림1" /></p>

<h2 id="5-conclusion">5 Conclusion</h2>

<ul>
  <li>Mixture of Universals (MoU)
    <ul>
      <li>Mixture of Feature Extractors (MoF)
        <ul>
          <li>an adaptive method specifically designed to enhance time series patch representations for capturing short-term dependencies</li>
        </ul>
      </li>
      <li>Mixture of Architectures (MoA)
        <ul>
          <li>hierarchically integrates multiple architectures in a structured sequence to model long-term dependencies from a hybrid perspective</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2408.15997)]]></summary></entry><entry><title type="html">SEQUENTIAL ORDER-ROBUST MAMBA FOR TIME SERIES FORECASTING (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2024-11-14-SOR-Mamba/" rel="alternate" type="text/html" title="SEQUENTIAL ORDER-ROBUST MAMBA FOR TIME SERIES FORECASTING (Arxiv 2024)" /><published>2024-11-14T00:00:00+09:00</published><updated>2025-01-08T22:03:01+09:00</updated><id>http://localhost:4000/mamba/SOR-Mamba</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-14-SOR-Mamba/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Mamba : near-linear complexity in processing sequential data
    <ul>
      <li><strong>하지만 일반적으로 Time series에서 변수의 순서는 존재하지 않기 때문에</strong></li>
      <li><strong>Mamba에서 channel dependencies (CD)를 잡다보면 sequential order bias가 발생</strong></li>
    </ul>
  </li>
  <li>그러므로 본 논문에서는 <strong>SOR-Mamba</strong>를 제안
    <ul>
      <li><strong>Regularization strategy</strong>
        <ul>
          <li>to minimize the discrepancy btw two embedding vectors (reversed channel orders)</li>
          <li>\(\to\)  robustness to channel order</li>
        </ul>
      </li>
      <li><strong>Eliminates the 1D-convolution</strong>
        <ul>
          <li>(originally designed to capture local information in sequential data)</li>
        </ul>
      </li>
      <li><strong>Channel correlation modeling (CCM)</strong>
        <ul>
          <li>pretraining task aimed at preserving <strong>correlations between channels</strong>
            <ul>
              <li>from the data space to the latent space</li>
              <li>in order to enhance the ability to capture CD</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer
    <ul>
      <li>ability to capture long-term dependencies but, quadratic computational complexity</li>
      <li>reduce the complexity 하려다보니 performance degradations</li>
    </ul>
  </li>
  <li><strong>SSM</strong>
    <ul>
      <li>employing <strong>convolutional</strong> operations to process sequences with linear complexity</li>
    </ul>
  </li>
  <li><strong>Mamba</strong>
    <ul>
      <li>incorporating a <strong>selective</strong> mechanism to prioritize important information</li>
      <li>balance btw performance and computational efficiency</li>
    </ul>
  </li>
  <li>Temporal dependencies (TD), channel dependencies (CD) 둘 다 잡아야 하는데,
    <ul>
      <li>iTransformer에서는 CD는 complex attention mechanisms으로,</li>
      <li>TD는 simple multi-layer perceptrons (MLPs)으로 했었음</li>
    </ul>
  </li>
  <li>Mamba는 <strong>sequential order bias</strong>가 있다보니 <strong>Bidirectional Mamba로는 충분하지 않음</strong></li>
  <li>그렇다고 MambaTS처럼 channel을 섞자니 추가적인 작업이 소요됨</li>
</ul>

<p><img src="/assets/img/Mamba/SOR-Mamba/fig1.png" alt="그림1" /></p>

<ul>
  <li>그래서 <strong>Sequential Order-Robust Mamba for TS forecasting (SOR-Mamba)</strong>를 제안
    <ul>
      <li>(간단한 설명은 abstract에 잘 정리되어 있으므로 pass)</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li><strong>CD-Mamba Block</strong>
    <ul>
      <li>1D Conv 제거</li>
      <li>Time series의 channels는 애초에 inherent sequential order가 존재하지 않기 때문</li>
    </ul>
  </li>
  <li><strong>Regulararization with CD-Mamba Block</strong>
    <ul>
      <li>Reversed channel order로 발생하는 두 embedding vectors의 차이를 줄이도록 학습</li>
      <li>\(\to\) Enhancing robustness to channel order !</li>
    </ul>
  </li>
  <li><strong>Channel correlation modeling</strong>
    <ul>
      <li>Data space에서의 correlation (btw channels)과</li>
      <li>Embedding space에서의 correlation (btw channels)의 차이를 줄이도록 학습</li>
    </ul>
  </li>
</ul>

<h2 id="3-preliminaries">3. Preliminaries</h2>

<p>pass</p>

<h2 id="4-methodology">4. Methodology</h2>

<p><img src="/assets/img/Mamba/SOR-Mamba/fig2.png" alt="그림1" /></p>

<h3 id="41-architecture-of-sor-mamba">4.1. Architecture of SOR-Mamba</h3>

<ul>
  <li>Embedding layer: \(\mathbf{x} \in \mathbb{R}^{L \times C}\) into \(\mathbf{Z} \in \mathbb{R}^{C \times D}\) using a single linear layer.</li>
  <li>Mamba for CD: CD-Mamba block (<em>1D-conv</em> 제거)</li>
  <li>MLP for TD: (with layer normalization (LN))</li>
  <li>Prediction head: linear prediction head, resulting in \(\hat{\mathbf{y}} \in \mathbb{R}^{H \times C}\)</li>
</ul>

<h3 id="42-regularization-with-cd-mamba-block">4.2. Regularization with CD-Mamba Block</h3>

<ul>
  <li>\(L_{\mathrm{reg}}(\mathbf{z})=d\left(\mathbf{z}_1, \mathbf{z}_2\right)\), 여기서는 \(d\)를 MSE 사용</li>
  <li>최종적으로 \(L(\mathbf{x}, \mathbf{y})=L_{\mathrm{fcst}}(\mathbf{x}, \mathbf{y})+\lambda \cdot \sum_{i=1}^m L_{\mathrm{reg}}\left(\mathbf{z}^{(i)}\right)\)</li>
</ul>

<h3 id="43-channel-correlation-modeling">4.3. Channel Correlation modeling</h3>

<ul>
  <li>Temporal Dependencies보다는 Channel Dependencies를 강조하는 pre-training task</li>
  <li>CCM: preserve the (Pearson) correlation between channels from the data space to the latent space</li>
</ul>

<p><img src="/assets/img/Mamba/SOR-Mamba/fig3.png" alt="그림1" /></p>

<ul>
  <li>input token on the data space에서의 correlation matrices가 있고
    <ul>
      <li>output token on the latent space에의 correlation matrices가 있으면</li>
      <li>loss function for CCM은 distance between these two matrices:
        <ul>
          <li>\(L_{\mathrm{CCM}}(\mathbf{x})=d\left(\mathbf{R}_{\mathbf{x}}, \mathbf{R}_{\mathbf{z}}\right)\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-experiments">5. Experiments</h2>

<h3 id="51-experimental-settings">5.1. Experimental settings</h3>

<p>pass</p>

<h3 id="52-time-series-forecasting">5.2. Time series Forecasting</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table2.png" alt="그림1" /></p>

<h3 id="53-transfer-learning">5.3. Transfer Learning</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table4.png" alt="그림1" /></p>

<ul>
  <li>SimMTM에 transfer learning 한 걸 S-Mamba와 비교</li>
</ul>

<h3 id="54-ablation-study">5.4. Ablation Study</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table5.png" alt="그림1" /></p>

<h2 id="6-analysis">6. Analysis</h2>

<h3 id="61-sequential-order-bias">6.1. Sequential order bias</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/fig4.png" alt="그림1" /></p>

<ul>
  <li>Channels가 많을수록, correlated 되어있을수록, Sequential order bias 큼</li>
</ul>

<h3 id="62-effect-of-regularization">6.2. Effect of regularization</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table6.png" alt="그림1" /></p>

<h3 id="63-effect-of-1d-conv">6.3. Effect of 1D-conv</h3>

<p><img src="/assets/img/Mamba/SOR-Mamba/table7.png" alt="그림1" /></p>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2410.23356)]]></summary></entry><entry><title type="html">Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)</title><link href="http://localhost:4000/mamba/2024-11-12-BiMamba+/" rel="alternate" type="text/html" title="Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (Arxiv 2024)" /><published>2024-11-12T00:00:00+09:00</published><updated>2025-01-08T22:02:59+09:00</updated><id>http://localhost:4000/mamba/BiMamba+</id><content type="html" xml:base="http://localhost:4000/mamba/2024-11-12-BiMamba+/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>LTSF :  long-term dependencies capturing and <strong>sparse semantic characteristics</strong></li>
  <li>Mamba
    <ul>
      <li>the selective capability on input data</li>
      <li>the hardware-aware parallel computing algorithm</li>
    </ul>
  </li>
  <li><strong>Mamba+ block</strong>
    <ul>
      <li>by adding a forget gate inside Mamba,</li>
      <li>to selectively combine the new features with the historical features in a complementary manner</li>
    </ul>
  </li>
  <li><strong>Bi-Mamba+</strong>
    <ul>
      <li>apply Mamba+ both forward and backward</li>
    </ul>
  </li>
  <li>MTS는 시나리오마다 dependency가 다름
    <ul>
      <li>(varying emphasis on intra- or inter-series dependencies)</li>
      <li>\(\to\) series-relation-aware decider
        <ul>
          <li>: controls the utilization of channel-independent or channel-mixing tokenization strategy</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer-based models
    <ul>
      <li>implicitly models the <strong>inter-series dependencies</strong> through channel-mixing embeddings</li>
      <li>However the <strong>quadratic complexity</strong> of the self-attention mechanism</li>
      <li>Informer, Autoformer : sparse attention
        <ul>
          <li>But balancing computational efficiency and predicting performance는 본질적 해결 X</li>
          <li>게다가 not explicitly capture the inter-series dependencies</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>state-space models (SSM) : design of selective scanning</li>
  <li><strong>Long-term time series modeling</strong> : patching manner (patch-wise tokens)으로 하겠다</li>
  <li><strong>Emphasis on intra- or inter-series dependencies</strong> : 데이터셋마다 intra- or inter-sequence dependencies 둘 중 뭐가 중요한지가 다름</li>
  <li>그래서 <strong>Mamba+</strong>를 디자인함
    <ul>
      <li>adding a forget gate in Mamba,</li>
      <li>to selectively combine the new features with the historical features in a complementary manner</li>
      <li>therefore preserving historical information in a longer range</li>
    </ul>
  </li>
  <li><strong>Mamba+</strong>에 기반한 <strong>Bidirectional Mamba+ (BiMamba+)</strong>를 제안
    <ul>
      <li>to model the MTS data from both forward and backward,
        <ul>
          <li>enhancing the model’s robustness and ability to capture interactions between time series elements</li>
        </ul>
      </li>
      <li>Series-Relation-Aware (SRA) decider
        <ul>
          <li>measures the proportion of highly correlated series pairs in the MTS data</li>
          <li>to automatically choose channel-independent or channelmixing tokenization strategies</li>
        </ul>
      </li>
      <li>patch-wise tokens
        <ul>
          <li>contain richer semantic information</li>
          <li>and encourage the model to learn the long-term dependencies</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="time-series-forecasting">Time Series Forecasting</h3>

<ul>
  <li>Transformer-based models : quadratic complexity to the length of the sequence
    <ul>
      <li>Informer(Zhou et al. 2021) : ProbSparse mechanism</li>
      <li>Autoformer(Wu et al. 2021) : time series decomposition</li>
      <li>Pyraformer(Liu et al. 2021) : pyramidal attention module</li>
      <li>FEDformer(Zhou et al. 2022) : frequency enhanced Transformer through frequency domain mapping</li>
      <li>PatchTST(Nie et al. 2023) : divides each univariate sequence into patches
        <ul>
          <li>and uses patch-wise self-attention to model temporal dependencies</li>
        </ul>
      </li>
      <li>Crossformer(Zhang and Yan 2023) : Cross-Dimension attention</li>
      <li>iTransformer(Liu et al. 2023) : inverts the attention layers
        <ul>
          <li>to straightly model inter-series dependencies</li>
          <li>But, the tokenization approach is simply passing the whole sequence through a Multilayer Perceptron (MLP),
            <ul>
              <li>which overlooks the complex evolutionary patterns inside the time series</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="ssm-based-models">SSM-based models</h3>

<ul>
  <li>RNN-based models :
    <ul>
      <li>maintain a hidden state which is updated with each input element</li>
      <li>limits the training speed and leads to forgetting long-term information</li>
    </ul>
  </li>
  <li>CNN-based models :
    <ul>
      <li>parallel computing and have faster training speed</li>
      <li>limits the inference speed and overlook the long-term global information</li>
    </ul>
  </li>
  <li>State Space Models (SSM) :
    <ul>
      <li>trained in parallel like CNN and inferences fastly like RNN</li>
    </ul>
  </li>
  <li>Mamba
    <ul>
      <li>parameterized matrices and a hardware-aware parallel computing algorithm to SSM</li>
      <li><strong>S-Mamba</strong>(Wang et al. 2024)</li>
      <li>embeds each univariate time series like iTransformer</li>
      <li>and feeds the embeddings into Mamba blocks
        <ul>
          <li>to model the relationships of different time series</li>
        </ul>
      </li>
      <li>However, the tokenization approach may overlook the complex evolutionary patterns</li>
      <li><strong>MambaMixer</strong>(Behrouz et al. 2024)
        <ul>
          <li>adjusts the Mamba block to bidirectional</li>
          <li>and uses two improved blocks to capture inter/intra-series dependencies simultaneously</li>
          <li>However, the gating branch is used to filter new features
            <ul>
              <li>(of both forward and backward directions)</li>
              <li>which may cause challenges for extracting new features</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>TimeMachine</strong>(Ahamed and Cheng 2024)
        <ul>
          <li>a multi-scale quadruple-Mamba architecture
            <ul>
              <li>to unify the handling of channel-mixing and channelindependence situations</li>
            </ul>
          </li>
          <li>However, simply based on the length of historical observations and variable number of different datasets
            <ul>
              <li>the characteristics of the MTS data are not fully considered.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<h3 id="31-preliminaries">3.1. Preliminaries</h3>

<ul>
  <li>Long-term multivariate time series forecasting
    <ul>
      <li>\(\mathbf{X}_{i n}=\left[x_1, x_2, \ldots, x_L\right] \in \mathbb{R}^{L \times M}\), we predict the future values \(\mathbf{X}_{\text {out }}=\left[x_{L+1}, x_{L+2}, \ldots, x_{L+H}\right] \in \mathbb{R}^{H \times M}\)</li>
    </ul>
  </li>
  <li>State Space Models
    <ul>
      <li>using first-order differential equations, \(h^{\prime}(t)=\mathbf{A} h(t)+\mathbf{B} x(t), \quad y(t)=\mathbf{C} h(t)\)
        <ul>
          <li>where \(\mathbf{A} \in \mathbb{R}^{N \times N}, \mathbf{B} \in \mathbb{R}^{D \times N} \text { and } \mathbf{C} \in \mathbb{R}^{N \times D}\)</li>
        </ul>
      </li>
      <li>can be discretized :
        <ul>
          <li>\(\begin{aligned}
&amp; \overline{\mathbf{A}}=\exp (\Delta \mathbf{A}), \\
&amp; \overline{\mathbf{B}}=(\Delta \mathbf{A})^{-1}(\exp (\Delta \mathbf{A})-\mathbf{I}) \cdot \Delta \mathbf{B}
\end{aligned}\),</li>
          <li>\(h_k=\overline{\mathbf{A}} h_{k-1}+\overline{\mathbf{B}} x_k, \quad y_k=\mathbf{C} h_k\),</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>S4</strong>(Gu et al. 2021b) :
    <ul>
      <li>HIPPO Matrix(Gu et al. 2020) to the initialization of matrix A</li>
    </ul>
  </li>
  <li><strong>Mamba</strong>(Gu and Dao 2023) :
    <ul>
      <li>parameterizes the matrices \(\mathbf{B}, \mathbf{C}\) and \(\Delta\) in a data-driven manner,</li>
      <li>introducing a selection mechanism into S4 model</li>
    </ul>
  </li>
</ul>

<h3 id="32-overview">3.2. Overview</h3>

<p><img src="/assets/img/Mamba/BiMamba+/fig1.png" alt="그림1" /></p>

<ul>
  <li>step 1) calculate the tokenization strategy indicator through the SRA decider</li>
  <li>step 2) divide the input series into patches and generate patch-wise tokens
    <ul>
      <li>based on the tokenization strategy indicator</li>
    </ul>
  </li>
  <li>step 3) obtained tokens are fed into multiple Bi-Mamba+ encoders</li>
  <li>step 4) a flatten head and a linear projector to get the final output</li>
</ul>

<h3 id="33-instance-normalization">3.3. Instance Normalization</h3>

<ul>
  <li>the input sequence의 non-stationary statistics를 제거하기 위해 RevIN (Kim et al. 2022) 사용</li>
</ul>

<h3 id="34-token-generalization">3.4. Token Generalization</h3>

<ul>
  <li>SRA Decider
    <ul>
      <li>Channel-independence / dependence는 데이터셋마다 다름</li>
      <li>\(\to\) automatic tokenization process
        <ul>
          <li>데이터셋마다 \(T=\left\{t^1, t^2, \ldots, t^M\right\}\)에 대해
            <ul>
              <li>Spearman correlation coefficients of different series \(t^i \text { and } t^j\) 계산 \(\rho_{i, j}\)</li>
              <li>\(\rho_{i, j}=1-\frac{6 \sum_{k=0}^n\left(\operatorname{Rank}\left(t_k^i\right)-\operatorname{Rank}\left(t_k^j\right)\right)^2}{n\left(n^2-1\right)}\),
                <ul>
                  <li>where \(n\) : the number of observations</li>
                  <li>\(\operatorname{Rank}\left(t_k^i\right)\) : the rank level of the \(k\)-th element in the specific time series \(t^i\)</li>
                </ul>
              </li>
              <li>threshold \(\lambda\)를 정하고 relevant series \(\rho_{\max }^\lambda\) and \(\rho_{\max }^0\)를 센 다음
                <ul>
                  <li>relation ratio \(r=\rho_{\max }^\lambda / \rho_{\max }^0 \geq 1-\lambda\)이면 channel-mixing</li>
                  <li>그렇지 않으면 channelindependent strategy</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/alg1.png" alt="그림1" /></p>

<h3 id="35-tokenization-process">3.5. Tokenization Process</h3>

<ul>
  <li>PatchTST처럼 \(x_{1: L}^i\)를 \(J = \left\lceil\frac{L-P}{S}+1\right\rceil\)개의 patch로 나누고
    <ul>
      <li>(S는 stride, P는 patch에 들어가는 시점의 개수)</li>
      <li>channel-independent strategy에서는 각 patch를 D차원으로 embedding
        <ul>
          <li>\(\to\) \(\mathbb{E}_{\text {ind }} \in \mathbb{R}^{M \times J \times D}\)</li>
        </ul>
      </li>
      <li>channel-mixing strategy에서는 같은 시점의 다른 변수들도 group으로 만들고 각 group을 tokenization
        <ul>
          <li>\(\to \mathbb{E}_{\operatorname{mix}} \in\mathbb{R}^{J \times M \times D}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="36-mamba-block">3.6. Mamba+ Block</h3>

<p><img src="/assets/img/Mamba/BiMamba+/mamba.png" alt="그림1" /></p>

<ul>
  <li><strong>기존 Mamba</strong>는 2개의 branches를 사용
    <ul>
      <li>\(b_1\)에서는 1d-conv와 SSM을 통과, 다른 하나 \(b_2\)에서는 그냥 SiLU activation 통과</li>
      <li>\(b_1\)의 SSM 안에 HIPPO가 있긴 해도 \(b_2\) 때문에 최근 정보가 더 우선시되는 문제</li>
    </ul>
  </li>
  <li>그래서 <strong>Mamba+ block</strong>에서는
    <ul>
      <li>forget gate \(\text{gate}_f=1-\text{gate}_{b_2}\)를 추가</li>
      <li>\(\text{gate}_f\)와 \(\text{gate}_{b_2}\)는 new features와 forgotten historical features를 선택적 결합
        <ul>
          <li>\(\to\) preserving historical information !</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/fig2.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/BiMamba+/alg2.png" alt="그림1" /></p>

<h3 id="37-bidirectional-mamba-encoder">3.7. Bidirectional Mamba+ Encoder</h3>

<ul>
  <li>Channel-mixing이면 \(\mathbb{E}_x^{(l)} \in \mathbb{R}^{J \times M \times D}\) and \(\mathbb{E}_x^{(0)}=\mathbb{E}_{m i x}\)
    <ul>
      <li>otherwise \(\mathbb{E}_x^{(l)} \in \mathbb{R}^{M \times J \times D}\) and \(\mathbb{E}_x^{(0)}=\mathbb{E}_{\text {ind }}\)</li>
    </ul>
  </li>
  <li>2개의 Bi-Mamba+ block, 각각 forward and backward
    <ul>
      <li>각각의 input을 \(\mathbb{E}_{x, d i r}^{(l)}\) where \(\operatorname{dir} \in\{\text{forward,backward}\}\)이라 하면</li>
      <li>\(\mathbb{E}_x^{(l+1)}=\sum_{\text {dir }}^{\{\text {forward,backward }\}} \mathcal{F}\left(\mathbb{E}_{y, \text { dir }}^{(l)}, \mathbb{E}_{x, d i r}^{(l)}\right)\)가 다음 layer의 input이 됨</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/alg3.png" alt="그림1" /></p>

<h3 id="38-loss-function">3.8. Loss Function</h3>

<ul>
  <li>MSE : \(\mathcal{L}(Y, \hat{Y})=\frac{1}{\mid Y \mid} \sum_{i=1}^{\mid Y \mid}\left(y_{(i)}-\hat{y}_{(i)}\right)^2\)</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<h3 id="41-datasets">4.1. Datasets</h3>

<p><img src="/assets/img/Mamba/BiMamba+/table1.png" alt="그림1" /></p>

<h3 id="42-baseline-models">4.2. Baseline Models</h3>

<ul>
  <li>Autoformer (Wu et al. 2021)
    <ul>
      <li>series decomposition technique with Auto-Correlation mechanism</li>
    </ul>
  </li>
  <li>PatchTST (Nie et al. 2023)
    <ul>
      <li>patching and channel independent techniques</li>
    </ul>
  </li>
  <li>Crossformer (Zhang and Yan 2023)
    <ul>
      <li>PatchTST + Attention layer (for capture inter-series dependencies)</li>
    </ul>
  </li>
  <li>iTransformer (Liu et al. 2023)
    <ul>
      <li>inverts the modeling method of Transformer</li>
    </ul>
  </li>
  <li>DLinear (Zeng et al. 2023)
    <ul>
      <li>decomposes time series into two different components</li>
    </ul>
  </li>
  <li>TimesNet (Wu et al. 2022)
    <ul>
      <li>transforming the 1-D time series into a set of 2-D tensors</li>
    </ul>
  </li>
  <li>WITRAN (Jia et al. 2024)
    <ul>
      <li>RNN structure that process the univariate input sequence
        <ul>
          <li>in the 2-D space with a fixed scale</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>CrossGNN (Huang et al. 2024)
    <ul>
      <li>time series in a multi-scale way</li>
      <li>GNN to capture both cross-scale and cross-series dependencies</li>
    </ul>
  </li>
  <li>S-Mamba (Wang et al. 2024)
    <ul>
      <li>generates embeddings for each time series through a simple MLP layer</li>
      <li>and uses Mamba to extract inter-series dependencies</li>
    </ul>
  </li>
</ul>

<h3 id="43-experimental-settings">4.3. Experimental Settings</h3>

<ul>
  <li>\(L=96\) for all models on all datasets, \(H \in\{96,192,336,720\}\)</li>
  <li>\(S=\frac{1}{2} P\) and use patch length \(P=\frac{1}{4} L\)</li>
  <li>SRA decider, we set \(\lambda=0.6\)</li>
  <li>for <strong>Bi-Mamba+, PatchTST and Crossformer</strong> that use patching technique, we set \(D=128\) for Weather, Traffic, Electricity, Solar and \(D=64\) for ETT datasets,
    <ul>
      <li>while for <strong>S-Mamba and iTransformer</strong> that map the whole sequence to tokens, we set \(D=512\) for Weather, Traffic, Electricity, Solar and \(D=256\) for ETT datasets.</li>
    </ul>
  </li>
  <li>As for parameters within Mamba+ block은 Ahamed and Cheng 2024; Wang et al. 2024처럼</li>
  <li><strong>convolutional kernel size</strong> d_conv =2 and <strong>hidden state expansion</strong> expand =1 on all datasets.</li>
  <li><strong>hidden dimension</strong> d_state =16 for Weather, Electricity and Traffic and d_state =8 for ETT datasets.</li>
  <li><strong>encoder layer</strong> \(l \in\{1,2,3\}\),</li>
  <li><strong>learning rate</strong>는 \([5 \mathrm{e}-5,1 \mathrm{e}-4,2 \mathrm{e}-4,5 \mathrm{e}-4,1 \mathrm{e}-3, 2 \mathrm{e}-3,5 \mathrm{e}-3]\)</li>
</ul>

<h3 id="44-main-results">4.4. Main Results</h3>

<p><img src="/assets/img/Mamba/BiMamba+/table3.png" alt="그림1" /></p>

<h3 id="45-ablation-study">4.5. Ablation Study</h3>

<p><img src="/assets/img/Mamba/BiMamba+/table4.png" alt="그림1" /></p>

<ul>
  <li>(a) w/o SRA-I which use channel-independent strategy only</li>
  <li>(b) w/o SRA-M which use channelmixing strategy only</li>
  <li>(c) w/o Bi which use forward direction Mamba block only</li>
  <li>(d) w/o Residual that removes the residual connection</li>
  <li>(e) S-Mamba</li>
  <li>
    <p>(f) PatchTST used for the benchmark models</p>
  </li>
  <li><strong>filter threshold</strong> \(\lambda\)에 따른 tokenization strategy indicator</li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/fig3.png" alt="그림1" /></p>

<ul>
  <li>length of patches \(P\)에 따른 MSE</li>
</ul>

<p><img src="/assets/img/Mamba/BiMamba+/fig4.png" alt="그림1" /></p>

<h3 id="46-model-efficiency">4.6. Model efficiency</h3>

<p><img src="/assets/img/Mamba/BiMamba+/fig7-1.png" alt="그림1" /></p>

<p><img src="/assets/img/Mamba/BiMamba+/fig7-2.png" alt="그림1" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li><strong>Bi-Mamba+</strong></li>
  <li>adding forget gate in Mamba
    <ul>
      <li>to selectively combine the added new features with the forgotten historical features in a complementary manner,</li>
      <li>therefore preserving historical information in a longer range</li>
    </ul>
  </li>
  <li>dividing the time series into patches
    <ul>
      <li>for inter-series dependencies at a finer granularity</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="mamba" /><summary type="html"><![CDATA[[Arxiv 2024](https://arxiv.org/pdf/2404.15772)]]></summary></entry></feed>