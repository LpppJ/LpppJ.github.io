---
layout: post
related_posts:
  _
title: 
description: >
  [ICML 2023](https://arxiv.org/pdf/2306.05043)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# Timediff : Non-autoregressive Conditional Diffusion Models for Time Series Prediction (ICML 2023)

## Abstract

- TimeDiff : non-autoregressive diffusion model, w/ two novel conditioning mechanisms
  - future mixup : future prediction의 ground-truth의 일부를 conditioning하는 것을 허용
  - autoregressive initialization : time series의 basic pattern (short term trends 등)을 모델 initialization에 사용

## 1. Introduction

- Diffusion model (iterative denoising)은 이미지 생성에서 뛰어난 quality
  - 하지만 time series prediction을 위해 어떻게 쓸지에 대한 연구는 아직
  - time series는 **complex dynamics, nonlinear patterns, long-temporal dependencies**
- 기존 diffusion model들은 decoding strategy에 따라 구분됨
  - **Autoregressive** : future prediction이 one by one으로 generated (ex. Timegrad)
    - 하지만 error accumulation 때문에 long range prediction 성능이 떨어지고
    - 하나씩 예측하다보니 inference가 느리다는 단점이 있음
  - **Non-autoregressive** : CSDI, SSSD처럼 denoising networks에 intermediate layers를 conditioning으로 넣고 the denoising objective에 inductive bias를 introduce
    - 하지만 long-range prediction performance는 Fedformer, NBeats보다 떨어짐
    - 왜냐하면 conditioning 전략이 image, textf를 위한 것이지 time series를 위한 것이 아니기 때문
    - inductive bias를 위해 denoising objective를 사용하는 것만으로는 lookback window에서 유용한 정보를 알아내기 어렵다.
- 본 논문에서는 long time series prediction을 위한 conditional non-autoregressive diffusion model인 TimeDiff 제안
  - CSDI, SSSD와 다르게 conditioning module에 time series를 위한 additional inductive bias 도입
    - **future mixup**: randomly reveals parts of the ground-truth future pre- dictions during training
    - **autoregressive initialization**: better initializes the model with basic components in the time series

## 2. Preliminaries

### 2.1. Diffusion Models

pass

### 2.2. Conditional DDPMs for Time Series Prediction

- $$\mathbf{x}_{-L+1: 0}^0 \in \mathbb{R}^{d \times L}$$를 보고 $$\mathbf{x}_{1: H}^0 \in \mathbb{R}^{d \times H}$$를 예측하는 문제
- $$p_\theta\left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}\right)=p_\theta\left(\mathbf{x}_{1: H}^K\right) \prod_{k=1}^K p_\theta\left(\mathbf{x}_{1: H}^{k-1} \mid \mathbf{x}_{1: H}^k, \mathbf{c}\right)$$,
  - where $$\mathbf{x}_{1: H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)$$
- 아직 efficient denoising network $$\mu_{\theta}$$와 conditioning network $$\mathcal F$$ in time series diffusion models를 어떻게 디자인할 것인지 명확하지 않음
- [TimeGrad (ICML 2021)](https://arxiv.org/pdf/2101.12072)
  - autoregressive manner :
     $$\begin{aligned}
    p_\theta & \left(\mathbf{x}_{1: H}^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)\right) \\
    & =\prod_{t=1}^H p_\theta\left(\mathbf{x}_t^{0: K} \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right) \\
    & =\prod^H p_\theta\left(\mathbf{x}_t^K\right) \prod^K p_\theta\left(\mathbf{x}_t^{k-1} \mid \mathbf{x}_t^k, \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{-L+1: t-1}^0\right)\right)
    \end{aligned}$$
  - training objectives : $$\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_t^k, k \mid \mathbf{h}_t\right)\right\|^2\right]$$
  - autoregressive decoding 때문에 error accumulation이 발생하고 inference가 느리고 부정확함
- [CSDI(NeurIPS 2021)](https://arxiv.org/pdf/2107.03502)
  - time series $$\mathbf{x}_{-L+1: H}^0$$ 전체를 한 번에 diffusing and denoising
  - binary mask $$\mathbf{m} \in\{0,1\}^{d \times(L+H)}$$를 사용하여 self-supervised strategy 제안
  - training objectives : $$\mathcal{L}_\epsilon=\mathbb{E}_{k, \mathbf{x}^0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{x}_{\text {target }}^k, k \mid \mathbf{c}=\mathcal{F}\left(\mathbf{x}_{\text {observed }}^k\right)\right)\right\|^2\right]$$
- 하지만 CSDI의 한계는
  - Denoising networks가 2개의 transformers를 사용해서 complexity가 높다.
  - conditioning에 사용되는 masking은 vision의 inpainting이랑 비슷한데
    - [(Lugmayr et al., 2022)](https://arxiv.org/abs/2201.09865)에서는 이 방식이 masking과 observed사이의 부조화 발생한다고 밝힘
- [SSSD (TMLR 2022)](https://arxiv.org/pdf/2208.09399)
  - Transfermer를 structured state space model로 대체
  - 하지만 여전히 non-autoregressive strategy이라서 boundary disharmony가 발생할 수 있음

## 3. Proposed Model

- Conditional Diffusion의 conditioning은 semantic similarities across modalities 파악에 중점
- 하지만 현실에서의 non-stationary time series는 complex temporal dependencies 파악이 중요함

### 3.1. Forward Diffusion Process

- Forward process : $$\mathbf{x}_{1: H}^k=\sqrt{\bar{\alpha}_k} \mathbf{x}_{1: H}^0+\sqrt{1-\bar{\alpha}_k} \epsilon$$
  - where $$\epsilon$$ is sampled from $$\mathcal{N}(0, \mathbf{I})$$ with the same size as $$\mathbf{x}_{1: H}^0$$

### 3.2. Conditioning the Backward Denoising Process

- Illustration

![그림1](/assets/img/timeseries/timediff/fig1.png)

#### 3.2.1. FUTURE MIXUP

- 먼저 *future mixup*으로 $$\mathbf{z}_{\text {mix }}=\mathbf{m}^k \odot \mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)+\left(1-\mathbf{m}^k\right) \odot \mathbf{x}_{1: H}^0$$를 만든다.
  - past information’s mapping $$\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)$$과 the future ground-truth $$\mathbf{x}_{1: H}^0$$를 combine
  - training에서 적용되는 것이고, inference에서는 $$\mathbf{z}_{\text {mix }}=\mathcal{F}\left(\mathbf{x}_{-L+1: 0}^0\right)$$

#### 3.2.2. AUTOREGRESSIVE MODEL

- Non-autoregressive models는 masked와 observed의 경계에서 disharmony
  - 그래서 linear autoregressive (AR) model $$\mathcal{M}_{a r}$$ 사용. $$\mathbf{z}_{a r}=\sum_{i=-L+1}^0 \mathbf{W}_i \odot \mathbf{X}_i^0+\mathbf{B}$$
    - $$\mathbf{X}_i^0 \in \mathbb{R}^{d \times H}$$ is a matrix containing $$H$$ copies of $$\mathbf{x}_i^0$$,
    - $$\mathbf{W}_i$$ s $$\in \mathbb{R}^{d \times H}, \mathbf{B} \in \mathbb{R}^{d \times H}$$ are trainable parameters
- complex nonlinear time series는 approximate 못하는 건 사실이지만
  - simple patterns (short-term trends) 정도는 잘 잡으니까
  - 그리고 one by one으로 하는 것이 아니라 $$\mathbf{z}_{a r}$$의 모든 columns는 동시에 계산됨

#### 3.3. Denoising Network

- 먼저 the transformer’s sinusoidal position embedding으로 the diffusion-step embedding $$\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}$$​ 얻음
  - 즉 $$\begin{aligned}
    k_{\text {embedding }}= & {\left[\sin \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \sin \left(10^{\frac{w \times 4}{w-1}} t\right),\right.} \\
    & \left.\cos \left(10^{\frac{0 \times 4}{w-1}} t\right), \ldots, \cos \left(10^{\frac{w \times 4}{w-1}} t\right)\right],
    \end{aligned}$$​
    - where $$w=\frac{d^{\prime}}{2}$$
  - 그리고 $$\mathbf{p}^k=\operatorname{SiLU}\left(\mathrm{FC}\left(\operatorname{SiLU}\left(\mathrm{FC}\left(k_{\text {embedding }}\right)\right)\right)\right) \in \mathbb{R}^{d^{\prime} \times 1}$$
- 그 다음 $$\mathbf{p}^k\in \mathbb{R}^{d^{\prime}}$$는 diffused input $$\mathbf{x}_{1: H}^k$$의 embedding $$\mathbf{z}_1^k \in \mathbb{R}^{d^{\prime} \times H}$$에 합쳐짐 ($$2 d^{\prime} \times H$$이 됨)
  - $$\mathbf{z}_1^k$$는 여러 개의 convolution layers로 이루어진 input projection block을 통과시켜 얻음

- 그 다음 multilayer convolution-based **encoder** 통과하면 $$\mathbf{z}_2^k \in \mathbb{R}^{d^{\prime \prime} \times H}$$로 representation
- 그 다음 $$\mathbf{c}$$와 $$\mathbf{z}_2^k$$를 fuse해서 $$\left(2 d+d^{\prime \prime}\right) \times H$$로 만들고
  - multiple convolution layers **decoder**에 넣어서 $$\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right)\in \mathbb{R^{d \times H}}$$로 만듬 ($$\mathbf{x}_{1: H}^k$$와 같은 size)
- 마지막으로 $$\mu_{\mathbf{x}}\left(\mathbf{x}_\theta\right)=\frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}^k, k \mid \mathbf{c}\right)$$을 통해 denoised output $$\begin{aligned}
  \hat{\mathbf{x}}_{1: H}^{k-1}= & \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
  +\sigma_k \epsilon
  \end{aligned}$$얻음
- 흔히 아는 Diffusion에서는 noise $$\epsilon_\theta\left(\mathbf{x}_{1: H}^k, k\right)$$를 예측하지만, time series에서는  highly irregular noisy components라서 데이터 $$\mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k\right)$$를 예측

### 3.4. Training

- Pesudo code

![그림3](/assets/img/timeseries/timediff/code.png)

- 각각의 $$\mathbf{x}_{1: H}^0$$에 대해 batch of diffusion steps $$k$$’s를 sampling하고
  - conditioned variant of loss를 minimize: $$\min _\theta \mathcal{L}(\theta)=\min _\theta \mathbb{E}_{\mathbf{x}_{1 . H}^0, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), k} \mathcal{L}_k(\theta)$$

### 3.5. Inference

- Pesudo code

![그림4](/assets/img/timeseries/timediff/code.png)

- 먼저 noise vector $$\mathbf{x}_{1 \cdot H}^K \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \in \mathbb{R}^{d \times H}$$를 생성하고
- $$\begin{aligned}
  \hat{\mathbf{x}}_{1: H}^{k-1}= & \frac{\sqrt{\alpha_k}\left(1-\bar{\alpha}_{k-1}\right)}{1-\bar{\alpha}_k} \mathbf{x}_{1: H}^k+\frac{\sqrt{\bar{\alpha}_{k-1}} \beta_k}{1-\bar{\alpha}_k} \mathbf{x}_\theta\left(\mathbf{x}_{1: H}^k, k \mid \mathbf{c}\right) 
  +\sigma_k \epsilon
  \end{aligned}$$을 반복 ($$k=1$$ 까지) 
  - when $$k=1$$, $$\epsilon=0$$이므로 $$\hat{\mathbf{x}}_{1: H}^0$$를 final prediction으로 얻을 수 있음

## 4. Experiments

![그림12](/assets/img/timeseries/timediff/table2.png)

![그림13](/assets/img/timeseries/timediff/table3.png)

![그림2](/assets/img/timeseries/timediff/fig2.png)

### 4.3. Ablation study

- **The Effectiveness of Future mixup**

![그림14](/assets/img/timeseries/timediff/table4.png)

- 특히 ETTh1 데이터셋에서 future mixup을 안썼을 때 성능이 많이 떨어진다.

- **The Mixup Strategies in Future mixup**
  - Hard mixup : The sampled values in $$\mathbf{m}^k$$ are binarized by a threshold $$\tau \in (0,1)$$
  - Segment mixup : The mask  $$\mathbf{m}^k$$, Each masked segment has a length following the geometric distribution with a mean of 3. This is then followed by an unmasked segment with mean length $$3(1 − \tau)/\tau$$

![그림15](/assets/img/timeseries/timediff/table5.png)

- **Predicting $$\mathbf{x}_\theta$$ vs Predicting $$\epsilon_\theta$$**

![그림16](/assets/img/timeseries/timediff/table6.png)

### 4.4. Integration into Existing Diffusion Models

![그림17](/assets/img/timeseries/timediff/table7.png)

### 4.5. Inference Efficiency

![그림18](/assets/img/timeseries/timediff/table8.png)

## 5. Conclusion

- Timediff : diffusion model for time series prediction,
  - 1)future mixup과 2)autoregressive initialization이라는 conditioning mechanisms으로
  - conditioning network에 useful inductive bias를 추가
  - 한계점으로 변수의 개수가 많을 때 multivariate dependencies를 학습하기 어렵다
    - graph 사용 ? 