<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-07-06T12:22:55+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting (Arxiv 2023)</title><link href="http://localhost:4000/timeseries/2024-07-06-MultiResFormer/" rel="alternate" type="text/html" title="MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting (Arxiv 2023)" /><published>2024-07-06T00:00:00+09:00</published><updated>2024-07-06T12:22:53+09:00</updated><id>http://localhost:4000/timeseries/MultiResFormer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-07-06-MultiResFormer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer-based models는 TS를 segments로 나눠서 encode (<strong>patches</strong>)
    <ul>
      <li>다양한 <strong>scale</strong>(i.e.. <strong>resolutions</strong>)에서의 TS를 모델링</li>
    </ul>
  </li>
  <li>하지만 pre-defined scale(patch의 길이)은…
    <ul>
      <li>variety of intricate temporal dependencies를 찾기 어려움</li>
    </ul>
  </li>
  <li>그래서 MultiResFormer는 adaptive time scale.
    <ul>
      <li>patch length를 주어진 데이터의 주기성을 보고 찾겠다</li>
      <li>그 다음 intraperiod and interperiod dependencies 학습</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>TSF를 위한 vanila Transformer의 modification 3:</li>
  <li>첫째. Efficient attention mechanisms for sub-quadratic attention computation
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2012.07436">Informer(2021)</a>, <a href="https://openreview.net/pdf?id=0EXmFzUn5I">Pyraformer(2022)</a></li>
    </ul>
  </li>
  <li>둘째. Breaking the point-wise nature of dot-product attention for segment or series level dependency modeling
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2106.13008">Autoformer(2021)</a>, <a href="https://arxiv.org/pdf/2201.12740">Fedformer(2022)</a>, <a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>, <a href="https://openreview.net/pdf?id=vSVLM2j9eie">Crossformer(2023)</a></li>
    </ul>
  </li>
  <li>셋째. Modeling sequences at multiple time scales with hierarchical representation learning
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a>, <a href="https://arxiv.org/pdf/2204.13767">Triformer(2022)</a>, <a href="https://arxiv.org/pdf/2206.04038">Scaleformer(2023)</a>, <a href="https://openreview.net/pdf?id=lJkOCMP2aW">Pathformer(2024)</a></li>
    </ul>
  </li>
  <li>하지만 세 번째 multi-scale methods의 경우 pre-defined resolution으로 인해 generalization이 안된다.</li>
  <li>그래서 본 논문에서는 데이터의 underlying periodicities을 파악해서 데이터에 맞게 multi-resolution view</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig1.png" alt="그림1" /></p>

<ul>
  <li>본 논문에서는 two core Transformer sublayers를 repurpose:
    <ul>
      <li><strong>Multi-headed attention</strong> for “interperiod” variation modeling
        <ul>
          <li>MHA는 전역적인 패치 간 의존성을 모델링하는 데 강점을 가지고 있어 interperiod 변동을 모델링하는 데 적합</li>
        </ul>
      </li>
      <li><strong>Position-wise Feed-Forward network</strong> for “intraperiod” variation modeling
        <ul>
          <li>FFN은 각 위치 내의 복잡한 의존성을 모델링하는 데 강점을 가지고 있어 intraperiod 변동을 모델링하는 데 적합</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>고려해야 할 사항들
    <ul>
      <li>어떻게 different resolution branches끼리 parameter-sharing을 할까 ?
        <ul>
          <li>일단 parameter-sharing을 해야 특정 scale에 overfitting되는 걸 방지하는 건 맞음</li>
          <li>patch length 모르니까 linear projection 못 쓰고, 그냥 padding하는 건 모델 학습을 방해함</li>
          <li>그래서 각 scale에서의 patches의 길이를 맞추기 위한 <strong>interpolation scheme</strong> 사용</li>
          <li>그리고 <strong>resolution embedding</strong>으로 scale-awareness</li>
        </ul>
      </li>
      <li>계산 복잡도를 어떻게 줄일까 ?
        <ul>
          <li>이미 interpolation scheme으로 patches의 길이를 맞춰줬으니 별도의 embedding 필요 없음
            <ul>
              <li>Dlinear model의 성공 사례에서 영감 받음</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="time-series-transformer">Time series Transformer</h3>

<ul>
  <li>Transformer의 quadratic complexity 때문에 longer series는 모델링하기 어려웠음
    <ul>
      <li><a href="https://arxiv.org/pdf/1907.00235">LogTrans(2019)</a> : sparse attention blocks where each token attends to others with an exponential step size</li>
      <li><a href="https://arxiv.org/pdf/2012.07436">Informer(2021)</a> : entropy-based measurement to filter out uninformative keys for \(O(N)\)</li>
      <li><a href="https://arxiv.org/pdf/2204.13767">Triformer(2022)</a>, <a href="https://openreview.net/pdf?id=0EXmFzUn5I">Pyraformer(2022)</a> : adopt CNN-like approaches for local attention operations</li>
      <li><a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a> : patch단위로 attention 연산 하니까 \(O(N^2/S^2)\)</li>
    </ul>
  </li>
  <li>단일 시점의 데이터는 정보가 별로 없음 (<a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>)
    <ul>
      <li>단일 시점을 토큰으로 하는 transformer는 localized patterns을 간과할 수 있음</li>
    </ul>
  </li>
  <li>channel-mixing embedding은 over-fitting 발생시킬 수 있음 (<a href="https://arxiv.org/pdf/2211.14730">PatchTST(2023)</a>)</li>
</ul>

<h3 id="multi-resolution-time-series-modeling">Multi-Resolution Time Series Modeling</h3>

<ul>
  <li><a href="https://arxiv.org/pdf/2210.02186">TimesNet(2023)</a>에서 adaptive multi-resolution modeling 하긴 함
    <ul>
      <li>하지만 input length를 맞춰줘야 해서 flatten해야 하고 longer series 예측 못함</li>
      <li>channel-mixing embedding이 불가피해서 overfitting</li>
    </ul>
  </li>
</ul>

<h2 id="3-adaptive-multi-resolution-time-series-modeling-with-transformers">3. Adaptive Multi-Resolution Time Series Modeling with Transformers</h2>

<ul>
  <li>\(\mathbf{X}_{1 \ldots I}=\left(\mathbf{x}_1, \ldots, \mathbf{x}_I\right) \in \mathbb{R}^{I \times V}\)로 \(\mathbf{X}_{I+1 \ldots I+O}=\left(\mathbf{x}_{I+1}, \ldots, \mathbf{x}_{I+O}\right) \in \mathbb{R}^{O \times V}\)​ 예측</li>
</ul>

<h3 id="31-multiresformer">3.1. MultiResFormer</h3>

<p><img src="/assets/img/timeseries/MultiResFormer/fig2.png" alt="그림2" /></p>

<ul>
  <li>The periodicity-aware patching module, detecting salient periodicities (section 3.2.)</li>
  <li>The Transformer Encoder block, shared across all resolution branches (section 3.3.)</li>
  <li>aggregate the representations derived within each resolution branch into \(\mathbf{X}^{(l)} \in \mathbf{R}^{I \times V}\)​ (section 3.4.)</li>
</ul>

<h3 id="32-salient-periodicity-detection">3.2. Salient Periodicity Detection</h3>

<ul>
  <li>
    <p>salient periodicites of the input series는 Fast Fourier Transform (FFT)으로 찾음</p>

\[\begin{aligned}
\mathbf{A} &amp; =\operatorname{Avg}(\operatorname{Amp}(\operatorname{FFT}(\mathbf{X}))) \\
\left\{f_1, \ldots, f_k\right\} &amp; =\underset{f_* \in\left\{1, \ldots,\left\lfloor\frac{I}{2}\right\rfloor\right\}}{\operatorname{argTopk}}(\mathbf{A}) \\
\text { Period }_i &amp; =\left\lceil\frac{I}{f_i}\right\rceil
\end{aligned}\]
  </li>
  <li>
    <p>Gradient-based 방식이 아니라서 미분이 필요없음</p>
  </li>
</ul>

<h3 id="33-multi-resolution-modeling-with-a-shared-transformer-block">3.3. Multi-Resolution Modeling with a Shared Transformer Block</h3>

<ul>
  <li>PatchTST처럼 fixed patch length 사용할 땐 high-dimensional embeddings을 위한 linear transformations가 필요했는데, 이제는 patch embedding layers가 필요없어짐 (efficient)</li>
  <li>다양한 patch length를 사용한다고 해서 길이를 맞춰주기 위해 padding을 한다고 하더라도, MHA와 FFN에는 masking mechanism이 없기 때문에 모델 성능 저하
    <ul>
      <li>그러므로 interpolation으로 original patches의 temporal characteristics 보존</li>
    </ul>
  </li>
  <li>one resolution branch의 Periodi가 주어지면 길이 Periodi의 겹치치 않는 patch로 분할
    <ul>
      <li>그 다음 길이 d가 되도록 linearly interpolate</li>
      <li>shape of the patch-based representation of the input series : \(V \times\left\lceil\frac{I}{\text { Period }_i}\right\rceil \times d\)</li>
    </ul>
  </li>
  <li>각 resolution branch에서 same-sized patches로 연산이 이루어지기 때문에 resolution embedding을 linearly interpolate에 더해줌 (transformer block에게 해상도 알려주기 위해)</li>
  <li>MHA to capture patch-wise dependencies (interperiod variation modeling)
    <ul>
      <li>FFN layers for capturing dependencies within each patch (intraperiod variation modeling)</li>
    </ul>
  </li>
</ul>

<h3 id="34-adaptive-aggregation">3.4. Adaptive Aggregation</h3>

<ul>
  <li>i 번째 resolution에서 representation의 shape은 \(V \times\left\lceil\frac{I}{\text { Period }_i}\right\rceil \times d\)​</li>
  <li>interpolation으로 \(I \times V\)로 representation</li>
  <li>
    <p>\(I \times V\)으로 표현된 모든 k개의 resolution에서의 representation을 adaptive aggregation:</p>

\[\begin{aligned}
  &amp; \left\{A_1, \ldots, A_k\right\}=\underset{f_* \in\left\{1, \ldots,\left\lfloor\frac{I}{2}\right\rfloor\right\}}{\operatorname{Topk}}(\mathbf{A}) \\
  &amp; \left\{w_1, \ldots, w_k\right\}=\operatorname{Softmax}\left(\left\{A_1, \ldots, A_k\right\}\right)
  \end{aligned}\]
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>Main Results</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/table12.png" alt="그림3" /></p>

<ul>
  <li>Ablation Study</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/table4.png" alt="그림4" /></p>

<ul>
  <li>Varying Look-back Window Size</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig3.png" alt="그림5" /></p>

<ul>
  <li>Representation analysis</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig4.png" alt="그림6" /></p>

<ul>
  <li>Efficiency Comparison</li>
</ul>

<p><img src="/assets/img/timeseries/MultiResFormer/fig5.png" alt="그림7" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>각 transformer blocks 내에서 FFT로 데이터의 underlying periodicities를 파악하고 resolution 결정</li>
  <li>각 transformer blocks 내에서 interpolation 덕분에 resolution끼리 parameter sharing</li>
  <li>블록 내의 encoder는 interpolation으로 input size와 output representation의 size가 같아서
    <ul>
      <li>embedding layer가 필요없고 final linear prediction head layer에서의 parameters 개수가 적음</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Arxiv 2023](https://arxiv.org/pdf/2311.18780)]]></summary></entry><entry><title type="html">Neyman-Pearson Hypothesis Testing</title><link href="http://localhost:4000/stat/2024-07-02-NPtest/" rel="alternate" type="text/html" title="Neyman-Pearson Hypothesis Testing" /><published>2024-07-02T00:00:00+09:00</published><updated>2024-07-04T09:50:22+09:00</updated><id>http://localhost:4000/stat/NPtest</id><content type="html" xml:base="http://localhost:4000/stat/2024-07-02-NPtest/"><![CDATA[<h2 id="basics-of-hypothesis-testing">Basics of Hypothesis Testing</h2>

<ul>
  <li>일반적으로 가설 검정을 construction하는 과정은 아래와 같다.
    <ul>
      <li>Test statistic \(T_n=T_n\left(X_1, \ldots, X_n\right)\) 선택</li>
      <li>Rejection region (기각역) 설정</li>
      <li>만약 \(T_n \in R\)이면 귀무가설 기각, 그렇지 않으면 기각할 수 없다.</li>
    </ul>
  </li>
  <li>가설 검정을 하는 이유는 귀무가설이 참인지 거짓인지 판단하기 위함이 아니다.
    <ul>
      <li>정확히는 <strong>귀무가설을 기각할 충분한 evidence</strong>가 있는지를 판단하기 위함이다.</li>
    </ul>
  </li>
</ul>

<h2 id="neyman-pearson-paradigm">Neyman-Pearson paradigm</h2>

<ul>
  <li>
    <p>\(H_0: \theta \in \Theta_0 \quad\) versus \(\quad H_1: \theta \in \Theta_1\)</p>
  </li>
  <li>Pick an \(\alpha \in(0,1)\)​</li>
  <li>Then try to maximize \(\beta(\theta)\) over \(\Theta_1\) subject to \(\sup _{\theta \in \Theta_0} \beta(\theta) \leq \alpha\)
    <ul>
      <li>where Power function : \(\beta(\theta)=P_\theta\left(\left(X_1, \ldots, X_n\right) \in R\right)\)​</li>
    </ul>
  </li>
  <li>가설 검정할 때 test statistic이 \(Z_{1-\alpha}\)보다 크면 귀무가설 기각하고… 어쩌고 그게 어디서 나온 것인지를 아래 예시를 통해 알아보자.</li>
</ul>

<h3 id="example--one-sided-test">example : One sided test</h3>

<ul>
  <li>Suppose \(X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, \sigma^2\right)\) with \(\sigma^2\) known</li>
  <li>\(H_0: \theta=\theta_0 \quad\) versus \(\quad H_1: \theta&gt;\theta_0\) (one sided)</li>
  <li>natural test statistic은 \(T_n\left(X_1, \ldots, X_n\right)=\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta_0}{\sigma / \sqrt{n}}\)이다.</li>
  <li>Power function : \(\beta(\theta)=P_\theta\left(T_n&gt;t\right)=P_\theta\left(\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta}{\sigma / \sqrt{n}}&gt;t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\)에서
    <ul>
      <li>Thresholdld \(t\)를 결정해야 한다.</li>
    </ul>
  </li>
  <li>\(\theta\)는 true parameter라는 점에서 \(\frac{\frac{1}{n} \sum_{i=1}^n X_i-\theta}{\sigma / \sqrt{n}} \sim N(0,1)\)이므로 \(\beta(\theta)=P\left(Z&gt;t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)=1-\Phi\left(t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\)이다.</li>
  <li>이제 Neyman-Pearson paradigm을 implement한다.
    <ul>
      <li>\(\sup _{\theta \in \Theta_0}\left\{1-\Phi\left(t+\frac{\theta_0-\theta}{\sigma / \sqrt{n}}\right)\right\} \leq \alpha\)인데 귀무가설에서는 \(\theta = \theta_0\)이므로</li>
      <li>\(1-\Phi(t) \leq \alpha\)이고 \(t\)에 대해 정리하면 \(t=\Phi^{-1}(1-\alpha)\)이다.</li>
    </ul>
  </li>
</ul>

<h2 id="neyman-pearson-procedure">Neyman-Pearson Procedure</h2>

<ul>
  <li>먼저 test function을 정의한다.
    <ul>
      <li>\(\phi(\boldsymbol{x})= \begin{cases}1, &amp; \text { if } \boldsymbol{x} \in R \\ 0, &amp; \text { if } \boldsymbol{x} \notin R\end{cases}\) , 즉 \(\phi(\boldsymbol{x})=1\)은 귀무가설 기각을 의미한다.</li>
      <li>이 notation에 따르면 \(\text { power }=\int \phi(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x}\) 이고 \(\text { size }=\int \phi(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}\)이다.</li>
    </ul>
  </li>
  <li>Neyman–Pearson test statistic은 likelihood ratio이다.
    <ul>
      <li>\(\Lambda(\boldsymbol{x})=\frac{L\left(\theta_0 \mid \boldsymbol{x}\right)}{L\left(\theta_1 \mid \boldsymbol{x}\right)}=\frac{f_0(\boldsymbol{x})}{f_1(\boldsymbol{x})}\)로 setting하고 \(P_0\left(\Lambda(\boldsymbol{X}) \leq t^*\right)=\alpha\)가 되도록 \(t^*\)를 결정한다.</li>
      <li>이렇게 likelihood ratio로 test function을 결정하는 방식이 유의수준이 \(\alpha\)인 모든 test 중에서 가장 power가 높다.</li>
      <li>이것을 증명하는 것이 The Neyman–Pearson Lemma이다.</li>
    </ul>
  </li>
</ul>

<h2 id="the-neymanpearson-lemma">The Neyman–Pearson Lemma</h2>

<ul>
  <li>
    <p>한 마디로 말하자면 귀무가설 하에서 type 1 error를 유의수준 \(\alpha\)와 같게 했을 때, type 2 error의 확률이 최소화된다는 것 (most powerful)</p>
  </li>
  <li>Consider a test with hypotheses \(H_0: \theta=\theta_0\) and \(H_1: \theta=\theta_1\)
    <ul>
      <li>where the pdf (or pmf) is \(f_i(\boldsymbol{x})\) for \(i=0,1\).</li>
    </ul>
  </li>
  <li>Consider the Neyman-Pearson test \(\phi_{\mathrm{NP}}(\boldsymbol{x})=\mathbb{1}\left(\frac{f_0(\boldsymbol{x})}{f_1(\boldsymbol{x})} \leq t^*\right)\),
    <ul>
      <li>where \(t^*\) is chosen such that \(\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}=\alpha\).</li>
    </ul>
  </li>
  <li>Consider any arbitrary test \(\phi_A(\boldsymbol{x})\) such that \(\int \phi_A(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x} \leq \alpha\),
    <ul>
      <li>i.e. level \(\alpha\) test.</li>
    </ul>
  </li>
  <li>Then the power of \(\phi_A(\boldsymbol{x})\) is at most the power of the Neyman-Pearson test,
    <ul>
      <li>that is \(\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x} \geq \int \phi_A(\boldsymbol{x}) f_1(\boldsymbol{x}) d \boldsymbol{x}\)</li>
    </ul>
  </li>
</ul>

<h3 id="proof">Proof</h3>

<ul>
  <li>
    <p>먼저 \(\int \underbrace{\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right)}_{T_1} \underbrace{\left(f_1(\boldsymbol{x})-\frac{f_0(\boldsymbol{x})}{t^*}\right)}_{T_2} d \boldsymbol{x} \geq 0\)​를 보인다.</p>

    <ul>
      <li>NP와 arbitrary test가 같은 결정을 내렸다면 \(T_1=0\)이므로 성립한다.</li>
      <li>NP와 arbitrary test가 다른 결정을 내렸다면 \(T_1\)과 \(T_2\)의 부호가 같으므로 성립한다.</li>
    </ul>
  </li>
  <li>
    <p>이제 다음과 같이 전개가 가능하다.</p>

    <p>\(\begin{aligned}
\int\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right) f_1(\boldsymbol{x}) d \boldsymbol{x} &amp; \geq \frac{1}{t^*} \int\left(\phi_{\mathrm{NP}}(\boldsymbol{x})-\phi_A(\boldsymbol{x})\right) f_0(\boldsymbol{x}) d \boldsymbol{x} \\
&amp; =\frac{1}{t^*}(\underbrace{\int \phi_{\mathrm{NP}}(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}}_{=\alpha}-\underbrace{\int \phi_A(\boldsymbol{x}) f_0(\boldsymbol{x}) d \boldsymbol{x}}_{\leq \alpha} \geq 0
\end{aligned}\)​</p>
  </li>
  <li>
    <p>위 결과를 통해 the power of the NP test가 the power of any other test보다 크다는 것을 알 수 있다.</p>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[Basics of Hypothesis Testing]]></summary></entry><entry><title type="html">Minimax Estimator and Stein’s Paradox</title><link href="http://localhost:4000/stat/2024-06-27-minimax/" rel="alternate" type="text/html" title="Minimax Estimator and Stein’s Paradox" /><published>2024-06-27T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/stat/minimax</id><content type="html" xml:base="http://localhost:4000/stat/2024-06-27-minimax/"><![CDATA[<h2 id="1-minimax-estimator">1. Minimax Estimator</h2>

<ul>
  <li>Minimax estimator는 최악이 가장 좋은 estimator이다.</li>
  <li>최악이라는 말은 true parameter \(\theta\)에 대한 estimator의 risk이다.
    <ul>
      <li>Risk : \(R(\theta, \widehat{\theta}(X))=\mathbb{E}_{X \sim f_\theta}[(\widehat{\theta}(X) - \theta)^2]\)</li>
    </ul>
  </li>
  <li>즉 Minimax estimator \(\hat \theta\)는 아래를 만족하는 estimator이다.
    <ul>
      <li>: \(\sup _{\theta \in \Theta} R(\theta, \widehat{\theta})=\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta})\)</li>
    </ul>
  </li>
</ul>

<h3 id="boundig-the-minimax-risk">Boundig the Minimax Risk</h3>

<ul>
  <li>
    <p>Minimax Risk의 Upper bound와 Lower bound에 대해서 알아보자</p>
  </li>
  <li>
    <p>Upper bound는 그냥 다른 estimator의 maximum risk를 사용하면 된다.</p>
  </li>
  <li>
    <p>Lower bound를 찾는 방법은 아래와 같다.</p>

    <ul>
      <li>Bayes estimator의 Bayes risk는 (어떤 prior \(\pi\)를 사용하더라도) minimax risk의 lower bound가 된다.</li>
      <li>
        <p>수식으로 표현하면 \(B_\pi\left(\widehat{\theta}_{\text {low }}\right) \leq B_\pi\left(\hat{\theta}_{\text {minimax}}\right) \leq \sup _\theta R\left(\theta, \hat{\theta}_{\text {minimax}}\right)=\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta})\)</p>
      </li>
      <li>직관적인 의미는 minimax risk는 risk의 maximum을 고려하지만, bayes risk는 risk를 prior를 가중치로 한 weighted average이다.</li>
      <li>평균은 최댓값보다 작다.</li>
    </ul>
  </li>
</ul>

<h3 id="example--d-dim-gaussian">Example : d-dim Gaussian</h3>

<ul>
  <li>
    <p>\(X_1, \ldots, X_n \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, I_d\right)\), then the average \(\widehat{\theta}=\frac{1}{n} \sum_{i=1}^n X_i\)가 minimax estimator of \(\theta\) w.r.t the squared loss임을 보이자.</p>
  </li>
  <li>
    <p>Upper bound</p>

    <ul>
      <li>\(\widehat{\theta} \sim N\left(\theta, I_d / n\right)\)의 Risk는 \(R(\theta, \widehat{\theta})=\mathbb{E}\left[\sum_{i=1}^d\left(\hat{\theta}_i-\theta_i\right)^2\right]=\mathbb{E}\left[\sum_{i=1}^d Z_i^2\right]=\frac{d}{n}\)이므로</li>
      <li>\(\inf _{\widetilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta}) \leq R(\theta, \widehat{\theta})=\frac{d}{n}\)이다.</li>
    </ul>
  </li>
  <li>
    <p>Lower bound</p>

    <ul>
      <li>먼저 Bayes estimator를 구하고 bayes risk를 구할 것이다.</li>
      <li>\(\theta \in \mathbb R\)이므로 prior는 \(N(0, c^2I)\)를 사용한다. \(c^2\)가 매우 크면 non-informative prior이다.</li>
      <li>Bayes estimator는 \(\hat \theta_{\text{bayes}}=\frac{c^2}{c^2+\frac{1}{n}}\hat \theta\)이다.</li>
      <li>
\[R\left(\theta, \widehat{\theta}_{\text {Bayes }}\right)=\mathbb{E}_{X_1, \ldots, X_X \stackrel{\text { i.i.d. }}{\sim} N\left(\theta, I_d\right)}\left\|\frac{c^2}{c^2+1 / n} \widehat{\theta}-\theta\right\|^2 = \frac{\|\theta\|_2^2}{n^2 \beta^2}+\frac{c^4}{\beta^2} \frac{d}{n}\]
        <ul>
          <li>where \(\widehat{\theta}=\theta+W\) and \(W \sim N\left(0, I_d / n\right)\), and \(\beta:=c^2+1 / n\)</li>
        </ul>
      </li>
      <li>이제 Bayes risk는 다음과 같다.
        <ul>
          <li>
\[\begin{aligned}
B_\pi\left(\frac{c^2}{c^2+1 / n} \widehat{\theta}\right) &amp; =\mathbb{E}_{\theta \sim \pi}\left[R\left(\theta, \widehat{\theta}_{\text {Bayes }}\right)\right] \\
&amp; =\frac{c^2 d}{n^2 \beta^2}+\frac{c^4}{\beta^2} \frac{d}{n}=\frac{c^2 d}{n \beta}=\frac{d}{n\left(1+1 /\left(n c^2\right)\right)}
\end{aligned}\]
          </li>
        </ul>
      </li>
      <li>그러므로 \(\frac{d}{n\left(1+1 /\left(n c^2\right)\right)} \leq R_n \leq \frac{d}{n}\)인데 c는 arbitrary이니까 무한대로 보낼 수 있다.</li>
      <li>upper bound와 lower bound가 모두 \(\frac{d}{n}\)이므로 \(\hat \theta\)는 minimax estimator이다.</li>
    </ul>
  </li>
</ul>

<h2 id="2-steins-paradox">2. Stein’s Paradox</h2>

<ul>
  <li>
    <p>먼저 admissible의 의미를 이해해야 한다.</p>

    <ul>
      <li>Estimator가 admissible하다는 것은</li>
      <li>모든 true parameters에 대해 다른 estimator보다 risk가 작거나 같고,</li>
      <li>적어도 하나의 true parameters에 대해 다른 estimator보다 risk가 작다는 뜻이다.</li>
    </ul>
  </li>
  <li>
    <p>모든 Minimax estimator가 admissible한 것은 아니다. 특히 parameter의 dimension이 3 이상일 때 그렇다.</p>
  </li>
  <li>
    <p>예를 들어 \(Y_i \sim N\left(\theta_i, 1\right) \quad \text { for } i \in\{1, \ldots, d\}\) 세팅에서는 minimax estimator는 \(\widehat{\theta}=Y=\left(Y_1, \ldots, Y_d\right)^{\top}\)이다.</p>
  </li>
  <li>
    <p>하지만 James-Stein estimator \(\widehat{\theta}_{\mathrm{JS}}=\left(1-\frac{d-2}{\|Y\|^2}\right) Y\)는 \(\mathbb{E}\left[\left\|\widehat{\theta}_{\mathrm{JS}}-\theta\right\|^2\right]=d-(d-2)^2 \mathbb{E}\left[\frac{1}{\|Y\|^2}\right]&lt;d\)​이다.</p>

    <ul>
      <li>즉 James-Stein estimator의 risk가 더 작기 때문에 minimax estimator가 admissible하지 않다.</li>
    </ul>
  </li>
  <li>
    <p><strong>Proof</strong></p>

    <ul>
      <li>
        <p>Squared term을 전개하면 \(\mathbb{E}\left[\left\|\widehat{\theta}_{\mathrm{JS}}-\theta\right\|^2\right]=\mathbb{E}\left[\|Y-\theta\|^2\right]+\mathbb{E}\left[\frac{(d-2)^2}{\|Y\|^2}\right]-2(d-2) \sum_{i=1}^d \mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]\)</p>
      </li>
      <li>
        <p>가장 우측에 있는 \(\mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]\)를 보자</p>

        <ul>
          <li>
            <p>\(\|Y-\theta\|^2 \sim \chi^2(d)\)이므로 expectation이 d이다.</p>
          </li>
          <li>
            <p>Gaussian setting이므로 \(\mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]=\int \cdots \int \underbrace{\frac{y_i}{\|y\|^2}}_{=g_i(y)} \underbrace{\frac{y_i-\theta}{(2 \pi)^{d / 2}} e^{-\|y-\theta\|^2 / 2}}_{=-\frac{\partial f(y)}{\partial y_i}} d y_1 \cdots d y_d\)이다.</p>
          </li>
          <li>
            <p>아래와 같은 계산 과정을 통해 \(\sum_{i=1}^d \mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right]=(d-2) \mathbb{E}\left[\frac{1}{\|Y\|^2}\right]\)이다.</p>

\[\begin{aligned}
\mathbb{E}\left[\frac{Y_i\left(Y_i-\theta_i\right)}{\|Y\|^2}\right] &amp; =\int \cdots \int \frac{\partial g_i(y)}{\partial y_i} f(y) d y_1 \cdots d y_d \\
&amp; =\int \cdots \int \frac{\|y\|^2-2 y_i^2}{\|y\|^4} f(y) d y_1 \cdots d y_d \\
&amp; =\mathbb{E}\left[\frac{1}{\|Y\|^2}\right]-\mathbb{E}\left[\frac{2 Y_i^2}{\|Y\|^4}\right]
\end{aligned}\]
          </li>
          <li>
            <p>이걸 첫 줄의 식에 대입하면 증명 끝이다.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[1. Minimax Estimator]]></summary></entry><entry><title type="html">Analysis of non-parametric kernel regression</title><link href="http://localhost:4000/stat/2024-05-26-kernel_reg/" rel="alternate" type="text/html" title="Analysis of non-parametric kernel regression" /><published>2024-05-26T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/stat/kernel_reg</id><content type="html" xml:base="http://localhost:4000/stat/2024-05-26-kernel_reg/"><![CDATA[<ul>
  <li>일반적인 regression은 response variable Y와 covariate X의 관계를 이해하는 방법 중 하나로 일종의 conditional expectation이다. <br />
\(r(x)=\mathbb{E}[Y \mid X=x]=\int y f(y \mid x) dy\)</li>
  <li>non-parametric regession을 하는 기본적인 방법 중 하나는 kernel regression이다. <br />
estimator는 \(\widehat{r}(x)=\sum_{i=1}^n w_i(x) Y_i\) where \(w_i(x)=\frac{K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)}\)
    <ul>
      <li>weights는 x 주변에 높은 가중치를 두고, h는 smoothing을 결정하는 bandwidth이다.</li>
      <li>간단한 kernel의 예시로 Gaussian kernel을 사용한다. \(K(x)=\frac{1}{\sqrt{2 \pi}} \exp \left(-x^2 / 2\right)\)</li>
    </ul>
  </li>
</ul>

<h2 id="assumptions">Assumptions</h2>
<ul>
  <li>증명을 복잡하지 않게 하기 위한 가정 \(y_i=r\left(x_i\right)+\epsilon_i\), where
    <ul>
      <li><strong>Design Assumption</strong> : \(x_i\) is one-dimensional, and <strong>equally spaced</strong> on \([0,1]\)
        <ul>
          <li>사실 꼭 필요한 가정은 아니지만 특정 x 근처에 다른 x가 존재함을 가정하기 위한 가정이다.</li>
        </ul>
      </li>
      <li><strong>Regression function Assumption</strong> : \(r(x)=\mathbb{E}[Y \mid X=x] \text { is } L \text {-Lipschitz}\) i.e. \(\left\vert\frac{d}{d x} r(x)\right\vert \leq L\)</li>
      <li><strong>Noise Assumption</strong> : \(\mathbb{E}\left[\epsilon_i\right]=0, \operatorname{Var}\left[\epsilon_i\right]=\sigma^2\) and iid</li>
      <li><strong>Kernel Assumption</strong> : spherical kernel \(K(x)=\mathbb{1}(-1 \leq x \leq 1)\)</li>
    </ul>
  </li>
</ul>

<h2 id="kernel-regression">Kernel Regression</h2>
<ul>
  <li>Under the <strong>Assumptions</strong> with \(h \ge 1/(n-1)\), <br />
\(\begin{aligned} R(\widehat{r}, r) &amp; = MSE(\widehat{r}, r) \\ &amp; =\int_0^1\{\widehat{r}(x)-r(x)\}^2 d x \\ &amp; =\int_0^1 \operatorname{bias}^2(x) d x+\int_0^1 \operatorname{Var}(\widehat{r}(x)) d x \leq L^2 h^2+\frac{\sigma^2}{(n-1) h} \end{aligned}\)</li>
  <li>Proof : 적분 구간이 [0,1]이므로 \(\max _{x \in[0,1]}\left\vert\operatorname{bias}(x)\right\vert \leq L h\), 그리고 \(\max _{x \in[0,1]} \operatorname{Var}(\widehat{r}(x)) \leq \frac{\sigma^2}{(n-1) h}\)를 증명
    <ul>
      <li><strong>Bounding the bias</strong> <br />
\(\begin{aligned} \left\vert \operatorname{bias}(x)\right\vert=\left\vert \mathbb{E} \widehat{r}(x)-r(x)\right\vert &amp; \stackrel{\text { (i) }}{=}\left\vert \mathbb{E}\left[\sum_{i=1}^n\left(w_i(x)\left(Y_i-r(x)\right)\right)\right]\right\vert \\ &amp;  \stackrel{\text { (ii) }}{=} \left\vert\sum_{i=1}^n\left(w_i(x)\left(r\left(X_i\right)-r(x)\right)\right)\right\vert \\ &amp; \stackrel{\text { (iii) }}{\leq} \sum_{i=1}^n w_i(x)\left\vert r\left(X_i\right)-r(x)\right\vert \\ &amp; \stackrel{\text { (iv) }}{\leq} L h \sum_{i=1}^n w_i(x)=L h,\end{aligned}\) <br />
(i) is holds since \(\sum_{i=1}^n w_i(x)=\frac{\sum_{i=1}^n K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)}=1\) <br />
(ii) follows by \(\mathbb{E}\left[Y_i \mid X_i\right]=r\left(X_i\right)\) <br />
(iii) uses triangle inequality <br />
(iv) if \(\left\vert X_i-x\right\vert \leq h,\) then \(\left\vert r\left(X_i\right)-r(x)\right\vert \leq Lh\) by the Lipschitz  and if \(\left\vert X_i-x\right\vert&gt;h,\) then \(w_i(x)=0\)</li>
      <li><strong>Bounding the variance</strong>
        <ul>
          <li>먼저 weights에 대한 bound \(w_i(x)=\frac{K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)}=\frac{\mathbb{1}\left(\left\vert x-X_i\right\vert \leq h\right)}{\sum_{j=1}^n \mathbb{1}\left(\left\vert x-X_j\right\Vert \leq h\right)} \leq \frac{1}{(n-1) h}\)를 보인다.</li>
          <li>즉 \(\sum_{j=1}^n \mathbb{1}\left(\left\vert x-X_j\right\vert \leq h\right) \geq (n-1)h\)를 보인다.
            <ul>
              <li>모든 \(h \geq 1 /(n-1)\)에 대해 \(\min _{x \in[0,1]} \sum_{j=1}^n \mathbb{1}\left(\left\vert x-X_j\right\vert \leq h\right)=\sum_{j=1}^n \mathbb{1}\left(\left\vert X_1-X_j\right\vert \leq h\right)\)이므로 (sum is minimized at the boundary)</li>
              <li>\(h \in\left[\frac{k}{n-1}, \frac{k+1}{n-1}\right)\)의 경우에 대해 <br />
\(\\ \begin{aligned} \min _{x \in[0,1]} \sum_{j=1}^n \mathbb{1}\left(\left\vert x-X_j\right\vert \leq h\right) &amp; =\sum_{j=1}^n \mathbb{1}\left(\left\vert X_1-X_j\right\vert \leq h\right) \\ &amp; \geq \sum_{j=1}^n \mathbb{1}\left(\left\vert X_1-X_j\right\vert \leq \frac{k}{n-1}\right)=k+1 \\ &amp; \geq(n-1) h \end{aligned}\)</li>
              <li>그러므로 <br />
\(\\ \begin{aligned} \operatorname{Var}(\widehat{r}(x)) &amp; =\mathbb{E}\left[(\widehat{r}(x)-\mathbb{E}(\widehat{r}(x)))^2\right]=\mathbb{E}[(\sum_{i=1}^n(w_i(x) \underbrace{\left(Y_i-r\left(X_i\right)\right.}_{=\epsilon_i})))^2] \\ &amp; =\mathbb{E}\left[\left(\sum_{i=1}^n \epsilon_i w_i(x)\right)^2\right] \\ &amp; =\sum_{i=1}^n w_i(x)^2 \mathbb{E}\left(\epsilon_i^2\right)=\sigma^2 \sum_{i=1}^n w_i(x)^2 \\ &amp; \leq \sigma^2 \max _{1 \leq i \leq n} w_i(x) \sum_{i=1}^n w_i(x) \leq \frac{\sigma^2}{(n-1) h} \end{aligned}\)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Proof : 적분 구간이 [0,1]이므로 \(\max _{x \in[0,1]}\left\vert\operatorname{bias}(x)\right\vert \leq L h\), 그리고 \(\max _{x \in[0,1]} \operatorname{Var}(\widehat{r}(x)) \leq \frac{\sigma^2}{(n-1) h}\)이 증명되었다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[일반적인 regression은 response variable Y와 covariate X의 관계를 이해하는 방법 중 하나로 일종의 conditional expectation이다. \(r(x)=\mathbb{E}[Y \mid X=x]=\int y f(y \mid x) dy\) non-parametric regession을 하는 기본적인 방법 중 하나는 kernel regression이다. estimator는 \(\widehat{r}(x)=\sum_{i=1}^n w_i(x) Y_i\) where \(w_i(x)=\frac{K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)}\) weights는 x 주변에 높은 가중치를 두고, h는 smoothing을 결정하는 bandwidth이다. 간단한 kernel의 예시로 Gaussian kernel을 사용한다. \(K(x)=\frac{1}{\sqrt{2 \pi}} \exp \left(-x^2 / 2\right)\)]]></summary></entry><entry><title type="html">Chernoff Bound (upper bound on the tail probability)</title><link href="http://localhost:4000/stat/2024-05-23-ChernoffBound/" rel="alternate" type="text/html" title="Chernoff Bound (upper bound on the tail probability)" /><published>2024-05-23T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/stat/ChernoffBound</id><content type="html" xml:base="http://localhost:4000/stat/2024-05-23-ChernoffBound/"><![CDATA[<h2 id="prerequirement">Prerequirement</h2>
<ul>
  <li>Markov inequality \(\mathrm{P}(X \geq a) \leq \frac{\mathrm{E}(X)}{a}\)</li>
  <li>Chebyshev inequality \(\operatorname{Pr}(\mid X-\mu\mid \geq k \sigma) \leq \frac{1}{k^2}\)</li>
</ul>

<h2 id="chernoff-bound">Chernoff Bound</h2>
<ul>
  <li>확률변수 X에 대한 tail probability <br />
\(\begin{aligned}P(X \geqslant a) &amp; =P\left(e^{t X} \geqslant e^{t a}\right) \\ &amp; \leqslant \frac{\mathbb{E}\left[e^{t X}\right]}{e^{t a}} \text { by Markov inequality } . \\ &amp; =M(t) e^{-t a}\end{aligned}\)</li>
</ul>

<h2 id="example--bernoulli-confidence-sets-vs-hoeffdings-inequality">Example : Bernoulli Confidence sets vs. Hoeffding’s inequality</h2>
<ul>
  <li>Under asymptotic normality <br />
\(\begin{aligned}P_\theta\left(\theta \in C_n\right)&amp;=P_\theta\left(\widehat{\theta}-z_{\alpha / 2} \sqrt{\operatorname{Var}_\theta[\widehat{\theta}]} \leq \theta \leq \widehat{\theta}+z_{\alpha / 2} \sqrt{\operatorname{Var}_\theta[\widehat{\theta}]}\right)\\C_n&amp;=\left(\widehat{p}-z_{\alpha / 2} \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}, \widehat{p}+z_{\alpha / 2} \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\right)\end{aligned}\)</li>
  <li>Hoeffding’s inequality <br />
\(P(\mid \widehat{p}-p\mid \geq t) \leq 2 e^{-2 n t^2}\)</li>
  <li><strong>Under asymptotic normality로 얻은 Confidence sets은 항상 Hoeffding’s inequality 보다 tight(short)하다.</strong>
    <ul>
      <li><strong>증명</strong> :  \(z_{\alpha / 2} \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}} \leq \sqrt{\frac{\log (2 / \alpha)}{2 n}}\)에서 \(\widehat{p}(1-\widehat{p}) \le \frac{1}{4}\) 이므로</li>
      <li>(1) Exponential Markov Inequality<br />
\(\begin{aligned} P(z \geqslant z) &amp; =P\left(e^{t Z} e^{t z}\right) \\ &amp; \leqslant \frac{\mathbb{E}\left[e^{t Z}\right]}{e^{t z}}=e^{\frac{t^2}{2}-t z} \\ &amp; \leqslant e^{-z^2 / 2} \quad \text{ when }t=z\end{aligned}\)</li>
      <li>(2) Chernoff Bounds<br />
\(\begin{aligned}  p(Z \geqslant z) &amp;\leqslant e^{-z^2 / 2} \\ p\left(z \geqslant z_{\frac{\alpha}{2}}\right) &amp;\leqslant e^{-z_{\frac{\alpha}{2}}^2 / 2} \\  \alpha / 2 &amp;\leqslant e^{-z_{\frac{\alpha}{2}}^2 / 2} \\ \log{\frac{\alpha}{2}}&amp;\leqslant-\frac{z_{\frac{\alpha}{2}}^2}{2} \\ z_{\frac{\alpha}{2}}^2 &amp;\leq 2 \log{\frac{\alpha}{2}} \\ \end{aligned}\)</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[Prerequirement Markov inequality \(\mathrm{P}(X \geq a) \leq \frac{\mathrm{E}(X)}{a}\) Chebyshev inequality \(\operatorname{Pr}(\mid X-\mu\mid \geq k \sigma) \leq \frac{1}{k^2}\)]]></summary></entry><entry><title type="html">TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-05-23-Timemixer/" rel="alternate" type="text/html" title="TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting (ICLR 2024)" /><published>2024-05-23T00:00:00+09:00</published><updated>2024-07-05T18:29:41+09:00</updated><id>http://localhost:4000/timeseries/Timemixer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-05-23-Timemixer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Real world에서 time series는 복잡한 temporal variations 때문에 forecasting이 어려움</li>
  <li>지금까지는 그냥 decomposition하고 multiperiodicity 분석 했는데,</li>
  <li>본 논문에서는 time series를 다양한 scale로 sampling하면 별개의 patterns가 관찰될 것이라는 intuition에 따라 multiscale-mixing을 제안
    <ul>
      <li>microscopic information은 fine scale, macroscopic information은 coarse scales</li>
    </ul>
  </li>
  <li><strong>TimeMixer</strong> : fully MLP-based architecture
    <ul>
      <li>with <strong>Past-Decomposable-Mixing</strong> (PDM) for past extraction
        <ul>
          <li>applies the decomposition to multiscale series</li>
          <li>and mixes the decomposed seasonal and trend components
            <ul>
              <li>즉 the microscopic seasonal과 macroscopic trend를 통합</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>with <strong>Future-Multipredictor-Mixing</strong> (FMM) for future prediction
        <ul>
          <li>multiscale observation을 통한 완성도 있는 예측을 위해 multiple predictors를 ensemble</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>Time series의 representative models는 temporal variations를 파악
    <ul>
      <li>foundation backbone 별로 보면</li>
      <li>CNN 계열로는 <a href="https://openreview.net/forum?id=zt53IDUR1U">MICN(2023)</a>, <a href="https://arxiv.org/abs/2210.02186">TimesNet(2023)</a>, <a href="https://link.springer.com/article/10.1007/s00500-020-04954-0">TCN(2020)</a>, RNN 계열로는 <a href="https://arxiv.org/abs/1703.07015">LSTNet(2018)</a>, <a href="https://arxiv.org/abs/1704.02971">DA-RNN(2017)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0169207019301888">DeepAR(2020)</a>이 있고</li>
      <li>Transformer 계열로는 <a href="https://arxiv.org/abs/2012.07436">Informer(2021)</a>, <a href="https://arxiv.org/abs/2106.13008">Autoformer(2021)</a>, <a href="https://arxiv.org/abs/2201.12740">Fedformer(2022)</a>, <a href="https://arxiv.org/abs/2211.14730">PatchTST(2023)</a>가 있다.</li>
      <li>그리고 MLP 계열로는 <a href="https://arxiv.org/abs/2205.13504">DLinear(2023)</a>, <a href="https://arxiv.org/abs/2207.01186">LightTS(2022)</a>, <a href="https://arxiv.org/abs/2201.12886">N-hits(2023)</a>가 있다.</li>
    </ul>
  </li>
  <li>지금까지는 그냥 decomposition하고 multiperiodicity 분석
    <ul>
      <li>decomposition : predictable component(seasonal, trend)와 complex temporal patterns를 분리</li>
      <li>multiperiodicity analysis : mixed temporal variations을 주기가 다른 여러 components로 disentangle</li>
    </ul>
  </li>
  <li>본 논문의 intuition : sampling scales가 달라지면 temporal variations도 달라진다.
    <ul>
      <li>microscopic information을 담은 fine scale과 macroscopic information을 담고 있는 coarse scales에서의 변동이 joint하게 미래 변동을 결정한다.</li>
      <li>multiscale mixing을 통해 scale에 따른 변동을 구분하고 그걸 다 고려해서 complementary한 예측을 할 수 있을 것</li>
    </ul>
  </li>
  <li><strong>Past-Decomposable-Mixing</strong> (PDM) : average downsampling을 통해 multiscale observations 생성</li>
  <li><strong>Future-Multipredictor-Mixing</strong> (FMM) : multiscale에서의 seasonal, trend를 합쳐서 multiple predictors를 ensemble</li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="21-temporal-modeling-in-deep-tsf">2.1. Temporal Modeling in Deep TSF</h3>

<ul>
  <li>Foundation backbones에 따라 4가지 계열로 구분 : CNN, RNN, Transformer, MLP
    <ul>
      <li>CNN, RNN-based deep models는 limited receptive field 때문에 long-term forecasting이 어려움</li>
      <li>Transformer-based models는 long-term temporal depdendencies 파악</li>
      <li>MLP-based models는 단순한 구조로도 복잡한 모델의 성능만큼 도달할 수 있음을 제시</li>
    </ul>
  </li>
  <li>Temporal multiscale designs models이 있긴 하지만 <a href="https://openreview.net/forum?id=0EXmFzUn5I">Pyraformer(2021)</a>, <a href="https://arxiv.org/abs/2106.09305">SCINet(2022)</a>
    <ul>
      <li>예측 과정에서 multiscale information을 동시에 활용하지는 않음</li>
    </ul>
  </li>
</ul>

<h3 id="22-mixing-networks">2.2. Mixing Networks</h3>

<ul>
  <li>Computer Vision과 NLP 분야에서 사용되는 mixing <a href="https://arxiv.org/abs/2105.01601">MLP-Mixer(2021)</a>, <a href="https://arxiv.org/abs/2105.03824">FNet(2022)</a></li>
</ul>

<h2 id="3-timemixer">3. TimeMixer</h2>

<ul>
  <li>P길이의 과거를 보고 F길이의 미래를 예측</li>
  <li>disentangled variations(past information extraction) &amp; complementary forecasting(future prediction)</li>
</ul>

<h3 id="31-multiscale-mixing-architecture">3.1. Multiscale Mixing Architecture</h3>

<p><img src="/assets/img/timeseries/Timemixer/fig1.png" alt="그림1" /></p>

<ul>
  <li>
    <p>먼저 complex variations를 disentangle하기 위해 downsampling한다.</p>

    <ul>
      <li>\(\mathbf{x} \in \mathbb{R}^{P \times C}\)를 M개의 scale로 downsampling하면</li>
      <li>\(\mathcal{X}=\left\{\mathbf{x}_0, \cdots, \mathbf{x}_M\right\} \text {, where } \mathbf{x}_m \in \mathbb{R}^{\left\lfloor \frac{P}{2^m}\right\rfloor \times C}, m \in\{0, \cdots, M\}\)를 얻는다.</li>
      <li>\(C\)는 변수 개수</li>
      <li>즉 \(\mathbf{x}_0\)는 input 원본이고, \(\mathbf{x}_m\)은 m개씩 average pooling한 것이다.</li>
      <li>그리고 embedding layer 통과하면 \(\mathcal{X}^0=\operatorname{Embed}(\mathcal{X})\)를 얻는다.</li>
    </ul>
  </li>
  <li>
    <p>이제 <strong>Past-Decomposable-Mixing</strong> (PDM, for mixing past information)</p>

\[x^l=\operatorname{PDM}\left(x^{l-1}\right),\quad l \in\{0, \cdots, L\}, \quad \mathbf{x}_m^l \in \mathbb{R}^{\left\lfloor\frac{P}{2^m}\right\rfloor \times d_{\text {model }}}\]

    <ul>
      <li>(자세한 건 다음 section 3.2.)</li>
    </ul>
  </li>
  <li>
    <p>다음으로 <strong>Future-Multipredictor-Mixing</strong> (FMM, for future prediction)</p>

\[\widehat{\mathbf{x}}=\operatorname{FMM}\left(\mathcal{X}^L\right), \quad \widehat{\mathbf{x}} \in \mathbb{R}^{F \times C}\]

    <ul>
      <li>(자세한 건 다음 section 3.3.)</li>
    </ul>
  </li>
</ul>

<h3 id="32-past-decomposable-mixing">3.2. Past Decomposable Mixing</h3>

<ul>
  <li>
    <p>Seasonal과 trend는 scale에 따라 다르게 나타나므로</p>

    <ul>
      <li>Seasonal끼리 scale별로 구해서 mixing, trend끼리 scale별로 구해서 mixing</li>
    </ul>
  </li>
  <li>
    <p>\(l\)-번째 PDM block에서는</p>
    <ul>
      <li>ts \(\mathcal{X}_l\)를 seasonal part \(\mathcal{S}^l=\left\{\mathbf{s}_0^l, \cdots, \mathbf{s}_M^l\right\}\)과 trend parts \(\mathcal{T}^l=\left\{\mathbf{t}_0^l, \cdots, \mathbf{t}_M^l\right\}\)로 decompose</li>
    </ul>

\[\begin{gathered}
\mathbf{s}_m^l, \mathbf{t}_m^l=\text { SeriesDecomp }\left(\mathbf{x}_m^l\right), m \in\{0, \cdots, M\}, \\
\mathcal{X}^l=\mathcal{X}^{l-1}+\text { FeedForward }\left(\operatorname{S-Mix}\left(\left\{\mathbf{s}_m^l\right\}_{m=0}^M\right)+\text { T-Mix }\left(\left\{\mathbf{t}_m^l\right\}_{m=0}^M\right)\right)
\end{gathered}\]

    <ul>
      <li>\(\text { FeedForward(} \cdot)\) contains two linear layers w/ GELU</li>
      <li>\(\operatorname{S}-\operatorname{Mix}(\cdot), T-\operatorname{Mix}(\cdot)\)​는 지금부터 설명할 mixing</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/fig2.png" alt="그림2" /></p>

<ul>
  <li><strong>Seasonal Mixing</strong>
    <ul>
      <li>(Box &amp; Jenkins, 1970)의 seasonality analysis에 따르면
        <ul>
          <li>larger periods는 smaller periods의 aggregation</li>
          <li>그러므로 residual하게 bottom-up approach</li>
          <li>(coarser scales의 seasonality를 위해 lower-level fine-scale 사용)</li>
          <li>
\[\text { for } m: 1 \rightarrow M \text { do: } \quad \mathbf{s}_m^l=\mathbf{s}_m^l+\text { Bottom-Up-Mixing }\left(\mathbf{s}_{m-1}^l\right)\]
            <ul>
              <li>\(\text { Bottom-Up-Mixing(} \cdot \text {) }\): input dim \(\left\lfloor\frac{P}{2^{m-1}}\right\rfloor\), output dim \(\left\lfloor\frac{P}{2^{m}}\right\rfloor\)인 two linear layers with GELU</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Trend Mixing</strong>
    <ul>
      <li>seasonal parts와 반대로
        <ul>
          <li>detailed variations는 noise로 보일 뿐이고 coarser scales가 finer scales를 guide</li>
          <li>그러므로 residual하게 top-down mixing</li>
          <li>
\[\text { for } m:(M-1) \rightarrow 0 \text { do: } \quad \mathbf{t}_m^l=\mathbf{t}_m^l+\text { Top-Down-Mixing }\left(\mathbf{t}_{m+1}^l\right)\]
            <ul>
              <li>\(\text { Top-Down-Mixing(} \cdot \text {) }\) : : input dim \(\left\lfloor\frac{P}{2^{m+1}}\right\rfloor\), output dim \(\left\lfloor\frac{P}{2^{m}}\right\rfloor\)인 two linear layers with GELU</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>결론적으로 seasonality는 from fine to coarse, 반대로 trend는 coarse to fine하게 multiscale mixing in past information extraction</li>
</ul>

<h3 id="33-future-multipredictor-mixing">3.3. Future MultiPredictor Mixing</h3>

<ul>
  <li>\(L\)개의 PDM block으로 \(\mathcal{X}^L=\left\{\mathbf{x}_0^L, \cdots, \mathbf{x}_M^L\right\}, \mathbf{x}_m^L \in \mathbb{R}^{\left\lfloor\frac{P}{2^m}\right\rfloor \times d_{\text {model }}}\)를 얻었다.</li>
  <li>서로 다른 scale인 \(\mathbf{x}_m^L\)들은 서로 다른 variations를 present하고 있기 때문에 모든 scale에서 예측을 하고 각각의 예측을 aggregate한다. (ensemble)</li>
  <li>
\[\widehat{\mathbf{x}}_m=\operatorname{Predictor}_m\left(\mathbf{x}_m^L\right), m \in\{0, \cdots, M\}, \widehat{\mathbf{x}}=\sum_{m=0}^M \widehat{\mathbf{x}}_m\]
    <ul>
      <li>\(\widehat{\mathbf{x}}_m, \widehat{\mathbf{x}} \in \mathbb{R}^{F \times C}\)​</li>
    </ul>
  </li>
  <li>\(\text { Predictor }_m(\cdot)\)은 one single linear layer인데 \(\left\lfloor\frac{P}{2^m}\right\rfloor\)길이의 과거 정보로부터 F 길이의 future를 regress</li>
  <li>FMM은 mixed multiscale series로 complementary forecasting하는  ensemble of multiple predictors</li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<ul>
  <li>Summary</li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/table1.png" alt="그림11" /></p>

<ul>
  <li>Main results</li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/table2.png" alt="그림12" /></p>

<ul>
  <li>Ablations</li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/table5.png" alt="그림15" /></p>

<ul>
  <li>Decomposition과 multiscale의 각 components에 대한 visualization</li>
</ul>

<p><img src="/assets/img/timeseries/Timemixer/fig3.png" alt="그림3" /></p>

<p><img src="/assets/img/timeseries/Timemixer/fig4.png" alt="그림4" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>Empowered by Past-Decomposable-Mixing and Future- Multipredictor-Mixing blocks, TimeMixer took advantage of both disentangled variations and complementary forecasting capabilities.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=7oLshfEIC2)]]></summary></entry><entry><title type="html">Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-05-23-Pathformer/" rel="alternate" type="text/html" title="Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting (ICLR 2024)" /><published>2024-05-23T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/timeseries/Pathformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-05-23-Pathformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>기존 Transformer for TS 모델들은 limited or fixed scales</li>
  <li>Pathformer는 temporal resolution과 temporal distance를 합치는 방식
    <ul>
      <li>다양한 크기의 patches를 사용해서 서로 다른 temporal resolutions를 만들고</li>
      <li>dual attention으로 temporal dependencies(global correlation과 local details)</li>
    </ul>
  </li>
  <li>Input의 varying temporal dynamics에 따라 adaptive multi-scale</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>최근 simpler linear model의 성능이 transformer 보다 잘 나옴 - transformer 디자인 수정 필요</li>
  <li>Time Series에서는 multiple scales를 고려하는 것이 중요 (일, 월 등…)
    <ul>
      <li><strong>Temporal resolution</strong>: patch의 길이 (fine-grained or coarse grained)</li>
      <li><strong>Temporal distance</strong> : time steps 사이의 거리
<img src="/assets/img/timeseries/Pathformer/fig1.png" alt="그림1" /></li>
      <li>그림 왼쪽 위에서 blue patch와 orange patch는 Temporal resolution가 다른 것이고</li>
      <li>그림 오른쪽에서 black arrows와 colored arrows는 Temporal distance가 다른 것</li>
    </ul>
  </li>
  <li>그래서 Transformer로 multi-scale modeling 하려는데 challenges 2개
    <ul>
      <li><strong>incompleteness of multi-scale modeling</strong>
        <ul>
          <li>단순히 Temporal resolution (patch 길이) 늘리는 건 다양한 dependency 파악에 도움 안됨</li>
          <li>차라리 Temporal distance (time steps 간격) 다양하게 하면 좋은데 Temporal distance는 patch size (data division)에 따라 달라짐</li>
        </ul>
      </li>
      <li><strong>fixed multi-scale modeling process</strong>
        <ul>
          <li>different series prefer different scales !</li>
          <li>그림 왼쪽 위 series는 rapid fluctuation, fine-grained, short-term characteriestics</li>
          <li>그림 왼쪽 아래 series는 coarse-grained and long-term characteriestics</li>
          <li>모든 데이터에 fixed multi-scale modeling하면 안되는데 매번 optimal scale 찾으려니 오래걸림</li>
        </ul>
      </li>
      <li>그래서 <strong>adaptive multi-scale modeling</strong>이 필요하다.
        <ul>
          <li>adaptively models the current data from certain multiple scales</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>기존에는 그냥 다양한 길이의 patch로 자른 뒤에, patch 안에서 temporal dependencies를, patch 끼리 global correlation을 파악하는 dual attention</li>
  <li>본 논문에서 제시하는 Pathformer는
    <ul>
      <li>Multi-scale router가 seasonality와 trend를 보고 적절한 patch size들을 결정</li>
      <li>그 다음 multi-scale characteristics를 weighted aggregation</li>
    </ul>
  </li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>

<ul>
  <li>Time Series forecasting
    <ul>
      <li>Statistical modeling, GNN(spatial dependency), RNN(temporal dependency), CNN(sub-series features), TimesNet(1-dim \(\to\) 2-dim), LLM-based…</li>
    </ul>
  </li>
  <li>Transformer
    <ul>
      <li>Informer : prob-sparse self-attention to select important keys</li>
      <li>Triformer :  manages to reduce the complexity</li>
      <li>Autoformer : auto-correlation mechanisms to replace self- attention</li>
      <li>FEDformer : the perspective of frequency to model temporal dynamics</li>
      <li>하지만 simple linear model의 성능이 더 좋은 경우가 많았음</li>
      <li>PatchTST : patching and channel independence로 transformer의 가능성 제시</li>
    </ul>
  </li>
  <li>Multi-scale Modeling for Time Series
    <ul>
      <li>N-HiTs : multi-rate data sampling and hierarchical interpolation for diverse temporal resolutions</li>
      <li>Pyraformer : pyramid attention to extract features at different temporal resolutions</li>
      <li>하지만 fixed scale이었고, 본 논문에서는 adaptive multi scale 제안</li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<p><img src="/assets/img/timeseries/Pathformer/fig2.png" alt="그림2" /></p>

<ul>
  <li>Instance Norm - Stacking of <strong>Adaptive Multi-Scale Blocks</strong> - Predictor(FC)로 구성</li>
  <li>Adaptive Multi-Scale Blocks은 multi-scale Transformer block과 adaptive pathways으로 구성
    <ul>
      <li>multi-scale Transformer block
        <ul>
          <li>다양한 size의 patch division and dual attention으로 multi-scale temporal resolution and distances 통합</li>
        </ul>
      </li>
      <li>adaptive pathways
        <ul>
          <li>multi-scale router가 다양한 patch size르 고르고</li>
          <li>aggregator에서 mutli-scale characteristics를 weighted aggregation</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="31-multi-scale-transformer-block">3.1. Multi-scale Transformer Block</h3>

<ul>
  <li><strong>Multi-scale Division</strong>
    <ul>
      <li>\(\mathrm{X} \in \mathbb{R}^{H \times d}\)를 P개의 patch \(\left(\mathrm{X}^1, \mathrm{X}^2, \ldots, \mathrm{X}^P\right)\)로 나눔, patch size는 M개 \(\mathcal{S}=\left\{S_1, \ldots, S_M\right\}\) 즉 \(P = H/S\)</li>
      <li>다양한 크기의 patch size로 다양한 temporal resolutions를 dual attention</li>
    </ul>
  </li>
  <li><strong>Dual attention</strong>
<img src="/assets/img/timeseries/Pathformer/fig3.png" alt="그림3" />
    <ul>
      <li>Intra-patch attention : relationships btw time steps within each patch
        <ul>
          <li>먼저 \(X_{\text {intra }}^i \in \mathbb{R}^{S \times d_m}\) patch를 embedding하고 \(\operatorname{Attn}_{\text {intra }}^i=\operatorname{Softmax}\left(Q_{\text {intra }}^i\left(K_{\text {intra }}^i\right)^T / \sqrt{d_m}\right) V_{\text {intra }}^i \in \mathbb{R}^{1 \times d_m}\)</li>
          <li>모든 patches에 대해 다 합치면 \(\operatorname{Attn}_{\text {intra }}=\operatorname{Concat}\left(\operatorname{Attn}_{\text {intra }}^1, \ldots, \operatorname{Attn}_{\text {intra }}^P\right) \in \mathbb{R}^{P \times d_m}\)​</li>
          <li>Linear transformation으로 \(\operatorname{Attn}_{\text {intra }} \in \mathbb{R}^{P \times S \times d_m}\)</li>
        </ul>
      </li>
      <li>Inter-patch attention : relationships btw patches to capture global correlations
        <ul>
          <li>먼저 feature embedding and rearrange : \(\mathrm{X}_{\text {inter }} \in \mathbb{R}^{P \times d_m^{\prime}} \text {, where } d_m^{\prime}=S \cdot d_m\)</li>
          <li>아까처럼  linear mapping으로 \(\operatorname{Attn}_{\text {inter }}=\operatorname{Softmax}\left(Q_{\text {inter }}\left(K_{\text {inter }}\right)^T / \sqrt{d_m^{\prime}}\right) V_{\text {inter }} \in \mathbb{R}^{P \times S \times d_m}\)</li>
        </ul>
      </li>
      <li>둘을 더하면 \(\mathrm{Attn}_{\text {intra }} + \mathrm{Attn}_{\text {intra }}=\text { Attn } \in \mathbb{R}^{P \times S \times d_m}\)</li>
    </ul>
  </li>
</ul>

<h3 id="32-adaptive-pathways">3.2. Adaptive Pathways</h3>

<ul>
  <li>different series may prefer diverse scales \(\to\) model needs to figure out critical scales based on the input</li>
  <li><strong>Multi-scale router</strong> selects specific sizes of patch division based on the input data</li>
  <li>
    <p><strong>Multi-scale aggregator</strong> combines multi-scale characteristics(weighted aggregation) \(\to\) output of the Transformer block</p>
  </li>
  <li><strong>Multi-scale router</strong> : selects the optimal sizes for patch division
    <ul>
      <li>by its complex inherent characteristics and dynamic patterns</li>
      <li>seasonality and trend decomposition \(\to\) extract periodicity and trend patterns</li>
      <li><strong>Seasonality decomposition</strong>
        <ul>
          <li>Discern Fourier Transform (DFT) : X를 푸리에 basis로 decomopose하고 the largest amplitudes 선택 (to keep the sparsity of frequency domain)</li>
          <li>IDFT로 periodic patterns \(\mathrm{X}_{\text {sea }}=\operatorname{IDFT}\left(\left\{f_1, \ldots, f_{K_f}\right\}, A, \Phi\right)\) 얻음
            <ul>
              <li>\(\Phi\), \(A\) :the phase and amplitude of each frequency from \(\operatorname{DFT}(\mathrm{X})\)</li>
              <li>\(\left\{f_1, \ldots, f_{K_f}\right\}\) : the frequencies with the top \(K_f\) amplitudes</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Trend decomposition</strong>
        <ul>
          <li>seasonality를 제외한 부분 \(\mathrm{X}_{\mathrm{rem}}=\mathrm{X}-\mathrm{X}_{\text {sea }}\)에 대해 different kernels of average pooling for moving averages to extract trend patterns</li>
          <li>정리하면 \(\mathrm{X}_{\text {trend }}=\operatorname{Softmax}\left(L\left(\mathrm{X}_{\mathrm{rem}}\right)\right) \cdot\left(\operatorname{Avgpool}\left(\mathrm{X}_{\mathrm{rem}}\right)_{\text {kernel }_1}, \ldots, \operatorname{Avgpool}\left(\mathrm{X}_{\mathrm{rem}}\right)_{\text {kernel }_N}\right)\)</li>
        </ul>
      </li>
      <li>input \(X\)에 seasonality pattern and trend pattern를 더하고 linear mapping으로 \(\mathrm{X}_{\text {trans }} \in \mathbb{R}^d\)</li>
      <li>마지막으로 routing function \(R\left(\mathrm{X}_{\text {trans }}\right)=\operatorname{Softmax}\left(\mathrm{X}_{\text {trans }} W_r+\epsilon \cdot \operatorname{Softplus}\left(\mathrm{X}_{\text {trans }} W_{\text {noise }}\right)\right), \epsilon \sim \mathcal{N}(0,1)\) 을 통해 pathway weights \(\in \mathbb{R}^{M}\)을 구한다.
        <ul>
          <li>\(W_r \text { and } W_{\text {noise }} \in \mathbb{R}^{d \times M}\)은 leanable parameters, \(d\)는 feature dim, \(M\)은 patch size의 개수</li>
          <li>noise를 추가한 이유는 patch size가 같은 것만 나오는 것을 방지하기 위함</li>
          <li>이렇게 만든 M개의 pathway weights 중에서 top K개를 제외하고 0으로 만든 걸 \(\bar{R}\left(\mathrm{X}_{\text {trans }}\right)\)로 표기</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Multi-Scale Aggregator</strong>
    <ul>
      <li>\(\bar{R}\left(\mathrm{X}_{\text {trans }}\right)_i&gt;0\)이 의미하는 것은 patch size \(S_i\)로 나누고 dual attention을 수행함을 의미</li>
      <li>그러므로 AMS block의 final output은 \(\mathrm{X}_{\text {out }}=\sum_{i=1}^M \mathcal{I}\left(\bar{R}\left(\mathrm{X}_{\text {trans }}\right)_i&gt;0\right) R\left(\mathrm{X}_{\text {trans }}\right)_i T_i\left(\mathrm{X}_{\text {out }}^i\right)\)
        <ul>
          <li>\(\mathrm{X}_{\text {out }}^i\)는 output of the multi-scale Transformer with the patch size \(S_i\)</li>
          <li>\(T_i\)는 align the temporal dimension from different scales하는 transformation function</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>

<p><img src="/assets/img/timeseries/Pathformer/table1.png" alt="그림4" /></p>

<p><img src="/assets/img/timeseries/Pathformer/table3.png" alt="그림5" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>Pathformer : Multi-Scale Transformer with Adaptive Pathways for TSF
    <ul>
      <li>integrates multi-scale temporal resolutions and temporal distances</li>
      <li>by patch division with multiple patch sizes and dual attention (modeling multi-scale characteristics)</li>
      <li>adaptive pathways dynamically select and aggregate scale-specific characteristics</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/pdf?id=lJkOCMP2aW)]]></summary></entry><entry><title type="html">(Corrformer, NMI 2023) Code Review 3 - Encoder</title><link href="http://localhost:4000/pytorch/2024-05-05-corrformer3/" rel="alternate" type="text/html" title="(Corrformer, NMI 2023) Code Review 3 - Encoder" /><published>2024-05-05T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/pytorch/corrformer3</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-05-05-corrformer3/"><![CDATA[<p><img src="/assets/img/pytorch/corrformer0/corrformer010.jpeg" alt="사진10" /></p>
<ul>
  <li>우리는 <code class="language-plaintext highlighter-rouge">exp_main</code>의 <code class="language-plaintext highlighter-rouge">train</code> 메소드를 실행하고 있다.</li>
  <li><code class="language-plaintext highlighter-rouge">get_data</code>는 이미 살펴보았고 <code class="language-plaintext highlighter-rouge">self.model</code>에 들어가는 Corrformer를 이해하기 위해 <code class="language-plaintext highlighter-rouge">Corrformer.py</code>를 살펴보고 있다.
    <ul>
      <li>step 0. Initialization and Normalizaiton</li>
      <li>step 1. Data Embedding Instance</li>
      <li>step 2. Encoder Instance</li>
      <li>step 3. Decoder Instance</li>
      <li>step 4. forward</li>
    </ul>
  </li>
  <li>forward 전체 코드
<img src="/assets/img/pytorch/corrformer1/fig6.png" alt="사진6" /></li>
</ul>

<h3 id="step-2-encoder-instance">step 2. Encoder Instance</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">enc_out = self.encoder(enc_out, attn_mask=enc_self_mask)</code>
<img src="/assets/img/pytorch/corrformer2/corrformer23.png" alt="사진3" /></li>
  <li>지난 포스팅에서 봤던 <code class="language-plaintext highlighter-rouge">enc_out = self.enc_embedding(x_enc, x_mark_enc) #(350, 48, 768)</code>가 <code class="language-plaintext highlighter-rouge">encoder</code>의 input</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">encoder</code>는 `layers’에 있는 Encoder, MultiCorrelation, AutoCorrelation, CrossCorrelation, CausalConv를 import
<img src="/assets/img/pytorch/corrformer3/corrformer31.png" alt="사진31" /></p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">self.encoder</code> instance를 만드는데, parameters를 잠시 지우고 구조만 보면 아래와 같다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">Encoder</span><span class="p">([</span>
        <span class="nc">EncoderLayer</span><span class="p">(</span>
            <span class="nc">MultiCorrelation</span><span class="p">(</span>
                <span class="nc">AutoCorrelationLayer</span><span class="p">(</span><span class="nc">AutoCorrelation</span><span class="p">()),</span>
                <span class="nc">CrossCorrelationLayer</span><span class="p">(</span><span class="nc">CrossCorrelation</span><span class="p">(</span><span class="nc">CausalConv</span><span class="p">()))</span>
            <span class="p">))])</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Encoder</code> 안에 <code class="language-plaintext highlighter-rouge">EncoderLayer</code>가 있다. <code class="language-plaintext highlighter-rouge">EncoderLayer</code> 안에는 <code class="language-plaintext highlighter-rouge">MultiCorrelation</code>이 있는데, <code class="language-plaintext highlighter-rouge">MultiCorrelation</code> 안에는 <code class="language-plaintext highlighter-rouge">AutoCorrelationLayer(AutoCorrelation())</code>과 <code class="language-plaintext highlighter-rouge">CrossCorrelationLayer(CrossCorrelation(CausalConv()))</code>이 있다.</li>
</ul>

<h3 id="causalconv">CausalConv()</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Corrformer.py
</span><span class="nc">CausalConv</span><span class="p">(</span>
    <span class="n">num_inputs</span><span class="o">=</span><span class="n">configs</span><span class="p">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="n">configs</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">label_len</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pred_len</span><span class="p">),</span>
    <span class="n">num_channels</span><span class="o">=</span><span class="p">[</span><span class="n">configs</span><span class="p">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="n">configs</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">label_len</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pred_len</span><span class="p">)]</span> <span class="o">*</span> <span class="n">configs</span><span class="p">.</span><span class="n">dec_tcn_layers</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Corrformer.sh
# --d_model 768 \ --n_heads 16 \ --label_len 24 \ --pred_len 24 \ --dec_tcn_layers 1 \
</span>
<span class="c1"># Causal_Conv.py
</span><span class="k">class</span> <span class="nc">CausalConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CausalConv</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">num_levels</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">num_channels</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_levels</span><span class="p">):</span>
            <span class="n">dilation_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">**</span> <span class="n">i</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="n">num_inputs</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">num_channels</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">out_channels</span> <span class="o">=</span> <span class="n">num_channels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="nc">CausalBlock</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation_size</span><span class="p">,</span>
                                   <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dilation_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)]</span>

        <span class="n">self</span><span class="p">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Corrformer github](https://github.com/thuml/Corrformer)]]></summary></entry><entry><title type="html">(Corrformer, NMI 2023) Code Review 2 - Data Embedding</title><link href="http://localhost:4000/pytorch/2024-05-04-corrformer2/" rel="alternate" type="text/html" title="(Corrformer, NMI 2023) Code Review 2 - Data Embedding" /><published>2024-05-04T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/pytorch/corrformer2</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-05-04-corrformer2/"><![CDATA[<ul>
  <li>step 0. Initialization and Normalizaiton</li>
  <li>step 1. Data Embedding Instance</li>
  <li>step 2. Encoder Instance</li>
  <li>step 3. Decoder Instance</li>
  <li>step 0 ~ 3는 embedding, encoder, decoder를 정의한 것이고, forward에서 실행된다.
    <ul>
      <li>forward에서 각 step을 지날 때에 step 0 ~ 3을 자세히 보도록 한다.</li>
    </ul>
  </li>
</ul>

<h3 id="step-4-forward">step 4. forward</h3>
<p><img src="/assets/img/pytorch/corrformer1/fig6.png" alt="사진6" /></p>
<ul>
  <li>step 1. embedding은 <code class="language-plaintext highlighter-rouge">enc_out = self.enc_embedding(x_enc, x_mark_enc)</code> 부분</li>
  <li>step 2. encoder는 <code class="language-plaintext highlighter-rouge">enc_out = self.encoder(enc_out, attn_mask=enc_self_mask)</code> 부분</li>
  <li>step 3. decoder는 <code class="language-plaintext highlighter-rouge">seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask,trend=trend_init)</code> 부분이다.</li>
</ul>

<h3 id="step-0-initialization-and-normalizaiton">step 0. Initialization and Normalizaiton</h3>
<p><img src="/assets/img/pytorch/corrformer2/corrformer200.png" alt="사진0" /></p>
<ul>
  <li>이전 포스팅에서 봤던 <code class="language-plaintext highlighter-rouge">batch_x</code>가 <code class="language-plaintext highlighter-rouge">x_enc</code>로 들어간다. (<code class="language-plaintext highlighter-rouge">torch.Size([1, 48, 3850])</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">x_enc</code>를 normalization해주고 RevIN처럼 learnable parameters로 설정한다.</li>
  <li><code class="language-plaintext highlighter-rouge">self.decomp</code>에서는 seasonal(x - moving average)와 trend(moving average)로 분해한다. (shape은 똑같이 <code class="language-plaintext highlighter-rouge">torch.Size([1, 48, 3850])</code>)</li>
</ul>

<h3 id="step-1-data-embedding-instance">step 1. Data Embedding Instance</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">enc_out = self.enc_embedding(x_enc, x_mark_enc)</code>
<img src="/assets/img/pytorch/corrformer2/corrformer23.png" alt="사진3" /></li>
  <li><code class="language-plaintext highlighter-rouge">Corrformer.sh</code>에서 B:1 / L:48 / D:3850 / C:4 / node_num:350 이므로</li>
  <li><code class="language-plaintext highlighter-rouge">x_enc</code>는 <code class="language-plaintext highlighter-rouge">1, 48, 3850</code> –<code class="language-plaintext highlighter-rouge">view</code>–&gt; <code class="language-plaintext highlighter-rouge">(1, 48, 350, 11)</code> –<code class="language-plaintext highlighter-rouge">permute</code>–&gt; <code class="language-plaintext highlighter-rouge">(1, 350, 48, 11)</code> –<code class="language-plaintext highlighter-rouge">view</code>–&gt; <code class="language-plaintext highlighter-rouge">(350, 48, 11)</code>이고</li>
  <li><code class="language-plaintext highlighter-rouge">x_mark_enc</code>는  <code class="language-plaintext highlighter-rouge">(1, 48, 4)</code> –<code class="language-plaintext highlighter-rouge">unsqueeze</code>–&gt; <code class="language-plaintext highlighter-rouge">(1, 1, 48, 4)</code> –<code class="language-plaintext highlighter-rouge">repeat</code>–&gt; <code class="language-plaintext highlighter-rouge">(1, 350, 48, 4)</code> –<code class="language-plaintext highlighter-rouge">view</code>–&gt; <code class="language-plaintext highlighter-rouge">(350, 48, 4)</code>가 된다.</li>
  <li><code class="language-plaintext highlighter-rouge">self.enc_embedding</code>의 input이 된다.
<img src="/assets/img/pytorch/corrformer2/corrformer20.png" alt="사진0" /></li>
  <li>encoder embedding은 <code class="language-plaintext highlighter-rouge">DataEmbedding</code>으로 정의되는데 (그리고 decoder embedding도) <code class="language-plaintext highlighter-rouge">Embed.py</code>에서 import
<img src="/assets/img/pytorch/corrformer2/corrformer21.png" alt="사진1" /></li>
  <li>forward의 x는 <code class="language-plaintext highlighter-rouge">value_embedding(x) + temporal_embedding(x_mark) + national_embedding(national_position)</code></li>
  <li>논문에서 아래 부분에 해당한다.
<img src="/assets/img/pytorch/corrformer2/corrformer22.png" alt="사진2" /></li>
  <li>GeoPositionalEmbedding, TokenEmbedding, TemporalEmbedding의 코드를 첨부하지는 않겠지만 코드를 보면 각각 national, value, temporal을 <code class="language-plaintext highlighter-rouge">d_model=768</code> 차원으로 embedding한다.
    <ul>
      <li>GeoPositionalEmbedding은 1개의 linear layer 사용</li>
      <li>TokenEmbedding은 channel-wise self-attention + conv1d 사용</li>
      <li>TemporalEmbedding은 sin과 cos를 사용했다.</li>
      <li><code class="language-plaintext highlighter-rouge">self.enc_embedding(x_enc, x_mark_enc)</code>는 torch.Size([<code class="language-plaintext highlighter-rouge">350, 48, 768</code>])이다. (<code class="language-plaintext highlighter-rouge">d_model=768</code>)
        <ul>
          <li>x_enc       –TokenEmbedding–&gt; <code class="language-plaintext highlighter-rouge">(350, 48, 768)</code> <br />
\(+\) x_mark_enc  –TemporalEmbedding–&gt; <code class="language-plaintext highlighter-rouge">(350, 48, 768)</code> <br />
\(+\) GeoPositionalEmbedding <code class="language-plaintext highlighter-rouge">(350, 48, 768)</code></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>step 2부터는 다음 포스팅에서 다루도록 한다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Corrformer github](https://github.com/thuml/Corrformer)]]></summary></entry><entry><title type="html">(Corrformer, NMI 2023) Code Review 1 - Overall Framework</title><link href="http://localhost:4000/pytorch/2024-04-29-corrformer1/" rel="alternate" type="text/html" title="(Corrformer, NMI 2023) Code Review 1 - Overall Framework" /><published>2024-04-29T00:00:00+09:00</published><updated>2024-07-02T22:54:50+09:00</updated><id>http://localhost:4000/pytorch/corrformer1</id><content type="html" xml:base="http://localhost:4000/pytorch/2024-04-29-corrformer1/"><![CDATA[<p><img src="/assets/img/pytorch/corrformer0/corrformer011.png" alt="사진11" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">_get_data</code> 함수로 데이터를 불러오는데 예를 들어 train_data의 경로는 <code class="language-plaintext highlighter-rouge">Corrformer/dataset/global_temp/temp_global_hourly_train.np</code>이고 shape은 <code class="language-plaintext highlighter-rouge">(12280, 3850, 1)</code>이다.
    <ul>
      <li>12280은 timestep의 개수, 3850은 sensor의 개수이다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">train_steps = len(train_loader)</code>는 <code class="language-plaintext highlighter-rouge">len(self.data_x)</code> - <code class="language-plaintext highlighter-rouge">self.seq_len</code> - <code class="language-plaintext highlighter-rouge">self.pred_len</code> + <code class="language-plaintext highlighter-rouge">1</code> = <code class="language-plaintext highlighter-rouge">12280 - 48 - 24 + 1</code> = <code class="language-plaintext highlighter-rouge">12209</code>가 된다.
    <ul>
      <li>12209번째부터 마지막 12280째까지 (48+24=72)개가 마지막 배치에 들어가고 12210째부터는 72개를 만들 수 없기 때문</li>
    </ul>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">_select_optimizer</code>로 MSE, <code class="language-plaintext highlighter-rouge">_select_criterion</code>로 Adam을 사용한다.</p>

    <ul>
      <li>참고로 <code class="language-plaintext highlighter-rouge">_get_data</code>는 <code class="language-plaintext highlighter-rouge">data_factory.py</code>에 있는 <code class="language-plaintext highlighter-rouge">data_provider</code> 함수를 import한 것이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/pytorch/corrformer0/corrformer08.png" alt="사진8" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">data_factory.py</code> - <code class="language-plaintext highlighter-rouge">data_provider</code> 함수의 첫 줄에서 <code class="language-plaintext highlighter-rouge">arg.data</code>에는 <code class="language-plaintext highlighter-rouge">Corrformer.sh</code>에서 정한 Global_Temp 또는 Global_Wind가 들어간다.</li>
  <li>그러면 <code class="language-plaintext highlighter-rouge">data_loader.py</code>에서 실제로 데이터를 불러오는 코드가 실행된다.</li>
  <li><code class="language-plaintext highlighter-rouge">data_loader.py</code> 코드를 보지는 않겠지만 numpy version 호환성으로 인해 <code class="language-plaintext highlighter-rouge">astype(np.float)</code>로 적힌 두 곳을 <code class="language-plaintext highlighter-rouge">astype(np.float64)</code>로 바꿔주었다. (안바꾸면 버전 호환성으로 인한 에러 발생)</li>
  <li>이제 매 epoch에서 어떻게 실행되는지 알아보자.</li>
</ul>

<p><img src="/assets/img/pytorch/corrformer0/corrformer07.png" alt="사진7" /></p>

<ul>
  <li>모델을 train mode로 설정하고 <code class="language-plaintext highlighter-rouge">train_loader</code>에서 데이터를 배치 단위로 불러온다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Corrformer</span><span class="p">.</span><span class="n">sh에서</span> <span class="o">--</span><span class="n">seq_len</span> <span class="mi">48</span> \ <span class="o">--</span><span class="n">label_len</span> <span class="mi">24</span> \ <span class="o">--</span><span class="n">pred_len</span> <span class="mi">24</span> <span class="n">이므로</span>

<span class="n">i</span> <span class="p">:</span> <span class="mi">0</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">batch_x</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">3850</span><span class="p">])</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">batch_y</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">3850</span><span class="p">])</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">batch_x_mark</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">batch_y_mark</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="mi">48</span><span class="n">개를</span> <span class="n">보고</span> <span class="mi">24</span><span class="n">개를</span> <span class="n">예측하는데</span> <span class="n">y의</span> <span class="n">길이도</span> <span class="mi">48</span><span class="n">인</span> <span class="n">이유</span>
<span class="o">--&gt;</span> <span class="n">x는</span> <span class="p">[</span><span class="n">index</span> <span class="n">부터</span> <span class="n">index</span> <span class="o">+</span> <span class="n">seq_len</span> <span class="n">까지</span><span class="p">]</span> <span class="n">이렇게</span> <span class="mi">48</span><span class="n">개이고</span> <span class="n">ex</span><span class="p">.</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span>
<span class="o">--&gt;</span> <span class="n">y는</span> <span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="n">seq_len</span> <span class="o">-</span> <span class="n">label_len</span> <span class="n">부터</span> <span class="n">index</span> <span class="o">+</span> <span class="n">label_len</span> <span class="o">+</span> <span class="n">pred_len</span> <span class="n">까지</span><span class="p">]</span> <span class="n">이렇게</span> <span class="mi">48</span><span class="n">개</span> <span class="n">ex</span><span class="p">.</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">72</span><span class="p">]</span>
<span class="o">--&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="p">]</span><span class="n">는</span> <span class="n">예측에</span> <span class="n">사용되는</span> <span class="n">부분이지</span> <span class="n">맞춰야</span> <span class="n">할</span> <span class="n">부분은</span> <span class="n">아니기</span> <span class="n">때문에</span><span class="p">,</span> <span class="n">x만</span> <span class="n">있으면</span> <span class="n">되고</span> <span class="n">y에는</span> <span class="n">없어도</span> <span class="n">되는</span> <span class="n">것이고</span>
<span class="o">--&gt;</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span><span class="n">은</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="p">]</span><span class="n">로</span> <span class="n">예측도</span> <span class="n">해야하고</span> <span class="p">[</span><span class="mi">49</span><span class="p">,</span> <span class="mi">72</span><span class="p">]</span><span class="n">를</span> <span class="n">예측할</span> <span class="n">때</span> <span class="n">사용도</span> <span class="n">해야</span> <span class="n">하니</span> <span class="n">x와</span> <span class="n">y가</span> <span class="n">모두</span> <span class="n">있어야</span> <span class="n">하고</span>
<span class="o">--&gt;</span> <span class="p">[</span><span class="mi">49</span><span class="p">,</span> <span class="mi">72</span><span class="p">]</span><span class="n">는</span> <span class="n">맞추기만</span> <span class="n">하면</span> <span class="n">되지</span> <span class="n">다른</span> <span class="n">예측에는</span> <span class="n">안쓰이기</span> <span class="n">때문에</span> <span class="n">y만</span> <span class="n">있으면</span> <span class="n">되는</span> <span class="n">것</span>
</code></pre></div></div>

<ul>
  <li>decoder의 input <code class="language-plaintext highlighter-rouge">dec_inp</code>는 <code class="language-plaintext highlighter-rouge">label_len</code> 만큼의 <code class="language-plaintext highlighter-rouge">batch_y</code>와 그 뒤에 <code class="language-plaintext highlighter-rouge">pred_len</code>만큼의 zero를 붙여서 만든다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dec_inp</span> <span class="c1"># torch.Size([1, 48, 3850])
</span><span class="nf">tensor</span><span class="p">([[[</span> <span class="mf">42.</span><span class="p">,</span>  <span class="mf">67.</span><span class="p">,</span>  <span class="mf">87.</span><span class="p">,</span>  <span class="p">...,</span> <span class="mf">300.</span><span class="p">,</span> <span class="mf">310.</span><span class="p">,</span> <span class="mf">320.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">37.</span><span class="p">,</span>  <span class="mf">67.</span><span class="p">,</span>  <span class="mf">88.</span><span class="p">,</span>  <span class="p">...,</span> <span class="mf">310.</span><span class="p">,</span> <span class="mf">320.</span><span class="p">,</span> <span class="mf">340.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">31.</span><span class="p">,</span>  <span class="mf">70.</span><span class="p">,</span>  <span class="mf">91.</span><span class="p">,</span>  <span class="p">...,</span> <span class="mf">320.</span><span class="p">,</span> <span class="mf">330.</span><span class="p">,</span> <span class="mf">350.</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span>  <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>  <span class="p">...,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span>  <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>  <span class="p">...,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span>  <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>  <span class="p">...,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">]]])</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">output = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)</code>에서 <code class="language-plaintext highlighter-rouge">Model</code> 클래스의 <code class="language-plaintext highlighter-rouge">forward</code>도 호출
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Corrformer.py</code>에서 <code class="language-plaintext highlighter-rouge">class Model(nn.Module):</code>로 Model이라는 class를 정의할 때 <code class="language-plaintext highlighter-rouge">nn.Module</code>을 상속받았는데, <code class="language-plaintext highlighter-rouge">nn.Module</code>에 오버라이딩 된 <code class="language-plaintext highlighter-rouge">__call__</code> 메소드 덕분에 <code class="language-plaintext highlighter-rouge">class Model</code>의 <code class="language-plaintext highlighter-rouge">forward</code> 메소드도 같이 호출되기 때문이다.</li>
      <li>즉 <code class="language-plaintext highlighter-rouge">batch_x</code>가 <code class="language-plaintext highlighter-rouge">Model</code> 클래스 <code class="language-plaintext highlighter-rouge">forward</code>의 <code class="language-plaintext highlighter-rouge">x_enc</code>로 들어가게 된다.</li>
      <li>이 때 <code class="language-plaintext highlighter-rouge">nn.Module</code>의 <code class="language-plaintext highlighter-rouge">__call__</code>에 의해 <code class="language-plaintext highlighter-rouge">forward</code>라는 메소드만 특별하게 처리되는 것이고, 만약 <code class="language-plaintext highlighter-rouge">Corrformer.py</code>에 <code class="language-plaintext highlighter-rouge">forward</code> 말고 다른 함수 <code class="language-plaintext highlighter-rouge">custom_function</code>도 있었다면 <code class="language-plaintext highlighter-rouge">self.model.custom_function</code>이렇게 해줘야 한다.</li>
    </ul>
  </li>
  <li>지금까지는 모델에 대해서 알아본 건 아니고 기본적인 코드의 구조를 이해했다. (아래 사진)</li>
  <li><code class="language-plaintext highlighter-rouge">exp_main</code> \(\to\) <code class="language-plaintext highlighter-rouge">Corrformer.py</code> 부분은 다음 포스팅에서 다룰 것이다.</li>
</ul>

<p><img src="/assets/img/pytorch/corrformer0/corrformer010.jpeg" alt="사진10" /></p>

<p><img src="/assets/img/pytorch/corrformer0/corrformer09.png" alt="사진9" /></p>

<ul>
  <li>다음 포스팅부터는 <code class="language-plaintext highlighter-rouge">outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)</code>에서 무슨 일이 일어나는지 알아보기 위해 <code class="language-plaintext highlighter-rouge">Exp_Main</code>에서 <code class="language-plaintext highlighter-rouge">arg.model</code>로 사용하게 되는 <code class="language-plaintext highlighter-rouge">Corrformer.py</code>에 있는 <code class="language-plaintext highlighter-rouge">Model</code> 클래스를 살펴보면서 모델을 이해해보도록 한다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="pytorch" /><summary type="html"><![CDATA[[Corrformer github](https://github.com/thuml/Corrformer)]]></summary></entry></feed>