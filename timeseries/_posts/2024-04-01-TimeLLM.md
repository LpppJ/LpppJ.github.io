---
layout: post
related_posts:
  _
title: 
description: >
  [ICLR 2024](https://arxiv.org/pdf/2310.01728.pdf)
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# (Time-LLM) Time Series Forecasting by Reprogramming Large Language Models

## Abstract

- CV, NLP는 single large model (또는 pre-trained foundation model)이 거의 모든 tasks에서 성능이 좋음
- 반면 TS는 **dat sparsity** 때문에 tasks마다 모델 디자인이 다름
- 본 논문에서는 TS와 NLP의 modality gap을 align하기 위해 Time-LLM을 제안
- **Prompt-as-Prefix**(PaP, reprogramming the input TS) $$\to$$ frozen LLM $$\to$$ forecasting

## 1. Introduction
- LLM을 forecasting model로 발전시키기 위해 필요한 것들
  - **Generalizability** : Capability for few-shot and zero-shot transfer learning, w/o pre-task retraining
  - **Data efficiency** : Performance new tasks with only a few examples(limited data)
  - **Reasoning** : Sophisticated reasoning $$\to$$ learned higher-level concepts $$\to$$ highly precise forecasting
  - **Multimodal knowledge** :  Diverse knowledge across modalities $$\to$$ synergistic forecasting that fuses different data types
  - **Easy optimization** : Once on massive computing $$\to$$ can be applied to forecasting tasks (without learning from scratch)
- **Align the modalities of TS & NLP** : Why challenging ?
  - 첫째로 NLP는 discrete tokens, TS는 본질적으로 continuous
  - 둘째로 TS reasoning 지식이 LLM pre-training 안에 없다.
- So, **Time-LLM**
  - Core idea : TS input을 LLM이 활용하기 쉬운 **text prototype**으로 reprogramming하는 framework (backbone model은 그대로)
  - **Prompt-as-Prefix (PaP)** : 1) enrich the input TS with additional context and 2) providing task instructions in the modality of NLP

## 2. Related Work
![사진1](/assets/img/timeseries/TimeLLM/fig1.jpeg)
- TS models는 task-specific $$\to$$ ex. ARIMA는 UTS를, LSTM은 seq를, TCN과 Transformer는 longer temporal dependencies를 위해 디자인 $$\to$$ versatility and generalizability 부족
- In-modality Adaptation : Pre-training (representation learning) $$\to$$​ fine-tuning (for downstream tasks) ! But **TS data sparsity.**..
- Cross-modality Adaptation (multimodal fine tuning) : Voice2Series(2021)은 TS를 acoustic model에 맞게 editing했고 LLM4TS는 first supervised pre-training on time series, then task-specific fine-tuning
- Time-LLM은 1) input TS를 수정하지도 않고, 2) backbone LLM을 fine tuning하지도 않는다. LLM이 잠재력을 발휘할 수 있도록 TS를 reprogramming

## 3. Methodology
- Goal : **Reprogram an embedding-visible language foundation model for general time series forecasting** without requiring any fine-tuning of the backbone model.
- $$f(\ \mathbf{X} \in \mathbb{R}^{N \times T}\ )= \hat{\mathbf{Y}} \in \mathbb{R}^{N \times H}$$, $$f$$는 input TS를 이해하고 예측.
  Loss : $$\frac{1}{H} \sum_{h=1}^H\left\|\hat{\mathbf{Y}}_h-\mathbf{Y}_h\right\|_F^2$$​
![사진2](/assets/img/timeseries/TimeLLM/fig2.jpeg)
- 3-main components
  - (1) input transformation
  - (2) a pre-trained and frozen LLM
  - (3) output projection
- Step 1) Channel independence : MTS $$\to$$​ N개의 UTS \\
  Step 2) Normalization, Patching, Embedding prior \\
  Step 3) Augment the LLM's Ts reasoning ability \\
  Step 4) Project to the final forecast $$\hat{\mathbf{Y}}^{i}$$
- Efficiency
  - only the parameters of the **lightweight input transformation and output projection** are updated. (backbone LLM is frozen)
  - directly optimizing $$\to$$ **small set of TS and a few training epochs**
  - for reduce memory footprint, **off-the-shelf techniques (e.g., quantization)**

### 3.1. Model Structure
- **Input Embedding**
  - step 1) RevIN $$\mathbf X^{(i)}$$
  - step 2) Patching with length $$L_p$$
    - Total number of input patches : $$P=\left\lfloor\frac{\left(T-L_p\right)}{S}\right\rfloor+2$$
    - Underlying motivations :
      -  Better preserving local semantic information by aggregating local information into each patch
      - Serving as tokenization to form a compact sequence of input tokens, reducing computational burdens.
  - step 3) Embedding w/ simple linear layer
    - $$\mathbf{X}_P^{(i)} \in \mathbb{R}^{P \times L_p} \to \mathbf{\hat X}_P^{(i)} \in \mathbb{R}^{P \times d_m}$$​
- **Patch Reprogramming**
![사진3](/assets/img/timeseries/TimeLLM/fig3.png)
  - Goal : to align the modalities of TS and natural language (TS 직접적인 수정 없이)
  - How : pre-trained word embedding $$\mathbf E \in \mathbb R^{V \times D}$$ in backbone.
    - But! no prior knowledge indicating which source token are directly relevant.
    - So, simply leveraging small collection of text prototypes by linearly probing $$\mathbf E$$, denoted as $$\mathbf E' \in \mathbb R^{V' \times D}, V^{\prime} \ll V $$ 
    - : efficient & allows for the adaptive selection of relevant source information
  - Multi-head cross-attention
    - query matrices $$\mathbf{Q}_k^{(i)}=\hat{\mathbf{X}}_P^{(i)} \mathbf{W}_k^Q$$
      key matrices $$\mathbf{K}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^K$$
      value matrices $$\mathbf{V}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^V$$
      where $$\mathbf{W}_k^Q \in \mathbb{R}^{d_m \times d}$$ and $$\mathbf{W}_k^K, \mathbf{W}_k^V \in \mathbb{R}^{D \times d}$$
      $$D$$ is the hidden dimension of the backbone model, and $$d=\left\lfloor\frac{d_m}{K}\right\rfloor$$
    - i-layer : $$\mathbf{Z}_k^{(i)}=\operatorname{ATTENTION}\left(\mathbf{Q}_k^{(i)}, \mathbf{K}_k^{(i)}, \mathbf{V}_k^{(i)}\right)=\operatorname{SOFTmax}\left(\frac{\mathbf{Q}_k^{(i)} \mathbf{K}_k^{(i) \top}}{\sqrt{d_k}}\right) \mathbf{V}_k^{(i)}$$
    - By aggregating each $$\mathbf{Z}_k^{(i)} \in \mathbb{R}^{P \times d}$$ in every head, $$\mathbf{Z}^{(i)} \in \mathbb{R}^{P \times d_m}$$,
      then linearly projected $$\to \mathbf{O}^{(i)} \in \mathbb{R}^{P \times D}$$

- **Patch-as-Prefix** (constraints...)
  - natural language로 표현된 TS를 예측
  - Constraints :
    - LLM은 **high-precision numerals** 연산에 sensitivity 떨어지고
    - LLM별로 서로 다른 후처리가 필요 ex.  0.61이 [’ 0 ‘, ‘, ‘, 6 ‘, ‘ 1 ‘] 또는 [’ 0 ‘, ‘, ‘, ‘61’]로 표시
- **Prompt-as-Prefix** (avoid constraints !)
  - (1) Dataset context : input TS의 essential background information을 LLM에 제공
    (2) Task instruction : task에 따른 patch embedding 가이드를 LLM에 제공
    (3) Input statistics : Enrich the input TS with additional statistics (trends, lags, ...)
- **Output Projection**
  - Prefixal part 버리고 output representation 얻어서 flatten하고 linear projection  $$\to \hat{\mathbf{Y}}^{(i)}$$

## 4. Main Results
### 4.1, 4.2. Long/Short-term Forecasting
![사진4](/assets/img/timeseries/TimeLLM/table12.png)
### 4.3. Few-shot Forecasting
![사진5](/assets/img/timeseries/TimeLLM/table3.png)
![사진6](/assets/img/timeseries/TimeLLM/table4.png)
### 4.4. Zero-shot Forecasting
![사진7](/assets/img/timeseries/TimeLLM/table5.png)
### 4.5. Model Analysis
![사진8](/assets/img/timeseries/TimeLLM/table67.png)

## 5. Conclusion and Future work
- TS를 test prototype으로 reprogramming해서 frozen LLM 통과
- Prompt-as-Prefix로 guidance를 LLM에 제공
- TS forecasting을 language task로 casting
- 결론적으로 Patching + Prompting으로 성능을 더 올린 모델