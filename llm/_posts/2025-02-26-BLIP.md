---
layout: post
related_posts:
  _
title: 
description: >
  [Arxiv 2024]()
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---

# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (PMLR 2022)

## Abstract

- 대부분의 Vision-Language Pre-training (VLP)는
  - either **understanding-based** tasks or generation-based tasks
  - scaling up the dataset with **noisy** image-textpairs collected from the web (sub-optimal)

- 본 논문에서는 BLIP 제안
  - transfers flexibly to both vision-language **understanding** and **generation** tasks
  - the noisy web data 활용 by bootstrapping the captions
    - generates syntheticcaptions and a filter removes the noisy ones.

## 1. Introduction

- Existing methods have 2 major **limitations**:
  - **Model** perspective:
    - encoder-based models는 less straightforward to directly transfer to text generation tasks
    - encoder-decoder models는 have not beensuccess fully adopted for image-text retrieval tasks
  - **Data** perspective:
    - SOTA(CLIP, ALBEF, SimVLM) 다 noisy web text 쓰다보니 suboptimal
- 본 논문에서 제시하는 BLIP의 2 **contribution**:
  - Multimodal mixture of Encoder-Decoder (**MED**):
    - unimodal encoder도 되고 image-grounded text encoder/decoder 다 됨
    - jointly pre-train
      - **image-text contrastive learning**,
      - **image-text matching**,
      - **image-conditioned language modeling**
  - Captioning and Filtering (**CapFilt**):
    - a **captioner**: to produce synthetic captions given web images
    - a **filter**: to remove noisy captions from both the original web texts and the synthetic texts

![그림1](/assets/img/llm/BLIP/fig1.png)

## 2. Related Work

### 2.1. Vision-language Pre-training

- Vision-language pre-training (VLP)들은 data crawled from the web 쓰다보니 suboptimal
  - 그러므로 web datasets를 잘 활용하는 **CapFilt**를 제안하겠음
- 문제는 **understanding-based** tasks (e.g. image-text retrieval)and generation-based tasks (e.g. image captioning) 둘다 잘해야 한다는 건데
  - 기존 encoder-based, encoder-decoder 다 잘 못해서
  - 본 논문에서 **multimodal mixture ofencoder-decoder model**을 제안하겠음

### 2.2. Knowledge Distillation

- Self-distillation: teacher and student have **equal sizes**
- 기존 KD: simply enforce the student to have the **same** class predictions as the teacher,
- CapFilt: the **captioner** distills its knowledge through semantically-rich synthetic captions
  - the **filter** distills its knowledge by removing noisy captions

### 2.3. Data Augmentation

- synthesize examples for various NLP tasks
  - 기존: the low-resourcelanguage-only tasks
  - BLIP: the advantage of synthetic captions in large-scale vision-language pre-training

## 3. Method

- 기본적으로 noisy image-text pairs에서 학습. MED와 CapFilt를 알아보자

![그림1](/assets/img/llm/BLIP/fig2.png)

### 3.1. Model Architecture

- Multimodal mixture of encoder-decoder (MED)는 다음 3가지 가능한 multi-task model
  - (1) **Unimodal Encoder**: separately encodes imageand text (ViT, BERT)
  - (2) **Image-grounded text encoder**: text encoder인데 visual 정보 삽입
    - how? Self-attention과 FFN 사이에 cross-attention 하나 추가
  - (3) **Image-grounded text decoder**: (2)에서 bi-directional self-attention을 causal self-attention으로 대체

### 3.2. Pre-training Objectives

- 2 **understanding**-based objectives & 1 **generation**-based objective
- Each image-text pair는 **one** forward pass **visual** transformer & **three** forward passes **text** transformer

### Image-Text Contrastive Loss (ITC)

- unimodal encoder를 activate
  - **visual** transformer and the **text** transformer의 feature space를 align
  - **positive** image-text pairs가 similar representations 갖도록 (negative는 반대)
    - momentum encoder가 features와 soft labels만들고 pairs로 학습

### Image-Text Matching Loss (ITM)

- image grounded text encoder를 activate
- image-text multi-modal representation을 통해 vision-language align
  - predict whether an image-text pair is positive, given their multi-modal feature

### Language Modeling Loss (LM)

- image grounded text decoder를 activate
- generate textual descriptions given an image
  - optimizes a cross entropy loss, to maximize the likelihood of the text, in an autoregressive manner.

### 3.3. CapFilt

![그림1](/assets/img/llm/BLIP/fig3.png)

- **captioner**: to generate captions given web images
  - image-grounded text **de**coder
  - finetuned with the LM objective to **decode texts given images**
  - web images $$I_w$$보고 synthetic captions $$T_s$$ 생성
- **filter**: to remove noisy image-text pairs
  - image-grounded text **en**coder
  - finetune with the ITC and ITM objectives to learn **whether a text matches an image**
  - removes noisy texts in both the original web texts $$T_w$$ and the synthetic texts $$T_s$$
- both are initialized from the same pre-trained MED, finetuned individually on the COCO dataset

## 4. Experiments and Discussions

### 4.1. Pre-training Details

- image transformer is initialized from ViT pre-trained on ImageNet
- the text transformer is initialized from BERT-base

### 4.2. Effect of CapFilt

![그림1](/assets/img/llm/BLIP/table1.png)

- captioner와 filter 둘 중 하나만 써도 성능 개선, 둘 다 쓰면 더더욱 개선

![그림1](/assets/img/llm/BLIP/fig4.png)

- example captions and their corresponding images
  - the captioner to generate new textual descriptions
  - the filter to remove noisy captions from both the originalweb texts and the synthetic texts

### 4.3. Diversity is Key for Synthetic Captions

![그림1](/assets/img/llm/BLIP/table2.png)

- Nucleus sampling is astochastic decoding method
  - each token is sampled from a set of tokens whose cumulative probability mass exceeds a threshold $$p$$ ($$p=0.9$$)
  - beam search, a deterministic decoding method와 비교했을 때 더 뛰어남
- nucleus sampling generates more **diverse** and surprising captions
- beam search tends to generate **safe** captions that are common inthe dataset

### 4.4. Parameter Sharing and Decoupling

![그림1](/assets/img/llm/BLIP/table3.png)

-  Pre-training에서 the text encoder and decoder share all parameters
  - except for the SA(self-attention) layers
  - 만약 SA layer도 공유했으면 encoding과 decoding이 충돌해서 성능 안좋아짐

![그림1](/assets/img/llm/BLIP/table4.png)

- if the captioner and filter share parameters in the same way as pre-training,
  - **confirmatio-bias**: parameter sharing 때문에 captioner가 만든 noisy captions가 filter out이 잘 안됨

## 5. Comparison with State-of-the-arts

### 5.1. Image-Text Retrieval

- image-to-text retrieval (TR) & text-to-image retrieval (IR)
  - finetune the pre-trained modelusing ITC and ITM losses

![그림1](/assets/img/llm/BLIP/table5.png)

### 5.2. Image Captioning

![그림1](/assets/img/llm/BLIP/table7.png)

- 14M 데이터로 pre-train 해도 성능이 준수하고 129M로 pre-train하면 LEMON의 200M 만큼의 성능

### 5.3. Visual Question Answering (VQA)

![그림1](/assets/img/llm/BLIP/fig5.png)

- During finetuning, pre-trained model을 re-arrange (그림 5 (a))
  - image-question is first encoded into multi modal embeddings
  - 그 다음 given to an answer decoder

### 5.4. Natural Language Visual Reasoning (NLVR)

- NLVR: 한 문장이 두 그림 중 어떤 걸 설명하는지 맞추는 것
- 사진이 2장이다보니 two cross-attention layers가 있고 (그림 5 (b))
  - 2 CA layers are intialized from the same pre-trained weights
  - 각각의 outputs는 merged & fed to the FFN

![그림1](/assets/img/llm/BLIP/table8.png)

### 5.5. Visual Dialog (VisDial)

- VisDial: VQA의 확장. image-question pair뿐만 아니라 대화 기록, captions보고 예측
- concatenate image and caption embeddings해서 cross-attention 통해 dialog encoder에게 전달
- 그러면 ITM loss로 학습된 dialog encoder가 discriminate whether the answer is true or false for a question

### 5.6. Zero-shot Transfer to Video-Language Tasks

- Strong generalization abilityto video-language tasks
  - zero-shot transfer to text-to-video retrieval and video question answering

![그림1](/assets/img/llm/BLIP/table10.png)

![그림1](/assets/img/llm/BLIP/table11.png)

## 6. Additional Ablation Study

![그림1](/assets/img/llm/BLIP/table12.png)

- Improvement with CapFilt is not due to longer training

![그림1](/assets/img/llm/BLIP/table13.png)

- A new model should be trained on the bootstrapped dataset

## 7. Conclusion

- BLIP pre-trains a multimodal mixture of encoder-decoder model
  - using a dataset bootstrapped from large-scale noisy image-text pairs
  - by injecting diversesynthetic captions and removing noisy captions

- Potential directions that can further enhancethe performance of BLIP:
  - Multiple rounds of dataset bootstrapping
  - Generate multiple synthetic captions per image to further enlarge the pre-training corpus
  - Model ensemble by training multiple different captioners and filter and combining their forces in CapFilt

