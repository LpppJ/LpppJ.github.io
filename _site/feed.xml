<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-20T17:34:48+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">LpppJ</title><subtitle>This is blog about machine learning, deep learning, artificial intelligence.
</subtitle><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><entry><title type="html">(Crossformer) Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting (ICLR 2023)</title><link href="http://localhost:4000/timeseries/2024-03-19-crossformer/" rel="alternate" type="text/html" title="(Crossformer) Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting (ICLR 2023)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-19T21:34:48+09:00</updated><id>http://localhost:4000/timeseries/crossformer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-crossformer/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer-based models
    <ul>
      <li>focus on modeling the temporal dependency (cross-time dependency)</li>
      <li>yet often omit the dependency among different variables (cross- dimension dependency)</li>
    </ul>
  </li>
  <li>Crossformer는
    <ul>
      <li>Dimension-Segment-Wise (DSW) : MTS \(\to\) 2d vector array로 만들고</li>
      <li>Two-Stage Attention (TSA) : 2개의 attention을 거치는데 각각 cross-time and cross-dimension dependency를 학습한다.</li>
      <li>Hierarchical Encoder-Decoder (HED) : 그리고 서로 다른 scales의 정보를 사용해서 coarse, fine한 정보 모두 활용하여 forecasting한다.</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>MTS에서는 cross-time dependency 뿐만 아니라 cross-dimension dependency도 중요한데, Transformer에서 cross-dim dependency를 반영하는 방법은 embedding 뿐이다.</li>
  <li>본 논문에서는 cross-dim dependency를 explicitly하게 사용한다.</li>
  <li>Dimension-Segment-Wise (DSW) : the series(e.g. UTS)는 segments로 나뉘고, 각 segment는 feature vector가 된다. (series는 2d vector array가 된다.)</li>
  <li>Two-Stage Attention (TSA): 2d vector array로부터 cross-time and cross-dimension dependency 학습</li>
  <li>Hierarchical Encoder-Decoder (HED) : 각 layer에서 서로 다른 scale에 대한 dependency를 학습하게 된다.</li>
</ul>

<h2 id="2-related-works">2. Related Works</h2>

<ul>
  <li><strong>Multivariate Time Series Forecasting</strong>
    <ul>
      <li>Statistical models : Vector auto-regressive(VAR), Vector auto-regressive moving average (VARMA)</li>
      <li>Neural models : TCN, DeepAR, LSTnet(CNN+RNN), MTGNN, …</li>
    </ul>
  </li>
  <li><strong>Transformer-based model</strong> : LogTrans, Informer, Autoformer, Pyraformer, FEDformer, Preformer, …</li>
  <li><strong>Vision Transformers</strong> : Transformer를 vision에서 사용할 때 썼던 patching 방식</li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>

<ul>
  <li>\(\mathbf{x}_{1: T} \in \mathbb{R}^{T \times D}\)를 보고 \(\mathbf{x}_{T+1: T+\tau} \in \mathbb{R}^{\tau \times D}\)​ 예측하는 문제
    <ul>
      <li>\(\tau, T\) is the number of time steps in the future and past, respectively</li>
      <li>\(D&gt;1\) is the number of dimensions</li>
    </ul>
  </li>
</ul>

<h3 id="31-dimension-segment-wise-embedding">3.1. Dimension-Segment-wise Embedding</h3>
<p><img src="/assets/img/timeseries/crossformer/fig1.png" alt="사진1" /></p>
<ul>
  <li>t시점의 모든 dimension의 data point \(\mathbf{x}_t \in \mathbb{R}^D\)를 \(\mathbf{h}_t \in \mathbb{R}^{d_{\text {model }}}\)로 embedding한다.</li>
  <li>
\[\begin{aligned}
\mathbf{x}_{1: T} &amp; =\left\{\mathbf{x}_{i, d}^{(s)} \left\lvert\, 1 \leq i \leq \frac{T}{L_{\text {seg }}}\right., 1 \leq d \leq D\right\} \\
\mathbf{x}_{i, d}^{(s)} &amp; =\left\{x_{t, d} \mid(i-1) \times L_{\text {seg }}&lt;t \leq i \times L_{\text {seg }}\right\} \\ \mathbf{h}_{i, d}&amp;=\mathbf{E} \mathbf{x}_{i, d}^{(s)}+\mathbf{E}_{i, d}^{(p o s)} \end{aligned}\]
    <ul>
      <li>\(\mathbf{x}_{i, d}^{(s)} \in \mathbb{R}^{L_{\text {seg }}}\)  is the \(i\)-th segment in dimension \(d\) with length \(L_{\text {seg }}\)</li>
      <li>\(\mathbf{E} \in \mathbb{R}^{d_{\text {model }} \times L_{\text {seg }}}\) : the learnable projection matrix</li>
      <li>\(\mathbf{E}_{i, d}^{(\text {pos })} \in \mathbb{R}^{d_{\text {model }}}\) : the learnable position embedding for position $(i, d)$.</li>
    </ul>
  </li>
  <li>
\[\mathbf{H}=\left\{\mathbf{h}_{i, d} \mid, 1 \leq i \leq \frac{T}{L_{\text {seg }}}, 1 \leq d \leq D\right\}\]
    <ul>
      <li>where each \(\mathbf{h}_{i, d}\) represents a univariate time series segment.</li>
    </ul>
  </li>
  <li>수식으로 표현하다보니 어려운데 아래 그림과 같고, \(\mathbf{H}\)는 오른쪽처럼 생겼다.
<img src="/assets/img/timeseries/crossformer/myfig1.jpeg" alt="사진2" /></li>
</ul>

<h3 id="32-two-stage-attention-layer">3.2. Two-Stage Attention Layer</h3>
<ul>
  <li>이미지가 아니라 시계열이다보니 height와 width가 서로 바뀌면 의미가 달라지기 때문에 flatten시키면 안되고 바로 \(\mathbf{H}\)에 self-attention을 적용한다.</li>
  <li><strong>Cross-Time Stage</strong>
    <ul>
      <li>\(\mathbf{Z}_{i, \text { : }}\) : the vectors of all dimensions at time step \(i\)</li>
      <li>\(\mathbf{Z}_{:, d}\) the vectors of all time steps in dimension \(d\)
<img src="/assets/img/timeseries/crossformer/myfig2.jpeg" alt="사진3" /></li>
      <li>
\[\begin{aligned} \hat{\mathbf{Z}}_{:, d}^{\text {time }}=\text { LayerNorm }\left(\mathbf{Z}_{:, d}+\operatorname{MSA}^{\text {time }}\left(\mathbf{Z}_{:, d}, \mathbf{Z}_{:, d}, \mathbf{Z}_{:, d}\right)\right) \\ \mathbf{Z}^{\text {time }}=\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {time }}+\operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {time }}\right)\right) \end{aligned}\]
      </li>
      <li>\(\mathbf{Z}^{time}\)이 다음 stage인 Cross-Dimension Stage의 input이 된다.</li>
    </ul>
  </li>
  <li><strong>Cross-Dimension Stage</strong>
    <ul>
      <li>\(\begin{aligned} \mathbf{B}_{i,:} &amp; =\mathrm{MSA}_1^{\operatorname{dim}}\left(\mathbf{R}_{i,:}, \mathbf{Z}_{i,:}^{\text {time }}, \mathbf{Z}_{i,:}^{\text {time }}\right), 1 \leq i \leq L \\ \overline{\mathbf{Z}}_{i,:}^{\text {dim }} &amp; =\mathrm{MSA}_2^{\text {dim }}\left(\mathbf{Z}_{i,:}^{\text {time }}, \mathbf{B}_{i,:}, \mathbf{B}_{i,:}\right), 1 \leq i \leq L \\ \hat{\mathbf{Z}}^{\text {dim }} &amp; =\text { LayerNorm }\left(\mathbf{Z}^{\text {time }}+\overline{\mathbf{Z}}^{\text {dim }}\right) \\ \mathbf{Z}^{\text {dim }} &amp; =\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {dim }}+\operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {dim }}\right)\right) \end{aligned}\)
<img src="/assets/img/timeseries/crossformer/fig2.png" alt="사진4" /></li>
      <li>\(D\)가 클 때에는 router mechanism을 사용하여 fixed number \(c &lt;&lt; D\) vectors에 정보를 모았다가 다시 뿌려준다.</li>
      <li>\(\mathbf{R} \in \mathbb{R}^{L \times c \times d_{\text {model }}}\) : the learnable vector array serving as routers</li>
      <li>\(\mathbf{B} \in \mathbb{R}^{L \times c \times d_{\text {model }}}\) : the aggregated messages from all dimensions</li>
      <li>\(\overline{\mathbf{Z}}^{\text {dim }}\) : output of the router mechanism.</li>
      <li>All time steps \((1 \leq i \leq L)\) share the same \(\mathbf{M S A}_1^{\text {dim }}, \mathbf{M S A}_2^{\text {dim }}\)</li>
      <li>\(\hat{\mathbf{Z}}^{\text {dim }}, \mathbf{Z}^{\text {dim }}\) : output of skip connection and MLP respectively</li>
    </ul>
  </li>
</ul>

<h3 id="33-hierarchical-encoder-decoder">3.3. Hierarchical Encoder-Decoder</h3>
<p><img src="/assets/img/timeseries/crossformer/fig3.png" alt="사진5" /></p>
<ul>
  <li>Upper layer일수록 coarser scale을 사용한 정보를 얻고, 서로 다른 scale로 얻은 정보들로 예측한 값들은 final result에서 더해진다.</li>
  <li><strong>Encoder</strong> : upper layer일수록 coarser scale을 사용한다는 말은 인접한 두 vector(segment)를 merge한다는 것과 같다.</li>
  <li>\(\mathbf{Z}^{e n c, l}=\operatorname{Encoder}\left(\mathbf{Z}^{e n c, l-1}\right)\)의 연산은 아래와 같다.
\(\begin{aligned} &amp; \begin{cases}l=1: &amp; \hat{\mathbf{Z}}^{e n c, l}=\mathbf{H} \\ l&gt;1: &amp; \hat{\mathbf{Z}}_{i, d}^{e n c, l}=\mathbf{M}\left[\mathbf{Z}_{2 i-1, d}^{e n c, l-1} \cdot \mathbf{Z}_{2 i, d}^{e n c, l-1}\right], 1 \leq i \leq \frac{L_{l-1}}{2}, 1 \leq d \leq D\end{cases} \\&amp; \mathbf{Z}^{\text {enc,l}}=\operatorname{TSA}\left(\hat{\mathbf{Z}}^{\text {enc,l}}\right) \end{aligned}\)
    <ul>
      <li>
        <p>\(\mathbf{H}\) denotes the 2D array obtained by DSW embedding</p>
      </li>
      <li>
        <p>\(\mathbf{Z}^{e n c, l}\) denotes the output of the \(l\)-th encoder layer</p>
      </li>
      <li>\(\mathbf{M} \in \mathbb{R}^{d_{\text {model }} \times 2 d_{\text {model }}}\) denotes a learnable matrix for segment merging</li>
      <li>\([\cdot]\) denotes the concatenation operation</li>
      <li>\(L_{l-1}\) denotes the number of segments in each dimension in layer \(l-1\)</li>
      <li>\(\hat{\mathbf{Z}}^{e n c, l}\) denotes the array after segment merging in the \(i\)-th layer</li>
      <li>\(\mathbf{Z}^{\text {enc }, 0}, \mathbf{Z}^{\text {enc }, 1}, \ldots, \mathbf{Z}^{\text {enc }, N},\left(\mathbf{Z}^{\text {enc }, 0}=\mathbf{H}\right)\) is used to represent the \(N+1\) outputs of the encoder</li>
    </ul>
  </li>
  <li><strong>Decoder</strong> : Emcoder에서 얻은 \(N+1\)개의 feature array가 있으면, \(N+1\)개의 layers로 예측한다.
    <ul>
      <li>decoder의 process : \(\mathbf{Z}^{\text {dec, } l}=\operatorname{Decoder}\left(\mathbf{Z}^{\text {dec, },-1}, \mathbf{Z}^{\text {enc, },}\right)\)
\(\begin{aligned} &amp; \left\{\begin{array}{lll} l=0: &amp; \tilde{\mathbf{Z}}^{\text {dec }, l}=\operatorname{TSA}\left(\mathbf{E}^{(d e c)}\right) \\ l&gt;0: &amp; \tilde{\mathbf{Z}}^{\text {dec, }, l}=\operatorname{TSA}\left(\mathbf{Z}^{\text {dec, },-1}\right) \end{array}\right. \\ &amp; \overline{\mathbf{Z}}_{:, d}^{\text {dec, }, l}=\operatorname{MSA}\left(\tilde{\mathbf{Z}}_{:, d}^{\text {dec, }, l}, \mathbf{Z}_{:, d}^{e n c, l}, \mathbf{Z}_{:, d}^{e n c, l}\right), 1 \leq d \leq D \\ &amp; \hat{\mathbf{Z}}^{\text {dec, } l}=\text { LayerNorm }\left(\tilde{\mathbf{Z}}^{\text {dec, }, l}+\overline{\mathbf{Z}}^{\text {dec, } l}\right) \\ &amp; \mathbf{Z}^{\text {dec,l}}=\text { LayerNorm }\left(\hat{\mathbf{Z}}^{\text {dec, }, l} \operatorname{MLP}\left(\hat{\mathbf{Z}}^{\text {dec,l}}\right)\right) \\ &amp; \end{aligned}\)</li>
      <li>\(\mathbf{E}^{(\text {dec })} \in \mathbb{R}^{\frac{\tau}{L_{s e g}} \times D \times d_{\text {model }}}\) denotes the learnable position embedding for decoder</li>
      <li>\(\tilde{\mathbf{Z}}^{\text {dec,l } l}\) is the output of TSA</li>
      <li>The MSA layer takes \(\tilde{\mathbf{Z}}_{:, d}^{d e c,l }\) as query and \(\mathbf{Z}_{:, d}^{e n c, l}\) as the key and value to build the connection between encoder and decoder</li>
      <li>The output of MSA is denoted as \(\overline{\mathbf{Z}}_{:, d}^{\text {dec, },} . \hat{\mathbf{Z}}^{\text {dec,l}, ~} \mathbf{Z}^{\text {dec, }, l}\) denote the output of skip connection and MLP respectively.</li>
      <li>\(\mathbf{Z}^{\text {dec, 0},}, \mathbf{Z}^{e n c, 1}, \ldots, \mathbf{Z}^{\text {dec, } N}\) : is used to represent decoder output</li>
      <li><strong>Linear projection</strong> : 각 layer에서는 linear projection으로 prediction을 만들고, 각 layer의 prediction을 다 더하면 최종 prediction이 된다.
\(\begin{gathered} \text { for } l=0, \ldots, N: \mathbf{x}_{i, d}^{(s), l}=\mathbf{W}^l \mathbf{Z}_{i, d}^{\text {dec,l }} \quad \mathbf{x}_{T+1: T+\tau}^{\text {pred, } l}=\left\{\mathbf{x}_{i, d}^{(s), l} \left\lvert\, 1 \leq i \leq \frac{\tau}{L_{\text {seg }}}\right., 1 \leq d \leq D\right\} \\ \mathbf{x}_{T+1: T+\tau}^{\text {pred }}=\sum_{l=0}^N \mathbf{x}_{T+1: T+\tau}^{\text {pred, }}\end{gathered}\)
        <ul>
          <li>\(\mathbf{W}^l \in \mathbb{R}^{L_{\text {seg }} \times d_{\text {model }}}\) : learnable matrix to project a vector to a ts segment</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<h3 id="41-protocols">4.1. Protocols</h3>
<ul>
  <li>Dataset : 1) ETTh1 (Electricity Transformer Temperature-hourly), 2) ETTm1 (Electricity Transformer Temperature-minutely), 3) WTH (Weather), 4) ECL (Electricity Consuming Load), 5) ILI (Influenza-Like Illness), 6) Traffic</li>
  <li>Baselines : 1) LSTMa (Bah- danau et al., 2015), 2) LSTnet (Lai et al., 2018), 3) MTGNN (Wu et al., 2020), and recent Transformer-based models for MTS forecasting: 4) Transformer (Vaswani et al., 2017), 5) In- former (Zhou et al., 2021), 6) Autoformer (Wu et al., 2021a), 7) Pyraformer (Liu et al., 2021a) and 8) FEDformer (Zhou et al., 2022)
    <h3 id="42-main-results">4.2. Main Results</h3>
    <p><img src="/assets/img/timeseries/crossformer/table1.png" alt="사진6" /></p>
    <h3 id="43-ablation-study">4.3. Ablation Study</h3>
    <p><img src="/assets/img/timeseries/crossformer/table2.png" alt="사진7" /></p>
    <h3 id="44-effect-of-hyper-parameters">4.4. Effect of Hyper-parameters</h3>
    <p><img src="/assets/img/timeseries/crossformer/fig4.png" alt="사진8" /></p>
    <h3 id="45-computational-efficiency-analysis">4.5. Computational Efficiency Analysis</h3>
    <p><img src="/assets/img/timeseries/crossformer/table3.png" alt="사진9" /></p>
  </li>
</ul>

<h3 id="5-conclusion">5. Conclusion</h3>
<ul>
  <li>Crossformer : Transformer-based model utilizing cross-dimension dependency for MTS forecasting</li>
  <li><strong>Dimension-Segment-Wise (DSW)</strong> embedding embeds the input data into a 2D vector array to preserve the information of both time and dimension</li>
  <li><strong>The Two-Stage-Attention (TSA)</strong> layer is devised to capture the cross-time and cross- dimension dependency of the embedded array</li>
  <li>Using DSW embedding and TSA layer, a <strong>Hierarchical Encoder-Decoder (HED)</strong> is devised to utilize the information at different scales</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2023](https://openreview.net/forum?id=vSVLM2j9eie)]]></summary></entry><entry><title type="html">(Survey paper) Transformers in Time Series: A Survey (IJCAI 2023)</title><link href="http://localhost:4000/timeseries/2024-03-19-TFinTSsurvey/" rel="alternate" type="text/html" title="(Survey paper) Transformers in Time Series: A Survey (IJCAI 2023)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-20T17:18:03+09:00</updated><id>http://localhost:4000/timeseries/TFinTSsurvey</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-TFinTSsurvey/"><![CDATA[<h2 id="abstract">Abstract</h2>

<ul>
  <li>Transformer는 long-range dependencies and interactions를 학습할 수 있다.</li>
  <li>본 논문에서는 Network structure 관점에서 Transformer를 TS forecasting에 사용하기 위해 어떤 adaptaion and modification을 했는지 알아보고</li>
  <li>Application 관점에서 forecasting, anomaly detection, and classification을 포함한 task에 대해 얼마나 잘 작동하는지 알아본다.</li>
  <li>마지막으로 future direction을 제시한다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<ul>
  <li>Transformer : ability for long-range dependencies and interactions in sequential data</li>
  <li>Time series : How to effectively model long-range and short-range temporal dependency and capture seasonality simultaneously ?</li>
  <li>Network modification 관점 : low-level(i.e. module)부터 high-level(i.e. architecture)</li>
  <li>Application 관점 : summarize Transformer for forecasting, anomaly detection, and classification</li>
</ul>

<h2 id="2-preliminaries-of-the-transformer">2. Preliminaries of the Transformer</h2>

<h3 id="21-vanilla-transformer">2.1. Vanilla Transformer</h3>

<ul>
  <li>Encoder : a multi- head self-attention module and a position-wise feed-forward network</li>
  <li>Decoder : cross-attention models between the multi-head self-attention module and the position-wise feed-forward network</li>
</ul>

<h3 id="22-input-encoding-and-positional-encoding">2.2. Input Encoding and Positional Encoding</h3>

<ul>
  <li>No recurrence, instead positional encoding</li>
  <li>Absolute Positional Encoding : \(PE(t)_i= \begin{cases}\sin \left(\omega_i t\right) &amp; i \% 2=0 \\ \cos \left(\omega_i t\right) &amp; i \% 2=1\end{cases}\)
    <ul>
      <li>\(\quad \omega_i\) is the hand-crafted frequency for each dim</li>
    </ul>
  </li>
  <li>Relative Positional Encoding : input의 상대적인 위치에 대해 learnable하지만 train에서 본 적 없는, 더 긴 길이의 input에 대해서 확장이 어려움</li>
</ul>

<h3 id="23-multi-head-attention">2.3. Multi-head Attention</h3>

<ul>
  <li>
    <p>scaled dot-product : \(\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q K}^{\mathbf{T}}}{\sqrt{D_k}}\right) \mathbf{V}\)</p>
  </li>
  <li>
    <p>Multi-head Attention :</p>

\[\begin{aligned}MultiHeadAttn (\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Concat}\left(\right. head _1, \cdots, head \left._H\right) \mathbf{W}^O \\ \text{where } head _i= Attention \left(\mathbf{Q} \mathbf{W}_i^Q, \mathbf{K} \mathbf{W}_i^K, \mathbf{V} \mathbf{W}_i^V\right)\end{aligned}\]
  </li>
</ul>

<h3 id="24-feed-forward-and-residual-network">2.4. Feed-forward and Residual Network</h3>

<ul>
  <li>The feed-forward network(FFN) : \(FFN\left(\mathbf{H}^{\prime}\right)=\operatorname{ReLU}\left(\mathbf{H}^{\prime} \mathbf{W}^1+\mathbf{b}^1\right) \mathbf{W}^2+\mathbf{b}^2\)
    <ul>
      <li>\(\mathbf{H}^{\prime}\) is outputs of previous layer</li>
      <li>\(\mathbf{W}^1 \in \mathcal{R}^{D_m \times D_f}\), \(\mathbf{W}^2 \in \mathcal{R}^{D_f \times D_m}, \mathbf{b}^1 \in \mathcal{R}^{D_f}, \mathbf{b}^2 \in \mathcal{R}^{D_m}\) are trainable parameters</li>
    </ul>
  </li>
  <li>Residual connection module followed by a layer normalization module : \(\begin{aligned} \mathbf{H}^{\prime} &amp; =\operatorname{LayerNorm}(\operatorname{Self} \operatorname{Attn}(\mathbf{X})+\mathbf{X}), \\ \mathbf{H} &amp; =\operatorname{LayerNorm}\left(FFN\left(\mathbf{H}^{\prime}\right)+\mathbf{H}^{\prime}\right)\end{aligned}\)
    <ul>
      <li>SelfAttn(.) : self-attention module</li>
      <li>LayerNorm(.) : the layer normalization operation</li>
    </ul>
  </li>
</ul>

<h2 id="3-taxonomy-of-transformers-in-time-series">3. Taxonomy of Transformers in Time Series</h2>

<p><img src="/assets/img/timeseries/TFinTSsurvey/fig1.png" alt="사진1" /></p>

<h2 id="4-network-modifications-for-time-series">4. Network Modifications for Time Series</h2>

<h3 id="41-positional-encoding">4.1. Positional Encoding</h3>

<ul>
  <li>Vanilla Positional Encoding : fixed, hand-crafted</li>
  <li>Learnable Positional Encoding : more flexible and can adapt to spe- cific tasks</li>
  <li>Timestamp Encoding : calendar timestamps (e.g., second, minute, hour, week, month, and year) and special times- tamps (e.g., holidays and events) \(\to\) additional position encoding</li>
</ul>

<h3 id="42-attention-module">4.2. Attention Module</h3>

<ul>
  <li>Self-attention module : FC layer w weights that are dynamically generated based on the pairwise similarity of input patterns</li>
  <li>Memory complexity \(O(N^2)\)
    <ul>
      <li>explicitly introducing a sparsity bias into the attention mechanism
        <ul>
          <li>e.g. LogTrans [Li <em>et al.</em>, 2019, Pyraformer [Liu <em>et al.</em>, 2022a]</li>
        </ul>
      </li>
      <li>exploring the low-rank property of the self-attention matrix to speed up the computation,
        <ul>
          <li>e.g. Informer [Zhou <em>et al.</em>, 2021], FEDformer [Zhou <em>et al.</em>, 2022].</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table1.png" alt="사진2" /></p>

<h3 id="43-architecture-based-attention-innovation">4.3. Architecture-based Attention Innovation</h3>

<ul>
  <li>Hierarchical architecture : multi-resolution aspect of TS
    <ul>
      <li>Informer [Zhou <em>et al.</em>, 2021]
        <ul>
          <li>max-pooling layers with stride 2 btw attention blocks (down-sample series into its half slice)</li>
        </ul>
      </li>
      <li>Pyraformer [Liu <em>et al.</em>, 2022a]
        <ul>
          <li>C-ary tree-based attention mechanism (finest-origin / coarser-lower resolutions)</li>
          <li>both intra-scale and inter-scale attentions \(\to\) temporal dependencies across different resolutions</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-applications-of-time-series-transformers">5. Applications of Time Series Transformers</h2>

<p><img src="/assets/img/timeseries/TFinTSsurvey/fig2.jpeg" alt="사진9" /></p>

<h3 id="51-transformers-in-forecasting">5.1. Transformers in Forecasting</h3>

<ul>
  <li><strong>Module-level variants</strong> : main architectures는 비슷한데, minor changes
    <ul>
      <li>(1) designing new attention modules
        <ul>
          <li>LogTrans [Li <em>et al.</em>, 2019] : convolution self-attention, sparse bias (Logsparse mask)</li>
          <li>Informer [Zhou <em>et al.</em>, 2021] : selects dominant queries based on queries and key similarities</li>
          <li>AST [Wu <em>et al.</em>, 2020a] : generative adversarial encoder- decoder framework to train a sparse Transformer</li>
          <li>Pyraformer [Liu <em>et al.</em>, 2022a] : hierarchical pyramidal attention module with a binary tree following the path</li>
          <li>Quatformer [Chen <em>et al.</em>, 2022] : learning-to-rotate attention (LRA) based on quaternions that introduce learnable period and phase information</li>
          <li>FEDformer [Zhou <em>et al.</em>, 2022] : attention operation in the frequency domain with Fourier trans- form and wavelet transform</li>
        </ul>
      </li>
      <li>(2) exploring the innovative way to normalize time series data
        <ul>
          <li>Non-stationary Transformer [Liu <em>et al.</em>, 2022b]</li>
        </ul>
      </li>
      <li>(3) utilizing the bias for token inputs
        <ul>
          <li>Autoformer [Wu <em>et al.</em>, 2021] : segmentation-based representation mechanism (auto-correlation mechanism)</li>
          <li>PatchTST [Nie <em>et al.</em>, 2023] : subseries-level patch design which are served as input tokens w/ channel-independency</li>
          <li>Cross- former [Zhang and Yan, 2023] : input is embedded into a 2D vector array and then two-stage attention layer is used to efficiently capture the cross-time and cross-dimension dependency</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Architecture-level variants</strong>
    <ul>
      <li>Triformer [Cirstea <em>et al.</em>, 2022] : triangular,variable-specific patch attention \(\to\) lightweight and linear complex- ity</li>
      <li>Scaleformer [Shabani <em>et al.</em>, 2023] : iteratively refine the forecasted time series at multiple scales with shared weights.</li>
    </ul>
  </li>
  <li><strong>Spatio-temporal Forecasting, Evnet Forecasting</strong>
    <ul>
      <li>Pass</li>
    </ul>
  </li>
</ul>

<h3 id="52-transformers-in-anomaly-detection">5.2. Transformers in Anomaly Detection</h3>

<ul>
  <li>
    <p>Transformer + Generative models</p>

    <ul>
      <li>
        <p>TranAD [Tuli <em>et al.</em>, 2022] : Transformer는 small deviation of anomaly는 놓치므로 reconstruction errors를 amplify하는 adversarial training</p>
      </li>
      <li>
        <p>MT-RVAE [Wang <em>et al.</em>, 2022], TransAnomaly [Zhang <em>et al.</em>, 2021] : Transformer + VAE \(\to\) reduce training costs, 다양한 scale의 정보 통합</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="53-transformers-in-classification">5.3 Transformers in Classification</h3>

<ul>
  <li>GTN [Liu <em>et al.</em>, 2021] : two-tower Transformer(time-step-wise attention and channel-wise attention) \(\to\)  learnable weighted concatenation</li>
  <li>TARNet [Chowdhury <em>et al.</em>, 2022] : utilizes attention score for important timestamps masking and reconstruction</li>
</ul>

<h2 id="6-experimental-evaluation-and-discussion">6. Experimental Evaluation and Discussion</h2>

<ul>
  <li><strong>Robustness Analysis</strong>
    <ul>
      <li>대부분의 attention-based models는 lower the quadratic calculation and memory complexity를 위해 module을 수정했고, 좋은 실험 결과를 위해 짧은 input을 사용했는데, 긴 input을 넣어도 MSE가 커지지 않고 잘 유지되는지 확인했다.</li>
      <li>대부분의 모델들이 긴 input에 대해서는 잘 처리하지 못한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table2.png" alt="사진3" /></p>

<ul>
  <li><strong>Model Size Analysis</strong>
    <ul>
      <li>일반적으로 model size가 커지면 prediction power도 좋아지는데 attention-based models에서 그렇지 않음을 확인했다.</li>
      <li>지금까지의 Transformer 자체가 features를 잘 뽑아내지 못하는 구조일 수 있겠다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table3.png" alt="사진4" /></p>

<ul>
  <li><strong>Seasonal-Trend Decomposition Analysis</strong>
    <ul>
      <li>seasonal-trend decomposition는 Transformer에서 필수적인 부분 : model performance가 50% - 80% boosting</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/timeseries/TFinTSsurvey/table4.png" alt="사진5" /></p>

<h2 id="7-future-research-opportunities">7. Future Research Opportunities</h2>

<h3 id="71-inductive-biases-for-time-series-transformers">7.1. Inductive Biases for Time Series Transformers</h3>

<ul>
  <li>Channel-independence와 Cross-channel(dim) dependency은 서로 반대 inductive bias이지만 둘 다 실험 결과가 좋았다.</li>
  <li>즉 cross-channel learning에는 noise도 있고 signal도 있다는 의미</li>
  <li>어떤 inductive bias를 어떻게 induce할지 고려할 필요가 있음</li>
</ul>

<h3 id="72-transformers-and-gnn-for-time-series">7.2. Transformers and GNN for Time Series</h3>

<ul>
  <li>Traffic forecsting처럼 spatial dependency (relationship among dim)이 강한 경우에는 GNN + Transformer의 성능이 좋을 수 있다.</li>
</ul>

<h3 id="73-pre-trained-transformers-for-time-series">7.3. Pre-trained Transformers for Time Series</h3>

<ul>
  <li>Large-scale pre-trained Transformer model의 성능이 좋긴 한데 대부분 classification을 위한 pre-train이라는 점에서, 다른 tasks를 위한 pre-train도 고려할 수 있다.</li>
</ul>

<h3 id="74-transformers-with-architecture-level-variants">7.4. Transformers with Architecture Level Variants</h3>

<ul>
  <li>지금까지는 attention module에 대한 modification이 주로 등장했지만, TS를 위한 architecture-level design도 고려할 수 있다.</li>
</ul>

<h3 id="75-transformers-with-nas-for-time-series">7.5. Transformers with NAS for Time Series</h3>

<ul>
  <li>Neural architecture search(NAS)과 같은 AutoML을 통해, 성능에 영향을 주는 embedding dimension이나 head/layer의 개수 등 효율적인 architecture를 고려할 수 있다.</li>
</ul>

<h2 id="8-conclusion">8. Conclusion</h2>

<ul>
  <li>new taxonomy consisting of network design and application</li>
  <li>strengths and limitations of representative methods by experimental evaluation</li>
  <li>highlight future research directions.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[IJCAI 2023](https://arxiv.org/pdf/2202.07125.pdf)]]></summary></entry><entry><title type="html">(Survey paper) Time Series Forecasting With Deep Learning A Survey (Philos Trans R Soc A. 2020)</title><link href="http://localhost:4000/timeseries/2024-03-19-TSwDLsurvey/" rel="alternate" type="text/html" title="(Survey paper) Time Series Forecasting With Deep Learning A Survey (Philos Trans R Soc A. 2020)" /><published>2024-03-19T00:00:00+09:00</published><updated>2024-03-19T13:15:52+09:00</updated><id>http://localhost:4000/timeseries/TSwDLsurvey</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-19-TSwDLsurvey/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Time series forecasting
    <ul>
      <li>traditional methods : parametric models informed by domain expertise (ex. autoregressive(AR))</li>
      <li>machine learning : learn temporal dynamics in a puerly data-driven manner</li>
      <li>deep learning : learn complex data representation
        <ul>
          <li>CNN, RNN, Attention-based mechanism</li>
        </ul>
      </li>
      <li>hybrid model : Quantitative TS model + deep learning model</li>
    </ul>
  </li>
  <li>Time series forecasting의 application
    <ul>
      <li>interpretability and counterfactual prediction</li>
    </ul>
  </li>
</ul>

<h2 id="2-deep-learning-architectures-for-tsf">2. Deep Learning Architectures for TSF</h2>
<ul>
  <li>One-step-ahead forecasting : \(\hat{y}_{i, t+1}=f\left(y_{i, t-k: t}, \boldsymbol{x}_{i, t-k: t}, \boldsymbol{s}_i\right)\)
    <ul>
      <li>\(\hat{y}_{i, t+1}\) : model forecast</li>
      <li>\(y_{i, t-k: t}=\left\{y_{i, t-k}, \ldots, y_{i, t}\right\}, \boldsymbol{x}_{i, t-k: t}=\left\{\boldsymbol{x}_{i, t-k}, \ldots, \boldsymbol{x}_{i, t}\right\}\) : observation over look-back window</li>
      <li>\(f(\cdot)\) : the prediction function learntby the model
        <h3 id="2a-basic-building-blocks">2.(a) Basic building blocks</h3>
      </li>
    </ul>
  </li>
  <li>Encoder : \(\boldsymbol{z}_t=g_{\mathrm{enc}}\left(y_{t-k: t}, \boldsymbol{x}_{t-k: t}, \boldsymbol{s}\right)\)</li>
  <li>Decoder : \(f\left(y_{t-k: t}, \boldsymbol{x}_{t-k: t}, \boldsymbol{s}\right)=g_{\mathrm{dec}}\left(\boldsymbol{z}_t\right)\)
    <ul>
      <li>Encoder에서 observations를 latent vector로 representation</li>
      <li>(1) Convolution Neural Networks : \(\begin{aligned} \boldsymbol{h}_t^{l+1} &amp; =A((\boldsymbol{W} * \boldsymbol{h})(l, t)) \\ (\boldsymbol{W} * \boldsymbol{h})(l, t) &amp; =\sum_{\tau=0}^k \boldsymbol{W}(l, \tau) \boldsymbol{h}_{t-\tau}^l \end{aligned}\)
        <ul>
          <li>Convolution과 pooling을 반복하는 구조. TS에서는 과거의 값만 보도록 설계</li>
          <li>\(\boldsymbol{h}_t^l \in \mathbb{R}^{\mathcal{H}_{i n}}\) : intermediate state at layer \(l\) at time \(t\)</li>
          <li>
            <p>\(*\) : convolution operator</p>
          </li>
          <li>\(\boldsymbol{W}(l, \tau) \in$ $\mathbb{R}^{\mathcal{H}_{\text {out }} \times \mathcal{H}_{\text {in }}}\) : fixed filter weight at layer \(l\)</li>
          <li>\(A(.)\) : activation function</li>
        </ul>
      </li>
      <li>Dilated Convolution : \((\boldsymbol{W} * \boldsymbol{h})\left(l, t, d_l\right)=\sum_{\tau=0}^{\left\lfloor k / d_l\right\rfloor} \boldsymbol{W}(l, \tau) \boldsymbol{h}_{t-d_l \tau}^l\)
        <ul>
          <li>\(d_l\) : layer-specific dilation rate</li>
          <li>(WaveNet) \(d_l = 2^l\) at layer \(l\) (fig1.(a))
<img src="/assets/img/timeseries/TSwDLsurvey/fig1.png" alt="사진1" /></li>
        </ul>
      </li>
      <li>(2) Recurrent Neural Networks
        <ul>
          <li>Memory state를 통해 과거 정보를 기억하는 sequential data에 적합한 구조</li>
          <li>
            <p>Gradient vanishing으로 인한 long-range dependency \(\to\) LSTM</p>
          </li>
          <li>
            <p>Memory update funciton : \(\boldsymbol{z}_t=\nu\left(\boldsymbol{z}_{t-1}, y_t, \boldsymbol{x}_t, \boldsymbol{s}\right)\)</p>
          </li>
          <li>Network : \(\begin{aligned} y_{t+1} &amp; =\gamma_y\left(\boldsymbol{W}_y \boldsymbol{z}_t+\boldsymbol{b}_y\right) \\ \boldsymbol{z}_t &amp; =\gamma_z\left(\boldsymbol{W}_{z_1} \boldsymbol{z}_{t-1}+\boldsymbol{W}_{z_2} y_t+\boldsymbol{W}_{z_3} \boldsymbol{x}_t+\boldsymbol{W}_{z_4} \boldsymbol{s}+\boldsymbol{b}_z\right) \end{aligned}\)
            <ul>
              <li>\(W_{.}, \boldsymbol{b}\) : the linear weights and bias</li>
              <li>\(\gamma_y(.), \gamma_z(.)\) : network activation functions</li>
            </ul>
          </li>
          <li>Long Short Term Memory(LSTM)
<img src="/assets/img/timeseries/TSwDLsurvey/fig2.png" alt="사진2" /></li>
        </ul>
      </li>
      <li>(3) Attention mechanisms
        <ul>
          <li>form : \(\boldsymbol{h}_t=\sum_{\tau=0}^k \alpha\left(\boldsymbol{\kappa}_t, \boldsymbol{q}_\tau\right) \boldsymbol{v}_{t-\tau}\)
            <ul>
              <li>key \(\boldsymbol{\kappa}_t\), query \(\boldsymbol{q}_\tau\) and value \(\boldsymbol{v}_{t-\tau}\) are intermediate features produced at different time steps by lower levels of the network</li>
              <li>\(\alpha\left(\boldsymbol{\kappa}_t, \boldsymbol{q}_\tau\right) \in[0,1]\) is the attention weight for \(t-\tau\) generated at time \(t\)</li>
              <li>\(\boldsymbol{h}_t\) is the context vector output of the attention layer
                <h3 id="2b-multi-horizon-forecasting-models">2.(b) Multi-horizon Forecasting Models</h3>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>단순히 다음 한 시점에 대한 예측이 아닌 미래 여러 시점에 대한 예측</li>
  <li>(1) Iterative Methods : Autoregressive forecasting. 각 time step에서의 작은 오차가 누적된다는 단점이 있다.</li>
  <li>(2) Direct Methods : Encoder의 정보를 활용해서 한 번에 target time steps를 예측</li>
</ul>

<h2 id="3-incorporating-domain-knowledge-with-hybrid-models">3. Incorporating Domain Knowledge with Hybrid Models</h2>
<ul>
  <li>Machine learning의 underperformance의 이유는 1) flexibility로 인한 overfitting, 2) pre-processed input에 대한 sensitivity</li>
  <li>Hybrid models
    <ul>
      <li>combine well-studied quantitative time series models together with deep learning</li>
      <li>use domain knowledge \(\to\) hypothesis space를 줄여준다</li>
      <li>(a) Non-probabilistic Hybrid models : forecasting equations를 modify</li>
      <li>(b) Probabilistic Hybrid models : predictive distribution으로 parameters 생성</li>
    </ul>
  </li>
</ul>

<h2 id="4-facilitating-decision-support-using-deep-neural-networks">4. Facilitating Decision Support Using Deep Neural Networks</h2>
<ul>
  <li>연구하는 입장에서는 model의 성능(MSE, Accuracy, …)가 중요하지만, user는 future action에 대한 guide의 지표</li>
  <li>그러므로 Local Interpretable Model-Agnostic Explanations (LIME), Shapley additive explanations (SHAP)과 같은 post-hoc 분석, Attention weights를 통한 inherent interpretability를 이해할 필요가 있다.</li>
  <li>그러면 counterfactual forecast(determining what would have happened if a different set of circumstances had occurred) 가능</li>
</ul>

<h2 id="5-conclusions-and-future-directions">5. Conclusions and Future Directions</h2>
<ul>
  <li>Survey the main architectures used for TS forecasting</li>
  <li>Hybrid DL models : combine statistical and deep learning components</li>
  <li>Limitation : irregular TS나 hierarchical structure에 대한 고민은 하기 이전</li>
</ul>

<h2 id="추가">추가</h2>
<ul>
  <li>2020년에 발표된 survey 논문이지만 최근 Long-term Time Series Forecasting(LTSF)에 활용되는 모델에 대한 내용을 잘 정리한 논문이다.</li>
  <li>본 논문 이후 현재까지 최신 연구들을 이해한 상태로 읽는다면 최신 연구들의 motivation을 이해하는 데에 도움이 되는 논문이다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[Philos Trans R Soc A. 2020](https://arxiv.org/pdf/2004.13408.pdf)]]></summary></entry><entry><title type="html">(SoftCLT) Soft Contrastive Learning for Time Series (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-03-13-softCLT/" rel="alternate" type="text/html" title="(SoftCLT) Soft Contrastive Learning for Time Series (ICLR 2024)" /><published>2024-03-13T00:00:00+09:00</published><updated>2024-03-19T13:15:52+09:00</updated><id>http://localhost:4000/timeseries/softCLT</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-13-softCLT/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>TS instance만으로 contrasting하거나, 하나의 TS의 adjacent timestamps만으로 contrasting하면 TS의 inherent correlation을 제대로 반영하지 못한다.</li>
  <li><strong>SoftCLT</strong> : instance-wise &amp; temporal Contrastive loss를 사용하여, 0 또는 1이 아닌 0 ~ 1 사이의 값으로 assign
    <ul>
      <li>Instance-wise contrastive loss : Data space에서 두 시계열의 distance</li>
      <li>Temporal contrastive loss : 서로 다른 시점에 대한 loss</li>
    </ul>
  </li>
</ul>

<h2 id="1-introduciton">1. Introduciton</h2>
<ul>
  <li>Self-supervised learning - Contrastive learning에서는 두 instance의 유사도로 pair를 설정하기보다는, 각 데이터에 대해 2개의 view를 augmentation하고 둘을 positive pair로 설정한다. (다른 데이터에서 augmentation된 view와는 negative)</li>
  <li>TS의 inherent correlations는 유사한 instance 뿐만 아니라, 인접한 timestamps에도 있다.</li>
  <li>설령 같은 data에서 augmentation 되었거나, 같은 timestamp의 value에만 positive(1)로 설정하더라도, 다른 data에서 augmentation 된 다른 timestamp의 value에는 다 똑같의 negative(0)으로 설정하는 것은 optimal하지 않다.</li>
  <li><a href="https://arxiv.org/pdf/1807.03748.pdf">InfoNCE</a>의 loss처럼 positive pair 뿐만 아니라 negative pair에 대해서도 weight를 고려하는 Soft Contrastive Learning for TS를 제안
<img src="/assets/img/timeseries/softclt/table1.png" alt="사진1" /></li>
</ul>

<h2 id="2-related-work">2. Related Work</h2>
<ul>
  <li>Self-supervised learning : 많은 양의 unlabeled data를 활용하는 pretext task를 수행하는 모델을 훈련시키고, 해당 모델을 downstream task의 앞쪽에 가져와서 사용한다. pretext task로는 <code class="language-plaintext highlighter-rouge">next token prediction</code>, <code class="language-plaintext highlighter-rouge">masked token prediction</code>, <code class="language-plaintext highlighter-rouge">jigsaw puzzles</code>, <code class="language-plaintext highlighter-rouge">rotation prediction</code> 등</li>
  <li>Contrastive learning in TS
    <ul>
      <li><a href="https://arxiv.org/pdf/1901.10738.pdf">T-Loss</a> : TS에서 subseries 샘플링, subseries가 속한 TS와는 positive 다른 TS와는 negative</li>
      <li><a href="https://arxiv.org/pdf/2011.13548.pdf">Self-Time</a> : augmented sample로 inter-sample relation 학습, temporal distance로 label을 만들고 classification 해서 intra-temporal relation 학습</li>
      <li><a href="https://arxiv.org/pdf/2106.00750.pdf">TNC</a> : 정규분포 window로 정의한 temporal neighborhood를 positive로 설정</li>
      <li><a href="https://arxiv.org/pdf/2106.14112.pdf">TS-TCC</a> : augmentations가 서로의 미래 시점을 예측하도록 해서 temporal contrastive loss 설정</li>
      <li><a href="https://arxiv.org/pdf/2203.09270.pdf">Mixing-up</a> : 2개의 TS를 섞어서 새로운 TS를 만드는데 mixing weights를 predict</li>
      <li><a href="https://arxiv.org/pdf/2202.01575.pdf">CoST</a> : time, frequency domain의 contrastive losses를 사용하여 representation learning</li>
      <li><a href="https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705122X00075/1-s2.0-S0950705122002726/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCDSwasc4kb8LU0bgnJMwyHRmJ5Xp0qOkMGkgGRC101hgIgQu4zuleiKDecSs%2FYiwU6McTbx88zb7ZGNMt6fPGxxoAqvAUI0v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDF3vdKai06uU77VSpSqQBVBf8gEuvX4RtLZKv9hE3KxXK6cBABJ3MlBBkOHOQG9wKZz7o6d1XmLn2FZSjY4%2FX7pPyQcDRmkq2n%2FJq8%2Fsui0EeAASo4iyS840z2aCXativJcKRglZNYHjwhkBAax3A8xvkoahV70%2BnaX2GHQ5TWPuaUEwQkfUD1Y%2BKJW17Llm6SzL9NFiUMu9oRVuGLNfzeDz8H916pyvPQXFS5ltLq8PmDSlDFSwpmuvjRZvcGJVQCRYJ9QAqN4pjSkBwmRJtZ1zxeYYLfQU88%2FhHpCuqKY%2Fo5PlZqitF%2Fi5tN9wcjv%2BOaUu0e3H8K8qknd2hQhfYZ3mcE319ttggfYVT4PxT6jQv2hH%2BtWO%2BQVZ32moiyr2q2dfvqndSZ%2BcslmaJMEEGPfVbkcqz6OuRXKg9c6wHw%2BzGjJ0qF8lctSoDbLhT5IOZWG%2FmNF%2BMVvU5Wgorfa2swiBav99Hgn3vrf74u8mLsY5T0vx4NEyG%2BNVyPbgKqGOHsQAejW06Vq4ik5UIzPpsQ5HV4XPKK%2Fqymlem1XN6PxFHQeaf3vs0y8kVwp0rvnrfnnQ4LSrKfZc%2FNdpLU%2FXHGx%2BkUvIAHtVRX4a%2BC3sP8xSXKWTctA458XV7b5O7K5sXlyS36%2F3xnTrjC0NcJv80e3dYPRDhg3knQfYgDe2geRGQuU2COYu%2FKksZiGgBXhSAln%2Bud9LWeLVphgzjSipja81DInKiCBNOHOlulkaoOh0WcdIOFQeAQ2q6v4DeoIE1D8tTL5JNgWDLUk8sxQGq9zspYADXCEhc9Ke4hgL%2FuFvRA6Q12rCsxcWroPVYAf2el010OB%2BHSiqaCHw6xiykfcfjw7oO7bINDuWwMASTZbTgTVETF0hx1VpaYUnUbObMOCH4K8GOrEBiMpaUIWZ9pgzWe8VL74b8Keg8P4qJal5QND0KgcUzoVtZv4JMAmxEuey5Xggo3VjcjrCYsQ3sGOrJ9OJ570LbYowhBvMl7GSojd2kqdTrDSXd400eFg4uwE4Vb35B7htjgxzcxpZJeKmMPHzoEJdnMzI61T%2Fkl8%2FoIh3I9dws%2BUKd1pmrot0rGKx7EM68SBELBX2rcQ1SfmvbLKJAKBimMCFZrVhl4BAeMfpNyKSnkQM&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240318T093805Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYRLOY57VC%2F20240318%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=0e72e500b607b8e5538d7a3d24ad28c78f5bc82c4b10c8f43305eb800f767c1b&amp;hash=4ee078d9d743db9b7aed88f3efc12b11dbe7e497747d269063317fe1b35fad70&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0950705122002726&amp;tid=spdf-1cc819cf-9b45-4e50-834c-160b0fdd0a1e&amp;sid=cc5998cf8485c74f0b497fc9e83d21092089gxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=05125c535357540259&amp;rr=86643f6fd90f3079&amp;cc=kr">TimeCLR</a> : phase-shift and amplitude change augmentation based on DTW</li>
    </ul>
  </li>
  <li>Soft contrastive learning
    <ul>
      <li><a href="https://arxiv.org/pdf/2104.14548.pdf">NNCLR</a> : 각 view마다 feature space에서 k-neighbors를 찾아서 추가적인 positive pair를 고려</li>
      <li>non-TS 도메인에서는 soft assignment를 계산할 때 embedding space에서 했지만, TS 도메인에서는 data space에서 계산한다.</li>
    </ul>
  </li>
  <li>Masked modeling in TS
    <ul>
      <li><a href="https://arxiv.org/pdf/2010.02803.pdf">TST</a> : masked modeling paradigm을 TS에 도입</li>
      <li><a href="https://arxiv.org/pdf/2211.14730.pdf">PatchTST</a> :  masked subseries-level patche를 예측해서 local semantic information을 효율적으로 계산</li>
      <li><a href="https://arxiv.org/pdf/2302.00861.pdf">SimMTM</a> : 여러 개의 masked TS로부터 reconstruction</li>
    </ul>
  </li>
</ul>

<h2 id="3-methodology">3. Methodology</h2>
<ul>
  <li>instance-wise contrastive loss : inter-sample relationship 학습. Distance btw TS on data space</li>
  <li>temporal contrastive loss : intra-temporal relationship 학습. 하나의 TS에서 서로 다른 timestamps의 차이
<img src="/assets/img/timeseries/softclt/fig1.png" alt="사진2" />
    <h3 id="31-problem-definition">3.1. Problem Definition</h3>
  </li>
  <li>하나의 batch에는 N개의 TS : \(\mathcal{X}=\left\{x_1, \ldots, x_N\right\}\)가 있고
    <ul>
      <li>\(f_\theta = x_i \in \mathbb{R}^{T \times D} \to r_i=\left[r_{i, 1}, \ldots, r_{i, T}\right]^{\top} \in \mathbb{R}^{T \times M}\)를 학습</li>
      <li>\(D\)는 input feature dim, \(M\)은 embedded feature dim
        <h3 id="32-soft-instance-wise-contrastive-learning">3.2. Soft Instance-wise Contrastive learning</h3>
      </li>
    </ul>
  </li>
  <li>Vision에서는 pixel-by-pixel distance가 similarity와 관련이 없기 때문에 embedding space에서 similar instance를 학습하지만, TS에서는 data space에서의 거리가 similarity가 된다.</li>
  <li>soft assignment for a pair of data indice \((i, i')\) : \(w_I\left(i, i^{\prime}\right)=2 \alpha \cdot \sigma\left(-\tau_I \cdot D\left(x_i, x_{i^{\prime}}\right)\right)\)
    <ul>
      <li>\(D(\cdot, \cdot)\) : min-max normalized distance metric</li>
      <li>\(\tau_I\) : hyperparameter controlling the sharpness</li>
      <li>\(\alpha\) : the upper bound in the range of [0, 1]. 완전 똑같은 TS의 assignment</li>
      <li>augmented view끼리가 아니라 original TS끼리 계산하는 것</li>
    </ul>
  </li>
  <li>Contrasive loss는 cross-entropy loss로 해석될 수 있으므로(<a href="https://arxiv.org/pdf/2010.08887.pdf">i-Mix</a>), softmax probability of the relative similarity는 \(p_I\left(\left(i, i^{\prime}\right), t\right)=\frac{\exp \left(r_{i, t} \circ r_{i^{\prime}, t}\right)}{\sum_{j=1, j \neq i}^{2 N} \exp \left(r_{i, t} \circ r_{j, t}\right)}\)</li>
  <li>Soft instance-wise contrastive loss for \(x_i\), at \(t\)는 \(\ell_I^{(i, t)}=-\log p_I((i, i+N), t)-\sum_{j=1, j \neq\{i, i+N\}}^{2 N} w_I(i, j \bmod N) \cdot \log p_I((i, j), t)\)
    <ul>
      <li>첫째 term은 positive pair에 대한 loss, 둘째 term은 나머지에 대한 loss인데 \(w_I\left(i, i^{\prime}\right)\)로 weighted.</li>
      <li>\(\forall w_I\left(i, i^{\prime}\right)=0\)이면 hard instance-wise contrastive loss이므로 일반화 버전이다.
        <h3 id="33-soft-temporal-contrastive-learning">3.3. Soft Temporal Contrastive learning</h3>
      </li>
    </ul>
  </li>
  <li>soft assignment for a pair of timestamps \((t, t')\) : \(w_T\left(t, t^{\prime}\right)=2 \cdot \sigma\left(-\tau_T \cdot\mid t-t^{\prime}\mid\right)\)
    <ul>
      <li>\(\tau_I\) : hyperparameter controlling the sharpness</li>
    </ul>
  </li>
  <li>: 인접한 timestamp의 values는 비슷할 것이라는 직관</li>
  <li><strong>Hierarchical loss</strong> : <a href="https://arxiv.org/pdf/2106.10466.pdf">TS2Vec</a>의 방식처럼 maxpooling을 해서 loss 계산
<img src="/assets/img/timeseries/softclt/fig2.png" alt="사진3" /></li>
  <li>Softmax probability of the relative similarity는 \(p_T\left(i,\left(t, t^{\prime}\right)\right)=\frac{\exp \left(r_{i, t} \circ r_{i, t^{\prime}}\right)}{\sum_{s=1, s \neq t}^{2 T} \exp \left(r_{i, t} \circ r_{i, s}\right)}\),</li>
  <li>Soft temporal contrastive loss for \(x_i\) at \(t\)는 \(\ell_T^{(i, t)}=-\log p_T(i,(t, t+T))-\sum_{s=1, s \neq\{t, t+T\}}^{2 T} w_T(t, s \bmod T) \cdot \log p_T(i,(t, s))\)
    <ul>
      <li>마찬가지로 \(\forall w_T\left(t, t^{\prime}\right)=0\)이면 hard temporal contrastive loss이므로 일반화 버전이다.</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 SoftCLT의 Final loss : \(\mathcal{L}=\frac{1}{4 N T} \sum_{i=1}^{2 N} \sum_{t=1}^{2 T}\left(\lambda \cdot \ell_I^{(i, t)}+(1-\lambda) \cdot \ell_T^{(i, t)}\right)\)
    <ul>
      <li>\(\lambda\) : hyperparameter controlling the contribution of each loss</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<ul>
  <li>(1) Classification with UTS, MTS (2) Semi-supervised classification (3) Transfer learning in in-domain and cross-domain (4) Anomaly detection
    <h3 id="41-classification">4.1. Classification</h3>
    <p><img src="/assets/img/timeseries/softclt/fig23.png" alt="사진4" /></p>
    <h3 id="42-semi-supervised-classification">4.2. Semi-supervised classification</h3>
    <p><img src="/assets/img/timeseries/softclt/table3.png" alt="사진5" /></p>
    <h3 id="43-transfer-learning">4.3. Transfer learning</h3>
    <p><img src="/assets/img/timeseries/softclt/table4.png" alt="사진6" /></p>
    <h3 id="44-anomaly-detection">4.4. Anomaly detection</h3>
    <p><img src="/assets/img/timeseries/softclt/table5.png" alt="사진7" /></p>
    <h3 id="45-ablation-study">4.5. Ablation study</h3>
    <p><img src="/assets/img/timeseries/softclt/table6.png" alt="사진8" /></p>
    <ul>
      <li>(a) : soft assignment를 instance-wise와 temporal에 모두 적용했을 때 성능이 가장 좋다.</li>
      <li>(b) : \(W_T\)를 계산하는 방법들에 따른 비교. sigmoid를 사용하는 근거가 된다.</li>
      <li>(c) : \(\alpha=0.5\) 정도로 해서 같은 TS의 similarity of the pairs를 적절히 크게 할 때 성능이 좋다.</li>
      <li>(d) : Distance function에 따른 성능 비교. DTW와 TAM의 성능이 같지만 더 일반적인 DTW 사용했다.
        <h3 id="46-analysis">4.6. Analysis</h3>
      </li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Comparison with soft CL methods in computer vision</code> : 앞서 언급했듯이 embedding space에서 similarity를 계산하는 vision domain과 다르게, TS는 data space에서 계산하면 성능이 더 좋다.</li>
  <li><code class="language-plaintext highlighter-rouge">Robustness to seasonality</code> : Seasonality in TS는 extract하기 어렵지도 않고 고려 안해도 성능이 좋아서 직접적으로 고려하지 않았다.</li>
  <li><code class="language-plaintext highlighter-rouge">Instance-wise relationships</code> : layer가 깊어짐에 따라 SoftCLT가 Hard CL보다 TS instance 사이의 관계를 잘 보존한다.</li>
  <li><code class="language-plaintext highlighter-rouge">Temporal relationships</code> : 시간(training epoch)에 따라서도 t-SNE를 비교했을 때, Hard CL은 진한 색(large tarining epoch)을 잘 구분하지 못하는데, large training epoch에는 fine-grained relationship이 학습된다. 즉 Hard CL은 coarse-grained relationship은 잘 학습하지만 SoftCLT는 fine-grained relationship도 잘 학습한다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>SoftCLT : soft assignments based on the instance-wise and temporal relationships on the data space</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://arxiv.org/abs/2312.16424)]]></summary></entry><entry><title type="html">(TS2Vec) Towards Universal Representation of Time Series (AAAI 2022)</title><link href="http://localhost:4000/timeseries/2024-03-12-ts2vec/" rel="alternate" type="text/html" title="(TS2Vec) Towards Universal Representation of Time Series (AAAI 2022)" /><published>2024-03-12T00:00:00+09:00</published><updated>2024-03-19T13:15:52+09:00</updated><id>http://localhost:4000/timeseries/ts2vec</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-12-ts2vec/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>본 논문에서 풀고자 하는 문제는 unsupervised TS representation(TS \(\to\) vector)</li>
  <li>TS를 arbitrary semantic level에서 representation하는 방법을 학습하는 framework</li>
  <li>augmented context views를 hierarchical하게 contrastive learning한다.</li>
  <li>(선행연구(<a href="https://arxiv.org/abs/2106.00750">TNC</a>, <a href="https://arxiv.org/abs/1901.10738">T-Loss</a>)를 읽기 전이라면 abstract에서 등장하는 단어들의 의미가 와닿지 않을 수 있다.)</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>이미 instance-level representation을 학습하는 연구들도 있었고(<a href="https://arxiv.org/abs/2106.00750">TNC</a>, <a href="https://arxiv.org/abs/1901.10738">T-Loss</a>),</li>
  <li>contrastive loss를 사용해서 TS의 구조를 학습하는 연구들도 있었다.(<a href="https://arxiv.org/abs/2106.14112">TS-TCC</a>, <a href="https://arxiv.org/abs/1901.10738">T-Loss</a>)</li>
  <li>하지만 지금까지의 방법들의 한계는 아래 3가지로 정리할 수 있다.</li>
  <li>첫째, instance-level representation은 fine-grained representation(e.g. forecasting, anomaly detection)에 적합하지 않다.
    <ul>
      <li>왜냐하면 specific한 timestamp, sub-series를 타겟으로 inference해야 하는데, coarse-grained representation으로는 충분하지 않다.</li>
    </ul>
  </li>
  <li>둘째, 다양한 granularities에서의 multi-scale contextual information을 파악하기 어렵다.
    <ul>
      <li>granularity가 높을수록 더 자세한 정보를 포함 (일별 &lt; 시간별 &lt; 분별 &lt; 초별 …)</li>
      <li>multi-scale information은 다양한 granularities에서 나타나는 정보. Scale에 따라 달라질 수는 있지만, representation의 generalization capability를 향상시킨다는 점에서 TS task에서 필수적인 정보이다.</li>
    </ul>
  </li>
  <li>셋째, CV, NLP에서의 unsupervised representation은 강한 inductive bias가 있는데, TS는 그렇지 않다.
    <ul>
      <li>transformation-invariant : 강아지 사진은 거꾸로 뒤집어도 강아지 사진이지만, TS는 거꾸로 뒤집으면 아예 다른 데이터가 된다.</li>
      <li>cropping-invariant : 사진이나 문장은 일부분을 잘라도 본질적인 정보가 바뀌지 않는 경우가 많아 augmentation 방법으로 쓰이지만, TS는 일부분을 자르면 패턴이나 분포 자체가 달라지게 된다.</li>
    </ul>
  </li>
  <li>본 논문에서 제시하는 TS2Vec은 universal contrastive learning framework를 제안
    <ul>
      <li>maxpooling으로 다양한 granularity의 overall representation을 얻고</li>
      <li>instance-wise and temporal dim에서의 hierarchically contrastive learning을 통해</li>
      <li>all-semantic level에서의 representation을 얻는다.</li>
    </ul>
  </li>
</ul>

<h2 id="2-method">2. Method</h2>

<h3 id="21-problem-definition">2.1. Problem Definition</h3>
<ul>
  <li>N개의 시계열 \(\mathcal{X}=\left\{x_1, x_2, \cdots, x_N\right\}\)에 대해서 nonlinear embedding function \(f_\theta : x_i \in \mathbb{R}^{T \times F} \to r_i=\left\{r_{i, 1}, r_{i, 2}, \cdots, r_{i, T}\right\} \in \mathbb{R}^{T \times K}\)를 학습한다.</li>
</ul>

<h3 id="22-model-architecture">2.2. Model Architecture</h3>
<p><img src="/assets/img/timeseries/ts2vec/fig1.png" alt="사진1" /></p>
<ul>
  <li><strong>sub-series</strong> : input TS \(x_i\)에서 2개의 sub-series를 랜덤하게 sampling한다.
    <ul>
      <li>겹치는 부분이 있도록 (겹치는 부분의 contextual representation이 consistent하도록 할거니까)</li>
    </ul>
  </li>
  <li><strong>Encoder</strong> \(f_\theta\)는 3개의 modules로 구성
    <ul>
      <li><strong>projection layer</strong> : t시점의 input을 high-dim latent vector로 mapping하는 FC layer. \(x_{i,t} \to z_{i,t}\)</li>
      <li><strong>timestamp masking module</strong> : latent vectors의 random한 timestamps를 masking(=0)해서 augmented context view를 생성</li>
      <li><strong>dilated CNN module</strong> : 각 timestamp의 contextual representation을 extract. 이 때 1d dilated conv layer를 사용하는데, dilation parameter를 다양하게 해서(\(2^l\) for \(l\)-th block) larger receptive field</li>
    </ul>
  </li>
</ul>

<h3 id="23-contextual-consistency">2.3. Contextual Consistency</h3>
<ul>
  <li>Contrastive learning을 위한 positive pair를 만드는 전략들을 소개한다.
<img src="/assets/img/timeseries/ts2vec/fig2.jpeg" alt="사진2" /></li>
  <li><strong>Subseries consistency</strong> : 서로 sub-series 관계인 segments를 positive pair로 설정하고 representation을 가깝게 학습</li>
  <li><strong>Temporal consistency</strong> : 인접한 시점의 segments를 positive pair로 설정</li>
  <li><strong>Transformation consistency</strong> : scaling, permutation과 같은 transformation에 invariant한 representation을 학습
<img src="/assets/img/timeseries/ts2vec/fig3.jpeg" alt="사진3" /></li>
  <li>하지만 fig3을 보면 위와 같은 전략들이 시계열 데이터에는 적절하지 않다. sub-series라고 해서, 인접한 시점이라고 해서 패턴이 같은 것은 아니다.</li>
  <li><strong>Contextual consistency</strong> : 본 논문에서 제시하는 방법으로, 동일한 timestamp에 대해 random masking이나 random cropping으로 생성한 contexts를 positive pair로 설정한다.
    <ul>
      <li><strong>Timestamp masking</strong> : 각 timestamp에 대해 latent vector \(z_i = \{z_{i,t\}\}\)를 \(p=0.5\) bernoulli masking</li>
      <li><strong>Random cropping</strong> : input \(x_i \in \mathbb R^{T \times F}\)에 대해 \(0 &lt; a_1 \le a_2 \le b_1 \le b_2 \le T\)를 만족하는 segments \([a_1, b_1], [a_2, b_2]\)를 random하게 만들고, 각 segment에 대해 overlapping segment인 \([a_2, b_1]\)의 contextual representation이 consistent해지도록 학습한다.</li>
      <li>Masking과 random cropping은 시계열의 magnitude를 바꾸지도 않으면서, 각 timestamp에 대해 복원하도록 학습시키기 때문에 robust한 representation learning 방식이다.</li>
    </ul>
  </li>
</ul>

<h3 id="24-hierarchical-contrasting">2.4. Hierarchical Contrasting</h3>
<ul>
  <li><strong>Hierarchical contraastive loss</strong> : 본 논문에서 제시하는 학습 방식으로, 다양한 scales에서의 representation을 학습하기 위한 loss이다. (scales가 다양하기 때문에 max-pooling을 사용한다.)
    <ul>
      <li>instance-wise &amp; temporal contrastive losses 모두 leverage하는데, 이걸 모든 granularity levels에 대해서 hierarchical하게 적용한다.</li>
    </ul>
  </li>
  <li><strong>Temporal Contrastive Loss</strong>
    <ul>
      <li>\(\ell_{t e m p}^{(i, t)}=-\log \frac{\exp \left(r_{i, t} \cdot r_{i, t}^{\prime}\right)}{\sum_{t^{\prime} \in \Omega}\left(\exp \left(r_{i, t} \cdot r_{i, t^{\prime}}^{\prime}\right)+\mathbb{1}_{\left[t \neq t^{\prime}\right]} \exp \left(r_{i, t} \cdot r_{i, t^{\prime}}\right)\right)}, \quad \Omega \text{ is the set of timestamps}\)이다.</li>
      <li>동일한 input \(x_i\)에 대해서, timestamp가 같으면 positive이고 다르면 negative이다.
<img src="/assets/img/timeseries/ts2vec/myfig1.jpeg" alt="사진5" /></li>
      <li>위 그림에서 빨강색이 분모에 포함되는 timestamps이고, 파랑색이 분자에 해당하는 timestamp이다. 같은 input에 대해 서로 다른 augmentation의 같은 timestamp가 가깝게 representation되도록 학습한다는 의미이다.</li>
    </ul>
  </li>
  <li><strong>Instance-wise Contrastive Loss</strong>
    <ul>
      <li>\(\ell_{i n s t}^{(i, t)}=-\log \frac{\exp \left(r_{i, t} \cdot r_{i, t}^{\prime}\right)}{\sum_{j=1}^B\left(\exp \left(r_{i, t} \cdot r_{j, t}^{\prime}\right)+\mathbb{1}_{[i \neq j]} \exp \left(r_{i, t} \cdot r_{j, t}\right)\right)}, \quad B \text{ is the batch size}\)이다.</li>
      <li>한 시점 t와 모든 instance(input)에 대해 같은 instance이면 positive이고 다른 instance이면 negative이다.
<img src="/assets/img/timeseries/ts2vec/myfig2.jpeg" alt="사진6" /></li>
      <li>역시 빨강색이 분모에 포함되는 instance이고, 파랑색이 분자에 해당하는 instance이다. 한 timestamp에 대해 같은 instance의 augmentation가 가깝게 representation되도록 학습한다는 의미이다.</li>
    </ul>
  </li>
  <li>두 losses는 complementary하다. (예를 들어 다수의 전기 사용량 시계열 데이터라면, instance contrast는 user-specifc 정보를, temporal contrast는 시간에 따른 dynamic trends를 학습한다.)</li>
  <li>The overall loss : \(\mathcal{L}_{\text {dual }}=\frac{1}{N T} \sum_i \sum_t\left(\ell_{\text {temp }}^{(i, t)}+\ell_{\text {inst }}^{(i, t)}\right)\)</li>
</ul>

<h2 id="3-experiments">3. Experiments</h2>
<p><img src="/assets/img/timeseries/ts2vec/table2.jpeg" alt="사진7" />
<img src="/assets/img/timeseries/ts2vec/fig5.png" alt="사진8" /></p>
<ul>
  <li>Informer는 trends는 학습했지만 주기적 패턴을 학습하지 못했고, TCN은 반대로 주기적 패턴은 학습했지만 trends는 학습하지 못했다. (coarse-grained vs fine-grained 둘 다 잘 학습하기 어려움)</li>
</ul>

<h2 id="4-analysis">4. Analysis</h2>
<p><img src="/assets/img/timeseries/ts2vec/table5.png" alt="사진9" /></p>
<ul>
  <li>Ablation study를 통해 components를 justify하였다.
<img src="/assets/img/timeseries/ts2vec/fig7.png" alt="사진10" /></li>
  <li>Heatmap을 통해 급작스러운 spike에 대해서도 적절하게 representation할 수 있음을 확인하였다.</li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>TS2Vec : universial representation learning framework
    <ul>
      <li>hierarchical contrasting을 통해 scale-invariant representation을 학습하였고,</li>
      <li>instance-wise contrasting과 temporal contrasting으로 loss를 설정하였다.</li>
    </ul>
  </li>
  <li>Ablation study를 통해 모델의 components가 모두 필요함을 보여주었다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[AAAI 2022](https://arxiv.org/abs/2106.10466)]]></summary></entry><entry><title type="html">MCMC with Implementation (2) : Gibbs Sampling</title><link href="http://localhost:4000/stat/2024-03-06-gibbs/" rel="alternate" type="text/html" title="MCMC with Implementation (2) : Gibbs Sampling" /><published>2024-03-06T00:00:00+09:00</published><updated>2024-03-06T19:08:49+09:00</updated><id>http://localhost:4000/stat/gibbs</id><content type="html" xml:base="http://localhost:4000/stat/2024-03-06-gibbs/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>만약 sampling하고자 하는 parameter의 full conditional distribution을 알 수 있다면 Gibbs sampler를 사용할 수 있다.</li>
  <li>Gibbs sampler는 MH-algorithm의 special case라고 할 수 있다.
    <ul>
      <li>Acceptance rate = 1이 되어 accept/reject 과정이 없다.</li>
    </ul>
  </li>
  <li><strong>Full conditional distribution</strong> : 만약 model parameters가 \(\mathbf{\theta}=\left(\theta_1, \cdots, \theta_k\right)\)이라면, full conditional distribution은 \(\pi\left(\theta_i \mid \theta_1, \cdots, \theta_{i-1}, \theta_{i+1}, \cdots, \theta_k, \mathbf{X}\right)\)이다. 즉 다른 parameters는 모두 given일 때 관심있는 parameter의 분포를 의미한다.</li>
  <li>정확히는 full conditional distribution이 모두 closed-form이어야 한다. 즉 k개의 conditional distributions가 standard한 분포(Normal, Gamma, …)를 따른다.</li>
  <li>그러므로 Gibss sampling 방식은 k개의 parameters로 이루어지는 확률변수 \(\mathbf{\theta}\)를 sampling을 할 때 k개 parameters를 한 번에 sampling하는 것이 아니라, 하나씩 sampling한 다음 k개를 모아서 하나의 sample로 만든다.
    <ul>
      <li>
        <ol>
          <li>Choose starting values \(\mathbf{\theta}=\left(\theta_1^{(1)}, \cdots, \theta_k^{(1)}\right)\)</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>For \(i=2, \cdots, T\) sample <br />
\(\begin{aligned}
&amp; \theta_1^{(i)} \mid \theta_2^{(i-1)}, \cdots, \theta_k^{(i-1)}, \mathbf{X} \\
&amp; \theta_2^{(i)} \mid \theta_1^{(i)}, \theta_3^{(i-1)}, \cdots, \theta_k^{(i-1)}, \mathbf{X} \\
&amp; \theta_k^{(i)} \mid \theta_1^{(i)}, \cdots, \theta_{k-1}^{(i)}, \mathbf{X}
\end{aligned}\)</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>주의할 점은 \(\theta_2^{(i)}\)를 sampling할 때에는 아직 sampling하지 않은 \(\theta_3^{(i)}\)부터 \(\theta_k^{(i)}\)까지는 이전 시점의 값을 사용하지만, 이미 sampling을 한 \(\theta_1^{(i)}\)은 sampling한 값을 사용한다는 점이다.</li>
</ul>

<h2 id="2-full-conditional-distribution-for-gibb-sampler">2. Full conditional distribution for Gibb Sampler</h2>
<ul>
  <li>다음과 같은 간단한 Bayse rule을 보자. \(P(A \mid B)=\frac{P(A, B)}{P(B)}=\frac{P(A, B)}{P(A)} \times \frac{P(A)}{P(B)}=P(B \mid A) \times \frac{P(A)}{P(B)}\)
    <ul>
      <li>A를 sampling할 parameters, B를 주어진 데이터라고 하면 \(P(B \mid A)\)는 likelihood이고, \(P(A)\)는 prior가 된다. 그리고 \(P(B)\)는 주어진 상수가 되어 고려하지 않아도 된다. 왜냐하면 결국 A가 바뀔 때 posterior \(P(A \mid B)\)의 대소관계가 궁금하기 때문이다.</li>
      <li>그러므로 \(\text{posterior} \propto \text{likelihood} \times \text{prior}\) 관계가 성립한다.</li>
    </ul>
  </li>
</ul>

<h2 id="3-r-implementation-of-gibb-sampler">3. R Implementation of Gibb Sampler</h2>

<h3 id="31-ex1--bayesian-linear-regression">3.1. Ex.1 : Bayesian Linear Regression</h3>
<p>\(\begin{gathered}
Y_i=\beta_0+\beta_1 X_{1, i}+\beta_2 X_{2, i}+\beta_3 X_{1, i} X_{2, i}+\epsilon_i  \\
\text{where } \epsilon_i \sim N\left(0, \sigma^2\right) \text{independently for } i=1, \cdots, 300,000 \\
\text{We will use independent priors as} \\
\beta_j \sim N(0,10) \text { for } j=0,1,2,3 \\
\sigma^2 \sim \operatorname{IG}(0.01,0.01)
\end{gathered}\)</p>
<ul>
  <li>위 모형에서 임의의 parameters로 데이터를 생성하고, Gibbs sampler가 생성한 parameters samples와 임의로 정한 parameters를 비교한다.</li>
  <li><strong>Simulation the dataset</strong>
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># the size of dataset</span><span class="w">
</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">300000</span><span class="w">

</span><span class="c1"># Simulating (X_1,1, ..., X_1,n) and (X_2,1, ..., X_2,n) ~ N(0,1)</span><span class="w">
</span><span class="n">X</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mvrnorm</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">mu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">Sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="m">2</span><span class="p">))</span><span class="w">
  
</span><span class="c1"># Setting the true parameters (beta)</span><span class="w">
</span><span class="n">beta.true</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.5</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">-1</span><span class="p">)</span><span class="w">

</span><span class="c1"># Simulating (Y_1, ..., Y_n) using the design matrix for above model</span><span class="w">
</span><span class="n">X_design</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="n">n</span><span class="p">),</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="o">*</span><span class="n">X</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="p">)</span><span class="w">
</span><span class="n">Y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">X_design</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta.true</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
  </li>
  <li><strong>Calculation the full conditional distribution</strong>
    <ul>
      <li>For \(\beta\), the kernel of posterior for Gibbs sampler is following :
\(\begin{aligned}
&amp; P\left(\beta \mid Y, X, \sigma^2\right) \propto L\left(Y, X \mid \beta, \sigma^2\right) P(\beta) \\
&amp; \propto\left(\frac{1}{\sqrt{2 \pi}}\right)^n\left(\frac{1}{\operatorname{det}\left(\sigma^2 I\right)}\right)^{\frac{1}{2}} \exp \left(-\frac{(Y-X \beta)^{\top}(Y-X \beta)}{2 \sigma^2}\right)\left(\frac{1}{\sqrt{2 \pi}}\right)^p\left(\frac{1}{\operatorname{det}(10 I)}\right)^{\frac{1}{2}} \exp \left(-\frac{\beta^{\top} \beta}{20}\right) \\
&amp; \propto \exp \left(-\frac{(Y-X \beta)^{\top}(Y-X \beta)}{2 \sigma^2}-\frac{\beta^{\top} \beta}{20}\right) \\
&amp; =\exp \left(-\frac{1}{2 \sigma^2} Y^{\top} Y+\frac{1}{2 \sigma^2} Y^{\top} X \beta+\frac{1}{2 \sigma^2} \beta^{\top} X Y-\frac{\beta^{\top} X^{\top} X \beta}{2 \sigma^2}-\frac{\beta^{\top} \beta}{20}\right) \\
&amp; =\exp \left(-\frac{1}{2}\left(\beta^{\top}\left(\frac{X^{\top} X}{\sigma^2}+\frac{I}{10}\right) \beta-\beta^{\top}\left(\frac{X^{\top} Y}{\sigma^2}\right)-\left(\frac{Y^{\top} X}{\sigma^2}\right) \beta+\frac{Y^{\top} Y}{6^2}\right)\right) \\
&amp; \therefore V_\beta=\left[\frac{X^{\top} X}{\sigma^2}+\frac{I}{10}\right]^{-1}, \quad m_\beta=V_\beta \frac{X^{\top} Y}{\sigma^2} \\
&amp; \beta \sim N\left(m_\beta, V_\beta\right) \\
\end{aligned}\)</li>
    </ul>
  </li>
  <li>For \(\sigma^2\), the kernel of posterior for Gibbs sampler is following :
\(\begin{aligned}
P\left(\sigma^2 \mid, \beta\right) &amp; \propto L\left(Y, X \mid \beta, \sigma^2\right) P\left(\sigma^2\right) \\
&amp; \propto\left(\frac{1}{\sigma \sqrt{2 \pi}}\right)^n \exp \left(-\frac{(Y-X \beta)^{\top}(Y-X \beta)}{2 \sigma^2}\right) \frac{\beta^\alpha}{\Gamma(\alpha)}\left(\frac{1}{\sigma^2}\right)^{\alpha+1} \exp \left(-\frac{\beta}{\sigma^2}\right) \\
&amp; \propto\left(\frac{1}{\sigma^2}\right)^{\frac{n}{2}+\alpha+1} \exp \left(-\frac{\beta+\frac{1}{2}(Y-X \beta)^{\top}(Y-X \beta)}{\sigma^2}\right) \\
&amp; \\
&amp; \therefore \alpha_{\sigma^2}=\frac{n}{2}+\alpha, \quad \beta_{\sigma^2}=\beta+\frac{1}{2}(Y-X \beta)^{\top}(Y-X \beta) \\
&amp; \sigma^2 \sim I G\left(\alpha_{\sigma^2}, \beta_{\sigma^2}\right)
\end{aligned}\)
    <ul>
      <li>위 결과를 바탕으로 sampling하는 R 코드를 작성한다.</li>
    </ul>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># the number of variates</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ncol</span><span class="p">(</span><span class="n">X_design</span><span class="p">)</span><span class="w">

</span><span class="c1"># The prior parameters</span><span class="w">
</span><span class="n">m.beta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="p">;</span><span class="w"> </span><span class="n">v.beta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="n">a.s2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.01</span><span class="p">;</span><span class="w"> </span><span class="n">b.s2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.01</span><span class="w">
  
</span><span class="c1"># 각각의 parameters를 1000개씩 sampling한다.</span><span class="w">
</span><span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="w">

</span><span class="c1"># sample space with initialization for each parameters</span><span class="w">
</span><span class="n">beta.samps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">)</span><span class="w">
</span><span class="n">s2.samps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">)</span><span class="w">
</span><span class="n">beta.samps</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="n">p</span><span class="p">)</span><span class="w">
</span><span class="n">s2.samps</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w">

</span><span class="c1"># Gibbs sampler</span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="n">B</span><span class="p">){</span><span class="w">
    
  </span><span class="c1">## beta[i] | s2[i-1]</span><span class="w">
  </span><span class="n">V</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">solve</span><span class="p">(</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">X_design</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">X_design</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">s2.samps</span><span class="p">[</span><span class="n">i</span><span class="m">-1</span><span class="p">]</span><span class="o">+</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">v.beta</span><span class="w"> </span><span class="p">)</span><span class="w">
  </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">V</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">X_design</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">s2.samps</span><span class="p">[</span><span class="n">i</span><span class="m">-1</span><span class="p">]</span><span class="w"> </span><span class="p">)</span><span class="w">
  </span><span class="n">beta.samps</span><span class="p">[,</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rmvnorm</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">V</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"svd"</span><span class="p">)</span><span class="w">
    
  </span><span class="c1"># s2[i] | beta[i]</span><span class="w">
  </span><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">n</span><span class="o">/</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">a.s2</span><span class="w">
  </span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">b.s2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">X_design</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta.samps</span><span class="p">[,</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">X_design</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta.samps</span><span class="p">[,</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">2</span><span class="w">
  </span><span class="n">s2.samps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rinvgamma</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">scale</span><span class="o">=</span><span class="n">b</span><span class="p">)</span><span class="w">
  </span><span class="n">s2.samps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">s2.samps</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
  </span><span class="c1"># burn-in</span><span class="w">
  </span><span class="n">beta.samps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">beta.samps</span><span class="p">[,</span><span class="m">51</span><span class="o">:</span><span class="n">B</span><span class="p">]</span><span class="w">
  </span><span class="n">s2.samps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">s2.samps</span><span class="p">[</span><span class="m">51</span><span class="o">:</span><span class="n">B</span><span class="p">]</span><span class="w">
</span></code></pre></div>    </div>
  </li>
  <li><strong>Checking the Samples</strong>
    <ul>
      <li>density plot, trace plot 등 다양한 tools를 활용하여 결과를 확인해야 하지만, posterior mean(samples의 평균)만 확인한다. 상당히 정확하게 sampling 되었다는 것을 알 수 있다.</li>
    </ul>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># posterior mean</span><span class="w">
</span><span class="n">cat</span><span class="p">(</span><span class="s2">"posterior mean of beta0:"</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">beta.samps</span><span class="p">[</span><span class="m">1</span><span class="p">,]),</span><span class="w"> </span><span class="s2">"\n"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"posterior mean of beta1:"</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">beta.samps</span><span class="p">[</span><span class="m">2</span><span class="p">,]),</span><span class="w"> </span><span class="s2">"\n"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"posterior mean of beta2:"</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">beta.samps</span><span class="p">[</span><span class="m">3</span><span class="p">,]),</span><span class="w"> </span><span class="s2">"\n"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"posterior mean of beta3:"</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">beta.samps</span><span class="p">[</span><span class="m">4</span><span class="p">,]),</span><span class="w"> </span><span class="s2">"\n"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"posterior mean of sigma2:"</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">s2.samps</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w">

</span><span class="c1">##### result #####</span><span class="w">
</span><span class="c1">## posterior mean of beta0: 0.4976068</span><span class="w">
</span><span class="c1">##  posterior mean of beta1: 0.9989775</span><span class="w">
</span><span class="c1">##  posterior mean of beta2: 2.00346</span><span class="w">
</span><span class="c1">##  posterior mean of beta3: -1.00101</span><span class="w">
</span><span class="c1">##  posterior mean of sigma2: 0.9986912</span><span class="w">
</span></code></pre></div>    </div>
    <ul>
      <li>Gibbs Sampler는 proposed sample이 항상 accept된다는 점에서 MH-algorithm의 speecial case라고 할 수 있다. samples의 unique value의 개수와 B=1000의 비율을 출력한다.</li>
    </ul>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># acceptance ratio</span><span class="w">
</span><span class="c1"># (Because I used Gibbs sampler, samples was always accepted !)</span><span class="w">
</span><span class="n">cat</span><span class="p">(</span><span class="s2">"Acceptance ratio of beta0 :"</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">unique</span><span class="p">(</span><span class="n">beta.samps</span><span class="p">[</span><span class="m">1</span><span class="p">,]))</span><span class="o">/</span><span class="n">B</span><span class="w"> </span><span class="p">,</span><span class="s2">"\n"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Acceptance ratio of beta1 :"</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">unique</span><span class="p">(</span><span class="n">beta.samps</span><span class="p">[</span><span class="m">2</span><span class="p">,]))</span><span class="o">/</span><span class="n">B</span><span class="w"> </span><span class="p">,</span><span class="s2">"\n"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Acceptance ratio of beta2 :"</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">unique</span><span class="p">(</span><span class="n">beta.samps</span><span class="p">[</span><span class="m">3</span><span class="p">,]))</span><span class="o">/</span><span class="n">B</span><span class="w"> </span><span class="p">,</span><span class="s2">"\n"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Acceptance ratio of beta3 :"</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">unique</span><span class="p">(</span><span class="n">beta.samps</span><span class="p">[</span><span class="m">4</span><span class="p">,]))</span><span class="o">/</span><span class="n">B</span><span class="w"> </span><span class="p">,</span><span class="s2">"\n"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Acceptance ratio of sigma2 :"</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">unique</span><span class="p">(</span><span class="n">s2.samps</span><span class="p">))</span><span class="o">/</span><span class="n">B</span><span class="w"> </span><span class="p">)</span><span class="w">
  
</span><span class="c1">##### result #####</span><span class="w">
</span><span class="c1">## Acceptance ratio of beta0 : 0.95</span><span class="w">
</span><span class="c1">##  Acceptance ratio of beta1 : 0.95</span><span class="w">
</span><span class="c1">##  Acceptance ratio of beta2 : 0.95</span><span class="w">
</span><span class="c1">##  Acceptance ratio of beta3 : 0.95</span><span class="w">
</span><span class="c1">##  Acceptance ratio of sigma2 : 0.95</span><span class="w">
</span></code></pre></div>    </div>
    <ul>
      <li>앞서 burn-in으로 각 parameters에서 50개의 samples를 버린 것을 감안하면 sample가 항상 accept되었음을 알 수 있다. (acceptance probabilties = 1)
        <ul>
          <li>Acceptance probabilies는 각 proposed sample이 accept될 확률이고</li>
          <li>Acceptance ratio는 전체 sampling 횟수 중에서 accept된 비율을 의미한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="32-ex2--truncated-exponential-distribution">3.2. Ex.2 : Truncated Exponential distribution</h3>
<ul>
  <li>to be continued…</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[1. Introduction 만약 sampling하고자 하는 parameter의 full conditional distribution을 알 수 있다면 Gibbs sampler를 사용할 수 있다. Gibbs sampler는 MH-algorithm의 special case라고 할 수 있다. Acceptance rate = 1이 되어 accept/reject 과정이 없다. Full conditional distribution : 만약 model parameters가 \(\mathbf{\theta}=\left(\theta_1, \cdots, \theta_k\right)\)이라면, full conditional distribution은 \(\pi\left(\theta_i \mid \theta_1, \cdots, \theta_{i-1}, \theta_{i+1}, \cdots, \theta_k, \mathbf{X}\right)\)이다. 즉 다른 parameters는 모두 given일 때 관심있는 parameter의 분포를 의미한다. 정확히는 full conditional distribution이 모두 closed-form이어야 한다. 즉 k개의 conditional distributions가 standard한 분포(Normal, Gamma, …)를 따른다. 그러므로 Gibss sampling 방식은 k개의 parameters로 이루어지는 확률변수 \(\mathbf{\theta}\)를 sampling을 할 때 k개 parameters를 한 번에 sampling하는 것이 아니라, 하나씩 sampling한 다음 k개를 모아서 하나의 sample로 만든다. Choose starting values \(\mathbf{\theta}=\left(\theta_1^{(1)}, \cdots, \theta_k^{(1)}\right)\) For \(i=2, \cdots, T\) sample \(\begin{aligned} &amp; \theta_1^{(i)} \mid \theta_2^{(i-1)}, \cdots, \theta_k^{(i-1)}, \mathbf{X} \\ &amp; \theta_2^{(i)} \mid \theta_1^{(i)}, \theta_3^{(i-1)}, \cdots, \theta_k^{(i-1)}, \mathbf{X} \\ &amp; \theta_k^{(i)} \mid \theta_1^{(i)}, \cdots, \theta_{k-1}^{(i)}, \mathbf{X} \end{aligned}\) 주의할 점은 \(\theta_2^{(i)}\)를 sampling할 때에는 아직 sampling하지 않은 \(\theta_3^{(i)}\)부터 \(\theta_k^{(i)}\)까지는 이전 시점의 값을 사용하지만, 이미 sampling을 한 \(\theta_1^{(i)}\)은 sampling한 값을 사용한다는 점이다.]]></summary></entry><entry><title type="html">(SimMTM) A Simple Pre-Training Framework for Masked Time-Series Modeling (NeurIPS 2023)</title><link href="http://localhost:4000/timeseries/2024-03-06-SimMTM/" rel="alternate" type="text/html" title="(SimMTM) A Simple Pre-Training Framework for Masked Time-Series Modeling (NeurIPS 2023)" /><published>2024-03-06T00:00:00+09:00</published><updated>2024-03-19T13:15:52+09:00</updated><id>http://localhost:4000/timeseries/SimMTM</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-06-SimMTM/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Labeling 비용을 줄이고 다양한 downstream tasks의 성능을 위해 self-supervised pre-training 방식이 사용된다.
    <ul>
      <li>Contrastive learning : positive and negative pairs를 통해 representation space 최적화</li>
      <li>Masked modeling : unmasked part를 보고 masked content를 reconstruct</li>
    </ul>
  </li>
  <li>하지만 시계열에서는 randomly masking하면 temporal variations(trend, periodicity, peak valley …)가 망가져서 reconstruction task가 너무 어려워진다.</li>
  <li>그래서 본 논문에서 제시하는 SimMTM은 한 개가 아니라 여러 개의 masked series를 assembling해서 reconstruction한다.</li>
</ul>

<h2 id="1-intnroduction">1. Intnroduction</h2>
<ul>
  <li>Self-supervised pre-training(SSL) : 대량의 unlabeled 데이터로 pretext knowledge를 학습하고, 다양한 downstream task에 맞게 개선 (Linear probing / Fine tuning)</li>
  <li>pre-training 방법 중 하나인 Masked modeling을 시계열에 적용
    <ul>
      <li>Masked modeling : 데이터의 일부를 masking하고 unmasked part를 보고 masked part를 reconstruct하는 방식을 학습</li>
    </ul>
  </li>
  <li>이미지나 자연어는 불필요한 정보도 많이 있지만(이미지의 빈 공간, 수식어 등), 시계열에는 temporal variations(trend, periodicity, peak vally…)가 있어서 단순하게 일부를 masking하면 시계열의 본질적인 부분이 변형되거나 망가질 수 있다.</li>
  <li>그래서 multiple masking series로 original data를 reconstruction하면 개별 maksing series에서는 temporal variations가 변형될 수 있지만 각 maksing series는 서로서로 complement하기 때문에 multiple masking series를 봤을 때에는 본질적인 부분이 사라지지 않는다.
<img src="/assets/img/timeseries/SimMTM/fig1.jpeg" alt="사진1" /></li>
  <li>요약하자면 SimMTM은 neighborhood aggregation design for reconstruction이라고 할 수 있고,
    <ul>
      <li>풀어서 설명하자면 SimMTM은 masked part를 reconstruct하기 위해서 series-wise representation의 simailarity가 높은 point-wise representations을 aggregate한다고 할 수 있다.</li>
    </ul>

    <h2 id="2-related-work">2. Related Work</h2>
    <h3 id="21-self-supervised-pre-training">2.1. Self-supervised Pre-training</h3>
    <ul>
      <li>Self-supervised Pre-training(SSL)
        <ul>
          <li>Contrastive leaning : positive pairs는 가깝게, negative pairs는 멀게 representation하도록 학습</li>
          <li>Masked modeling
            <ul>
              <li>TST : learns to predict removed time points based on the remaining time points</li>
              <li>PatchTST : predict masked subseries-level patches to capture the local semantic information</li>
              <li>Ti-MAE : mask modeling as an auxiliary task to boost the forecasting and classification performances</li>
            </ul>
          </li>
          <li>하지만 directly masking time series 방식은 본질적인 temporal variations를 망가지게 할 수 있으니, multiple randomly masked series로 recunstruct한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-simmtm">3. SimMTM</h2>
<ul>
  <li>모델은 크게 2개의 단계로 구성
    <ul>
      <li>multiple time series의 series-wise representation space에서의 similarities를 학습</li>
      <li>학습된 similarities를 바탕으로 point-wise representations를 aggregate</li>
    </ul>
  </li>
</ul>

<h3 id="31-overall-architecture">3.1. Overall Architecture</h3>
<ul>
  <li>모델은 4개의 modules로 구성
    <ul>
      <li>Masking</li>
      <li>Representation learning</li>
      <li>Series-wise similarity learning</li>
      <li>Point-wise aggregation
<img src="/assets/img/timeseries/SimMTM/fig2.png" alt="사진2" /></li>
    </ul>
  </li>
  <li><strong>Masking</strong>
    <ul>
      <li>\(\left\{\mathbf{x}_i\right\}_{i=1}^N\) : a mini-batch of \(N\) time series samples, <br />
where \(\mathbf{x}_i \in \mathbb{R}^{L \times C}\) contains \(L\) time points and \(C\) observed variates</li>
      <li>\(\left\{\overline{\mathbf{x}}_i^j\right\}_{j=1}^M=\operatorname{Mask}_{r}\left(\mathbf{x}_i\right)\) <br />
where \(r \in[0,1]\) denotes the masked portion,
\(M\) is a hyperparameter for the number of masked time series</li>
      <li>
        <p>\(\overline{\mathbf{x}}_i^j \in \mathbb{R}^{L \times C}\) : the \(j\)-th masked time series of \(\mathbf{x}_i\)</p>
      </li>
      <li>All the \((N(M+1))\) input series in a set as \(\mathcal{X}=\bigcup_{i=1}^N\left(\left\{\mathbf{x}_i\right\} \cup\left\{\overline{\mathbf{x}}_i^j\right\}_{j=1}^M\right)\).
        <ul>
          <li>\(N\)은 mini-batch에 있는 시계열 데이터 sample의 개수,</li>
          <li>\(M\)은 multiple masked time series의 개수</li>
          <li>\(1\)은 masking 하지 않은 원본 시계열을 의미한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Representation learning</strong>
    <ul>
      <li>Encoder : Transformer and ResNet (to obtain the point-wise representations \(\mathcal{Z}\))
        <ul>
          <li>\(\mathcal{Z}=\bigcup_{i=1}^N\left(\left\{\mathbf{z}_i\right\} \cup\left\{\overline{\mathbf{z}}_i^j\right\}_{j=1}^M\right)=\operatorname{Enocder}(\mathcal{X})\) <br />
where \(\mathbf{z}_i, \overline{\mathbf{z}}_i^j \in \mathbb{R}^{L \times d_{\text {model }}}\)</li>
          <li>Detail : input 시계열마다 separately하게 통과 : \(\bigcup_{i=1}^N\left(\operatorname{Encoder}\left(\mathbf{x}_i\right) \cup\left\{\text { Encoder }\left(\overline{\mathbf{x}}_i^j\right)\right\}_{j=1}^M\right)\)</li>
        </ul>
      </li>
      <li>Projector : MLP layer along the temporal dim (to obtain the series-wise representations \(\mathcal{S}\))
        <ul>
          <li>\(\mathcal{S}=\bigcup_{i=1}^N\left(\left\{\mathbf{s}_i\right\} \cup\left\{\overline{\mathbf{s}}_i^j\right\}_{j=1}^M\right)=\operatorname{Projector}(\mathcal{Z})\) <br />
where \(\mathbf{s}_i, \overline{\mathbf{s}}_i^j \in \mathbb{R}^{1 \times d_{\text {model }}}\)</li>
        </ul>
      </li>
      <li>Note : \(\mathbf{z}_i, \overline{\mathbf{z}}_i^j \in \mathbb{R}^{L \times d_{\text {model }}}, \mathbf{s}_i, \overline{\mathbf{s}}_i^j \in \mathbb{R}^{1 \times d_{\text {model }}}\)
<img src="/assets/img/timeseries/SimMTM/myfig1.jpeg" alt="사진3" /></li>
    </ul>
  </li>
  <li><strong>Series-wise similarity learning</strong>
    <ul>
      <li>Multiple masked time series를 단순하게 averaging하면 over-smoothing problem이 있기 때문에, similarities among series-wise representation로 weighted aggregation한다.</li>
      <li>
\[\mathbf{R}=\operatorname{Sim}(\mathcal{S}) \in \mathbb{R}^{D \times D}, D=N(M+1), \quad \mathbf{R}_{\mathbf{u}, \mathbf{v}}=\frac{\mathbf{u v}^{\top}}{\|\mathbf{u}\|\|\mathbf{v}\|}, \mathbf{u}, \mathbf{v} \in \mathcal{S}\]
        <ul>
          <li>\(\mathbf{R}=\operatorname{Sim}(\mathcal{S}) \in \mathbb{R}^{D \times D}\)은 \(N(M+1)\)개의 input 각각에 대해 series-wise representation space에서의 similarities가 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Point-wise aggregation</strong>
    <ul>
      <li>The aggregation process는 다음과 같다 : \(\widehat{\mathbf{z}}_i=\sum_{\mathbf{s}^{\prime} \in \mathcal{S} \backslash\left\{\mathbf{s}_i\right\}} \frac{\exp \left(\mathbf{R}_{\mathbf{s}_i, \mathbf{s}^{\prime}} / \tau\right)}{\sum_{\mathbf{s}^{\prime \prime} \in \mathcal{S} \backslash\left\{\mathbf{s}_i\right\}} \exp \left(\mathbf{R}_{\mathbf{s}_i, \mathbf{s}^{\prime \prime}} / \tau\right)} \mathbf{z}^{\prime}\)
        <ul>
          <li>where \(\mathbf{z}^{\prime}=\text { Projector }\left(\mathbf{s}^{\prime}\right)\), \(\tau\) denotes the temperature hyperparameter of softmax normalization for series-wise similarities</li>
          <li>의미적으로는 \(\mathbf{x}_i\)를 reconstruction하기 위해서 \(\mathbf{x}_i\)에 대한 M개의 masked series \(\left\{\overline{\mathbf{x}}_i^j\right\}_{j=1}^M\) 뿐만 아니라, similarities가 높은 다른 series(samples)도 참고하겠다는 것으로, 시계열의 structure를 더 잘 학습하도록 의도했다.</li>
        </ul>
      </li>
      <li>그리고 마지막으로 Decoder를 통과시키면 reconstruction 값을 얻는다 : \(\left\{\widehat{\mathbf{x}}_i\right\}_{i=1}^N=\operatorname{Decoder}\left(\left\{\widehat{\mathbf{z}}_i\right\}_{i=1}^N\right)\)
        <ul>
          <li>\(\operatorname{Decoder}\)는 simple MLP layer (along the channel dim)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="32-self-supervised-pre-training">3.2. Self-supervised Pre-training</h3>
<ul>
  <li>SimMTM의 reconstruction loss는 \(\mathcal{L}_{\text {reconstruction }}=\sum_{i=1}^N\left\|\mathbf{x}_i-\widehat{\mathbf{x}}_i\right\|_2^2\)이다.</li>
  <li>The series-wise representation space에 constraints가 없으면 trivial aggregation이 발생할 수 있기 때문에, 한 series에 대한 multiple masked series끼리는 positive pair, 서로 다른 series에 대해서는 negative pair로 가정하고 (neighborhood assumption) contrastive하게 학습할 수 있도록 loss를 추가해주었다. : \(\mathcal{L}_{\text {constraint }}=-\sum_{\mathbf{s} \in \mathcal{S}}\left(\sum_{\mathbf{s}^{\prime} \in \mathcal{S}^{+}} \log \frac{\exp \left(\mathbf{R}_{\mathbf{s}, \mathbf{s}^{\prime}} / \tau\right)}{\sum_{\mathbf{s}^{\prime \prime} \in \mathcal{S} \backslash\{\mathbf{s}\}} \exp \left(\mathbf{R}_{\mathbf{s}, \mathbf{s}^{\prime \prime}} / \tau\right)}\right)\)</li>
  <li>SimMTM의 overall optimization loss는 다음과 같다 : \(\min _{\Theta} \mathcal{L}_{\text {reconstruction }}+\lambda \mathcal{L}_{\text {constraint }}\)
    <ul>
      <li>\(\mathcal{L}_{\text {constraint }}\)이 trivial aggregation이 발생하는 것에 대한 regularization 역할을 한다.</li>
    </ul>
  </li>
</ul>

<h2 id="4-experiments">4. Experiments</h2>
<p><img src="/assets/img/timeseries/SimMTM/table1.jpeg" alt="사진4" /></p>
<ul>
  <li>Low-level downstream task인 forecasting, high-level downstream task인 classification을 수행하였다.</li>
  <li>비교한 SOTA 모델들
    <ul>
      <li>contrastive learning methd : TF-C, CoST, TS2Vec, LaST</li>
      <li>masked modeling method : <strong>Ti-MAE</strong>, TST, TF-C
<img src="/assets/img/timeseries/SimMTM/fig3.png" alt="사진5" /></li>
      <li>(x-axis) 왼쪽에 있을수록 MSE가 낮고, (y-axis) 위쪽에 있을수록 Accuracy가 높다.</li>
    </ul>
  </li>
  <li><img src="/assets/img/timeseries/SimMTM/table2.png" alt="사진6" /></li>
  <li><img src="/assets/img/timeseries/SimMTM/table3.png" alt="사진7" /></li>
  <li><img src="/assets/img/timeseries/SimMTM/table4.png" alt="사진8" />
    <ul>
      <li>SimMTM은 학습 데이터와 테스트 데이터가 다른 cross-domain setting에서도 forecasting과 classification 모두 다른 모델보다 뛰어나기 때문에 좋은 baseline 모델이라 할 수 있다.</li>
    </ul>
  </li>
  <li><img src="/assets/img/timeseries/SimMTM/fig4.png" alt="사진9" />
    <ul>
      <li>\(\min _{\Theta} \mathcal{L}_{\text {reconstruction }}+\lambda \mathcal{L}_{\text {constraint }}\) 두 항 모두 loss term에 있을 때에 성능이 더 좋았다.</li>
    </ul>
  </li>
  <li><img src="/assets/img/timeseries/SimMTM/fig5.png" alt="사진10" />
    <ul>
      <li>(left) SimMTM은 학습의 effectiveness가 다른 모델보다 높다. 즉 적은 데이터만으로도 valuable knowledge를 잘 파악한다.</li>
      <li>(right) SimMTM에서 masked ratio가 높을수록 많은 multiple masked series를 만들 때 성능이 높다는 직관과 부합하는 결과이다.</li>
    </ul>
  </li>
</ul>

<h2 id="5-conclusion">5. Conclusion</h2>
<ul>
  <li>SimMTM은 new masked modeling 방법을 제시
    <ul>
      <li>reconstructs the original series from its multiple neighbor masked series</li>
      <li>aggregates the point-wise representations based on the series-wise similarities</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[NeurIPS 2023](https://arxiv.org/abs/2302.00861)]]></summary></entry><entry><title type="html">MCMC with Implementation (1) : Metropolis Hastings algorithm</title><link href="http://localhost:4000/stat/2024-03-04-mcmc/" rel="alternate" type="text/html" title="MCMC with Implementation (1) : Metropolis Hastings algorithm" /><published>2024-03-04T00:00:00+09:00</published><updated>2024-03-07T15:13:15+09:00</updated><id>http://localhost:4000/stat/mcmc</id><content type="html" xml:base="http://localhost:4000/stat/2024-03-04-mcmc/"><![CDATA[<h2 id="1-probabilistic-ml-model">1. Probabilistic ML Model</h2>
<ul>
  <li>\(x\) : set of observed variables <br />
\(y\) : set of hidden / latent variables <br />
\(\theta\) : model parameters</li>
  <li>
    <p>Discriminative probabilistic ML model</p>

    <ul>
      <li>\(p(Y \mid X)\) : 데이터 \(X\)가 주어졌을 때 결과 \(y\) 예측하기 (Classification, Regression, …)</li>
    </ul>
  </li>
  <li>
    <p>Generative probabilistic ML model</p>

    <ul>
      <li>Bayes Theorem : \(p(Y \mid X)=\frac{p(X, Y)}{p(X)}=\frac{p(X \mid Y) p(Y)}{p(X)}=\frac{p(X \mid Y) p(Y)}{\int p(X \mid Y) p(Y) d Y}\)
<img src="/assets/img/stat/vi/fig1.png" alt="그림1" /></li>
    </ul>
  </li>
  <li>Discriminative model은 클래스(y) 사이의 차이를 의미하는 decision boundary를 학습하고, (\(p(Y\mid X)\))
Generative model은 분포 \(p(X), p(X,Y)\)를 학습하여 posterior \(p(Y\mid X)\)를 추정한다.</li>
  <li>\(Y\)의 차원이 높아질수록 분모에 있는 \(Y\)에 대한 적분이 어려워지기 때문에(intractable), 아래 두 가지 방법으로 \(p(Y\mid X)\)를 추정한다.
    <ul>
      <li>Variational Inference (optimization)</li>
      <li>Markov chain Monte Carlo (sampling)</li>
      <li>MCMC는 분포를 근사하기 위해 sampling으로 inference하는 것이고, VI는 분포를 근사하기 위해 optimization 문제로 바꾼 것이다.</li>
      <li>일반적으로 VI는 빠르고, MCMC는 정확하다.(상대적으로 그렇다는 것)</li>
    </ul>
  </li>
</ul>

<h2 id="2-markov-chain-monte-carlo">2. Markov Chain Monte Carlo</h2>
<h3 id="21-the-properties-of-mc">2.1. The properties of MC</h3>
<ul>
  <li><strong>Markov Chain</strong> : \(\operatorname{Pr}\left\{X_{t+1}=j_{t+1} \mid X_t=j_t, X_{t-1}=j_{t-1}, \cdots, X_0=j_0\right\} = \operatorname{Pr}\left\{X_{t+1}=j_{t+1} \mid X_t=j_t\right\}\)</li>
  <li><strong>Transition probability</strong> : \(\mathbf{P}=\left(\left(p_{i j}\right)\right) ; \quad \operatorname{Pr}\left\{X_t=j \mid X_0=i\right\}=\left(\mathbf{P}^t\right)_{i j}\) <br />
where \(p_{i j}=\operatorname{Pr}\left\{X_{t+1}=j \mid X_t=i\right\}\)
<img src="/assets/img/stat/mcmc/fig1.jpeg" alt="그림2" /></li>
  <li><strong>Irreducible</strong> : \(\left(\mathbf{P}^t\right)_{i j}=\operatorname{Pr}\left\{X_t=j \mid X_0=i\right\}&gt;0\) for some \(t&gt;0, and all $i$ and $j$ in $\Omega$.\)
    <ul>
      <li>현재 state가 무엇이든, 모든 state에 언젠가는 도달할 수 있어야 한다.</li>
      <li>Markov Chain이 irreducible하면 unique한 stationary probabilities를 가진다. <br />
i.e. \(\pi_j=\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{t=1}^n I\left(X_t=j\right) \text { w.p.1 for all initial states}\)</li>
      <li>Indicator의 expectation은 probability이므로 \(\pi_j=\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{t=1}^n \operatorname{Pr}\left\{X_t=j \mid X_0=i\right\}, \text { for all initial states}\)</li>
    </ul>
  </li>
  <li><strong>Aperiodic</strong> : \(\operatorname{Pr}\left\{X_t=j \mid X_0=j\right\}&gt;0 \text { and } \operatorname{Pr}\left\{X_{t+1}=j \mid X_0=j\right\}&gt;0\)
    <ul>
      <li>주기가 없다는 것을 수학적으로 표현하면 위와 같다. Irreducible한 Markov Chain이 하나 이상의 self-loop를 가진다면 aperiodic하다.</li>
    </ul>
  </li>
  <li><strong>Time-reversible</strong> : \(\pi_i p_{i j}=\pi_j p_{j i}, \quad \forall i \neq j\)
    <ul>
      <li><code class="language-plaintext highlighter-rouge">i에 도달하고 i에서 j가 될 확률</code>과 <code class="language-plaintext highlighter-rouge">j에 도달하고 j에서 i가 될 확률</code>이 같다는 의미</li>
    </ul>
  </li>
  <li>많은 경우의 Markov Chains는 irreducible, aperiodic, time-reversible하다.</li>
  <li>결국 하고자 하는 것은 \(E[h(X)]=\sum_{j=1}^N h(j) \pi_j\)를 \(\frac{1}{n} \sum_{t=1}^n h\left(X_t\right)\)로 추정하는 것이다.</li>
</ul>

<h3 id="22-metropolis-hastings-algorithm">2.2. Metropolis-Hastings algorithm</h3>
<ul>
  <li>Goal : (MCMC를 왜 할까?)
    <ul>
      <li>\(X \sim F\) with density f를 sampling하고 싶은데 independent sampling이 불가능하기 때문에 최대한 independent한 samples를 만드는 것</li>
    </ul>
  </li>
  <li>\(F\)로부터 직접 sampling하기 어려운 경우에, sampling이 쉬운 proposal density \(q(x' \mid x)\)에서 propose</li>
  <li>\(q(x' \mid x)\)에서의 proposed sample은 \(\alpha=\min \left(\frac{f\left(x^{\prime}\right) q\left(x \mid x^{\prime}\right)}{f(x) q\left(x^{\prime} \mid x\right)}, 1\right)\)확률로 sample에 추가, 아니면 stay \(x\)</li>
  <li>정리하면
    <ul>
      <li>Step1. Proposal density \(q(x' \mid x)\)를 선택한다.</li>
      <li>Step2. 적당한 initial state \(X_0 = x_0\)를 정한다.</li>
      <li>
        <p>Step3. \(X' \sim q(x' \mid X_t)\)와 \(U \sim Unif(0,1)\)을 생성한다.</p>
      </li>
      <li>Step4. \(U&lt;\frac{f\left(X^{\prime}\right) q\left(X_t \mid X^{\prime}\right)}{f\left(X_t\right) q\left(X^{\prime} \mid X_t\right)}, \text { then } X_{t+1}=X^{\prime} . \text { Else } X_{t+1}=X_t\)</li>
      <li>Step5. \(t = t + 1\), Go to Step3</li>
    </ul>
  </li>
  <li>이 때 proposal density가 symmetric density이면 \(\frac{q\left(X_t \mid X^{\prime}\right)}{q\left(X^{\prime} \mid X_t\right)}=1\)이 되어 계산할 필요가 없어진다.</li>
  <li><img src="/assets/img/stat/mcmc/fig2.png" alt="그림3" /></li>
  <li>예를 들어, MCMC로 생성한 samples의 trace plot이 위와 같다면 target distribution은 \((-2,2)\)에서 높은 density를 갖는다는 것을 알 수 있고, initial state와 무관하게 빠르게 수렴하는 것을 볼 수 있다.</li>
</ul>

<h2 id="3-r-implementation--mh-algorithm">3. R Implementation : MH-algorithm</h2>
<ul>
  <li>예시를 통해 MH-algorithm을 구현해본다.
    <ul>
      <li>Target density : \(g(x)=\exp \left(-(x+1)^2-y^2\right)+\exp \left(-150\left(x^2-y\right)^2-150\left(x-y^2\right)^2\right)\)
        <ul>
          <li>복잡한 un-normalized density이므로 직접 sampling하기가 어렵기 때문에 MH-algorithm으로 sampling한다.</li>
        </ul>
      </li>
      <li>Proposal distribution : \(\left(\begin{array}{l} x_{new} \\ y_{new} \end{array}\right) \sim N\left(\left(\begin{array}{l} x_{now} \\ y_{now} \end{array}\right), \sigma^2\left(\begin{array}{ll} 1 &amp; \rho \\ \rho &amp; 1 \end{array}\right)\right)\) where \(\sigma^2 &gt; 0, \rho \in (-1,1)\)’</li>
    </ul>
  </li>
  <li>먼저 target density를 visualization해서 파악한다.</li>
  <li>
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="m">-150</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">150</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)}</span><span class="w">

</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">);</span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">g</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"X"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Y"</span><span class="p">,</span><span class="w">
        </span><span class="n">main</span><span class="o">=</span><span class="s2">"The contour plot of density g"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
    <p><img src="/assets/img/stat/mcmc/fig3.png" alt="그림4" /></p>
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">);</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">g</span><span class="p">)</span><span class="w">
</span><span class="n">res1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">persp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">30</span><span class="p">,</span><span class="w"> </span><span class="n">phi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">expand</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">,</span><span class="w">
              </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"green"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"X"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Y"</span><span class="p">,</span><span class="w"> </span><span class="n">zlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Z"</span><span class="p">,</span><span class="w">
              </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"The density plot of g"</span><span class="p">)</span><span class="w">
</span><span class="n">res2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">persp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">70</span><span class="p">,</span><span class="w"> </span><span class="n">phi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">expand</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">,</span><span class="w">
              </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"green"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"X"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Y"</span><span class="p">,</span><span class="w"> </span><span class="n">zlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Z"</span><span class="p">,</span><span class="w">
              </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"The density plot of g"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
    <p><img src="/assets/img/stat/mcmc/fig4.png" alt="그림5" /></p>
  </li>
  <li>MH-algorithm을 함수로 작성한다.
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MCMC.MH</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">init</span><span class="p">,</span><span class="w"> </span><span class="n">sigma</span><span class="p">,</span><span class="w"> </span><span class="n">rho</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  
</span><span class="c1">############################################################</span><span class="w">
</span><span class="c1"># n : sample size</span><span class="w">
</span><span class="c1"># init : starting value (initialization)</span><span class="w">
</span><span class="c1"># sigma, rho : parameters for covariance matrix of proposal distribution</span><span class="w">
</span><span class="c1">############################################################</span><span class="w">

</span><span class="c1"># sample space for x, y</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">);</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w">

</span><span class="c1"># initialization</span><span class="w">
</span><span class="n">x</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">init</span><span class="p">[</span><span class="m">1</span><span class="p">];</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">init</span><span class="p">[</span><span class="m">2</span><span class="p">]</span><span class="w">
  
</span><span class="c1"># run the loop until sample space is full</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">

  </span><span class="c1"># 현재값(=current, =now)이 new x, new y의 분포를 결정한다. (Markov chain의 property)</span><span class="w">
  </span><span class="n">current</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="m">-1</span><span class="p">],</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="m">-1</span><span class="p">])</span><span class="w">

  </span><span class="c1"># Proposed sample</span><span class="w">
  </span><span class="n">proposal</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rmvnorm</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">current</span><span class="p">,</span><span class="w">
                         </span><span class="n">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">rho</span><span class="p">,</span><span class="w"> </span><span class="n">rho</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">sigma</span><span class="o">^</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">))</span><span class="w">

  </span><span class="c1"># Accept probability</span><span class="w">
  </span><span class="c1"># Note: Proposal function이 normal(symmetric)이므로 q항이 약분된다.                       </span><span class="w">
  </span><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">g</span><span class="p">(</span><span class="n">proposal</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">proposal</span><span class="p">[</span><span class="m">2</span><span class="p">])</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">g</span><span class="p">(</span><span class="n">current</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">current</span><span class="p">[</span><span class="m">2</span><span class="p">])</span><span class="w">
    
  </span><span class="c1"># Accept or Reject</span><span class="w">
  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">runif</span><span class="p">(</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">proposal</span><span class="p">[</span><span class="m">1</span><span class="p">];</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">proposal</span><span class="p">[</span><span class="m">2</span><span class="p">]}</span><span class="w">
  </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">current</span><span class="p">[</span><span class="m">1</span><span class="p">];</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">current</span><span class="p">[</span><span class="m">2</span><span class="p">]}}</span><span class="w">

</span><span class="c1"># when sample space is full, we will return  </span><span class="w">
</span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">))}</span><span class="w">
</span></code></pre></div>    </div>
  </li>
  <li>이제 MCMC를 실행하고 결과를 본다.
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1e6</span><span class="w">
</span><span class="n">samples.1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">MCMC.MH</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">init</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span><span class="w"> </span><span class="m">-10</span><span class="p">),</span><span class="w"> </span><span class="n">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">rho</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">samples.1</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">samples.1</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Scatter plot of x and y"</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
    <p><img src="/assets/img/stat/mcmc/fig5.png" alt="그림6" /></p>
    <ul>
      <li>생성된 samples가 점점 target density가 높은 방향으로 움직인다.</li>
    </ul>
  </li>
  <li>Density plot가 target density와 비슷한지, trace plot으로 chain이 잘 수렴하는지, acf plot으로 얼마나 dependent한지 눈으로 확인할 수 있다.
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># density plot</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">hist</span><span class="p">(</span><span class="n">samples.1</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Histogram of x"</span><span class="p">,</span><span class="w"> </span><span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="o">=</span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">hist</span><span class="p">(</span><span class="n">samples.1</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Histogram of y"</span><span class="p">,</span><span class="w"> </span><span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="o">=</span><span class="m">100</span><span class="p">)</span><span class="w">

</span><span class="c1"># trace plot</span><span class="w">
</span><span class="n">ts.plot</span><span class="p">(</span><span class="n">samples.1</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Trace plot of x"</span><span class="p">)</span><span class="w">
</span><span class="n">ts.plot</span><span class="p">(</span><span class="n">samples.1</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Trace plot of y"</span><span class="p">)</span><span class="w">

</span><span class="c1"># acf plot</span><span class="w">
</span><span class="n">acf</span><span class="p">(</span><span class="n">samples.1</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Autocorrelation of x"</span><span class="p">)</span><span class="w">
</span><span class="n">acf</span><span class="p">(</span><span class="n">samples.1</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Autocorrelation of y"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
    <p><img src="/assets/img/stat/mcmc/fig6.jpg" alt="그림7" /></p>
  </li>
  <li>Tuning :
    <ul>
      <li>initial point가 (-10, 10)이므로 x와 y가 모두 커져야 target density가 높은 방향으로 움직인다.
        <ul>
          <li>\(rho&gt;0\)으로 설정</li>
        </ul>
      </li>
      <li>initial point가 target density가 높은 곳과 멀리 떨어져있다
        <ul>
          <li>\(sigma &gt;&gt; 0.1\)으로 설정</li>
        </ul>
      </li>
      <li>결과는 아래와 같다.
        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples.4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">MCMC.MH</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">init</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span><span class="w"> </span><span class="m">-10</span><span class="p">),</span><span class="w"> </span><span class="n">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">rho</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.8</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">samples.4</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">samples.4</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Scatter plot of x and y"</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w">
</span><span class="c1"># 얼마나 빠르게 수렴하는지 보기 위해 첫 10개 sample을 색칠을 해주었다.</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">){</span><span class="n">points</span><span class="p">(</span><span class="n">samples.4</span><span class="o">$</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">samples.4</span><span class="o">$</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)}</span><span class="w">
</span></code></pre></div>        </div>
        <p><img src="/assets/img/stat/mcmc/fig7.png" alt="그림8" /></p>
      </li>
      <li>Scatter plot으로부터 수렴이 훨씬 빠르게 되었음을 확인할 수 있다. sigma로 보폭을, rho로 방향을 적절하게 설정했기 때문이다.</li>
      <li>물론 burn-in을 통해 수렴하기 전의 samples를 제거할 수 있다. 하지만 target distribution이 퍼져있는 정도를 고려했을 때, sigma를 늘리지 않을 이유는 없다.
<img src="/assets/img/stat/mcmc/fig8.jpeg" alt="그림9" /></li>
      <li>Acf plot이 유의미하게 낮아졌다. sigma가 커짐에 따라 현재값으로부터 멀리 떨어진 new sample이 proposed되었기 때문이다.</li>
      <li>그렇다고 sigma가 클수록 좋다는 것은 아니다. sigma가 충분히 큰 덕분에 빠르게 수렴했지만, sigma가 너무 크면 target density가 낮은 위치에서 sample이 생성될 확률이 높아지기 때문에 acceptance rate가 낮아지고 sampling에 시간이 지나치게 오래 걸린다.</li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[1. Probabilistic ML Model \(x\) : set of observed variables \(y\) : set of hidden / latent variables \(\theta\) : model parameters Discriminative probabilistic ML model \(p(Y \mid X)\) : 데이터 \(X\)가 주어졌을 때 결과 \(y\) 예측하기 (Classification, Regression, …) Generative probabilistic ML model]]></summary></entry><entry><title type="html">Advanced Variational Inference(VI), Variational Autoencoder(VAE)</title><link href="http://localhost:4000/stat/2024-03-02-vi/" rel="alternate" type="text/html" title="Advanced Variational Inference(VI), Variational Autoencoder(VAE)" /><published>2024-03-02T00:00:00+09:00</published><updated>2024-03-04T16:34:37+09:00</updated><id>http://localhost:4000/stat/vi</id><content type="html" xml:base="http://localhost:4000/stat/2024-03-02-vi/"><![CDATA[<ul>
  <li>Paper : <a href="https://arxiv.org/abs/1601.00670">Variational Inference : A Review for Statisticians</a></li>
  <li>Paper : <a href="https://arxiv.org/pdf/1711.05597.pdf">Advances in Variational Inference</a></li>
  <li>Paper : <a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></li>
</ul>

<h2 id="1-probabilistic-ml-model">1. Probabilistic ML Model</h2>
<ul>
  <li>\(x\) : set of observed variables <br />
\(y\) : set of hidden / latent variables <br />
\(\theta\) : model parameters</li>
  <li>Discriminative probabilistic ML model
    <ul>
      <li>\(p(Y \mid X)\) : 데이터 \(X\)가 주어졌을 때 결과 \(y\) 예측하기 (Classification, Regression, …)</li>
    </ul>
  </li>
  <li>
    <p>Generative probabilistic ML model</p>

    <ul>
      <li>Bayes Theorem : \(p(Y \mid X)=\frac{p(X, Y)}{p(X)}=\frac{p(X \mid Y) p(Y)}{p(X)}=\frac{p(X \mid Y) p(Y)}{\int p(X \mid Y) p(Y) d Y}\)
<img src="/assets/img/stat/vi/fig1.png" alt="그림1" /></li>
    </ul>
  </li>
  <li>Discriminative model은 클래스(y) 사이의 차이를 의미하는 decision boundary를 학습하고, (\(p(Y\mid X)\))
Generative model은 분포 \(p(X), p(X,Y)\)를 학습하여 posterior \(p(Y\mid X)\)를 추정한다.</li>
  <li>\(Y\)의 차원이 높아질수록 분모에 있는 \(Y\)에 대한 적분이 어려워지기 때문에(intractable), 아래 두 가지 방법으로 \(p(Y\mid X)\)를 추정한다.
    <ul>
      <li>Variational Inference (optimization)</li>
      <li>Markov chain Monte Carlo (sampling)</li>
      <li>MCMC는 분포를 근사하기 위해 sampling으로 inference하는 것이고, VI는 분포를 근사하기 위해 optimization 문제로 바꾼 것이다.</li>
      <li>일반적으로 VI는 빠르고, MCMC는 정확하다.(상대적으로 그렇다는 것)</li>
    </ul>
  </li>
</ul>

<h2 id="2-variational-inference">2. Variational Inference</h2>
<ul>
  <li>The model: \(p_\theta(x)\) <br />
The data: \(\stackrel{}{D}=\left\{x_1, \ldots, x_N\right\}\) <br />
Maximum likelihood fit: \(\theta \leftarrow \operatorname{argmax}_\theta \frac{1}{N} \sum_i \log p_\theta\left(x_i\right)\) 
    <ul>
      <li>i.e. \(\theta \leftarrow \operatorname{argmax}_\theta \frac{1}{N} \sum_i \log \left(\int p_\theta\left(x_i \mid z\right) p(z) d z\right)\)</li>
      <li>i.e. \(\theta \leftarrow \operatorname{argmax}_\theta \frac{1}{N} \sum_i E_{z \sim p\left(z \mid x_i\right)}\left[\log p_\theta\left(x_i, z\right)\right]\)</li>
    </ul>
  </li>
  <li>log-likelihood \(\log p_\theta\)를 maximize : 
\(\begin{aligned}
  \log p\left(x_i\right) &amp; =\log \int p\left(x_i \mid z\right) p(z) d z \\
  &amp; =\log \int p\left(x_i \mid z\right) p(z) \frac{q_i(z)}{q_i(z)} d z \\
  &amp; =\log E_{z \sim q_i(z)}\left[\frac{p\left(x_i \mid z\right) p(z)}{q_i(z)}\right] \quad \left(\because E_{q(z)}[f(Z)]=\int f(z)q(z) dz\right) \\
  &amp; \geq E_{z \sim q_i(z)}\left[\log \frac{p\left(x_i \mid z\right) p(z)}{q_i(z)}\right] \quad (\because \text{Jensen's Inequality} : \varphi(E[X]) \geq E[\varphi(X)] (\varphi \ \text{is concave fn})) \\
  &amp; =E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right]-E_{z \sim q_i(z)}\left[\log q_i(z)\right] \\
  &amp; =E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right]+H\left(q_i\right) \quad (\text{where} \ H \ \text{is Entropy}) \\
  &amp; =E_{z \sim q_i(z)}\left[\log p_\theta \left(x_i, z\right)\right]+H\left(q_i\right) \\
  \\
  &amp; = \mathcal{L}_{i}\left(p, q_{i}\right)
  \end{aligned}\)</li>
  <li>위 식에서 \(E_{z \sim q_i(z)}\left[\log p\left(x_i, z\right)\right]\)은 \(q_i(z)\)가 \(p(x_i, z)\)의 density가 높은 곳에서 높은 density를 가질 때 커지고, \(H\left(q_i\right)\)는 \(q_i(z)\)가 고르게 퍼져있을 때 커진다.</li>
  <li>
    <p>위 전개식에서 부등식 앞뒤 식의 차이가 \(D_{\mathrm{KL}}\left(q_i\left(z_i\right) \| p\left(z \mid x_i\right)\right)\)가 되고, 그러므로 \(\mathcal{L}_{i}\left(p, q_{i}\right)\)를 maximize한다는 것은 \(D_{\mathrm{KL}}\left(q_i\left(z_i\right) \| p\left(z \mid x_i\right)\right)\)를 minimize한다는 것과 같다. (아래 전개식 참고)</p>

    <p><br />
\(\begin{aligned}
  D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) &amp; =E_{z \sim q_i(z)}\left[\log \frac{q_i(z)}{p\left(z \mid x_i\right)}\right]=E_{z \sim q_i(z)}\left[\log \frac{q_i(z) p\left(x_i\right)}{p\left(x_i, z\right)}\right] \\
  &amp; =-E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right]+E_{z \sim q_i(z)}\left[\log q_i(z)\right]+E_{z \sim q_i(z)}\left[\log p\left(x_i\right)\right] \\
  \\
  &amp; =-E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right]-\mathcal{H}\left(q_i\right)+\log p\left(x_i\right) \\
  &amp; =-\mathcal{L}_i\left(p, q_i\right)+\log p\left(x_i\right)
  \end{aligned}\)
\(\begin{aligned}
  &amp;\log p\left(x_i\right)=D_{\mathrm{KL}}\left(q_i(z) \| p\left(z \mid x_i\right)\right)+\mathcal{L}_i\left(p, q_i\right)
  \end{aligned}\)</p>
  </li>
  <li>\(\log p\left(x_i\right)\)가 고정되어 있다면, \(\mathcal{L}_i\left(p, q_i\right)\)를 maximize할 때 \(D_{\mathrm{KL}}\left(q_i(z) \| p\left(z \mid x_i\right)\right)\)가 minimize된다. <br />
(\(\mathcal{L}_i\left(p, q_i\right)\)은 ELBO이고, \(D_{\mathrm{KL}}\left(q_i(z) \| p\left(z \mid x_i\right)\right)\)은 variational distribution)</li>
  <li>이 때 학습되는 parameters는 아래와 같다.
    <ul>
      <li>\(z\)를 \(\hat x\)으로 mapping시키는 \(\theta\) (\(\theta\)는 \(\hat x\)가 \(x\)와 비슷해지도록 학습)</li>
      <li>\(z\)의 분포인 \(q_i(z)\)의 평균과 분산 (Gaussian을 가정)</li>
      <li>총 \(\mid \theta \mid +\left(\mid \mu_i\mid +\mid \sigma_i\mid \right) \times N\)개</li>
    </ul>
  </li>
</ul>

<h2 id="21-amortized-variational-inference">2.1. Amortized Variational Inference</h2>
<ul>
  <li>\(\mid \theta \mid +\left(\mid \mu_i\mid +\mid \sigma_i\mid \right) \times N\)개의 parameters는 데이터 개수가 늘어날수록 커진다는 단점이 있다.</li>
  <li>
    <p>Amortized Variational Inference는 \(q_i(z)\)가 아니라 \(q_\phi(z \mid x)=\mathcal{N}\left(\mu_\phi(x), \sigma_\phi(x)\right)\)가 \(p(x \mid z)\)와 비슷해지도록 학습한다.</p>

    <p><img src="/assets/img/stat/vi/fig2.png" alt="그림2" /></p>
  </li>
  <li>Basic Variational Inference는 sampled \(z\)로 \(\hat x\)를 만들어내는 것인데, Amortized Variational Inference는 \(x\)가 input으로 들어가면 latent vector \(z\)의 분포가 결정되고 그 분포에서 \(z\)를 sampling해서 \(\hat x\)를 만들어내기 때문에, autoencoder의 아이디어와 같다. 즉 VAE는 Amortized Variational Inference의 예시 중 하나이다.</li>
  <li>\(x\)가 input으로 들어가서 \(z\)의 분포가 결정되는 네트워크(\(\phi\))를 encoder, inference network가 되고, \(z\)로 \(\hat x\)을 만드는 네트워크(\(\theta\))를 decoder, generative network가 된다.</li>
  <li>\(p_\theta(x_i \mid z)\)를 Gaussian으로 가정한다는 것은, log를 씌웠을 때 exp 안에 있는 L2 term만 남기 때문에 \(\hat x\)과 \(x\)를 비교할 때 euclidean distance를 사용한다는 의미이다.</li>
  <li>이외에도 많은 variants of VI가 있지만 Amortized VI만 소개하는 이유는 VAE와 관련이 있기 때문이다.</li>
</ul>

<h2 id="22-mean-field-variational-inference">2.2. Mean-field Variational Inference</h2>
<ul>
  <li>Assumption : all latent variables are mutually independent (i.e. \(q(\mathbf{z})=\prod_{j=1}^m q_j\left(z_j\right) .\))</li>
  <li>Mean-field 가정을 하면 true posterior의 variables가 highly dependent인 경우에 approximation의 정확도가 다소 떨어지는 단점이 있지만, fully factorized distribution으로 계산이 간단해진다.</li>
  <li>ELBO를 maximize하는 \(q(z_i)\)들을 각각 찾고 다 곱해서 \(q(z)=q(z_1) \times ... \times q(z_M)\)을 계산하기 때문이다</li>
  <li>Mean-filed 가정을 하면 아까 봤던 식이 아래와 같이 전개된다.
\(\begin{aligned}
\mathrm{ELBO} &amp; =E_{q(z)}[\log p(x, z)-\log q(z)] \\
&amp; =E_{\prod_i q_i}\left[\log p(x, z)-\log \prod_i q_i\right] \ (\because \text{Mean-field assumption})\\
&amp; =\int \prod_i q_i\left\{\log p(x, z)-\log \prod_i q_i\right\} d z \ (\text{The definition of Expectation})\\
\\
&amp; =\int \prod_i q_i\left\{\log p(x, z)-\sum_i \log q_i\right\} d z \ (\text{Property of logarithm}) \\
&amp; =\int q_j\left\{\int \log p(x, z) \prod_{i \neq j} q_i d z_i\right\} d z_j-\int \prod_i q_i \sum_i \log q_i d z \ (\text{j번째 적분만 바깥으로 뺀 것})\\
&amp; =\int q_j\left\{\int \log p(x, z) \prod_{i \neq j} q_i d z_i\right\} d z_j-\int \prod_i q_i \log q_1 d z+\ldots \int \prod_i q_i \log q_M d z \\
&amp; =\int q_j\left\{\int \log p(x, z) \prod_{i \neq j} q_i d z_i\right\} d z_j-\int \prod_i q_i \log q_j d z+\text{(Constant)} \\
&amp; =\int q_j\left\{\int \log p(x, z) \prod_{i \neq j} q_i d z_i\right\} d z_j-\int q_j \log q_j d z_j+\text{(Constant)} \ (q_j\text{와 무관한 항들은 constant}) \\
&amp; =\int q_j E_{i \neq j}[\log p(x, z)] d z_j-\int q_j \log q_j d z_j+\text{(Constant)} \\
\\
&amp; =\int q_j \log \widetilde{p}\left(x, z_j\right) d z_j-\int q_j \log q_j d z_j+\text{(Constant)} \\
&amp; =\int q_j \log \frac{\widetilde{p}\left(x, z_j\right)}{q_j} d z_j+\text{(Constant)} \\
&amp; =-\mathrm{KL}\left[q_j \| \widetilde{p}\left(x, z_j\right)\right]+\text{(Constant)}
\end{aligned}\)</li>
  <li>위 결과로부터 \(q_j\)가 \(\tilde p(x,z_j)\)와 비슷해져야 한다는 것을 알 수 있다. 그러므로 \(q_j\)를 전개하고 normalization하면 아래와 같다. <br />
\(\begin{aligned}
q_j &amp; =\widetilde{p}\left(x, z_j\right) \\
\log q_j &amp; =\log \widetilde{p}\left(x, z_j\right) \\
\log q_j &amp; \propto E_{i \neq j}[\log p(x, z)] \\
q_j &amp; \propto \exp \left(E_{i \neq j}[\log p(x, z)]\right) \\
q_j &amp; =\frac{\exp \left(E_{i \neq j}[\log p(x, z)]\right)}{\int \exp \left(E_{i \neq j}[\log p(x, z)]\right) d z_j}
\end{aligned}\)</li>
  <li>optimal \(q^*_j\)를 알기 위해서 \(i\ne j\)에 대해 \(log\ p(x,z)\)의 expectation을 계산한다.</li>
</ul>

<h2 id="3-variational-autoencoder">3. Variational Autoencoder</h2>
<ul>
  <li>VAE는 2개의 neural network로 구성된다. 을 사용한다. (2개의 NN)
    <ul>
      <li>1) top-down generative model(=decoder) : mapping from the latent variable \(z\) to the data \(x\)</li>
      <li>2) bottom-up inference model(=encoder) : approximates the posterior \(p(z \mid x)\) (using amortized mean-field variational distribution)
<img src="/assets/img/stat/vi/fig3.png" alt="그림3" /></li>
    </ul>
  </li>
  <li>위 그림에서 나와있듯이 encoder를 거치면 deterministic하게 latent variable이 output으로 나오는 것이 아니다. output은 mean vector와 std dev vector이고, 이렇게 결정된 gaussian 분포에서 sampling을 통해 latent variable을 만든다. (VAE가 generative model인 이유이다.)</li>
  <li>Reparameterization trick : \(N(mean, std)\)에서 sampling하지 않고 \(N(0,1)\)에서 생성한 뒤 std를 곱하고 mean을 더해줌으로써 미분이 가능하도록 (backpropagation이 가능하도록) 한다.</li>
  <li>latent variable이 decoder를 거치면 input과 유사한(ideally) 새로운 데이터가 생성된다.</li>
  <li>
    <p>VAE의 loss는 다음과 같다.</p>

    <p>\(\arg \min _{\theta, \phi} \sum_i-\mathbb{E}_{q_\phi\left(z \mid x_i\right)}\left[\log \left(p\left(x_i \mid g_\theta(z)\right)\right)\right] \oplus K L\left(q_\phi\left(z \mid x_i\right) \| p(z)\right)\)</p>
    <ul>
      <li>학습 parameters는 encoder의 \(\phi\)와 decoder의 \(\theta\)이다.</li>
      <li>Reconstruction Error \(-\mathbb{E}_{q_\phi\left(z \mid x_i\right)}\left[\log \left(p\left(x_i \mid g_\theta(z)\right)\right)\right]\)
        <ul>
          <li>\(x\)가 주어졌을 때 encoder \(q_\phi\)를 지나 \(z\)생성 : \(q_\phi(z \mid x)\)</li>
          <li>그 \(z\)가 decoder \(g_\theta\)를 지나 \(x\) 생성 : \(g_\theta(z)\)</li>
          <li>이렇게 생성한 \(\hat x\)에 대한 \(x\)의 negative log-likelihood</li>
        </ul>
      </li>
      <li>Regularization Error \(K L\left(q_\phi\left(z \mid x_i\right) \| p(z)\right)\)
        <ul>
          <li>\(x\)가 주어졌을 때 encoder \(q_\phi\)를 지나 \(z\)생성 : \(q_\phi(z \mid x)\)</li>
          <li>그 \(z\)와 gaussian의 KL-divergence</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="stat" /><summary type="html"><![CDATA[Paper : Variational Inference : A Review for Statisticians Paper : Advances in Variational Inference Paper : Auto-Encoding Variational Bayes]]></summary></entry><entry><title type="html">(GAFormer) Enhancing Timeseries Transformers Through Group-Aware Embeddings (ICLR 2024)</title><link href="http://localhost:4000/timeseries/2024-03-01-GAFormer/" rel="alternate" type="text/html" title="(GAFormer) Enhancing Timeseries Transformers Through Group-Aware Embeddings (ICLR 2024)" /><published>2024-03-01T00:00:00+09:00</published><updated>2024-03-19T13:15:52+09:00</updated><id>http://localhost:4000/timeseries/GAFormer</id><content type="html" xml:base="http://localhost:4000/timeseries/2024-03-01-GAFormer/"><![CDATA[<h2 id="abstract">Abstract</h2>
<ul>
  <li>Multivariate TS의 복잡한 inter-channel relationship과 dynamic shifts로 인해 Robust and generalizable representation을 학습하기 어렵다.</li>
  <li>본 논문에서 제시하는 GAFormer는 set of group tokens를 학습하고 instance-specific group embedding layer를 만든다.</li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<ul>
  <li>Multivariate TS의 temporal dynamics(temporal structure) of each channel, 그리고 relationship across channels(channel-wise structure)는 TS의 representation을 만드는 요소</li>
  <li>TS는 <strong>no predetermined ordering</strong>, 그리고 <strong>instance-specific relationships across channels and time</strong>으로 인해 position embedding을 그대로 사용하기에 적절하지 않다.</li>
  <li>본 논문에서 제시하는 GAFormer는 channel structure와 temporal structure를 통합하여 token에 ‘group embedding’한다.</li>
</ul>

<h2 id="2-method">2. Method</h2>
<ul>
  <li>Instance-specific group embeddings : grouping across different tokens, either channel-wise (spatially) or time-wise (temporally)
    <h3 id="21-group-embeddings">2.1. Group Embeddings</h3>
  </li>
  <li>Sequence of tokens : \(X=\left[\mathbf{x}_1, \ldots, \mathbf{x}_N\right] \in \mathbb{R}^{N \times D}\) (시계열의 채널, 길이와 다름)
    <ul>
      <li>\(N\) : the total # of tokens in a seq</li>
      <li>\(D\) : token dim</li>
    </ul>
  </li>
  <li>Linear weight matrix : \(W \in \mathbb{R}^{D \times K}\) project each token down to a space of \(K\) dim
    <ul>
      <li><code class="language-plaintext highlighter-rouge">operation</code> \(\operatorname{Encoder}(X) W \in \mathbb{R}^{N \times K}\)</li>
    </ul>
  </li>
  <li>그 다음 softmax : sparsify the coefficients that assign group tokens to input tokens (group awareness)
    <ul>
      <li><code class="language-plaintext highlighter-rouge">operation</code> \(\mathbb{S}(\operatorname{Encoder}(X) W)\)
        <ul>
          <li>where \(\mathbb{S}\) represents the softmax (along \((D)\) dim)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>각 tokens를 K차원으로 줄였기 때문에,  \(\mathbf{G} \in \mathbb{R}^{N \times D}\)을 곱해줄 수 있다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">operation</code> \(\operatorname{GE}(X)=\mathbb{S}(\operatorname{Encoder}(X) W) \cdot G\)</li>
    </ul>
  </li>
  <li>이제 \(X\)에 더해준다
    <ul>
      <li><code class="language-plaintext highlighter-rouge">operation</code> \(X \leftarrow X+\operatorname{GE}(X)\)</li>
    </ul>
  </li>
</ul>

<h3 id="22-gaformer-a-group-aware-spatiotemporal-transformer">2.2. GAFormer: A Group-Aware SpatioTemporal Transformer</h3>
<ul>
  <li><strong>Tokenization Layer</strong> : 시계열 \(X \in \mathbb{R}^{C \times T}\)를 P개의 패치로 잘라서 tensor \(X \in \mathbb{R}^{C \times P \times L}\)를 만든다. 그리고 <code class="language-plaintext highlighter-rouge">Token</code>이라는 encoder를 통과시키면 \(Z=\operatorname{Token}(X) \in \mathbb{R}^{C \times P \times D}\)가 된다. 이 때 channel-wise separation이 유지되고 각 channel에서의 temporal semantics는 알아낼 수 있다.</li>
  <li><strong>Spatial (Channel-Wise) Group Awareness</strong> : <code class="language-plaintext highlighter-rouge">Trans-S</code>(Transformer encoder)와 <code class="language-plaintext highlighter-rouge">SGE</code>(spatial group embedding)으로 channel-wise group embedding을 학습한다. <br />
<img src="/assets/img/timeseries/GAFormer/fig2.jpeg" alt="사진2" />
(\(Z^{\prime}=\operatorname{Trans}-\mathrm{S}\left(Z_S+\operatorname{SGE}\left(Z_S\right)\right)\)) Group embedding을 하지 않고 fixed positional embedding을 하면 두 시계열에 같은 사건이 발생했는데도 발생 시점이 다르다는 이유로 두 시계열의 structure를 다르게 학습하는데, group embedding을 하면 사건이 발생한 인접 시점들에 대해 embedding을 하기 때문에 같은 structure로 학습할 수 있다.
<img src="/assets/img/timeseries/GAFormer/fig1.jpeg" alt="사진1" /></li>
  <li><strong>Temporal Group Awareness</strong> : 먼저 dimension reduction layer \(H\)를 통과시켜 \(D\)차원 token을 \(D'\)차원 token으로 압축할 수 있다. 그리고 Spatial Group Awareness layer와 유사하게 학습한다. <br />
(\(Z^{\text {final}}=\operatorname{Trans}-\mathrm{T}\left(Z_T+\operatorname{TGE}\left(Z_T\right)\right)\)) 이렇게 학습된 \(Z^{\text {final}}\)은 linear classifier로 들어간다.</li>
</ul>

<h2 id="3-results">3. Results</h2>
<h3 id="31-an-intuitive-example--noisy-many-body-systems">3.1. An Intuitive example : Noisy many-body systems</h3>
<ul>
  <li>본 논문에서는 multivariate TS를 생성하기 위해 상호 작용하는 입자들로 구성된 many-body system의 궤적을 기반으로 한 실험 결과를 제시한다.</li>
  <li>시스템의 총 에너지를 분류하여, 고에너지 시스템인지 저에너지 시스템인지를 결정하는 것인데, 실험의 복잡성을 증가시키기 위해 상호 작용하지 않는 무관한 body들이 시스템을 오염시키는 상황을 가정했다.</li>
  <li>3가지의 embedding 방식을 실험했다.
    <ul>
      <li>1) learnable positional embedding</li>
      <li>2) parameter-free sin-cos positional embedding</li>
      <li>3) Group embedding</li>
    </ul>
  </li>
  <li>3가지의 setting을 실험했다.
    <ul>
      <li>1) Stable : the relative position of object(channels) never shifts</li>
      <li>2) Shuffle : the observed objects could be in any position and are randomized similarly</li>
      <li>3) Biased : the observed objects have different position that are randomly sampled from non-overapping set</li>
    </ul>
  </li>
  <li>실험 결과 Group embedding은 channel mismatch와 distribution shifts가 있을 때 다른 embedding보다 robust했다.
<img src="/assets/img/timeseries/GAFormer/fig3.jpeg" alt="사진3" />
    <h3 id="32-time-series-classification-tasks">3.2. Time series Classification tasks</h3>
  </li>
  <li>table1, table2 : TGE(temporal dimension embedding)에 Group embedding을 했을 때 classification 성능이 향상된다.
<img src="/assets/img/timeseries/GAFormer/table1.jpeg" alt="사진3" />
<img src="/assets/img/timeseries/GAFormer/table2.jpeg" alt="사진4" />
    <h3 id="33-classification-and-regression-tasks-on-neural-recordings">3.3. Classification and Regression tasks on Neural Recordings</h3>
    <p><img src="/assets/img/timeseries/GAFormer/table3.jpeg" alt="사진5" /></p>
    <h3 id="34-ablation-studies">3.4. Ablation Studies</h3>
  </li>
  <li>table4 :  temporal group embedding 또는 spatial group embedding 둘 중 하나만 적용하거나 둘 다 적용한 결과 둘 다 성능 향상에 필요하다.
<img src="/assets/img/timeseries/GAFormer/table4.jpeg" alt="사진6" /></li>
</ul>

<h2 id="4-related-work">4. Related Work</h2>
<h2 id="5-discussion">5. Discussion</h2>
<ul>
  <li>본 논문에서는 TS의 group-level structure를 다루는 새로운 프레임워크를 제안하였다.</li>
  <li>Group embedding은 group token across channel and temporal dimension을 학습해서 representation space로 보낸다.</li>
</ul>]]></content><author><name>GW Jeong</name><email>wjdrjsdn39@yonsei.ac.kr</email></author><category term="timeseries" /><summary type="html"><![CDATA[[ICLR 2024](https://openreview.net/forum?id=c56TWtYp0W)]]></summary></entry></feed>